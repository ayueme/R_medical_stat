[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R语言实战医学统计",
    "section": "",
    "text": "前言",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#本书缘起",
    "href": "index.html#本书缘起",
    "title": "R语言实战医学统计",
    "section": "本书缘起",
    "text": "本书缘起\nR语言是一门编程语言，但同时也是一个统计软件，R语言是由统计学家开发的，所以天生就适合做统计。\n很多刚接触R语言的朋友不知道如何入手，只知道目前R语言在临床医学领域很火爆，做统计分析、画图、做生信分析、孟德尔随机化、数据库挖掘等都离不开R语言。\n万事开头难，我非常理解新手面对R语言的痛苦，因为我也是从0开始的，作为从未接触过编程的医学生/医生来说，初学R语言简直就是读天书！我最开始接触R语言是因为偶然间听师兄师姐说R语言可以做统计学，当时的我对SPSS的使用不熟练，觉得SPSS的使用步骤太多，难以记住，于是入了R语言的坑…没想到从此一发不可收拾，打开了新世界的大门。\n这个系列也是我最开始学习R语言时的笔记，在我的公众号：医学和生信笔记，都可以找到，现在对原内容进行重新整理，并把数据一起打包，方便有需要的同学学习。\n\n\n\n\n\n\n提醒\n\n\n\n本书不适合R语言零基础的人。 如果你是刚入门的小白，我首先推荐你了解下R语言的基础知识，比如R语言和R包安装、Rstudio的界面、R语言中数据类型（向量、矩阵、数据框、列表等）、R语言中的数据导入导出、R语言的基础数据操作等。\n\n\n我结合自己学习R语言时的经验，也专门为编程零基础的医学生/医生等群体录制了R语言零基础入门的视频教程，已放在B站，且配套文档、数据都是免费的，无任何套路。各种在初学R时遇到的“坑”，我都替你踩过了，并且也在视频中指出来了。强烈建议没接触过R语言的朋友先去了解下基础知识，切勿直接上手实操！\n然后你就可以跟着本系列一起学习R语言在医学统计学中的使用。这个系列非常适合初学者，因为有很多内容是按照课本来的，尤其是基础统计分析部分，完全使用R语言复现课本中的例题，得到结果后可以与课本对照！我使用的课本是孙振球主编的《医学统计学》第4版（第5版和第4版内容变化不大），封面如下：\n\n\n\n\n\n由于R和SPSS在进行统计分析时的一些数学计算方面并不是完全一致，所以导致有些结果和课本中的结果有些出入，但是并不影响结果的正确性。\n本系列还有配套的视频教程，也在B站，免费观看，点击直达：R语言实战医学统计\n\n\n\n\n\n\n注意\n\n\n\n本书实际上是我公众号历史推文的整理和汇总（部分内容有改动），书中涉及的所有数据都可以在相关历史推文中免费获取！历史推文合集链接：医学统计学\n我也准备了一个PDF版合集，内容和网页版一致，只是打包了所有的数据，付费获取（10元），介意勿扰！PDF版合集获取链接：R语言实战医学统计\n\n\n限于本人水平等问题，难免会有一些错误，欢迎大家以各种方式批评指正，比如公众号留言、粉丝QQ群、github、个人微信等。\n本书会不定期更新，内容和格式都会不断完善。\n更新日志：\n\n20241018：本次更新是一次大更新！\n\n首先是细节修改，错别字改正，并修正一些错误内容，主要涉及以下章节：t检验、方差分析（所有方差分析内容皆有改动）、卡方检验、秩和检验、双变量回归与相关等；\n新增“三线表和统计绘图”一章，对应课本第十章：统计图和统计表；\n精简一些和医学统计关联性较小的内容，把这部分内容放在参考链接中；\n合并一些章节的内容，比如ROC曲线和三线表等；\n重新安排章节顺序，和课本中的顺序更加对应；\n增加配套的视频教程（b站：阿越就是我：R语言零基础入门）！\n\n20231230：全部内容从Rmarkdown改为quarto；增加三线表内容\n20230905：\n\n格式升级，改为3列式；\nRCS增加推荐阅读；\nt检验增加正态性检验和方差齐性检验；\ntidy风格医学统计增加秩和检验和计数资料统计分析；\n增加亚组分析和森林图绘制；\n\n20230612：改正样条回归中的一个笔误（age的Nonlinear的P&lt;0.05……）\n20230407：首次上传",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#作者简介",
    "href": "index.html#作者简介",
    "title": "R语言实战医学统计",
    "section": "作者简介",
    "text": "作者简介\n\n阿越，外科医生，R语言爱好者，长期分享R语言和医学统计学、临床预测模型、生信数据挖掘、R语言机器学习等知识。\n公众号：医学和生信笔记\n哔哩哔哩：阿越就是我\n知乎：医学和生信笔记\nCSDN：医学和生信笔记\nGithub：ayueme",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "1001-ttest.html",
    "href": "1001-ttest.html",
    "title": "1  t检验",
    "section": "",
    "text": "1.1 单样本t检验\n使用课本例3-5的数据。\n首先是读取数据，可以自己录入，也可以使用课本光盘里的数据，我这里直接使用了光盘里的数据。\n# 使用foreign包读取SPSS数据\nlibrary(foreign)\ndf &lt;- read.spss('datasets/例03-05.sav',to.data.frame = T)\n\nhead(df)\n##   no  hb\n## 1  1 112\n## 2  2 137\n## 3  3 129\n## 4  4 126\n## 5  5  88\n## 6  6  90\n进行单样本t检验，与140进行比较：\nst &lt;- t.test(df$hb,mu=140,alternative = 'two.sided') # 双侧检验\n\nst\n## \n##  One Sample t-test\n## \n## data:  df$hb\n## t = -2.1367, df = 35, p-value = 0.03969\n## alternative hypothesis: true mean is not equal to 140\n## 95 percent confidence interval:\n##  122.1238 139.5428\n## sample estimates:\n## mean of x \n##  130.8333\n结果显示t=-2.1367，自由度df=35，p=0.03969,和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t检验</span>"
    ]
  },
  {
    "objectID": "1001-ttest.html#配对样本t检验",
    "href": "1001-ttest.html#配对样本t检验",
    "title": "1  t检验",
    "section": "1.2 配对样本t检验",
    "text": "1.2 配对样本t检验\n使用课本例3-6的数据，首先是读取数据。\n\nlibrary(foreign)\ndf &lt;- read.spss('datasets/例03-06.sav',to.data.frame = T)\n\nhead(df)\n##   no    x1    x2\n## 1  1 0.840 0.580\n## 2  2 0.591 0.509\n## 3  3 0.674 0.500\n## 4  4 0.632 0.316\n## 5  5 0.687 0.337\n## 6  6 0.978 0.517\n\n数据一共3列10行，第1列是样本编号，第2列和第3列是要比较的值。\n进行配对样本t检验：\n\npt &lt;- t.test(df$x1,df$x2,paired = T,var.equal = T)\npt\n## \n##  Paired t-test\n## \n## data:  df$x1 and df$x2\n## t = 7.926, df = 9, p-value = 2.384e-05\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  0.1946542 0.3501458\n## sample estimates:\n## mean difference \n##          0.2724\n\n结果显示t=7.926，自由度df=9，p&lt;0.001，结果和课本一致。\n如果数据格式是两列数据，R语言也支持formula的形式。\n首先我们要把数据变成formula形式需要的格式，也就是长数据：\n\nsuppressMessages(library(tidyverse))\n\ndf.l &lt;- df |&gt;\n  pivot_longer(2:3,names_to = \"group\",values_to = \"x\")\n\n# 数据一列是分组，另一列是数值，还有一列id没什么用\ndf.l\n## # A tibble: 20 × 3\n##       no group     x\n##    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n##  1     1 x1    0.84 \n##  2     1 x2    0.58 \n##  3     2 x1    0.591\n##  4     2 x2    0.509\n##  5     3 x1    0.674\n##  6     3 x2    0.5  \n##  7     4 x1    0.632\n##  8     4 x2    0.316\n##  9     5 x1    0.687\n## 10     5 x2    0.337\n## 11     6 x1    0.978\n## 12     6 x2    0.517\n## 13     7 x1    0.75 \n## 14     7 x2    0.454\n## 15     8 x1    0.73 \n## 16     8 x2    0.512\n## 17     9 x1    1.2  \n## 18     9 x2    0.997\n## 19    10 x1    0.87 \n## 20    10 x2    0.506\n\n这时可以用formula形式：\n\nt.test(x ~ group, data = df.l, paired = T, var.equal = T)\n\n    Two Sample t-test\n\ndata:  x by group\nt = 3.2894, df = 18, p-value = 0.004076\nalternative hypothesis: true difference in means between group x1 and group x2 is not equal to 0\n95 percent confidence interval:\n 0.09841834 0.44638166\nsample estimates:\nmean in group x1 mean in group x2 \n          0.7952           0.5228 \n\n结果也是一模一样的。\n\n\n\n\n\n\n警告\n\n\n\nt.test在R4.4.0及以后不再支持在公式形式中使用paired参数，所以如果你的R版本在4.4.0及以后的，在公式形式中使用paired参数会得到以下报错：\nError in t.test.formula(x ~ group, data = df.l, paired = T, var.equal = T) : \n  cannot use 'paired' in formula method\n逆天更新！！\n\n\n\n在R语言中，绝大多数统计检验函数都是支持多种形式的输入样式的，需要自己注意。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t检验</span>"
    ]
  },
  {
    "objectID": "1001-ttest.html#两样本t检验",
    "href": "1001-ttest.html#两样本t检验",
    "title": "1  t检验",
    "section": "1.3 两样本t检验",
    "text": "1.3 两样本t检验\n使用课本例3-7的数据。\n首先是读取数据.\n\nlibrary(foreign)\ndf &lt;- read.spss('datasets/例03-07.sav',to.data.frame = T)\ndf$group &lt;- c(rep('阿卡波糖',20),rep('拜糖平',20))\nattributes(df)[3] &lt;- NULL\n\nhead(df)\n##   no    x    group\n## 1  1 -0.7 阿卡波糖\n## 2  2 -5.6 阿卡波糖\n## 3  3  2.0 阿卡波糖\n## 4  4  2.8 阿卡波糖\n## 5  5  0.7 阿卡波糖\n## 6  6  3.5 阿卡波糖\n\n一共有3列40行，第1列是编号，第2列是血糖值，第3列是组别（阿卡波糖组和拜糖平组，每组20个人）。\n进行两样本t检验：\n\ntt &lt;- t.test(x ~ group, data = df, var.equal = T)\ntt\n## \n##  Two Sample t-test\n## \n## data:  x by group\n## t = -0.64187, df = 38, p-value = 0.5248\n## alternative hypothesis: true difference in means between group 阿卡波糖 and group 拜糖平 is not equal to 0\n## 95 percent confidence interval:\n##  -2.326179  1.206179\n## sample estimates:\n## mean in group 阿卡波糖   mean in group 拜糖平 \n##                  2.065                  2.625\n\n结果显示t=-0.64187，自由度df=38，p=0.5248，结果和课本一致。\n假如两样本方差不等，可以使用近似t检验，或者使用Mann-Whitney-U检验。课本中给出了3种近似t检验的方法（例3-8），而t.test只能给出Welch校正的结果。\n课本中的例3-8只给出了均值和方差，没给原始数据，所以没法复现其结果。下面使用例3-7的数据演示下如何实现近似t检验：\n\ntt &lt;- t.test(x ~ group, data = df, var.equal = F)#这里指定方差不等即可\ntt\n## \n##  Welch Two Sample t-test\n## \n## data:  x by group\n## t = -0.64187, df = 36.086, p-value = 0.525\n## alternative hypothesis: true difference in means between group 阿卡波糖 and group 拜糖平 is not equal to 0\n## 95 percent confidence interval:\n##  -2.32926  1.20926\n## sample estimates:\n## mean in group 阿卡波糖   mean in group 拜糖平 \n##                  2.065                  2.625",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t检验</span>"
    ]
  },
  {
    "objectID": "1001-ttest.html#正态性检验和两样本方差比较的f检验",
    "href": "1001-ttest.html#正态性检验和两样本方差比较的f检验",
    "title": "1  t检验",
    "section": "1.4 正态性检验和两样本方差比较的F检验",
    "text": "1.4 正态性检验和两样本方差比较的F检验\n\n1.4.1 正态性检验\n例3-9\n\n# 使用foreign包读取SPSS数据\nlibrary(foreign)\ndf &lt;- read.spss('datasets/例03-01.sav',to.data.frame = T)\n\nhead(df)\n##   no   mean   sd\n## 1  1 167.41 2.74\n## 2  2 165.56 6.57\n## 3  3 168.20 5.36\n## 4  4 166.67 4.81\n## 5  5 164.89 5.41\n## 6  6 166.36 4.50\n\n进行Shapiro-Wilk正态性检验：\n\nshapiro.test(df$mean)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  df$mean\n## W = 0.99409, p-value = 0.9444\n\n这个结果和SPSS的结果是一样的。\n计算偏度和峰度：\n\nlibrary(moments) # psych\n\nskewness(df$mean)\n## [1] 0.1423707\nkurtosis(df$mean)\n## [1] 3.045566\n\n# 偏度和峰度检验\nagostino.test(df$mean)\n## \n##  D'Agostino skewness test\n## \n## data:  df$mean\n## skew = 0.14237, z = 0.61614, p-value = 0.5378\n## alternative hypothesis: data have a skewness\nanscombe.test(df$mean)\n## \n##  Anscombe-Glynn kurtosis test\n## \n## data:  df$mean\n## kurt = 3.04557, z = 0.41992, p-value = 0.6745\n## alternative hypothesis: kurtosis is not equal to 3\n\n\n\n1.4.2 方差齐性检验\n课本中的例3-11没给数据（使用了例3-8的数据，但是只给了均值和方差，没给原始数据），所以我们用例3-7的数据演示F检验。\n\nlibrary(foreign)\ndf &lt;- read.spss('datasets/例03-07.sav',to.data.frame = T)\ndf$group &lt;- c(rep('阿卡波糖',20),rep('拜糖平',20))\nattributes(df)[3] &lt;- NULL\n\nhead(df)\n##   no    x    group\n## 1  1 -0.7 阿卡波糖\n## 2  2 -5.6 阿卡波糖\n## 3  3  2.0 阿卡波糖\n## 4  4  2.8 阿卡波糖\n## 5  5  0.7 阿卡波糖\n## 6  6  3.5 阿卡波糖\n\n进行F检验：\n\nvar.test(x ~ group, data = df)\n## \n##  F test to compare two variances\n## \n## data:  x by group\n## F = 1.5984, num df = 19, denom df = 19, p-value = 0.3153\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 95 percent confidence interval:\n##  0.6326505 4.0381795\n## sample estimates:\n## ratio of variances \n##           1.598361\n\np大于0.05，不拒绝原假设，可以认为方差齐。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t检验</span>"
    ]
  },
  {
    "objectID": "1002-anova.html",
    "href": "1002-anova.html",
    "title": "2  多样本均数比较的方差分析",
    "section": "",
    "text": "2.1 完全随机设计资料的方差分析\n使用课本例4-2的数据。\n首先是构造数据，本次数据自己从书上摘录。\ntrt&lt;-c(rep(\"group1\",30),rep(\"group2\",30),rep(\"group3\",30),rep(\"group4\",30))\n\nweight&lt;-c(3.53,4.59,4.34,2.66,3.59,3.13,3.30,4.04,3.53,3.56,3.85,4.07,1.37,\n          3.93,2.33,2.98,4.00,3.55,2.64,2.56,3.50,3.25,2.96,4.30,3.52,3.93,\n          4.19,2.96,4.16,2.59,2.42,3.36,4.32,2.34,2.68,2.95,2.36,2.56,2.52,\n          2.27,2.98,3.72,2.65,2.22,2.90,1.98,2.63,2.86,2.93,2.17,2.72,1.56,\n          3.11,1.81,1.77,2.80,3.57,2.97,4.02,2.31,2.86,2.28,2.39,2.28,2.48,\n          2.28,3.48,2.42,2.41,2.66,3.29,2.70,2.66,3.68,2.65,2.66,2.32,2.61,\n          3.64,2.58,3.65,3.21,2.23,2.32,2.68,3.04,2.81,3.02,1.97,1.68,0.89,\n          1.06,1.08,1.27,1.63,1.89,1.31,2.51,1.88,1.41,3.19,1.92,0.94,2.11,\n          2.81,1.98,1.74,2.16,3.37,2.97,1.69,1.19,2.17,2.28,1.72,2.47,1.02,\n          2.52,2.10,3.71)\n\ndata1&lt;-data.frame(trt,weight)\n\nhead(data1)\n##      trt weight\n## 1 group1   3.53\n## 2 group1   4.59\n## 3 group1   4.34\n## 4 group1   2.66\n## 5 group1   3.59\n## 6 group1   3.13\n数据一共两列，第一列是分组（一共四组），第二列是低密度脂蛋白测量值。\n先简单看下数据分布：\nboxplot(weight ~ trt, data = data1)\n进行完全随机设计资料的方差分析，或者叫单因素方差分析（one-factor ANOVA）：\nfit &lt;- aov(weight ~ trt, data = data1)\nsummary(fit)\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## trt           3  32.16  10.719   24.88 1.67e-12 ***\n## Residuals   116  49.97   0.431                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n结果显示组间自由度为3，组内自由度为116，组间离均差平方和为32.16，组内离均差平方和为49.97，组间均方为10.719，组内均方为0.431，F值=24.88，p=1.67e-12，和课本一致。\n再简单介绍一下可视化的平均数和可信区间的方法：\nlibrary(gplots)\nplotmeans(weight~trt,xlab = \"treatment\",ylab = \"weight\",\n          main=\"mean plot\\nwith95% CI\")",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "1002-anova.html#随机区组设计资料的方差分析",
    "href": "1002-anova.html#随机区组设计资料的方差分析",
    "title": "2  多样本均数比较的方差分析",
    "section": "2.2 随机区组设计资料的方差分析",
    "text": "2.2 随机区组设计资料的方差分析\n使用例4-4的数据。\n首先是构造数据，本次数据自己从书上摘录。。\n\nweight &lt;- c(0.82,0.65,0.51,0.73,0.54,0.23,0.43,0.34,0.28,0.41,0.21,\n            0.31,0.68,0.43,0.24)\nblock &lt;- c(rep(c(\"1\",\"2\",\"3\",\"4\",\"5\"),each=3))\ngroup &lt;- c(rep(c(\"A\",\"B\",\"C\"),5))\n\ndata4_4 &lt;- data.frame(weight,block,group)\n\nhead(data4_4)\n##   weight block group\n## 1   0.82     1     A\n## 2   0.65     1     B\n## 3   0.51     1     C\n## 4   0.73     2     A\n## 5   0.54     2     B\n## 6   0.23     2     C\n\n数据一共3列，第一列是小白鼠肉瘤重量，第二列是区组因素（5个区组），第三列是分组（一共3组）\n进行随机区组设计资料的方差分析：\n\nfit &lt;- aov(weight ~ block + group,data = data4_4)#随机区组设计方差分析，注意顺序\nsummary(fit)\n##             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \n## block        4 0.2284 0.05709   5.978 0.01579 * \n## group        2 0.2280 0.11400  11.937 0.00397 **\n## Residuals    8 0.0764 0.00955                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果显示区组间自由度为4，分组间自由度为2，组内自由度为8，区组间离均差平方和为0.2284，分组间离均差平方和为0.2280，组内离均差平方和为0.0764，区组间均方为0.05709，分组间均方为0.1140，组内均方为0.00955，区组间F值=5.798，p=0.01579，分组间F值=11.937，p=0.00397，和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "1002-anova.html#拉丁方设计方差分析",
    "href": "1002-anova.html#拉丁方设计方差分析",
    "title": "2  多样本均数比较的方差分析",
    "section": "2.3 拉丁方设计方差分析",
    "text": "2.3 拉丁方设计方差分析\n使用课本例4-5的数据。\n首先是构造数据，本次数据自己从书上摘录。\n\npsize &lt;- c(87,75,81,75,84,66,73,81,87,85,64,79,73,73,74,78,73,77,77,68,69,74,76,73,\n           64,64,72,76,70,81,75,77,82,61,82,61)\ndrug &lt;- c(\"C\",\"B\",\"E\",\"D\",\"A\",\"F\",\"B\",\"A\",\"D\",\"C\",\"F\",\"E\",\"F\",\"E\",\"B\",\"A\",\"D\",\"C\",\n          \"A\",\"F\",\"C\",\"B\",\"E\",\"D\",\"D\",\"C\",\"F\",\"E\",\"B\",\"A\",\"E\",\"D\",\"A\",\"F\",\"C\",\"B\")\ncol_block &lt;- c(rep(1:6,6))\nrow_block &lt;- c(rep(1:6,each=6))\nmydata &lt;- data.frame(psize,drug,col_block,row_block)\nmydata$col_block &lt;- factor(mydata$col_block)\nmydata$row_block &lt;- factor(mydata$row_block)\nstr(mydata)\n## 'data.frame':    36 obs. of  4 variables:\n##  $ psize    : num  87 75 81 75 84 66 73 81 87 85 ...\n##  $ drug     : chr  \"C\" \"B\" \"E\" \"D\" ...\n##  $ col_block: Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 1 2 3 4 ...\n##  $ row_block: Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 2 2 2 2 ...\n\n数据一共4列，第一列是皮肤疱疹大小，第二列是不同药物（处理因素，共5种），第三列是列区组因素，第四列是行区组因素。\n进行拉丁方设计的方差分析：\n\nfit &lt;- aov(psize ~ drug + row_block + col_block, data = mydata)\nsummary(fit)\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## drug         5  667.1  133.43   3.906 0.0124 *\n## row_block    5  250.5   50.09   1.466 0.2447  \n## col_block    5   85.5   17.09   0.500 0.7723  \n## Residuals   20  683.2   34.16                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果显示行区组间自由度为5，列区组间自由度为5，分组（处理因素）间自由度为5，组内自由度为20； 行区组间离均差平方和为250.5，列区组间离均差平方和为85.5，分组间离均差平方和为667.1，组内离均差平方和为0.0683.2； 行区组间均方为50.09，列区组间均方为17.09，分组间均方为133.43，组内均方为34.16， 行区组间F值=1.466，p=0.2447，列区组间F值=0.5，p=0.7723，分组间F值=3.906，p=0.0124，和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "1002-anova.html#两阶段交叉设计资料方差分析",
    "href": "1002-anova.html#两阶段交叉设计资料方差分析",
    "title": "2  多样本均数比较的方差分析",
    "section": "2.4 两阶段交叉设计资料方差分析",
    "text": "2.4 两阶段交叉设计资料方差分析\n使用课本例4-6的数据。\n首先是构造数据，本次数据自己从书上摘录。\n\ncontain &lt;- c(760,770,860,855,568,602,780,800,960,958,940,952,635,650,440,450,\n             528,530,800,803)\nphase &lt;- rep(c(\"phase_1\",\"phase_2\"),10)\ntype &lt;- c(\"A\",\"B\",\"B\",\"A\",\"A\",\"B\",\"A\",\"B\",\"B\",\"A\",\"B\",\"A\",\"A\",\"B\",\"B\",\"A\",\n          \"A\",\"B\",\"B\",\"A\")\ntestid &lt;- rep(1:10,each=2)\nmydata &lt;- data.frame(testid,phase,type,contain)\n\nstr(mydata)\n## 'data.frame':    20 obs. of  4 variables:\n##  $ testid : int  1 1 2 2 3 3 4 4 5 5 ...\n##  $ phase  : chr  \"phase_1\" \"phase_2\" \"phase_1\" \"phase_2\" ...\n##  $ type   : chr  \"A\" \"B\" \"B\" \"A\" ...\n##  $ contain: num  760 770 860 855 568 602 780 800 960 958 ...\n\nmydata$testid &lt;- factor(mydata$testid)\n\n数据一共4列，第一列是受试者id，第二列是不同阶段，第三列是测定方法，第四列是测量值。\n简单看下2个阶段情况：\n\ntable(mydata$phase,mydata$type)\n##          \n##           A B\n##   phase_1 5 5\n##   phase_2 5 5\n\n进行两阶段交叉设计资料方差分析：\n\nfit &lt;- aov(contain~phase+type+testid,mydata)\nsummary(fit)\n##             Df Sum Sq Mean Sq  F value   Pr(&gt;F)    \n## phase        1    490     490    9.925   0.0136 *  \n## type         1    198     198    4.019   0.0799 .  \n## testid       9 551111   61235 1240.195 1.32e-11 ***\n## Residuals    8    395      49                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果和课本一致！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "1002-anova.html#多个样本均数间的多重比较",
    "href": "1002-anova.html#多个样本均数间的多重比较",
    "title": "2  多样本均数比较的方差分析",
    "section": "2.5 多个样本均数间的多重比较",
    "text": "2.5 多个样本均数间的多重比较\n课本例4-7，使用了课本例4-2的数据。\n首先是构造数据，本次数据自己从书上摘录。\n\ntrt&lt;-c(rep(\"group1\",30),rep(\"group2\",30),rep(\"group3\",30),rep(\"group4\",30))\n\nweight&lt;-c(3.53,4.59,4.34,2.66,3.59,3.13,3.30,4.04,3.53,3.56,3.85,4.07,1.37,\n          3.93,2.33,2.98,4.00,3.55,2.64,2.56,3.50,3.25,2.96,4.30,3.52,3.93,\n          4.19,2.96,4.16,2.59,2.42,3.36,4.32,2.34,2.68,2.95,2.36,2.56,2.52,\n          2.27,2.98,3.72,2.65,2.22,2.90,1.98,2.63,2.86,2.93,2.17,2.72,1.56,\n          3.11,1.81,1.77,2.80,3.57,2.97,4.02,2.31,2.86,2.28,2.39,2.28,2.48,\n          2.28,3.48,2.42,2.41,2.66,3.29,2.70,2.66,3.68,2.65,2.66,2.32,2.61,\n          3.64,2.58,3.65,3.21,2.23,2.32,2.68,3.04,2.81,3.02,1.97,1.68,0.89,\n          1.06,1.08,1.27,1.63,1.89,1.31,2.51,1.88,1.41,3.19,1.92,0.94,2.11,\n          2.81,1.98,1.74,2.16,3.37,2.97,1.69,1.19,2.17,2.28,1.72,2.47,1.02,\n          2.52,2.10,3.71)\n\ndata1&lt;-data.frame(trt,weight)\ndata1$trt &lt;- factor(data1$trt)\n\nstr(data1)\n## 'data.frame':    120 obs. of  2 variables:\n##  $ trt   : Factor w/ 4 levels \"group1\",\"group2\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ weight: num  3.53 4.59 4.34 2.66 3.59 3.13 3.3 4.04 3.53 3.56 ...\n\n数据一共两列，第一列是分组（一共四组），第二列是低密度脂蛋白测量值。\n进行完全随机设计资料的方差分析：\n\nfit &lt;- aov(weight ~ trt, data = data1)\nsummary(fit)\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## trt           3  32.16  10.719   24.88 1.67e-12 ***\n## Residuals   116  49.97   0.431                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n2.5.1 LSD-t检验\n使用超级全能的PMCMRplus包实现，需要自己安装。\n\nlibrary(PMCMRplus)\n\nres &lt;- lsdTest(fit)\n# lsdTest(weight ~ trt, data = data1) 也可以\n\nsummary(res)\n##                      t value   Pr(&gt;|t|)    \n## group2 - group1 == 0  -4.219 4.8872e-05 ***\n## group3 - group1 == 0  -4.322 3.2889e-05 ***\n## group4 - group1 == 0  -8.639 3.5772e-14 ***\n## group3 - group2 == 0  -0.102    0.91871    \n## group4 - group2 == 0  -4.420 2.2345e-05 ***\n## group4 - group3 == 0  -4.318 3.3397e-05 ***\n\n结果比SPSS的结果更加直接，给出了统计量和P值，可以非常直观的看出哪两个组之间有差别。group2和group1的t值是-4.219，和课本的-4.18略有差别，问题不大。\n从结果中可知：group2和 group3是没差别的，和另外两组有差别。\n还可以可视化结果（字母相同的是没差别的）：\n\nplot(res)\n\n\n\n\n\n\n\n\n\n\n2.5.2 TukeyHSD\n这里介绍一种 TukeyHSD方法：\n\nTukeyHSD(fit) ### 每个组之间进行比较,多重比较\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = weight ~ trt, data = data1)\n## \n## $trt\n##                      diff        lwr        upr     p adj\n## group2-group1 -0.71500000 -1.1567253 -0.2732747 0.0002825\n## group3-group1 -0.73233333 -1.1740587 -0.2906080 0.0001909\n## group4-group1 -1.46400000 -1.9057253 -1.0222747 0.0000000\n## group3-group2 -0.01733333 -0.4590587  0.4243920 0.9996147\n## group4-group2 -0.74900000 -1.1907253 -0.3072747 0.0001302\n## group4-group3 -0.73166667 -1.1733920 -0.2899413 0.0001938\n\n这个结果更直观，可以直接看到每个组之间的比较，后面给出了P值。\n可视化结果：\n\npar(las=2)#旋转纵坐标标签\npar(mar=c(5,8,4,2))#设置四个画图边距\nplot(TukeyHSD(fit))\n\n\n\n\n\n\n\n\n图形中置信区间包含0的疗法说明差异不显著。\n\n\n2.5.3 Dunnett-t检验\n使用超级全能的PMCMRplus包实现\n\nlibrary(PMCMRplus)\n\nres &lt;- dunnettTest(fit)\n# 或者 dunnettTest(weight ~ trt, data = data1)\n\nsummary(res)\n##                      t value   Pr(&gt;|t|)    \n## group2 - group1 == 0  -4.219 0.00014294 ***\n## group3 - group1 == 0  -4.322 9.8849e-05 ***\n## group4 - group1 == 0  -8.639 3.1530e-14 ***\n\n结果也是非常明显，所有组和安慰剂组相比都有意义（3个t值和课本也是略有差别，问题不大）。\n可视化结果：\n\nplot(res)\n\n\n\n\n\n\n\n\nDunnett-t检验用于g-1个实验组和一个对照组的均数差别的多重比较，所以从上图看：group1和其他3组都是有差别的。\n\n\n2.5.4 SNK-q检验\n课本例4-9，使用了例4-4的数据。\n还是使用超级全能的PMCMRplus包实现。\n\nlibrary(PMCMRplus)\n\n# 需要把字符型变成因子型\n#data4_4$block &lt;- factor(data4_4$block)#没用到区组\ndata4_4$group &lt;- factor(data4_4$group)\n\nfit &lt;- aov(weight ~ group,data = data4_4)\nres &lt;- snkTest(fit)\n\nsummary(res)\n##            q value Pr(&gt;|q|)  \n## B - A == 0  -2.526 0.099390 .\n## C - A == 0  -4.209 0.028913 *\n## C - B == 0  -1.684 0.256834\n#summaryGroup(res)\n\n这个结果也很直观，可以直接看到每个组之间的比较，给出了q值和P值(但是结果和课本不一样，试了多种方法，q值全都不一样)。\n可视化结果：\n\nplot(res)",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "1002-anova.html#多样本方差比较的bartlett检验和levene检验",
    "href": "1002-anova.html#多样本方差比较的bartlett检验和levene检验",
    "title": "2  多样本均数比较的方差分析",
    "section": "2.6 多样本方差比较的Bartlett检验和Levene检验",
    "text": "2.6 多样本方差比较的Bartlett检验和Levene检验\n\n2.6.1 多样本方差比较的Bartlett检验\n课本例4-10，使用课本例4-2的数据。\n\ntrt&lt;-c(rep(\"group1\",30),rep(\"group2\",30),rep(\"group3\",30),rep(\"group4\",30))\n\nweight&lt;-c(3.53,4.59,4.34,2.66,3.59,3.13,3.30,4.04,3.53,3.56,3.85,4.07,1.37,\n          3.93,2.33,2.98,4.00,3.55,2.64,2.56,3.50,3.25,2.96,4.30,3.52,3.93,\n          4.19,2.96,4.16,2.59,2.42,3.36,4.32,2.34,2.68,2.95,2.36,2.56,2.52,\n          2.27,2.98,3.72,2.65,2.22,2.90,1.98,2.63,2.86,2.93,2.17,2.72,1.56,\n          3.11,1.81,1.77,2.80,3.57,2.97,4.02,2.31,2.86,2.28,2.39,2.28,2.48,\n          2.28,3.48,2.42,2.41,2.66,3.29,2.70,2.66,3.68,2.65,2.66,2.32,2.61,\n          3.64,2.58,3.65,3.21,2.23,2.32,2.68,3.04,2.81,3.02,1.97,1.68,0.89,\n          1.06,1.08,1.27,1.63,1.89,1.31,2.51,1.88,1.41,3.19,1.92,0.94,2.11,\n          2.81,1.98,1.74,2.16,3.37,2.97,1.69,1.19,2.17,2.28,1.72,2.47,1.02,\n          2.52,2.10,3.71)\n\ndata1&lt;-data.frame(trt,weight)\ndata1$trt &lt;- factor(data1$trt)\n\nstr(data1)\n## 'data.frame':    120 obs. of  2 variables:\n##  $ trt   : Factor w/ 4 levels \"group1\",\"group2\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ weight: num  3.53 4.59 4.34 2.66 3.59 3.13 3.3 4.04 3.53 3.56 ...\n\n进行Bartlett检验：\n\nbartlett.test(weight ~ trt, data = data1)\n## \n##  Bartlett test of homogeneity of variances\n## \n## data:  weight by trt\n## Bartlett's K-squared = 5.2192, df = 3, p-value = 0.1564\n\n由结果可知，K-squared（卡方）=5.2192，P值为0.1564，不拒绝H0，还不能认为4个实验组的低密度脂蛋白值不满足方差齐性！\n\n\n2.6.2 多样本方差比较的Levene检验\n使用car包实现。\n\nlibrary(car)\n\nleveneTest(weight ~ trt, data = data1)\n## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value Pr(&gt;F)\n## group   3   1.493 0.2201\n##       116\n\n由结果可知，不能认为不满足方差齐性！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html",
    "href": "1006-chisq.html",
    "title": "3  卡方检验",
    "section": "",
    "text": "3.1 不同类型卡方检验的选择\n课本中关于四格表资料的卡方检验的方法选择以及RxC表资料的检验方法选择做了非常好的总结，在这里一并和大家分享一下：\n四格表资料的方法选择：\nR×C表资料的分类及其检验方法的选择：\nR×C表资料可以分为双向无序、单向有序、双向有序属性相同和双向有序属性不同4类。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#不同类型卡方检验的选择",
    "href": "1006-chisq.html#不同类型卡方检验的选择",
    "title": "3  卡方检验",
    "section": "",
    "text": "当 n(样本量)≥40 且所有的T(期望频数)≥5时，用χ2检验的基本公式或四格表资料之χ2检验的专用公式；当P ≈ α时，改用四格表资料的 Fisher 确切概率法；\n当 n≥40 但有 1≤T&lt;5 时，用四格表资料χ2检验的校正公式，或改用四格表资料的 Fisher 确切概率法。\n当 n&lt;40，或 T&lt;1时，用四格表资料的 Fisher 确切概率法。\n\n\n\n\n双向无序R×C表资料 R×C表资料中两个分类变量皆为无序分类变量对于该类资料，若研究目的为多个样本率（或构成比）的比较，可用行×列表资料的χ2检验：若研究目的为分析两个分类变量之间有无关联性以及关系的密切程度时，可用行×列表资料的χ2检验以及Pearson列联系数进行分析。\n单向有序R×C表资料 有两种形式。一种是R×C表资料中的分组变量（如年龄）是有序的，而指标变量（如传染病的类型）是无序的。其研究目的通常是分析不同年龄组各种传染病的构成情况，此种单向有序R×C表资料可用行×列表资料的χ2检验进行分析。另一种情况是R×C表资料中的分组变量(如疗法)为无序的，而指标变量（如疗效按等级分组）是有序的。其研究目的为比较不同疗法的疗效，此种单向有序R×C表资料宜用秩转换的非参数检验进行分析。\n双向有序属性相同的R×C表资料 R×C表资料中的两个分类变量皆为有序且属性相同。实际上是配对四格表资料的扩展，即水平数≥3的配伍资料，如用两种检测方法同时对同一批样品的测定结果。其研究目的通常是分析两种检测方法的一致性，此时宜用一致性检验或称Kappa检验；也可用特殊模型分析方法（可用SAS软件）。\n双向有序属性不同的R×C表资料 R×C表资料中两个分类变量皆为有序的，但属性不同。对于该类资料，若研究目的为分析不同年龄组患者疗效之间有无差别时，可把它视为单向有序R×C表资料，选用秩转换的非参数检验；若研究目的为分析两个有序分类变量间是否存在相关关系，宜用等级相关分析：若研究目的为分析两个有序分类变量间是否存在线性变化趋势，宜用前述的双向有序分组资料的线性趋势检验(test for linear trend)。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#四格表资料的卡方检验",
    "href": "1006-chisq.html#四格表资料的卡方检验",
    "title": "3  卡方检验",
    "section": "3.2 四格表资料的卡方检验",
    "text": "3.2 四格表资料的卡方检验\n使用课本例7-1的数据。\n首先是构造数据，本次数据自己从书上摘录。\n\nID&lt;-seq(1,200)\ntreat&lt;-c(rep(\"treated\",104),rep(\"placebo\",96))\ntreat&lt;- factor(treat)\nimpro&lt;-c(rep(\"marked\",99),rep(\"none\",5),rep(\"marked\",75),rep(\"none\",21))\nimpro&lt;-factor(impro)\ndata1&lt;-data.frame(ID,treat,impro)\n\nstr(data1)\n## 'data.frame':    200 obs. of  3 variables:\n##  $ ID   : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ treat: Factor w/ 2 levels \"placebo\",\"treated\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ impro: Factor w/ 2 levels \"marked\",\"none\": 1 1 1 1 1 1 1 1 1 1 ...\n\n数据一共3列，第一列是id，第二列是治疗方法，第三列是等级（有效和无效）。\n简单看下各组情况：\n\ntable(data1$treat,data1$impro)\n##          \n##           marked none\n##   placebo     75   21\n##   treated     99    5\n\n做卡方检验有2种方法，分别演示：\n\n3.2.1 方法1\n直接使用 gmodels包里面的 CrossTable()函数，非常强大，直接给出所有结果，和SPSS差不多。\n\nlibrary(gmodels)\n\nCrossTable(data1$treat, data1$impro, digits = 4, \n           expected = T, chisq = T, fisher = T, mcnemar = T, \n           format = \"SPSS\")\n## \n##    Cell Contents\n## |-------------------------|\n## |                   Count |\n## |         Expected Values |\n## | Chi-square contribution |\n## |             Row Percent |\n## |          Column Percent |\n## |           Total Percent |\n## |-------------------------|\n## \n## Total Observations in Table:  200 \n## \n##              | data1$impro \n##  data1$treat |   marked  |     none  | Row Total | \n## -------------|-----------|-----------|-----------|\n##      placebo |       75  |       21  |       96  | \n##              |  83.5200  |  12.4800  |           | \n##              |   0.8691  |   5.8165  |           | \n##              |  78.1250% |  21.8750% |  48.0000% | \n##              |  43.1034% |  80.7692% |           | \n##              |  37.5000% |  10.5000% |           | \n## -------------|-----------|-----------|-----------|\n##      treated |       99  |        5  |      104  | \n##              |  90.4800  |  13.5200  |           | \n##              |   0.8023  |   5.3691  |           | \n##              |  95.1923% |   4.8077% |  52.0000% | \n##              |  56.8966% |  19.2308% |           | \n##              |  49.5000% |   2.5000% |           | \n## -------------|-----------|-----------|-----------|\n## Column Total |      174  |       26  |      200  | \n##              |  87.0000% |  13.0000% |           | \n## -------------|-----------|-----------|-----------|\n## \n##  \n## Statistics for All Table Factors\n## \n## \n## Pearson's Chi-squared test \n## ------------------------------------------------------------\n## Chi^2 =  12.85707     d.f. =  1     p =  0.0003362066 \n## \n## Pearson's Chi-squared test with Yates' continuity correction \n## ------------------------------------------------------------\n## Chi^2 =  11.3923     d.f. =  1     p =  0.0007374901 \n## \n##  \n## McNemar's Chi-squared test \n## ------------------------------------------------------------\n## Chi^2 =  50.7     d.f. =  1     p =  1.076196e-12 \n## \n## McNemar's Chi-squared test with continuity correction \n## ------------------------------------------------------------\n## Chi^2 =  49.40833     d.f. =  1     p =  2.078608e-12 \n## \n##  \n## Fisher's Exact Test for Count Data\n## ------------------------------------------------------------\n## Sample estimate odds ratio:  0.1818332 \n## \n## Alternative hypothesis: true odds ratio is not equal to 1\n## p =  0.0005286933 \n## 95% confidence interval:  0.05117986 0.5256375 \n## \n## Alternative hypothesis: true odds ratio is less than 1\n## p =  0.0002823226 \n## 95% confidence interval:  0 0.4569031 \n## \n## Alternative hypothesis: true odds ratio is greater than 1\n## p =  0.9999541 \n## 95% confidence interval:  0.06281418 Inf \n## \n## \n##  \n##        Minimum expected frequency: 12.48\n\n可以看到这个函数直接给出所有结果，根据需要自己选择合适的即可。\n本例符合pearson卡方，卡方值为12.85707，p&lt;0.01，和课本一致。\n\n\n3.2.2 方法2\n先把数据变成2x2列联表，然后用 chisq.test函数做\n\nmytable &lt;- table(data1$treat,data1$impro)\n\nmytable\n##          \n##           marked none\n##   placebo     75   21\n##   treated     99    5\n\n\nchisq.test(mytable,correct = F) # 和SPSS一样\n## \n##  Pearson's Chi-squared test\n## \n## data:  mytable\n## X-squared = 12.857, df = 1, p-value = 0.0003362\n\n这个结果和课本也是一致的，和SPSS算出来的也是一样的。\n\n\n\n\n\n\n注释\n\n\n\n四格表资料卡方检验的专用公式/四格表资料卡方检验的校正公式/配对四格表资料的卡方检验/四格表资料的Fisher精确概率法，都可以用方法1直接解决。\n\n\n下面使用R语言自带的chisq.test()函数进行演示。\n使用课本例7-2的数据，这是一个连续校正卡方检验。\n\nper &lt;- matrix(c(46,6,18,8),\n              nrow = 2, byrow = T,\n              dimnames = list(group = c(\"胞磷胆碱\",\"神经节苷脂\"),\n                              effect = c(\"有效\",\"无效\")\n                              )\n              )\n\nper\n##             effect\n## group        有效 无效\n##   胞磷胆碱     46    6\n##   神经节苷脂   18    8\n\n进行连续校正的卡方检验：\n\nchisq.test(per, correct = T)\n## Warning in chisq.test(per, correct = T): Chi-squared approximation may be\n## incorrect\n## \n##  Pearson's Chi-squared test with Yates' continuity correction\n## \n## data:  per\n## X-squared = 3.1448, df = 1, p-value = 0.07617\n\n结果和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#配对四格表资料的卡方检验",
    "href": "1006-chisq.html#配对四格表资料的卡方检验",
    "title": "3  卡方检验",
    "section": "3.3 配对四格表资料的卡方检验",
    "text": "3.3 配对四格表资料的卡方检验\n使用课本例7-3的数据。\n\nana &lt;- matrix(c(11,12,2,33), nrow = 2, byrow = T,\n              dimnames = list(免疫荧光 = c(\"阳性\",\"阴性\"),\n                              乳胶凝集 = c(\"阳性\",\"阴性\")\n                              )\n              )\n\nana\n##         乳胶凝集\n## 免疫荧光 阳性 阴性\n##     阳性   11   12\n##     阴性    2   33\n\n配对四个表资料需要用McNemar检验：\n\nmcnemar.test(ana, correct = T)\n## \n##  McNemar's Chi-squared test with continuity correction\n## \n## data:  ana\n## McNemar's chi-squared = 5.7857, df = 1, p-value = 0.01616\n\n结果和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#四格表资料的fisher确切概率法",
    "href": "1006-chisq.html#四格表资料的fisher确切概率法",
    "title": "3  卡方检验",
    "section": "3.4 四格表资料的Fisher确切概率法",
    "text": "3.4 四格表资料的Fisher确切概率法\n使用课本例7-4的数据。\n\nhbv &lt;- matrix(c(4,18,5,6), nrow = 2, byrow = T,\n              dimnames = list(组别 = c(\"预防注射组\",\"非预防组\"),\n                              效果 = c(\"阳性\",\"阴性\")\n                              )\n              )\nhbv\n##             效果\n## 组别       阳性 阴性\n##   预防注射组    4   18\n##   非预防组      5    6\n\n进行 Fisher 检验：\n\nfisher.test(hbv)\n## \n##  Fisher's Exact Test for Count Data\n## \n## data:  hbv\n## p-value = 0.121\n## alternative hypothesis: true odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  0.03974151 1.76726409\n## sample estimates:\n## odds ratio \n##  0.2791061\n\nP值为0.121，和课本一样。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#行-x-列表资料的卡方检验",
    "href": "1006-chisq.html#行-x-列表资料的卡方检验",
    "title": "3  卡方检验",
    "section": "3.5 行 x 列表资料的卡方检验",
    "text": "3.5 行 x 列表资料的卡方检验\n行 x 列表资料的卡方检验有很多种情况，不是所有的列联表资料都可以直接用卡方检验，大家要注意甄别！方法选择可以参考本篇开头部分。\n\n3.5.1 多个样本率的比较\n使用课本例7-6的数据。\n首先是构造数据，本次数据直接读取，也可以自己手动摘录。\n\ndf &lt;- read.csv(\"datasets/例07-06.csv\", header = T)\n\nstr(df)\n## 'data.frame':    6 obs. of  3 variables:\n##  $ 疗法: int  1 1 2 2 3 3\n##  $ 疗效: int  1 2 1 2 1 2\n##  $ f   : int  199 7 164 18 118 26\n\nhead(df)\n##   疗法 疗效   f\n## 1    1    1 199\n## 2    1    2   7\n## 3    2    1 164\n## 4    2    2  18\n## 5    3    1 118\n## 6    3    2  26\n\n数据一共3列，第1列是疗法，第2列是有效无效，第3列是频数.\n进行 行 x 列表资料的卡方检验，首先要对数据格式转换一下，变成 table或者 矩阵：\n\nM &lt;- matrix(df$f,nrow = 3,byrow = T,\n            dimnames = list(trt = c(\"物理\", \"药物\", \"外用\"),\n                            effect = c(\"有效\",\"无效\")))\n\nM\n##       effect\n## trt    有效 无效\n##   物理  199    7\n##   药物  164   18\n##   外用  118   26\n\n这里教大家一个可视化列联表资料非常好用的马赛克图：\n\nmosaicplot(M)\n\n\n\n\n\n\n\n\n进行 行 x 列表资料的卡方检验：\n\nkf &lt;- chisq.test(M, correct = F)\n\nkf\n## \n##  Pearson's Chi-squared test\n## \n## data:  M\n## X-squared = 21.038, df = 2, p-value = 2.702e-05\n\n结果和课本一致。\n多个样本率的比较也可以使用以下函数进行检验：\n\n# 只适用于两列的，类似于 有效/无效 这种！\nprop.test(M, correct = TRUE)\n## \n##  3-sample test for equality of proportions without continuity correction\n## \n## data:  M\n## X-squared = 21.038, df = 2, p-value = 2.702e-05\n## alternative hypothesis: two.sided\n## sample estimates:\n##    prop 1    prop 2    prop 3 \n## 0.9660194 0.9010989 0.8194444\n\n可以看到两种结果是一样的，和课本一致的！\n\n\n3.5.2 样本构成比的比较\n使用课本例7-7的数据。\n\nace &lt;- matrix(c(42,48,21,30,72,36),nrow = 2,byrow = T,\n              dimnames = list(dn = c(\"dn组\",\"非dn组\"),\n                              idi = c(\"dd\",\"id\",\"ii\")\n                              )\n              )\nace\n##         idi\n## dn       dd id ii\n##   dn组   42 48 21\n##   非dn组 30 72 36\n\n进行卡方检验：\n\nchisq.test(ace, correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  ace\n## X-squared = 7.9127, df = 2, p-value = 0.01913\n\n卡方值为7.91，和课本一致。\n\n\n3.5.3 双向无序分类资料的关联性检验\n使用课本例例7-8的数据。\n\nblood &lt;- matrix(c(431,490,902,388,410,800,495,587,950,137,179,32),\n                nrow = 4,byrow = T,\n                dimnames = list(abo = c(\"o\",\"a\",\"b\",\"ab\"),\n                                mn = c(\"m\",\"n\",\"mn\")\n                                )\n                )\nblood\n##     mn\n## abo    m   n  mn\n##   o  431 490 902\n##   a  388 410 800\n##   b  495 587 950\n##   ab 137 179  32\n\n进行 关联性检验：\n\nchisq.test(blood,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  blood\n## X-squared = 213.16, df = 6, p-value &lt; 2.2e-16\n\n计算列联系数：\n\nlibrary(vcd)\n\nassocstats(blood)\n##                     X^2 df P(&gt; X^2)\n## Likelihood Ratio 248.14  6        0\n## Pearson          213.16  6        0\n## \n## Phi-Coefficient   : NA \n## Contingency Coeff.: 0.188 \n## Cramer's V        : 0.136\n\nPearson列联系数是0.188，和课本一样。\n\n\n3.5.4 双向有序分组资料的线性趋势检验\n使用课本例7-9的数据。\n\nather &lt;- matrix(c(70,22,4,2,27,24,9,3,16,23,13,7,9,20,15,14),\n                nrow = 4,byrow = T,\n                dimnames = list(age = c(\"20~\",\"30~\",\"40~\",\"≥50\"),\n                                level = c(\"-\",\"+\",\"++\",\"+++\")\n                                )\n                )\nather\n##      level\n## age    -  + ++ +++\n##   20~ 70 22  4   2\n##   30~ 27 24  9   3\n##   40~ 16 23 13   7\n##   ≥50  9 20 15  14\n\n进行卡方检验：\n\nchisq.test(ather)\n## \n##  Pearson's Chi-squared test\n## \n## data:  ather\n## X-squared = 71.432, df = 9, p-value = 7.97e-12\n\n课本中分别计算了线性回归分量和非线性回归分量的卡方值，并计算了其P值，但是目前在R中我没找到可以直接实现的方法，感兴趣的同学可以根据课本中给出的公式自己计算下试试看。\n课本是看两者之间有没有线性趋势，我们可以直接用lm()函数做，把age作为自变量，把level作为因变量即可，由于没有原始数据，这里就不演示了。\n对于这种双向有序的列联表资料，也可以用下一节介绍的MH卡方统计量检验行变量和列变量是否存在线性相关(以下代码参考：Mantel-Haenszel Test for Linear Trend)：\n\nsource(\"Mantel_Haenszel_test.R\")\n\nather &lt;- matrix(c(70,22,4,2,27,24,9,3,16,23,13,7,9,20,15,14),\n                nrow = 4,byrow = T)\n\nage &lt;- c(1,2,3,4)\nlevel &lt;- c(1,2,3)\n\nMH.test(ather,age,level)\n## Warning in margin.table(table, 2) * cscore: longer object length is not a\n## multiple of shorter object length\n## Warning in margin.table(table, 2) * (cdif^2): longer object length is not a\n## multiple of shorter object length\n## Warning in t(table * rdif) * cdif: longer object length is not a multiple of\n## shorter object length\n## $pcor\n## [1] 0.2955288\n## \n## $M2\n## [1] 24.19242\n## \n## $pval\n## [1] 8.717448e-07\n## \n## $rscore\n## [1] 1 2 3 4\n## \n## $cscore\n## [1] 1 2 3\n#MH.test.mid(ather)\n\n结果中P值小于0.05，可以认为行变量和列变量存在线性关系。\n或者可以进行Kappa一致性检验，Kappa的值的大小代表的一致性的程度，此值介于0到1之间，越大一致性程度越大。\n\n# 2选1\nDescTools::CohenKappa(ather, weight=\"Unweighted\")\n## [1] 0.2177295\nvcd::Kappa(ather)\n##             value     ASE     z  Pr(&gt;|z|)\n## Unweighted 0.2177 0.03799 5.732 9.953e-09\n## Weighted   0.3368 0.03949 8.529 1.477e-17",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#多个样本率间的多重比较",
    "href": "1006-chisq.html#多个样本率间的多重比较",
    "title": "3  卡方检验",
    "section": "3.6 多个样本率间的多重比较",
    "text": "3.6 多个样本率间的多重比较\n主要有卡方分割法、Scheffe可信区间法、SNK法等，这里主要演示卡方分割法。\n其实非常简单，就是把多个组手动拆分为多个两个组，分别进行卡方检验，和P值比较，只不过这里的P值不再是0.05，而是和组数（比较次数）有关。\n使用例7-10的数据。\n\ndf &lt;- read.csv(\"datasets/例07-06.csv\", header = T)\n\nM &lt;- matrix(df$f,nrow = 3,byrow = T,\n            dimnames = list(trt = c(\"物理\", \"药物\", \"外用\"),\n                            effect = c(\"有效\",\"无效\")))\n\nM\n##       effect\n## trt    有效 无效\n##   物理  199    7\n##   药物  164   18\n##   外用  118   26\n\n手动拆分，两两比较，直接取子集即可：\n\n# 物理治疗组和药物治疗组的卡方检验\nchisq.test(M[1:2,], correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  M[1:2, ]\n## X-squared = 6.756, df = 1, p-value = 0.009343\n\n\n# 物理治疗组和外用膏药组的卡方检验\nchisq.test(M[c(1,3),], correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  M[c(1, 3), ]\n## X-squared = 21.323, df = 1, p-value = 3.881e-06\n\n\n# 药物治疗组和外用膏药组的卡方检验\nchisq.test(M[2:3,], correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  M[2:3, ]\n## X-squared = 4.591, df = 1, p-value = 0.03214\n\n可以看到和课本是一样的。\n这时的 P’ = P / (K * (K - 1) / 2 + 1)，K是组数，一般情况下P=0.05，所以P’ = 0.05/(3*(3-1)/2+1) = 0.0125，上面3个卡方分析的P值和0.0125比较即可！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#cochran-mantel-haenszel卡方统计量检验",
    "href": "1006-chisq.html#cochran-mantel-haenszel卡方统计量检验",
    "title": "3  卡方检验",
    "section": "3.7 Cochran-Mantel-Haenszel卡方统计量检验",
    "text": "3.7 Cochran-Mantel-Haenszel卡方统计量检验\n有一个叫Mantel-Haenszel卡方统计量的方法是用来检验两个有序分类变量是否存在线性相关的，Cochran-Mantel-Haenszel卡方统计量是在其基础上提出的，用于高维列联表的分析，即控制了某一个或几个混杂因素（分层变量）之后，检验二维列联表的行变量和列变量是否存在统计学关联。Cochran-Mantel-Haenszel检验属于分层卡方检验方法。\n根据行变量X和列变量Y的类型不同，Cochran-Mantel-Haenszel卡方统计量包括以下几种：\n\n相关统计量：适用于X和Y均为有序分类变量的资料。对于一维列联表，CMH统计量即为MH卡方统计量。\n方差分析统计量：也称行平均得分统计量（行均分检验），适用于列变量Y为有序分类变量的资料。\n一般关联统计量：适用于X和Y均为无序分类变量的资料，目的是检验X和Y是否存在关联性。\n\n使用课本例7-12的数据。\n这个数据有3个变量，首先是年龄，根据年龄分成两层，然后是是否心肌梗死和是否口服避孕药，我们可以直接把这个数据录入成3维array的形式：\n\nmyo &lt;- array(c(17,47,\n               121,944,\n               12,158,\n               14,663),\n             dim = c(2,2,2),\n             dimnames = list(心肌梗死 = c(\"病例\",\"对照\"),\n                             口服避孕药 = c(\"是\",\"否\"),\n                             年龄分层 = c(\"&lt;40岁\",\"≥40岁\")\n                             )\n             )\nmyo\n## , , 年龄分层 = &lt;40岁\n## \n##         口服避孕药\n## 心肌梗死 是  否\n##     病例 17 121\n##     对照 47 944\n## \n## , , 年龄分层 = ≥40岁\n## \n##         口服避孕药\n## 心肌梗死  是  否\n##     病例  12  14\n##     对照 158 663\n\n这样就能直接进行Cochran-Mantel-Haenszel检验了，这个检验的函数是R语言自带的，不需要另外的包：\n\nmantelhaen.test(myo,correct = F)\n## \n##  Mantel-Haenszel chi-squared test without continuity correction\n## \n## data:  myo\n## Mantel-Haenszel X-squared = 24.184, df = 1, p-value = 8.755e-07\n## alternative hypothesis: true common odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  1.930775 4.933840\n## sample estimates:\n## common odds ratio \n##          3.086444\n\n这样就得到结果了，这个结果和课本也是一样的。\n课本还介绍了Breslow-Day对各层的效应值进行齐性检验，这个检验可以通过DescTools包实现：\n\nlibrary(DescTools)\n\nBreslowDayTest(myo)\n## \n##  Breslow-Day test on Homogeneity of Odds Ratios\n## \n## data:  myo\n## X-squared = 0.23409, df = 1, p-value = 0.6285\n\n结果也是和课本一模一样。\n如果你的是原始数据的形式，也是很简单的，我们构造一个和上面数据一样的原始数据：\n\nmyoo &lt;- data.frame(年龄分层 = c(rep(\"&lt;40岁\",1129),rep(\"≥40岁\",847)),\n                   心肌梗死 = c(rep(\"病例\",64),rep(\"对照\",1065),\n                                rep(\"病例\",170),rep(\"对照\",677)\n                                ),\n                   口服避孕药 = c(rep(\"是\",17),rep(\"否\",47),\n                                  rep(\"是\",121),rep(\"否\",944),\n                                  rep(\"是\",12),rep(\"否\",158),\n                                  rep(\"是\",14),rep(\"否\",663)\n                                  )\n                   )\n\n# 分类变量变为因子型\nmyoo$年龄分层 &lt;- factor(myoo$年龄分层,levels = c(\"&lt;40岁\",\"≥40岁\"))\nmyoo$心肌梗死 &lt;- factor(myoo$心肌梗死, levels = c(\"病例\",\"对照\"))\nmyoo$口服避孕药 &lt;- factor(myoo$口服避孕药, levels = c(\"是\",\"否\"))\n\nhead(myoo)\n##   年龄分层 心肌梗死 口服避孕药\n## 1    &lt;40岁     病例         是\n## 2    &lt;40岁     病例         是\n## 3    &lt;40岁     病例         是\n## 4    &lt;40岁     病例         是\n## 5    &lt;40岁     病例         是\n## 6    &lt;40岁     病例         是\n\n用xtabs查看数据，结果和我们的array的形式是一样的：\n\nmyoo.tab &lt;- xtabs(~口服避孕药+心肌梗死+年龄分层,data = myoo)\nmyoo.tab\n## , , 年龄分层 = &lt;40岁\n## \n##           心肌梗死\n## 口服避孕药 病例 对照\n##         是   17  121\n##         否   47  944\n## \n## , , 年龄分层 = ≥40岁\n## \n##           心肌梗死\n## 口服避孕药 病例 对照\n##         是   12   14\n##         否  158  663\n\n这样就可以直接进行Cochran-Mantel-Haenszel检验了：\n\nmantelhaen.test(myoo.tab, correct = F)\n## \n##  Mantel-Haenszel chi-squared test without continuity correction\n## \n## data:  myoo.tab\n## Mantel-Haenszel X-squared = 24.184, df = 1, p-value = 8.755e-07\n## alternative hypothesis: true common odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  1.930775 4.933840\n## sample estimates:\n## common odds ratio \n##          3.086444\n\n结果也是一样的。\n还可用woolf法检验不同分层之间的效应值有没有统计学显著性，通过使用?mantelhaen.test查看帮助文档，作者直接给了一个现成的计算方法：\n\nwoolf &lt;- function(x) {\n  x &lt;- x + 1 / 2\n  k &lt;- dim(x)[3]\n  or &lt;- apply(x, 3, function(x) (x[1,1]*x[2,2])/(x[1,2]*x[2,1]))\n  w &lt;-  apply(x, 3, function(x) 1 / sum(1 / x))\n  1 - pchisq(sum(w * (log(or) - weighted.mean(log(or), w)) ^ 2), k - 1)\n}\n\nwoolf(myoo.tab)\n## [1] 0.6400154\n\n直接给出了P值。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#频数分布拟合优度卡方检验",
    "href": "1006-chisq.html#频数分布拟合优度卡方检验",
    "title": "3  卡方检验",
    "section": "3.8 频数分布拟合优度卡方检验",
    "text": "3.8 频数分布拟合优度卡方检验\n使用课本例7-13的数据。\nR语言做卡方拟合优度检验非常简单，关键是概率的计算，这里我们直接用课本中的概率。\n\nx &lt;- c(26,51,75,63,38,17,9)\np &lt;- c(0.0854,0.2102,0.2585,0.2120,0.1304,0.0641,0.0394)\n\nchisq.test(x=x, p =p)\n## \n##  Chi-squared test for given probabilities\n## \n## data:  x\n## X-squared = 2.0377, df = 6, p-value = 0.9162\n\n结果和课本非常接近。\n这里贴一个网络教程的概率计算方法：\n\nx&lt;-0:6\ny&lt;-c(26,51,75,63,38,17,9)\nmean&lt;-mean(rep(x,y))\nq&lt;-ppois(x,mean)\nn&lt;-length(y)\np&lt;-c()\np[1]&lt;-q[1]\np[n]&lt;-1-q[n-1]\nfor(i in 2:(n-1))\n  p[i]&lt;-q[i]-q[i-1]\nchisq.test(y, p=p,correct=F)\n## \n##  Chi-squared test for given probabilities\n## \n## data:  y\n## X-squared = 2.0569, df = 6, p-value = 0.9144\n\n结果和课本非常接近。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1009-cochranarmitage.html",
    "href": "1009-cochranarmitage.html",
    "title": "4  Cochran-Armitage检验",
    "section": "",
    "text": "Cochran Armitage检验是一种线性趋势检验，常用于自变量是有序分类变量，而因变量是二分类变量的资料，可以用来检验自变量和因变量存不存在线性趋势。\n注意和Cochran-Mantel-Haenszel检验区分，CMH检验是研究两个分类变量之间关联性的一种检验方法。但有时数据除了我们研究的变量外，还混杂或隐含了其它的变量，如果将这些变量纳入分析中，则有可能得出完全不同的结论，著名的Simpson悖论就是这个问题的典型案例。\n换句话说，在2 x 2表格数据的基础上，引入了第三个分类变量，称之为混杂变量。混杂变量的引入使得CMH检验可以用于分析分层样本，作为生物统计学领域的一种常用技术，该检验常用于疾病对照研究。\n现在我们想要了解某种药物剂量和疗效之间的关系，药物剂量有50mg，100mg，200mg，300mg，500mg，5个水平，疗效分为有效/无效两个水平。这种情况可以使用Cochran Armitage检验。\n\ndf &lt;- matrix(c(13, 136, 17, 125, 16, 104, 32, 149, 9, 45), \n             nrow = 5, byrow = T,\n             dimnames = list(\"Dose\" = c(\"50\", \"100\", \"200\", \"300\", \"500\"),\n                             \"effect\" = c(\"Yes\", \"No\"))\n             )\n\ndf\n##      effect\n## Dose  Yes  No\n##   50   13 136\n##   100  17 125\n##   200  16 104\n##   300  32 149\n##   500   9  45\n\n首先可以计算一下不同药物剂量下的有效率是多少：\n\ndf[,1]/rowSums(df)\n##         50        100        200        300        500 \n## 0.08724832 0.11971831 0.13333333 0.17679558 0.16666667\n\n可以看到随着药物剂量增加，有效率整体也是增加的，下面使用CAM检验验证一下。\n使用DescTools包中的CochranArmitageTest()函数进行检验：\n\nDescTools::CochranArmitageTest(df)\n## \n##  Cochran-Armitage test for trend\n## \n## data:  df\n## Z = 2.2116, dim = 5, p-value = 0.02699\n## alternative hypothesis: two.sided\n\n结果显示P值为p-value = 0.02699，小于0.05，可以认为疗效会随着药物剂量增加而增加。\n现在的df是一个频数统计表类型的数据，我们可以把它变成每行一个患者的数据，然后进行logistic回归看看结果。\n\ndf1 &lt;- rstatix::counts_to_cases(df)\npsych::headTail(df1)\n##     Dose effect\n## 1     50    Yes\n## 2     50    Yes\n## 3     50    Yes\n## 4     50    Yes\n## ... &lt;NA&gt;   &lt;NA&gt;\n## 643  500     No\n## 644  500     No\n## 645  500     No\n## 646  500     No\n\n把Dose变成数值型：\n\ndf1$Dose &lt;-  as.numeric(factor(df1$Dose))\n\nsummary(glm(effect~Dose, data = df1,family = binomial()))\n## \n## Call:\n## glm(formula = effect ~ Dose, family = binomial(), data = df1)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  2.48493    0.29598   8.396   &lt;2e-16 ***\n## Dose        -0.21544    0.08985  -2.398   0.0165 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 510.57  on 645  degrees of freedom\n## Residual deviance: 504.71  on 644  degrees of freedom\n## AIC: 508.71\n## \n## Number of Fisher Scoring iterations: 4\n\nlogistic回归的结果也显示，剂量的P值是小于0.05的。\n下面是CMH检验的一个补充。\n默认的CMH检验只能进行3个变量的检验，vcdExtra中的CMHtest()可以进行两个变量的CMH检验。\n\nvcdExtra::CMHtest(df, types = \"cor\")\n## Cochran-Mantel-Haenszel Statistics for Dose by effect \n## \n##           AltHypothesis  Chisq Df     Prob\n## cor Nonzero correlation 5.8217  1 0.015829",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cochran-Armitage检验</span>"
    ]
  },
  {
    "objectID": "1007-wilcoxon.html",
    "href": "1007-wilcoxon.html",
    "title": "5  秩转换的非参数检验",
    "section": "",
    "text": "5.1 配对样本比较的Wilcoxon符号秩检验\n使用课本例8-1的数据，自己手动摘录：\ntest1&lt;-c(60,142,195,80,242,220,190,25,198,38,236,95)\ntest2&lt;-c(76,152,243,82,240,220,205,38,243,44,190,100)\n两列数据，和配对t检验的数据结果完全一样。\n简单看一下数据情况：\nboxplot(test1,test2)\n进行秩和检验：\nwilcox.test(test1,test2,paired = T,alternative = \"two.sided\",\n            exact = F, correct = F)\n## \n##  Wilcoxon signed rank test\n## \n## data:  test1 and test2\n## V = 11.5, p-value = 0.05581\n## alternative hypothesis: true location shift is not equal to 0\n结果和课本一致！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>秩转换的非参数检验</span>"
    ]
  },
  {
    "objectID": "1007-wilcoxon.html#两独立样本比较的wilcoxon符号秩检验",
    "href": "1007-wilcoxon.html#两独立样本比较的wilcoxon符号秩检验",
    "title": "5  秩转换的非参数检验",
    "section": "5.2 两独立样本比较的Wilcoxon符号秩检验",
    "text": "5.2 两独立样本比较的Wilcoxon符号秩检验\n和两样本t检验的数据格式完全一样！\n使用课本例8-3的数据，自己手动摘录。\n\nRD1&lt;-c(2.78,3.23,4.20,4.87,5.12,6.21,7.18,8.05,8.56,9.60)\nRD2&lt;-c(3.23,3.50,4.04,4.15,4.28,4.34,4.47,4.64,4.75,4.82,4.95,5.10)\n\n进行两独立样本比较的Wilcoxon符号秩检验：\n\nwilcox.test(RD1,RD2,paired = F, correct = F)\n## Warning in wilcox.test.default(RD1, RD2, paired = F, correct = F): cannot\n## compute exact p-value with ties\n## \n##  Wilcoxon rank sum test\n## \n## data:  RD1 and RD2\n## W = 86.5, p-value = 0.08049\n## alternative hypothesis: true location shift is not equal to 0\n\n结果取单侧检验，还是和课本一致！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>秩转换的非参数检验</span>"
    ]
  },
  {
    "objectID": "1007-wilcoxon.html#完全随机设计多个样本比较的-kruskal-wallis-h-检验",
    "href": "1007-wilcoxon.html#完全随机设计多个样本比较的-kruskal-wallis-h-检验",
    "title": "5  秩转换的非参数检验",
    "section": "5.3 完全随机设计多个样本比较的 Kruskal-Wallis H 检验",
    "text": "5.3 完全随机设计多个样本比较的 Kruskal-Wallis H 检验\n\n5.3.1 多样本比较的kruskal-wallis H检验\n使用课本例8-5的数据，手动摘录：\n\nrm(list = ls())\ndeath_rate &lt;- c(32.5,35.5,40.5,46,49,16,20.5,22.5,29,36,6.5,\n                9.0,12.5,18,24)\ndrug &lt;- rep(c(\"Drug_A\",\"drug_B\",\"drug_C\"),each=5)\nmydata &lt;- data.frame(death_rate,drug)\n\nstr(mydata)\n## 'data.frame':    15 obs. of  2 variables:\n##  $ death_rate: num  32.5 35.5 40.5 46 49 16 20.5 22.5 29 36 ...\n##  $ drug      : chr  \"Drug_A\" \"Drug_A\" \"Drug_A\" \"Drug_A\" ...\n\n数据一共2列，第1列是死亡率，第2列是药物（3种）。\n简单看下数据：\n\nboxplot(death_rate ~ drug, data = mydata)\n\n\n\n\n\n\n\n\n进行 Kruskal-Wallis H 检验：\n\nkruskal.test(death_rate ~ drug, data = mydata)\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  death_rate by drug\n## Kruskal-Wallis chi-squared = 9.74, df = 2, p-value = 0.007673\n\n算出来结果和课本一致！\n使用课本例8-6的数据，手动摘录：\n\ndata8_6 &lt;- data.frame(days=c(2,2,2,3,4,4,4,5,7,7,\n                             5,5,6,6,6,7,8,10,12,\n                             3,5,6,6,6,7,7,9,10,11,11),\n                      type=c(rep(\"9D\",10),rep(\"11C\",9),rep(\"DSC\",11)))\nhead(data8_6)\n##   days type\n## 1    2   9D\n## 2    2   9D\n## 3    2   9D\n## 4    3   9D\n## 5    4   9D\n## 6    4   9D\n\n进行 Kruskal-Wallis H 检验：\n\nkruskal.test(days ~ type, data = data8_6)\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  days by type\n## Kruskal-Wallis chi-squared = 9.9405, df = 2, p-value = 0.006941\n\n结果和课本一致（课本中Hc=9.97）。\n\n\n5.3.2 kruskal-Wallis H检验后的多重比较\n例8-8使用的是例8-6的数据。\n课本上是使用的 Nemenyi检验，我们使用非参数检验的全能R包：PMCMRplus实现。\n\nlibrary(PMCMRplus)\n\n# 首先要把分类变量因子化\ndata8_6$type &lt;- factor(data8_6$type)\n\n下面就可以使用 Nemenyi检验了。\n\n# 也可以把kwh检验的结果作为输入\nres &lt;- kwAllPairsNemenyiTest(days ~ type, data = data8_6)\n## Warning in kwAllPairsNemenyiTest.default(c(2, 2, 2, 3, 4, 4, 4, 5, 7, 7, : Ties\n## are present, p-values are not corrected.\nsummary(res)\n##                q value Pr(&gt;|q|)  \n## 9D - 11C == 0    3.628 0.027794 *\n## DSC - 11C == 0   0.177 0.991411  \n## DSC - 9D == 0    3.998 0.013057 *\n\n得到的数值和课本有差别，但是结论是一样的。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>秩转换的非参数检验</span>"
    ]
  },
  {
    "objectID": "1007-wilcoxon.html#随记区组设计多个样本比较的friedman-m检验",
    "href": "1007-wilcoxon.html#随记区组设计多个样本比较的friedman-m检验",
    "title": "5  秩转换的非参数检验",
    "section": "5.4 随记区组设计多个样本比较的Friedman M检验",
    "text": "5.4 随记区组设计多个样本比较的Friedman M检验\n\n5.4.1 多个相关样本比较的Friedman M检验\n使用课本例8-9的数据：\n\ndf &lt;- foreign::read.spss(\"datasets/例08-09.sav\", to.data.frame = T)\n\nstr(df)\n## 'data.frame':    8 obs. of  4 variables:\n##  $ a: num  8.4 11.6 9.4 9.8 8.3 8.6 8.9 7.8\n##  $ b: num  9.6 12.7 9.1 8.7 8 9.8 9 8.2\n##  $ c: num  9.8 11.8 10.4 9.9 8.6 9.6 10.6 8.5\n##  $ d: num  11.7 12 9.8 12 8.6 10.6 11.4 10.8\n##  - attr(*, \"codepage\")= int 65001\n\n数据一共4列，分别是4中不同频率下的反应率。\n简单看下数据：\n\nboxplot(df$a,df$b,df$c,df$d)\n\n\n\n\n\n\n\n\n进行 Friedman M 检验前先把数据格式转换一下：\n\nM &lt;- as.matrix(df) # 变成矩阵\n\n进行 Friedman M 检验：\n\nfriedman.test(M)\n## \n##  Friedman rank sum test\n## \n## data:  M\n## Friedman chi-squared = 15.152, df = 3, p-value = 0.001691\n\n结果和课本一致！\n\n\n5.4.2 多个相关样本两两比较的q检验\nP126页，多个相关样本两两比较的q检验。课本上说的这个q检验，应该是quade test。\n接下来就是使用R语言实现quade-test。但是自带的quade.test()函数不能进行两两比较，还是要借助第三方包。\n\n# 准备数据，也是用的课本例8-9的数据\ndf &lt;- matrix(\n  c(8.4, 11.6, 9.4, 9.8, 8.3, 8.6, 8.9, 7.8,\n    9.6, 12.7, 9.1, 8.7, 8, 9.8, 9, 8.2,\n    9.8, 11.8, 10.4, 9.9, 8.6, 9.6, 10.6, 8.5,\n    11.7, 12, 9.8, 12, 8.6, 10.6, 11.4, 10.8\n    ),\n  byrow = F, nrow = 8,\n  dimnames = list(1:8,LETTERS[1:4])\n  )\n\nprint(df)\n##      A    B    C    D\n## 1  8.4  9.6  9.8 11.7\n## 2 11.6 12.7 11.8 12.0\n## 3  9.4  9.1 10.4  9.8\n## 4  9.8  8.7  9.9 12.0\n## 5  8.3  8.0  8.6  8.6\n## 6  8.6  9.8  9.6 10.6\n## 7  8.9  9.0 10.6 11.4\n## 8  7.8  8.2  8.5 10.8\n\n先进行 Friedman M检验看看：\n\nfriedman.test(df)\n## \n##  Friedman rank sum test\n## \n## data:  df\n## Friedman chi-squared = 15.152, df = 3, p-value = 0.001691\n\n接下来进行quade检验：\n\nlibrary(PMCMRplus)\n\nquadeAllPairsTest(df, dist = \"Normal\")\n##   A       B       C     \n## B 0.2200  -       -     \n## C 0.0017  0.0644  -     \n## D 1.7e-07 7.7e-05 0.0860\n\n当然也可以有更加详细的结果：\n\nres &lt;- quadeAllPairsTest(df,dist = \"Normal\")\ntoTidy(res)\n##   group1 group2 statistic      p.value alternative\n## 1      B      A  1.226488 2.200150e-01   two.sided\n## 2      C      A  3.526154 1.686568e-03   two.sided\n## 3      C      B  2.299666 6.440153e-02   two.sided\n## 4      D      A  5.549859 1.715396e-07   two.sided\n## 5      D      B  4.323371 7.683144e-05   two.sided\n## 6      D      C  2.023706 8.600089e-02   two.sided\n##                                           method distribution p.adjust.method\n## 1 Quade's testwith standard-normal approximation            z            holm\n## 2 Quade's testwith standard-normal approximation            z            holm\n## 3 Quade's testwith standard-normal approximation            z            holm\n## 4 Quade's testwith standard-normal approximation            z            holm\n## 5 Quade's testwith standard-normal approximation            z            holm\n## 6 Quade's testwith standard-normal approximation            z            holm\n\n这个结果和课本上也不是完全一样，不过不影响结果。还有很多其他的方法可以选择，除了这个quade检验，还可以用Nemenyi等检验方法。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>秩转换的非参数检验</span>"
    ]
  },
  {
    "objectID": "1015-twocorrelation.html",
    "href": "1015-twocorrelation.html",
    "title": "6  双变量回归与相关",
    "section": "",
    "text": "6.1 直线回归\n例9-1。\ndf9_1 &lt;- data.frame(x = c(13,11,9,6,8,10,12,7),\n                    y = c(3.54,3.01,3.09,2.48,2.56,3.36,3.18,2.65))\n建立回归方程：\nfit &lt;- lm(y ~ x, data = df9_1)\nsummary(fit)\n## \n## Call:\n## lm(formula = y ~ x, data = df9_1)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.21500 -0.15937 -0.00125  0.09583  0.30667 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)  1.66167    0.29700   5.595  0.00139 **\n## x            0.13917    0.03039   4.579  0.00377 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.197 on 6 degrees of freedom\n## Multiple R-squared:  0.7775, Adjusted R-squared:  0.7404 \n## F-statistic: 20.97 on 1 and 6 DF,  p-value: 0.003774\n截距是1.66167，x的系数是0.13917。同时该结果也给出了回归方程的假设检验结果。\n例9-2：F-statistic: 20.97，p-value: 0.003774；回归系数：t=4.579,p=0.00377。\n例9-3，计算回归系数的95%的可信区间，直接使用broom计算即可：\nbroom::tidy(fit,conf.int = T)\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    1.66     0.297       5.59 0.00139   0.935      2.39 \n## 2 x              0.139    0.0304      4.58 0.00377   0.0648     0.214\n例9-4，计算总体均数的可信区间和个体Y值的预测区间，1行代码即可实现:\nnew_x &lt;- data.frame(x=12)\n\n# 总体均数的可信区间\npredict(fit, newdata = new_x,interval = \"confidence\",level = 0.95)\n##        fit      lwr      upr\n## 1 3.331667 3.079481 3.583852\n\n# 个体Y值的预测区间\npredict(fit, newdata = new_x,interval = \"prediction\",level = 0.95)\n##        fit      lwr      upr\n## 1 3.331667 2.787731 3.875602\n以上结果均和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>双变量回归与相关</span>"
    ]
  },
  {
    "objectID": "1015-twocorrelation.html#直线相关",
    "href": "1015-twocorrelation.html#直线相关",
    "title": "6  双变量回归与相关",
    "section": "6.2 直线相关",
    "text": "6.2 直线相关\n使用课本例9-5的数据。\n\ndf &lt;- data.frame(\n  weight = c(43,74,51,58,50,65,54,57,67,69,80,48,38,85,54),\n  kv = c(217.22,316.18,231.11,220.96,254.70,293.84,263.28,271.73,263.46,\n         276.53,341.15,261.00,213.20,315.12,252.08)\n)\n\nstr(df)\n## 'data.frame':    15 obs. of  2 variables:\n##  $ weight: num  43 74 51 58 50 65 54 57 67 69 ...\n##  $ kv    : num  217 316 231 221 255 ...\n\n两变量是否有关联？其方向和密切程度如何？\n直接用cor可计算相关系数r：\n\ncor(df$weight, df$kv)\n## [1] 0.8754315\n\n或者直接用cor.test，既可以计算相关系数，又可以计算相关系数的P值：\n\ncor.test(~ weight + kv, data = df)\n## \n##  Pearson's product-moment correlation\n## \n## data:  weight and kv\n## t = 6.5304, df = 13, p-value = 1.911e-05\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.6584522 0.9580540\n## sample estimates:\n##       cor \n## 0.8754315\n\n从结果可以看出，两者是正相关，相关系数r=0.8754，且P值小于0.05，并给出了相关系数的可信区间：（0.6584522 0.9580540），具有统计学意义！\n可视化结果：\n\nlibrary(ggplot2)\n\nggplot(df, aes(weight, kv)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\",se=F) +\n  geom_vline(xintercept = mean(df$weight),linetype=2)+\n  geom_hline(yintercept = mean(df$kv),linetype=2)+\n  labs(x=\"体重(kg)X\",y=\"双肾体积(ml)Y\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n相关性分析的过程比较简单，在选择方法时要注意是使用pearson相关还是秩相关。\nR2的计算：\n\nsummary(lm(weight ~ kv, data = df))\n## \n## Call:\n## lm(formula = weight ~ kv, data = df)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -9.947 -4.469 -1.338  4.285 12.500 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -23.1845    12.7869  -1.813    0.093 .  \n## kv            0.3109     0.0476   6.530 1.91e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.777 on 13 degrees of freedom\n## Multiple R-squared:  0.7664, Adjusted R-squared:  0.7484 \n## F-statistic: 42.65 on 1 and 13 DF,  p-value: 1.911e-05\n\nMultiple R-squared: 0.7664, Adjusted R-squared: 0.7484",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>双变量回归与相关</span>"
    ]
  },
  {
    "objectID": "1015-twocorrelation.html#秩相关",
    "href": "1015-twocorrelation.html#秩相关",
    "title": "6  双变量回归与相关",
    "section": "6.3 秩相关",
    "text": "6.3 秩相关\n例9-8\n\ndf9_8 &lt;- foreign::read.spss(\"datasets/例09-08.sav\", to.data.frame = T)\n\nstr(df9_8)\n## 'data.frame':    18 obs. of  3 variables:\n##  $ number: num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x     : num  0.03 0.14 0.2 0.43 0.44 0.45 0.47 0.65 0.95 0.96 ...\n##  $ y     : num  0.05 0.34 0.93 0.69 0.38 0.79 1.19 4.74 2.31 5.95 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:3] \"\\xb1\\xe0\\xba\\xc5\" \"\\xcb\\xc0\\xd2򹹳\\xc9\" \"WYPLL\\xb9\\xb9\\xb3\\xc9\"\n##   ..- attr(*, \"names\")= chr [1:3] \"number\" \"x\" \"y\"\n\n计算相关系数：\n\ncor(df9_8$x,df9_8$y, method = \"spearman\")\n## [1] 0.9050568\n\n计算P值：\n\ncor.test(df9_8$x,df9_8$y, method = \"spearman\")\n## \n##  Spearman's rank correlation rho\n## \n## data:  df9_8$x and df9_8$y\n## S = 92, p-value &lt; 2.2e-16\n## alternative hypothesis: true rho is not equal to 0\n## sample estimates:\n##       rho \n## 0.9050568\n\nP&lt;0.001。\n如何校正？直接使用continuity即可（看帮助文档）：\n\ncor.test(df9_8$x,df9_8$y, method = \"spearman\",continuity=T)\n## \n##  Spearman's rank correlation rho\n## \n## data:  df9_8$x and df9_8$y\n## S = 92, p-value &lt; 2.2e-16\n## alternative hypothesis: true rho is not equal to 0\n## sample estimates:\n##       rho \n## 0.9050568",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>双变量回归与相关</span>"
    ]
  },
  {
    "objectID": "1015-twocorrelation.html#两条回归直线的比较",
    "href": "1015-twocorrelation.html#两条回归直线的比较",
    "title": "6  双变量回归与相关",
    "section": "6.4 两条回归直线的比较",
    "text": "6.4 两条回归直线的比较\n正常儿童数据见例9-1，大骨节病儿童数据见例9-9，问：回归直线是否不平行？\n\n# 例9-1数据\ndf9_1 &lt;- data.frame(x = c(13,11,9,6,8,10,12,7),\n                    y = c(3.54,3.01,3.09,2.48,2.56,3.36,3.18,2.65))\n\n# 例9-9数据\ndf9_9 &lt;- foreign::read.spss(\"datasets/例09-09.sav\", to.data.frame = T)\n\n建立回归方程：\n\n# 例9-1的回归方程\nfit9_1 &lt;- lm(y ~ x, data = df9_1)\n\n# 例9-2的回归方程\nfit9_9 &lt;- lm(y ~ x, data = df9_9)\n\na1 &lt;- anova(fit9_1)\na1\n## Analysis of Variance Table\n## \n## Response: y\n##           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \n## x          1 0.81343 0.81343  20.968 0.003774 **\n## Residuals  6 0.23276 0.03879                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\na2 &lt;- anova(fit9_9)\na2\n## Analysis of Variance Table\n## \n## Response: y\n##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \n## x          1 2.61351 2.61351  58.362 6.076e-05 ***\n## Residuals  8 0.35825 0.04478                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n如果此时你直接使用anova进行F检验，会得到以下报错：\n\nanova(fit9_1,fit9_9)\n\nError in anova.lmlist(object, ...) : \n  models were not all fitted to the same size of dataset\n\n这是因为这个函数只能处理样本量完全一样的两个模型的比较，此时我们可以把两个数据集合并到一起，添加一个交互项，查看交互项的显著性，以此来判断两条回归直线是否平行(如果有现成的函数可以比较的，请大神告诉我)。\n\ndf9_1$group &lt;- \"group9_1\"\ndf9_9$group &lt;- \"group9_9\"\n\ndf9 &lt;- rbind(df9_1,df9_9[,-1])\ndf9$group &lt;- factor(df9$group)\nstr(df9)\n## 'data.frame':    18 obs. of  3 variables:\n##  $ x    : num  13 11 9 6 8 10 12 7 10 9 ...\n##  $ y    : num  3.54 3.01 3.09 2.48 2.56 3.36 3.18 2.65 3.01 2.83 ...\n##  $ group: Factor w/ 2 levels \"group9_1\",\"group9_9\": 1 1 1 1 1 1 1 1 2 2 ...\n\n建立回归方程，并比较：\n\n# 建立不包含交互项的模型\nmodel_no_interaction &lt;- lm(y ~ x + group, data = df9)\n\n# 建立包含交互项的模型\nmodel_interaction &lt;- lm(y ~ x * group, data = df9)\n\n# 使用anova函数比较两个模型\nanova(model_no_interaction, model_interaction)\n## Analysis of Variance Table\n## \n## Model 1: y ~ x + group\n## Model 2: y ~ x * group\n##   Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)\n## 1     15 0.62211                           \n## 2     14 0.59101  1  0.031103 0.7368 0.4052\n\n得到的P值大于0.05，还不能认为两条总体回归直线不平行。\n当认为两条总体回归直线平行时，如果能进一步认为其总体截距是相等的，在两组数据的自变量取值范围接近时，便可认为两条总体回归直线基本重合，这时可合并两组样本资料，计算一个统一的回归方程。\n下面我们检测其截距是否相等，可通过直接查看有交互项模型的结果：\n\n# 查看模型摘要，检查group的显著性\nsummary(model_no_interaction)\n## \n## Call:\n## lm(formula = y ~ x + group, data = df9)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.29885 -0.15905  0.01675  0.14186  0.34023 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    1.44893    0.18427   7.863 1.06e-06 ***\n## x              0.16156    0.01785   9.049 1.83e-07 ***\n## groupgroup9_9 -0.23256    0.10181  -2.284   0.0373 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2037 on 15 degrees of freedom\n## Multiple R-squared:  0.8457, Adjusted R-squared:  0.8252 \n## F-statistic: 41.12 on 2 and 15 DF,  p-value: 8.162e-07\n\n结果中的groupgroup9_9的P值小于0.05，说明其截距是有差异的，不相等的。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>双变量回归与相关</span>"
    ]
  },
  {
    "objectID": "1015-twocorrelation.html#曲线拟合",
    "href": "1015-twocorrelation.html#曲线拟合",
    "title": "6  双变量回归与相关",
    "section": "6.5 曲线拟合",
    "text": "6.5 曲线拟合\n有些情况两个变量的关系并不是直线形式的，有可能是曲线形式的，此时可以通过曲线拟合来刻画两变量之间的关系。主要方法就是对变量做转换，比如log、平方、多项式、样条等。\n例9-11。\n\ndf9_11 &lt;- foreign::read.spss(\"datasets/例09-11.sav\", to.data.frame = T)\nstr(df9_11)\n## 'data.frame':    5 obs. of  3 variables:\n##  $ number: num  1 2 3 4 5\n##  $ x     : num  0.005 0.05 0.5 5 25\n##  $ y     : num  34.1 58 94.5 128.5 170\n##  - attr(*, \"variable.labels\")= Named chr [1:3] \"编号\" \" CRF浓度\" \"ACTH的合成量\"\n##   ..- attr(*, \"names\")= chr [1:3] \"number\" \"x\" \"y\"\n##  - attr(*, \"codepage\")= int 936\n\n先画图查看趋势：\n\nlibrary(ggplot2)\n\nggplot(df9_11, aes(x,y))+\n  geom_point(size=4)\n\n\n\n\n\n\n\n\n可以发现这个趋势非常像高中学过的对数函数的图像，所以我们选择对自变量X做对数转换，再画图看一看：\n\nggplot(df9_11, aes(log10(x),y))+\n  geom_point(size=4)\n\n\n\n\n\n\n\n\n果然就基本上呈直线趋势了，所以我们选择对数转换后的X建立直线回归方程：\n\nf9_11 &lt;- lm(y ~ log10(x), data = df9_11)\nsummary(f9_11)\n## \n## Call:\n## lm(formula = y ~ log10(x), data = df9_11)\n## \n## Residuals:\n##      1      2      3      4      5 \n##  7.152 -5.083 -4.698 -6.804  9.433 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  110.060      4.095   26.88 0.000113 ***\n## log10(x)      36.115      2.968   12.17 0.001195 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 8.838 on 3 degrees of freedom\n## Multiple R-squared:  0.9801, Adjusted R-squared:  0.9735 \n## F-statistic: 148.1 on 1 and 3 DF,  p-value: 0.001195\n\n例9-12。\n\ndf9_12 &lt;- foreign::read.spss(\"datasets/例09-12.sav\", to.data.frame = T)\nstr(df9_12)\n## 'data.frame':    15 obs. of  2 variables:\n##  $ x: num  2 5 7 10 14 19 26 31 34 38 ...\n##  $ y: num  54 50 45 37 35 25 20 16 18 13 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:2] \"סԺ\\xcc\\xec\\xca\\xfd\" \"Ԥ\\xba\\xf3ָ\\xca\\xfd\"\n##   ..- attr(*, \"names\")= chr [1:2] \"x\" \"y\"\n\n先画图看趋势：\n\nggplot(df9_12, aes(x,y))+\n  geom_point(size=4)\n\n\n\n\n\n\n\n\n这个图有点像指数函数的图像，我们可以尝试对因变量Y做对数转换，再画图看看：\n\nggplot(df9_12, aes(x,log(y)))+\n  geom_point(size=4)\n\n\n\n\n\n\n\n\n果然就基本上呈直线趋势了，所以我们选择对数转换后的Y建立直线回归方程：\n\nf9_12 &lt;- lm(log(y) ~ x, data = df9_12)\nsummary(f9_12)\n## \n## Call:\n## lm(formula = log(y) ~ x, data = df9_12)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.37241 -0.07073  0.02777  0.05982  0.33539 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  4.037159   0.084103   48.00 5.08e-16 ***\n## x           -0.037974   0.002284  -16.62 3.86e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1794 on 13 degrees of freedom\n## Multiple R-squared:  0.9551, Adjusted R-squared:  0.9516 \n## F-statistic: 276.4 on 1 and 13 DF,  p-value: 3.858e-10",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>双变量回归与相关</span>"
    ]
  },
  {
    "objectID": "统计图表.html",
    "href": "统计图表.html",
    "title": "7  三线表和统计绘图",
    "section": "",
    "text": "7.1 统计表\n主要是各种三线表的绘制，但是目前的临床研究的三线表制作还是离不开Word、Excel等传统软件，目前没有任何一款R包可以做到：输出结果无需修改即可直接发表。或多或少都需要在Word中修改一下的。\n目前在R语言中绘制三线表的常见R包有：compareGroups、tableone、table1、gtSummary、gt、gtExtras等，我已写过多篇推文进行介绍（公众号后台回复三线表即可获取合集）：\n我使用下来在多数情况下还是compareGroups最方便，所以下面将会详细介绍compareGroups的用法，其他R包的用法请参考以上链接。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>三线表和统计绘图</span>"
    ]
  },
  {
    "objectID": "统计图表.html#统计表",
    "href": "统计图表.html#统计表",
    "title": "7  三线表和统计绘图",
    "section": "",
    "text": "compareGroups：compareGroups包1行代码生成基线资料表\ntableone:使用R语言快速绘制三线表\ntable1:tableone？table1？傻傻分不清楚\ngtSummary:超棒的三线表绘制工具，总有一款适合你\ngt:gt包绘制表格详细介绍\ngtExtras:使用gtExtras美化表格\n\n\n\n7.1.1 compareGroups\ncompareGroups可用一句代码生成基线资料表、单因素分析表、多因素分析表等，可直接把结果导出为csv、Excel、Word、Markdown、LaTeX、PDF，而且十分美观，大大提高工作效率。\n之前也做过介绍，但是最近发现R包更新后自带数据换了，之前的predimed数据没有了，变成了regicor，所以重新再整理一遍。之前的介绍：使用compareGroups包1行代码生成基线资料表\n官方文档的名字就叫：分组描述（Descriptives-by-groups），充分说明该包的强项就是分组描述，特别适合基线资料表、各类SCI中的table 1的绘制。支持生存数据，可以计算HR、OR、P-for-trend、多重检验的P值等。\n使用方法和之前介绍的基本一样，主要就是3个函数：\n\ncompareGroups：计算\ncreateTable：构建表格\nexport2xxx导出表格\n\n之前R包内置的predimed已经没有了，现在默认演示数据变成了regicor。\n\n为了说明这个软件包是如何工作的，我们从REGICOR研究中取了一部分数据。REGICOR是一个 对来自西班牙东北部的参与者进行的横断面研究，包括：人口统计学信息（年龄、性别、身高、体重、腰围等）、血脂特征（总胆固醇和胆固醇、甘油三酯等）、问卷调查信息（体格，活动，生活质量，…）等。此外，心血管事件和 死亡信息来自医院和官方登记处。\n\n查看数据:\n\nlibrary(compareGroups)\ndata(\"regicor\")\ndim(regicor)\n## [1] 2294   25\nstr(regicor)\n## 'data.frame':    2294 obs. of  25 variables:\n##  $ id      : num  2.26e+03 1.88e+03 3.00e+09 3.00e+09 3.00e+09 ...\n##   ..- attr(*, \"label\")= Named chr \"Individual id\"\n##   .. ..- attr(*, \"names\")= chr \"id\"\n##  $ year    : Factor w/ 3 levels \"1995\",\"2000\",..: 3 3 2 2 2 2 2 1 3 1 ...\n##   ..- attr(*, \"label\")= Named chr \"Recruitment year\"\n##   .. ..- attr(*, \"names\")= chr \"year\"\n##  $ age     : int  70 56 37 69 70 40 66 53 43 70 ...\n##   ..- attr(*, \"label\")= Named chr \"Age\"\n##   .. ..- attr(*, \"names\")= chr \"age\"\n##  $ sex     : Factor w/ 2 levels \"Male\",\"Female\": 2 2 1 2 2 2 1 2 2 1 ...\n##   ..- attr(*, \"label\")= chr \"Sex\"\n##  $ smoker  : Factor w/ 3 levels \"Never smoker\",..: 1 1 2 1 NA 2 1 1 3 3 ...\n##   ..- attr(*, \"label\")= Named chr \"Smoking status\"\n##   .. ..- attr(*, \"names\")= chr \"smoker\"\n##  $ sbp     : int  138 139 132 168 NA 108 120 132 95 142 ...\n##   ..- attr(*, \"label\")= Named chr \"Systolic blood pressure\"\n##   .. ..- attr(*, \"names\")= chr \"sbp\"\n##  $ dbp     : int  75 89 82 97 NA 70 72 78 65 78 ...\n##   ..- attr(*, \"label\")= Named chr \"Diastolic blood pressure\"\n##   .. ..- attr(*, \"names\")= chr \"dbp\"\n##  $ histhtn : Factor w/ 2 levels \"Yes\",\"No\": 2 2 2 2 2 2 1 2 2 2 ...\n##   ..- attr(*, \"label\")= Named chr \"History of hypertension\"\n##   .. ..- attr(*, \"names\")= chr \"histbp\"\n##  $ txhtn   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n##   ..- attr(*, \"label\")= chr \"Hypertension treatment\"\n##  $ chol    : num  294 220 245 168 NA NA 298 254 194 188 ...\n##   ..- attr(*, \"label\")= Named chr \"Total cholesterol\"\n##   .. ..- attr(*, \"names\")= chr \"chol\"\n##  $ hdl     : num  57 50 59.8 53.2 NA ...\n##   ..- attr(*, \"label\")= Named chr \"HDL cholesterol\"\n##   .. ..- attr(*, \"names\")= chr \"hdl\"\n##  $ triglyc : num  93 160 89 116 NA 94 71 NA 68 137 ...\n##   ..- attr(*, \"label\")= Named chr \"Triglycerides\"\n##   .. ..- attr(*, \"names\")= chr \"triglyc\"\n##  $ ldl     : num  218.4 138 167.4 91.6 NA ...\n##   ..- attr(*, \"label\")= Named chr \"LDL cholesterol\"\n##   .. ..- attr(*, \"names\")= chr \"ldl\"\n##  $ histchol: Factor w/ 2 levels \"Yes\",\"No\": 2 2 2 2 NA 2 1 2 2 2 ...\n##   ..- attr(*, \"label\")= chr \"History of hyperchol.\"\n##  $ txchol  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 NA 1 1 1 1 1 ...\n##   ..- attr(*, \"label\")= Named chr \"Cholesterol treatment\"\n##   .. ..- attr(*, \"names\")= chr \"txchol\"\n##  $ height  : num  160 163 170 147 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Height (cm)\"\n##   .. ..- attr(*, \"names\")= chr \"height\"\n##  $ weight  : num  64 67 70 68 NA 43.5 79.2 45.8 53 62 ...\n##   ..- attr(*, \"label\")= Named chr \"Weight (Kg)\"\n##   .. ..- attr(*, \"names\")= chr \"weight\"\n##  $ bmi     : num  25 25.2 24.2 31.5 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Body mass index\"\n##   .. ..- attr(*, \"names\")= chr \"bmi\"\n##  $ phyact  : num  304 160 553 522 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Physical activity (Kcal/week)\"\n##   .. ..- attr(*, \"names\")= chr \"phyact\"\n##  $ pcs     : num  54.5 58.2 43.4 54.3 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Physical component\"\n##   .. ..- attr(*, \"names\")= chr \"pcs\"\n##  $ mcs     : num  58.9 48 62.6 57.9 NA ...\n##   ..- attr(*, \"label\")= chr \"Mental component\"\n##  $ cv      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 NA 1 1 1 1 1 ...\n##   ..- attr(*, \"label\")= chr \"Cardiovascular event\"\n##  $ tocv    : num  1025 2757 1906 1055 NA ...\n##   ..- attr(*, \"label\")= chr \"Days to cardiovascular event or end of follow-up\"\n##  $ death   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 1 NA 1 2 1 1 1 ...\n##   ..- attr(*, \"label\")= chr \"Overall death\"\n##  $ todeath : num  1299.2 39.3 858.4 1833.1 NA ...\n##   ..- attr(*, \"label\")= chr \"Days to overall death or end of follow-up\"\n\n各个变量的信息如下：\n\n\n\nName\nLabel\nCodes\n\n\n\n\nid\nIndividual id\n\n\n\nyear\nRecruitment year\n1995; 2000; 2005\n\n\nage\nAge\n\n\n\nsex\nSex\nMale; Female\n\n\nsmoker\nSmoking status\nNever smoker; Current or former &lt; 1y; Former $\\geq$ 1y\n\n\nsbp\nSystolic blood pressure\n\n\n\ndbp\nDiastolic blood pressure\n\n\n\nhisthtn\nHistory of hypertension\nYes; No\n\n\ntxhtn\nHypertension treatment\nNo; Yes\n\n\nchol\nTotal cholesterol\n\n\n\nhdl\nHDL cholesterol\n\n\n\ntriglyc\nTriglycerides\n\n\n\nldl\nLDL cholesterol\n\n\n\nhistchol\nHistory of hyperchol.\nYes; No\n\n\ntxchol\nCholesterol treatment\nNo; Yes\n\n\nheight\nHeight (cm)\n\n\n\nweight\nWeight (Kg)\n\n\n\nbmi\nBody mass index\n\n\n\nphyact\nPhysical activity (Kcal/week)\n\n\n\npcs\nPhysical component\n\n\n\nmcs\nMental component\n\n\n\ncv\nCardiovascular event\nNo; Yes\n\n\ntocv\nDays to cardiovascular event or end of follow-up\n\n\n\ndeath\nOverall death\nNo; Yes\n\n\ntodeath\nDays to overall death or end of follow-up\n\n\n\n\n\n\n使用该R包的一些注意点：\n\n该包的重点是描述数据，不是对数据进行质量控制或其他目的。\n强烈建议数据框中只包含需要描述的数据，对于不需要的数据建议不要包含在数据框中。\n分类变量需要进行因子化。\n可以给各个变量增加label属性以展示其详细信息，该包默认会展示各个变量的label。\n\n如果是生存数据（time-to-event），必须使用Surv()包装数据。比如：\n\nlibrary(survival)\n\nregicor$tmain &lt;- with(regicor, Surv(tocv, cv == \"Yes\"))\nattr(regicor$tmain, \"label\") &lt;- \"Time to CV event or censoring\"\n\n这里新建的tmain是生存时间。\n以year为分组变量，统计各个变量的差异：\n\n# 不要id这个变量\ncompareGroups(year ~ . - id, data=regicor)\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##    var                                              N    p.value \n## 1  Age                                              2294 0.078*  \n## 2  Sex                                              2294 0.506   \n## 3  Smoking status                                   2233 &lt;0.001**\n## 4  Systolic blood pressure                          2280 &lt;0.001**\n## 5  Diastolic blood pressure                         2280 &lt;0.001**\n## 6  History of hypertension                          2286 &lt;0.001**\n## 7  Hypertension treatment                           2251 0.002** \n## 8  Total cholesterol                                2193 &lt;0.001**\n## 9  HDL cholesterol                                  2225 0.208   \n## 10 Triglycerides                                    2231 0.582   \n## 11 LDL cholesterol                                  2126 &lt;0.001**\n## 12 History of hyperchol.                            2273 &lt;0.001**\n## 13 Cholesterol treatment                            2239 &lt;0.001**\n## 14 Height (cm)                                      2259 0.003** \n## 15 Weight (Kg)                                      2259 0.150   \n## 16 Body mass index                                  2259 &lt;0.001**\n## 17 Physical activity (Kcal/week)                    2206 &lt;0.001**\n## 18 Physical component                               2054 0.032** \n## 19 Mental component                                 2054 &lt;0.001**\n## 20 Cardiovascular event                             2163 0.161   \n## 21 Days to cardiovascular event or end of follow-up 2163 0.099*  \n## 22 Overall death                                    2148 &lt;0.001**\n## 23 Days to overall death or end of follow-up        2148 0.252   \n## 24 Time to CV event or censoring                    2163 0.157   \n##    method            selection\n## 1  continuous normal ALL      \n## 2  categorical       ALL      \n## 3  categorical       ALL      \n## 4  continuous normal ALL      \n## 5  continuous normal ALL      \n## 6  categorical       ALL      \n## 7  categorical       ALL      \n## 8  continuous normal ALL      \n## 9  continuous normal ALL      \n## 10 continuous normal ALL      \n## 11 continuous normal ALL      \n## 12 categorical       ALL      \n## 13 categorical       ALL      \n## 14 continuous normal ALL      \n## 15 continuous normal ALL      \n## 16 continuous normal ALL      \n## 17 continuous normal ALL      \n## 18 continuous normal ALL      \n## 19 continuous normal ALL      \n## 20 categorical       ALL      \n## 21 continuous normal ALL      \n## 22 categorical       ALL      \n## 23 continuous normal ALL      \n## 24 Surv [Tmax=1718]  ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n比如第一个变量age，其实使用的方差分析，给出了age在不同year中的pvalue，method显示了该包把age这个变量看做连续型变量且符合正态分布。\n我们可以自己使用方差分析看下结果：\n\nsummary(aov(age ~ year, data = regicor))\n##               Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## year           2    623   311.6   2.556 0.0778 .\n## Residuals   2291 279320   121.9                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n可以看到结果是一样的哈~\nsex这个变量是用的卡方检验：\n\nchisq.test(regicor$sex, regicor$year)\n## \n##  Pearson's Chi-squared test\n## \n## data:  regicor$sex and regicor$year\n## X-squared = 1.364, df = 2, p-value = 0.5056\n\n结果也是一样的。\n支持之使用其中一部分数据，比如只看一下age、smoker、bmi这3个变量在不同的year中的差异，并且只选择性别为女性，对于bmi这个变量，只选择年龄大于50岁的：\n\ncompareGroups(year ~ age + smoker + bmi, data=regicor, \n              selec = list(bmi=age&gt;50), \n              subset = sex==\"Female\")\n## \n## \n## -------- Summary of results by groups of 'year'---------\n## \n## \n##   var             N    p.value  method           \n## 1 Age             1193 0.351    continuous normal\n## 2 Smoking status  1162 &lt;0.001** categorical      \n## 3 Body mass index  709 0.308    continuous normal\n##   selection                     \n## 1 sex == \"Female\"               \n## 2 sex == \"Female\"               \n## 3 (sex == \"Female\") & (age &gt; 50)\n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n连续型变量支持选择不同的统计方法（选择有限），通过method指定即可：\n\ncompareGroups(year ~ age + smoker + triglyc, data=regicor, \n              method = c(triglyc=NA), \n              alpha= 0.01)\n## Warning in cor.test.default(x, as.integer(y), method = \"spearman\"): Cannot\n## compute exact p-value with ties\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##   var            N    p.value  method                selection\n## 1 Age            2294 0.078*   continuous normal     ALL      \n## 2 Smoking status 2233 &lt;0.001** categorical           ALL      \n## 3 Triglycerides  2231 0.762    continuous non-normal ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n\nmethod：统计检验方法\n\n1：数值型变量，正态分布，\n2：数值型变量，非正态分布\n3：分类变量\nNA：使用shapiro.test()决定是否正态分布\n\nalpha：正态性检验的阈值\nmin.dis：所有非因子型向量都会被认为是连续型的，除非某个变量的取值少于5个，可通过此参数更改这个标准。\n\n\nregicor$age7gr &lt;- as.integer(cut(regicor$age, breaks = c(-Inf, 40, 45, 50, 55, 65, 70, Inf), right = TRUE))\n\ncompareGroups(year ~ age7gr, data = regicor, method = c(age7gr = NA), min.dis = 8)\n## Warning in compare.i(X[, i], y = y, selec.i = selec[i], method.i = method[i], :\n## variable 'age7gr' converted to factor since few different values contained\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##   var    N    p.value method      selection\n## 1 age7gr 2294 0.012** categorical ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n\nmax.ylev：分组变量的最大组数\n\n\ncompareGroups(age7gr ~ sex + bmi + smoker, data = regicor, max.ylev = 7)\n## \n## \n## -------- Summary of results by groups of 'age7gr'---------\n## \n## \n##   var             N    p.value  method            selection\n## 1 Sex             2294 0.950    categorical       ALL      \n## 2 Body mass index 2259 &lt;0.001** continuous normal ALL      \n## 3 Smoking status  2233 &lt;0.001** categorical       ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n显示列的名字，而不是label：\n\ncompareGroups(year ~ age + smoker + bmi, data = regicor, include.label = FALSE)\n## \n## \n## -------- Summary of results by groups of 'year'---------\n## \n## \n##   var    N    p.value  method            selection\n## 1 age    2294 0.078*   continuous normal ALL      \n## 2 smoker 2233 &lt;0.001** categorical       ALL      \n## 3 bmi    2259 &lt;0.001** continuous normal ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\nQ1/Q3：非正态分布的变量默认显示中位数（25%，75%），这个百分位数可以更改，设置为0和1则是最小值和最大值\n\nresu2 &lt;- compareGroups(year ~ age + triglyc, data = regicor, \n                       method = c(triglyc = 2), \n                       Q1 = 0.025, Q3 = 0.975)\n## Warning in cor.test.default(x, as.integer(y), method = \"spearman\"): Cannot\n## compute exact p-value with ties\ncreateTable(resu2)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## _______________________________________________________________________ \n##                    1995            2000            2005       p.overall \n##                    N=431           N=786          N=1077                \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age             54.1 (11.7)     54.3 (11.2)     55.3 (10.6)     0.078   \n## Triglycerides 94.0 [47.0;292] 98.0 [47.0;278] 98.0 [42.0;293]   0.762   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nsimplify：有时某个变量在某个分组中可能样本数为0，这时候要设置为TRUE来排除这样的亚组，因为卡方检验和Fisher精确概率法的计算不能为0。\n下面新建一个变量smk，其中unknown这个分组的样本数为0：\n\nregicor$smk &lt;- regicor$smoker\nlevels(regicor$smk) &lt;- c(\"Never smoker\", \"Current or former &lt; 1y\", \"Former &gt;= 1y\", \"Unknown\")\nattr(regicor$smk, \"label\") &lt;- \"Smoking 4 cat.\"\ncbind(table(regicor$smk))\n##                        [,1]\n## Never smoker           1201\n## Current or former &lt; 1y  593\n## Former &gt;= 1y            439\n## Unknown                   0\n\n如果不加simplify = FALSE会给出warning：\n\ncompareGroups(year ~ age + smk + bmi, data = regicor)\n## Warning in compare.i(X[, i], y = y, selec.i = selec[i], method.i = method[i], :\n## Some levels of 'smk' are removed since no observation in that/those levels\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##   var             N    p.value  method            selection\n## 1 Age             2294 0.078*   continuous normal ALL      \n## 2 Smoking 4 cat.  2233 &lt;0.001** categorical       ALL      \n## 3 Body mass index 2259 &lt;0.001** continuous normal ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n加simplify = FALSE之后就不再对这个变量进行统计检验了，也就没有P值显示了：\n\ncompareGroups(year ~ age + smk + bmi, data = regicor, simplify = FALSE)\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##   var             N    p.value  method            selection\n## 1 Age             2294 0.078*   continuous normal ALL      \n## 2 Smoking 4 cat.  2233 .        categorical       ALL      \n## 3 Body mass index 2259 &lt;0.001** continuous normal ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n支持更新对象：\n\nres &lt;- compareGroups(year ~ age + sex + smoker + bmi, data = regicor)\nres\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##   var             N    p.value  method            selection\n## 1 Age             2294 0.078*   continuous normal ALL      \n## 2 Sex             2294 0.506    categorical       ALL      \n## 3 Smoking status  2233 &lt;0.001** categorical       ALL      \n## 4 Body mass index 2259 &lt;0.001** continuous normal ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n\nres &lt;- update(res, . ~ . - sex + triglyc + cv + tocv, \n              subset = sex == \"Female\", \n              method = c(triglyc = 2, tocv = 2), \n              selec = list(triglyc = txchol == \"No\"))\n## Warning in cor.test.default(x, as.integer(y), method = \"spearman\"): Cannot\n## compute exact p-value with ties\n## Warning in cor.test.default(x, as.integer(y), method = \"spearman\"): Cannot\n## compute exact p-value with ties\nres\n## \n## \n## -------- Summary of results by groups of 'year'---------\n## \n## \n##   var                                              N    p.value \n## 1 Age                                              1193 0.351   \n## 2 Smoking status                                   1162 &lt;0.001**\n## 3 Body mass index                                  1169 0.084*  \n## 4 Triglycerides                                    1020 0.993   \n## 5 Cardiovascular event                             1121 0.139   \n## 6 Days to cardiovascular event or end of follow-up 1121 0.427   \n##   method                selection                           \n## 1 continuous normal     sex == \"Female\"                     \n## 2 categorical           sex == \"Female\"                     \n## 3 continuous normal     sex == \"Female\"                     \n## 4 continuous non-normal (sex == \"Female\") & (txchol == \"No\")\n## 5 categorical           sex == \"Female\"                     \n## 6 continuous non-normal sex == \"Female\"                     \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n支持提取其中的某些信息，比如P值、均值等，方便后续的操作：\n\ndata(SNPs)\ntab &lt;- createTable(compareGroups(casco ~ snp10001 + snp10002 + snp10005 +\n                                   snp10008 + snp10009, SNPs))\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\n\n# 提取P值，方便进行校正\npvals &lt;- getResults(tab, \"p.overall\")\np.adjust(pvals, method = \"BH\")\n##  snp10001  snp10002  snp10005  snp10008  snp10009 \n## 0.7051300 0.7072158 0.7583432 0.7583432 0.7072158\n\n4.6.0以后的版本还增加了计算调整P值的方法，和p.adjust的方法一样，比如Bonferroni/False Discovery Rate等。\n\ncg &lt;- compareGroups(casco ~ snp10001 + snp10002 + snp10005 + snp10008 + snp10009, SNPs)\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\ncreateTable(padjustCompareGroups(cg, method = \"BH\"))\n## \n## --------Summary descriptives table by 'casco'---------\n## \n## _________________________________________ \n##               0          1      p.overall \n##              N=47      N=110              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## snp10001:                         0.705   \n##     CC    2 (4.26%)  10 (9.09%)           \n##     CT    21 (44.7%) 32 (29.1%)           \n##     TT    24 (51.1%) 68 (61.8%)           \n## snp10002:                         0.707   \n##     AA    0 (0.00%)  5 (4.55%)            \n##     AC    25 (53.2%) 53 (48.2%)           \n##     CC    22 (46.8%) 52 (47.3%)           \n## snp10005:                         0.758   \n##     AA    0 (0.00%)  3 (2.73%)            \n##     AG    22 (46.8%) 48 (43.6%)           \n##     GG    25 (53.2%) 59 (53.6%)           \n## snp10008:                         0.758   \n##     CC    30 (63.8%) 74 (67.3%)           \n##     CG    15 (31.9%) 29 (26.4%)           \n##     GG    2 (4.26%)  7 (6.36%)            \n## snp10009:                         0.707   \n##     AA    21 (45.7%) 51 (46.4%)           \n##     AG    25 (54.3%) 54 (49.1%)           \n##     GG    0 (0.00%)  5 (4.55%)            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n支持计算OR值和HR值，如果是二分类会计算OR值，如果是time-to-event数据会计算HR值：\n\n# show.ratio = TRUE展示p.ratio和OR值\nres1 &lt;- compareGroups(cv ~ age + sex + bmi + smoker, data = regicor, ref = 1)\ncreateTable(res1, show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Cardiovascular event'---------\n## \n## ______________________________________________________________________________________ \n##                                 No          Yes            OR        p.ratio p.overall \n##                               N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        54.6 (11.1)  57.5 (11.0) 1.02 [1.00;1.04]  0.017    0.018   \n## Sex:                                                                           0.801   \n##     Male                   996 (48.1%)  46 (50.0%)        Ref.        Ref.             \n##     Female                 1075 (51.9%) 46 (50.0%)  0.93 [0.61;1.41]  0.721            \n## Body mass index            27.6 (4.56)  28.1 (4.48) 1.02 [0.98;1.07]  0.313    0.307   \n## Smoking status:                                                               &lt;0.001   \n##     Never smoker           1099 (54.3%) 37 (40.2%)        Ref.        Ref.             \n##     Current or former &lt; 1y 506 (25.0%)  47 (51.1%)  2.75 [1.77;4.32] &lt;0.001            \n##     Former &gt;= 1y           419 (20.7%)   8 (8.70%)  0.58 [0.25;1.19]  0.142            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nref可以更改参考水平，比如smoker以第一个水平为参考，sex以第二个水平为参考：\n\nres2 &lt;- compareGroups(cv ~ age + sex + bmi + smoker, data = regicor, \n                      ref = c(smoker = 1, sex = 2))\ncreateTable(res2, show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Cardiovascular event'---------\n## \n## ______________________________________________________________________________________ \n##                                 No          Yes            OR        p.ratio p.overall \n##                               N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        54.6 (11.1)  57.5 (11.0) 1.02 [1.00;1.04]  0.017    0.018   \n## Sex:                                                                           0.801   \n##     Male                   996 (48.1%)  46 (50.0%)  1.08 [0.71;1.64]  0.721            \n##     Female                 1075 (51.9%) 46 (50.0%)        Ref.        Ref.             \n## Body mass index            27.6 (4.56)  28.1 (4.48) 1.02 [0.98;1.07]  0.313    0.307   \n## Smoking status:                                                               &lt;0.001   \n##     Never smoker           1099 (54.3%) 37 (40.2%)        Ref.        Ref.             \n##     Current or former &lt; 1y 506 (25.0%)  47 (51.1%)  2.75 [1.77;4.32] &lt;0.001            \n##     Former &gt;= 1y           419 (20.7%)   8 (8.70%)  0.58 [0.25;1.19]  0.142            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n这里的p.overall是通过t检验计算的：(如果不符合正态分布是通过Kruskall-Wallis检验计算)\n\nt.test(age ~ cv, data = regicor)\n## \n##  Welch Two Sample t-test\n## \n## data:  age by cv\n## t = -2.4124, df = 99.303, p-value = 0.01768\n## alternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n## 95 percent confidence interval:\n##  -5.1660205 -0.5031794\n## sample estimates:\n##  mean in group No mean in group Yes \n##          54.62192          57.45652\n\n这个表格中的p.ratio和OR值就是做个逻辑回归得到的：\n\naa &lt;- glm(cv ~ age, data = regicor,family = binomial())\nbroom::tidy(aa,exponentiate = T,conf.int = T)\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   0.0120   0.572       -7.74 1.01e-14  0.00377    0.0357\n## 2 age           1.02     0.00980      2.39 1.69e- 2  1.00       1.04\n\nage的p.value就是表格中的p.ratio，estimate是OR值，conf.low和conf.high是可信区间。\nref.no表示，如果某个变量中有No（或者NO，no，不区分大小写）这个类别，那就以这个类别为参考，这是一个可适用于所有变量的参数。\n\nres &lt;- compareGroups(cv ~ age + sex + bmi + histhtn + txhtn, data = regicor, \n                     ref.no = \"NO\")\ncreateTable(res, show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Cardiovascular event'---------\n## \n## ____________________________________________________________________________________ \n##                               No          Yes            OR        p.ratio p.overall \n##                             N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.6 (11.1)  57.5 (11.0) 1.02 [1.00;1.04]  0.017    0.018   \n## Sex:                                                                         0.801   \n##     Male                 996 (48.1%)  46 (50.0%)        Ref.        Ref.             \n##     Female               1075 (51.9%) 46 (50.0%)  0.93 [0.61;1.41]  0.721            \n## Body mass index          27.6 (4.56)  28.1 (4.48) 1.02 [0.98;1.07]  0.313    0.307   \n## History of hypertension:                                                     0.058   \n##     Yes                  647 (31.3%)  38 (41.3%)  1.54 [1.00;2.36]  0.049            \n##     No                   1418 (68.7%) 54 (58.7%)        Ref.        Ref.             \n## Hypertension treatment:                                                      0.270   \n##     No                   1657 (81.3%) 70 (76.1%)        Ref.        Ref.             \n##     Yes                  382 (18.7%)  22 (23.9%)  1.37 [0.82;2.21]  0.223            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nsex/histhtn/txhtn这几个变量中都有No这个分类，所以都以这个类别为参考。\n对于连续性变量来说，OR值和HR值的解释是自变量每增加一个单位，因变量变化OR倍或HR倍，参数fact.ratio可以控制一个单位具体是多少。比如设置bmi的一个单位是2，age的一个单位是10：\n\nres &lt;- compareGroups(cv ~ age + bmi, data = regicor, \n                     fact.ratio = c(age = 10, bmi = 2))\ncreateTable(res, show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Cardiovascular event'---------\n## \n## __________________________________________________________________________ \n##                     No          Yes            OR        p.ratio p.overall \n##                   N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age             54.6 (11.1) 57.5 (11.0) 1.26 [1.04;1.53]  0.017    0.018   \n## Body mass index 27.6 (4.56) 28.1 (4.48) 1.05 [0.96;1.14]  0.313    0.307   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n通常对于OR值和HR值来说，是相对于因变量的某个类别来说的，比如自变量每增加一个单位，患癌症的风险增加OR倍，可以通过ref.y改成不患癌症的风险增加xx倍。\n\nres &lt;- compareGroups(cv ~ age + sex + bmi + txhtn, data = regicor, ref.y = 2)\ncreateTable(res, show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Cardiovascular event'---------\n## \n## ___________________________________________________________________________________ \n##                              No          Yes            OR        p.ratio p.overall \n##                            N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                     54.6 (11.1)  57.5 (11.0) 0.98 [1.00;0.96]  0.017    0.018   \n## Sex:                                                                        0.801   \n##     Male                996 (48.1%)  46 (50.0%)        Ref.        Ref.             \n##     Female              1075 (51.9%) 46 (50.0%)  1.08 [0.71;1.64]  0.721            \n## Body mass index         27.6 (4.56)  28.1 (4.48) 0.98 [1.02;0.93]  0.313    0.307   \n## Hypertension treatment:                                                     0.270   \n##     No                  1657 (81.3%) 70 (76.1%)        Ref.        Ref.             \n##     Yes                 382 (18.7%)  22 (23.9%)  0.73 [0.45;1.22]  0.223            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\ncreateTable函数用于把compareGroups计算的结果转换为表格，打印在屏幕上或者导出为CSV、LaTeX、HTML、Word、Excel等格式。\n\nres &lt;- compareGroups(year ~ age + sex + smoker + bmi + sbp, data = regicor, \n                     selec = list(sbp = txhtn == \"No\"))\nrestab &lt;- createTable(res)\n\nwhich.table = \"descr\"给出描述性三线表：\n\nprint(restab, which.table = \"descr\")\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ________________________________________________________________________ \n##                               1995        2000        2005     p.overall \n##                               N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex:                                                             0.506   \n##     Male                   206 (47.8%) 390 (49.6%) 505 (46.9%)           \n##     Female                 225 (52.2%) 396 (50.4%) 572 (53.1%)           \n## Smoking status:                                                 &lt;0.001   \n##     Never smoker           234 (56.4%) 414 (54.6%) 553 (52.2%)           \n##     Current or former &lt; 1y 109 (26.3%) 267 (35.2%) 217 (20.5%)           \n##     Former &gt;= 1y           72 (17.3%)  77 (10.2%)  290 (27.4%)           \n## Body mass index            27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  &lt;0.001   \n## Systolic blood pressure    129 (17.4)  130 (20.1)  124 (16.9)   &lt;0.001   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nwhich.table = \"avail\"给出可用数据和方法表：\n\nprint(restab, which.table = \"avail\")\n## \n## \n## \n## ---Available data----\n## \n## ____________________________________________________________________________ \n##                         [ALL] 1995 2000 2005      method          select     \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                     2294  431  786  1077 continuous-normal      ALL      \n## Sex                     2294  431  786  1077    categorical         ALL      \n## Smoking status          2233  415  758  1060    categorical         ALL      \n## Body mass index         2259  423  771  1065 continuous-normal      ALL      \n## Systolic blood pressure 1810  357  649  804  continuous-normal txhtn == \"No\" \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n某些二分类的变量，比如男性/女性这种，可以通过hide不展示其中某个类别，比如不展示sex中的male：\n\nupdate(restab, hide = c(sex = \"Male\"))\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ________________________________________________________________________ \n##                               1995        2000        2005     p.overall \n##                               N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex: Female                225 (52.2%) 396 (50.4%) 572 (53.1%)   0.506   \n## Smoking status:                                                 &lt;0.001   \n##     Never smoker           234 (56.4%) 414 (54.6%) 553 (52.2%)           \n##     Current or former &lt; 1y 109 (26.3%) 267 (35.2%) 217 (20.5%)           \n##     Former &gt;= 1y           72 (17.3%)  77 (10.2%)  290 (27.4%)           \n## Body mass index            27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  &lt;0.001   \n## Systolic blood pressure    129 (17.4)  130 (20.1)  124 (16.9)   &lt;0.001   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nhide.no和ref.no的含义类似，如果某个变量含有no这个类别，可以全部隐藏：\n\nres &lt;- compareGroups(year ~ age + sex + histchol + histhtn, data = regicor)\ncreateTable(res, hide.no = \"no\", hide = c(sex = \"Male\"))\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## _____________________________________________________________________ \n##                            1995        2000        2005     p.overall \n##                            N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                     54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex: Female             225 (52.2%) 396 (50.4%) 572 (53.1%)   0.506   \n## History of hyperchol.   97 (22.5%)  256 (33.2%) 356 (33.2%)  &lt;0.001   \n## History of hypertension 111 (25.8%) 233 (29.6%) 379 (35.5%)  &lt;0.001   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\ndigits用于控制表格中小数点的位数：\n\ncreateTable(res, digits = c(age = 2, sex = 3))\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ____________________________________________________________________________ \n##                              1995          2000          2005      p.overall \n##                              N=431         N=786        N=1077               \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.10 (11.72) 54.34 (11.22) 55.28 (10.63)   0.078   \n## Sex:                                                                 0.506   \n##     Male                 206 (47.796%) 390 (49.618%) 505 (46.890%)           \n##     Female               225 (52.204%) 396 (50.382%) 572 (53.110%)           \n## History of hyperchol.:                                              &lt;0.001   \n##     Yes                   97 (22.5%)    256 (33.2%)   356 (33.2%)            \n##     No                    334 (77.5%)   515 (66.8%)   715 (66.8%)            \n## History of hypertension:                                            &lt;0.001   \n##     Yes                   111 (25.8%)   233 (29.6%)   379 (35.5%)            \n##     No                    320 (74.2%)   553 (70.4%)   690 (64.5%)            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n对于分类变量，默认情况下表格中会给出计数和比例，可通过type参数更改只显示计数或比例：\n\n# 1只显示比例，默认是2都显示，3只显示计数\ncreateTable(res, type = 1)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ______________________________________________________________________ \n##                             1995        2000        2005     p.overall \n##                             N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex:                                                           0.506   \n##     Male                    47.8%       49.6%       46.9%              \n##     Female                  52.2%       50.4%       53.1%              \n## History of hyperchol.:                                        &lt;0.001   \n##     Yes                     22.5%       33.2%       33.2%              \n##     No                      77.5%       66.8%       66.8%              \n## History of hypertension:                                      &lt;0.001   \n##     Yes                     25.8%       29.6%       35.5%              \n##     No                      74.2%       70.4%       64.5%              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nshow.n = TRUE展示每个变量的所有可用数量：\n\n# 注意最后一列\ncreateTable(res, show.n = TRUE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ___________________________________________________________________________ \n##                             1995        2000        2005     p.overall  N   \n##                             N=431       N=786      N=1077                   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   2294 \n## Sex:                                                           0.506   2294 \n##     Male                 206 (47.8%) 390 (49.6%) 505 (46.9%)                \n##     Female               225 (52.2%) 396 (50.4%) 572 (53.1%)                \n## History of hyperchol.:                                        &lt;0.001   2273 \n##     Yes                  97 (22.5%)  256 (33.2%) 356 (33.2%)                \n##     No                   334 (77.5%) 515 (66.8%) 715 (66.8%)                \n## History of hypertension:                                      &lt;0.001   2286 \n##     Yes                  111 (25.8%) 233 (29.6%) 379 (35.5%)                \n##     No                   320 (74.2%) 553 (70.4%) 690 (64.5%)                \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nshow.descr = FALSE表示不展示描述统计部分，只显示P值：\n\ncreateTable(res, show.descr = FALSE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## __________________________________ \n##                          p.overall \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        0.078   \n## Sex:                               \n##     Male                   0.506   \n##     Female                         \n## History of hyperchol.:             \n##     Yes                   &lt;0.001   \n##     No                             \n## History of hypertension:           \n##     Yes                   &lt;0.001   \n##     No                             \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nshow.all = TRUE表示显示所有数量：（注意第一列）\n\ncreateTable(res, show.all = TRUE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ___________________________________________________________________________________ \n##                             [ALL]        1995        2000        2005     p.overall \n##                             N=2294       N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.7 (11.0)  54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex:                                                                        0.506   \n##     Male                 1101 (48.0%) 206 (47.8%) 390 (49.6%) 505 (46.9%)           \n##     Female               1193 (52.0%) 225 (52.2%) 396 (50.4%) 572 (53.1%)           \n## History of hyperchol.:                                                     &lt;0.001   \n##     Yes                  709 (31.2%)  97 (22.5%)  256 (33.2%) 356 (33.2%)           \n##     No                   1564 (68.8%) 334 (77.5%) 515 (66.8%) 715 (66.8%)           \n## History of hypertension:                                                   &lt;0.001   \n##     Yes                  723 (31.6%)  111 (25.8%) 233 (29.6%) 379 (35.5%)           \n##     No                   1563 (68.4%) 320 (74.2%) 553 (70.4%) 690 (64.5%)           \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nshow.p.overall = FALSE表示不显示P值：\n\ncreateTable(res, show.p.overall = FALSE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ____________________________________________________________ \n##                             1995        2000        2005     \n##                             N=431       N=786      N=1077    \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.1 (11.7) 54.3 (11.2) 55.3 (10.6) \n## Sex:                                                         \n##     Male                 206 (47.8%) 390 (49.6%) 505 (46.9%) \n##     Female               225 (52.2%) 396 (50.4%) 572 (53.1%) \n## History of hyperchol.:                                       \n##     Yes                  97 (22.5%)  256 (33.2%) 356 (33.2%) \n##     No                   334 (77.5%) 515 (66.8%) 715 (66.8%) \n## History of hypertension:                                     \n##     Yes                  111 (25.8%) 233 (29.6%) 379 (35.5%) \n##     No                   320 (74.2%) 553 (70.4%) 690 (64.5%) \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n如果因变量有2个以上的类别，可通过show.p.trend = TRUE展示p-for-trend，符合正态分布通过pearson计算，不符合通过spearman计算：\n\n# year这个变量是有3个类别的\ncreateTable(res, show.p.trend = TRUE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ______________________________________________________________________________ \n##                             1995        2000        2005     p.overall p.trend \n##                             N=431       N=786      N=1077                      \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078    0.032  \n## Sex:                                                           0.506    0.544  \n##     Male                 206 (47.8%) 390 (49.6%) 505 (46.9%)                   \n##     Female               225 (52.2%) 396 (50.4%) 572 (53.1%)                   \n## History of hyperchol.:                                        &lt;0.001   &lt;0.001  \n##     Yes                  97 (22.5%)  256 (33.2%) 356 (33.2%)                   \n##     No                   334 (77.5%) 515 (66.8%) 715 (66.8%)                   \n## History of hypertension:                                      &lt;0.001   &lt;0.001  \n##     Yes                  111 (25.8%) 233 (29.6%) 379 (35.5%)                   \n##     No                   320 (74.2%) 553 (70.4%) 690 (64.5%)                   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nshow.p.mul：分组变量多于两组可以进行两两比较，符合正态分布用Turkey，不符合用Benjamini & Hochberg\n\ncreateTable(res, show.p.mul = TRUE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ___________________________________________________________________________________________________________________ \n##                             1995        2000        2005     p.overall p.1995 vs 2000 p.1995 vs 2005 p.2000 vs 2005 \n##                             N=431       N=786      N=1077                                                           \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078       0.930          0.143          0.161      \n## Sex:                                                           0.506       0.794          0.794          0.792      \n##     Male                 206 (47.8%) 390 (49.6%) 505 (46.9%)                                                        \n##     Female               225 (52.2%) 396 (50.4%) 572 (53.1%)                                                        \n## History of hyperchol.:                                        &lt;0.001       &lt;0.001         &lt;0.001         1.000      \n##     Yes                  97 (22.5%)  256 (33.2%) 356 (33.2%)                                                        \n##     No                   334 (77.5%) 515 (66.8%) 715 (66.8%)                                                        \n## History of hypertension:                                      &lt;0.001       0.169          0.001          0.015      \n##     Yes                  111 (25.8%) 233 (29.6%) 379 (35.5%)                                                        \n##     No                   320 (74.2%) 553 (70.4%) 690 (64.5%)                                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n如果因变量是2分类或者是生存数据，show.ratio = TRUE可以展示Odds Ratios或者Hazard Ratios：\n\n# 展示OR和p.ratio\ncreateTable(update(res, subset = year != 1995), show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'year'---------\n## \n## ___________________________________________________________________________________ \n##                             2000        2005            OR        p.ratio p.overall \n##                             N=786      N=1077                                       \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.3 (11.2) 55.3 (10.6) 1.01 [1.00;1.02]  0.064    0.066   \n## Sex:                                                                        0.264   \n##     Male                 390 (49.6%) 505 (46.9%)       Ref.        Ref.             \n##     Female               396 (50.4%) 572 (53.1%) 1.12 [0.93;1.34]  0.245            \n## History of hyperchol.:                                                      1.000   \n##     Yes                  256 (33.2%) 356 (33.2%)       Ref.        Ref.             \n##     No                   515 (66.8%) 715 (66.8%) 1.00 [0.82;1.22]  0.988            \n## History of hypertension:                                                    0.010   \n##     Yes                  233 (29.6%) 379 (35.5%)       Ref.        Ref.             \n##     No                   553 (70.4%) 690 (64.5%) 0.77 [0.63;0.93]  0.008            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n生存数据展示HR值：\n\ncreateTable(compareGroups(tmain ~ year + age + sex, data = regicor),\n            show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Time to CV event or censoring'---------\n## \n## _____________________________________________________________________________ \n##                     No event      Event           HR        p.ratio p.overall \n##                      N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Recruitment year:                                                     0.157   \n##     1995          388 (18.7%)  10 (10.9%)        Ref.        Ref.             \n##     2000          706 (34.1%)  35 (38.0%)  1.95 [0.96;3.93]  0.063            \n##     2005          977 (47.2%)  47 (51.1%)  1.82 [0.92;3.59]  0.087            \n## Age               54.6 (11.1)  57.5 (11.0) 1.02 [1.00;1.04]  0.021    0.021   \n## Sex:                                                                  0.696   \n##     Male          996 (48.1%)  46 (50.0%)        Ref.        Ref.             \n##     Female        1075 (51.9%) 46 (50.0%)  0.92 [0.61;1.39]  0.696            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\ndigits.ratio控制小数点位数：\n\ncreateTable(compareGroups(tmain ~ year + age + sex, data = regicor),\n            show.ratio = TRUE,\n            digits.ratio = 3)\n## \n## --------Summary descriptives table by 'Time to CV event or censoring'---------\n## \n## ________________________________________________________________________________ \n##                     No event      Event            HR          p.ratio p.overall \n##                      N=2071       N=92                                           \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Recruitment year:                                                        0.157   \n##     1995          388 (18.7%)  10 (10.9%)         Ref.          Ref.             \n##     2000          706 (34.1%)  35 (38.0%)  1.946 [0.964;3.930]  0.063            \n##     2005          977 (47.2%)  47 (51.1%)  1.816 [0.918;3.593]  0.087            \n## Age               54.6 (11.1)  57.5 (11.0) 1.022 [1.003;1.041]  0.021    0.021   \n## Sex:                                                                     0.696   \n##     Male          996 (48.1%)  46 (50.0%)         Ref.          Ref.             \n##     Female        1075 (51.9%) 46 (50.0%)  0.922 [0.613;1.387]  0.696            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n在print或者导出表格时，header.labels可以修改某些关键列的名称：\n\ntab &lt;- createTable(compareGroups(tmain ~ year + age + sex, data = regicor),\n                   show.all = TRUE)\nprint(tab, header.labels = c(p.overall = \"p-value\", all = \"All\"))\n## \n## --------Summary descriptives table by 'Time to CV event or censoring'---------\n## \n## _______________________________________________________________ \n##                       All        No event      Event    p-value \n##                      N=2163       N=2071       N=92             \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Recruitment year:                                        0.157  \n##     1995          398 (18.4%)  388 (18.7%)  10 (10.9%)          \n##     2000          741 (34.3%)  706 (34.1%)  35 (38.0%)          \n##     2005          1024 (47.3%) 977 (47.2%)  47 (51.1%)          \n## Age               54.7 (11.1)  54.6 (11.1)  57.5 (11.0)  0.021  \n## Sex:                                                     0.696  \n##     Male          1042 (48.2%) 996 (48.1%)  46 (50.0%)          \n##     Female        1121 (51.8%) 1075 (51.9%) 46 (50.0%)          \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n支持按照行合并表格：\n\nrestab1 &lt;- createTable(compareGroups(year ~ age + sex, data = regicor))\nrestab2 &lt;- createTable(compareGroups(year ~ bmi + smoker, data = regicor))\nrbind(`Non-modifiable risk factors` = restab1, `Modifiable risk factors` = restab2)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ____________________________________________________________________________ \n##                                   1995        2000        2005     p.overall \n##                                   N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Non-modifiable risk factors:\n##     Age                        54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n##     Sex:                                                             0.506   \n##         Male                   206 (47.8%) 390 (49.6%) 505 (46.9%)           \n##         Female                 225 (52.2%) 396 (50.4%) 572 (53.1%)           \n## Modifiable risk factors:\n##     Body mass index            27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  &lt;0.001   \n##     Smoking status:                                                 &lt;0.001   \n##         Never smoker           234 (56.4%) 414 (54.6%) 553 (52.2%)           \n##         Current or former &lt; 1y 109 (26.3%) 267 (35.2%) 217 (20.5%)           \n##         Former &gt;= 1y           72 (17.3%)  77 (10.2%)  290 (27.4%)           \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n按照列合并表格，也就是分层表格：\n\nres &lt;- compareGroups(year ~ age + smoker + bmi + histhtn, data = regicor)\nalltab &lt;- createTable(res, show.p.overall = FALSE)\nfemaletab &lt;- createTable(update(res, subset = sex == \"Female\"), show.p.overall = FALSE)\nmaletab &lt;- createTable(update(res, subset = sex == \"Male\"), show.p.overall = FALSE)\n\ncbind(ALL = alltab, FEMALE = femaletab, MALE = maletab)\n## \n## --------Summary descriptives table ---------\n## \n## ________________________________________________________________________________________________________________________________________\n##                                            ALL                                FEMALE                                MALE                \n##                            ___________________________________  ___________________________________  ___________________________________\n##                               1995        2000        2005         1995        2000        2005         1995        2000        2005     \n##                               N=431       N=786      N=1077        N=225       N=396       N=572        N=206       N=390       N=505    \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n## Age                        54.1 (11.7) 54.3 (11.2) 55.3 (10.6)  54.1 (11.7) 54.4 (11.2) 55.2 (10.6)  54.1 (11.8) 54.3 (11.2) 55.4 (10.7) \n## Smoking status:                                                                                                                          \n##     Never smoker           234 (56.4%) 414 (54.6%) 553 (52.2%)  182 (83.1%) 302 (79.3%) 416 (74.0%)  52 (26.5%)  112 (29.7%) 137 (27.5%) \n##     Current or former &lt; 1y 109 (26.3%) 267 (35.2%) 217 (20.5%)  32 (14.6%)  68 (17.8%)  83 (14.8%)   77 (39.3%)  199 (52.8%) 134 (26.9%) \n##     Former &gt;= 1y           72 (17.3%)  77 (10.2%)  290 (27.4%)   5 (2.28%)  11 (2.89%)  63 (11.2%)   67 (34.2%)  66 (17.5%)  227 (45.6%) \n## Body mass index            27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  27.2 (4.57) 28.0 (5.25) 27.3 (5.39)  26.9 (3.64) 28.2 (3.89) 27.9 (3.58) \n## History of hypertension:                                                                                                                 \n##     Yes                    111 (25.8%) 233 (29.6%) 379 (35.5%)  61 (27.1%)  123 (31.1%) 198 (34.8%)  50 (24.3%)  110 (28.2%) 181 (36.2%) \n##     No                     320 (74.2%) 553 (70.4%) 690 (64.5%)  164 (72.9%) 273 (68.9%) 371 (65.2%)  156 (75.7%) 280 (71.8%) 319 (63.8%) \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n4.0版本以后提供了strataTable用于快速创建分层表格：\n\nres &lt;- compareGroups(year ~ age + bmi + smoker + histchol + histhtn, regicor)\nrestab &lt;- createTable(res, hide.no = \"no\")\n\nstrataTable(restab, \"sex\")\n## \n## --------Summary descriptives table ---------\n## \n## _______________________________________________________________________________________________________________________\n##                                                Male                                          Female                    \n##                            _____________________________________________  _____________________________________________\n##                               1995        2000        2005     p.overall     1995        2000        2005     p.overall \n##                               N=206       N=390       N=505                  N=225       N=396       N=572              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n## Age                        54.1 (11.8) 54.3 (11.2) 55.4 (10.7)   0.212    54.1 (11.7) 54.4 (11.2) 55.2 (10.6)   0.351   \n## Body mass index            26.9 (3.64) 28.2 (3.89) 27.9 (3.58)  &lt;0.001    27.2 (4.57) 28.0 (5.25) 27.3 (5.39)   0.084   \n## Smoking status:                                                 &lt;0.001                                         &lt;0.001   \n##     Never smoker           52 (26.5%)  112 (29.7%) 137 (27.5%)            182 (83.1%) 302 (79.3%) 416 (74.0%)           \n##     Current or former &lt; 1y 77 (39.3%)  199 (52.8%) 134 (26.9%)            32 (14.6%)  68 (17.8%)  83 (14.8%)            \n##     Former &gt;= 1y           67 (34.2%)  66 (17.5%)  227 (45.6%)             5 (2.28%)  11 (2.89%)  63 (11.2%)            \n## History of hyperchol.      48 (23.3%)  138 (35.8%) 167 (33.2%)   0.007    49 (21.8%)  118 (30.6%) 189 (33.3%)   0.006   \n## History of hypertension    50 (24.3%)  110 (28.2%) 181 (36.2%)   0.002    61 (27.1%)  123 (31.1%) 198 (34.8%)   0.097   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n以上介绍的创建表格基本上是两步，首先compareGroups，然后createTable，为了方便，作者提供了一个descrTable，直接完成以上两步：\n\ndescrTable(year ~ age + bmi + smoker + histchol + histhtn, data = regicor)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ________________________________________________________________________ \n##                               1995        2000        2005     p.overall \n##                               N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Body mass index            27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  &lt;0.001   \n## Smoking status:                                                 &lt;0.001   \n##     Never smoker           234 (56.4%) 414 (54.6%) 553 (52.2%)           \n##     Current or former &lt; 1y 109 (26.3%) 267 (35.2%) 217 (20.5%)           \n##     Former &gt;= 1y           72 (17.3%)  77 (10.2%)  290 (27.4%)           \n## History of hyperchol.:                                          &lt;0.001   \n##     Yes                    97 (22.5%)  256 (33.2%) 356 (33.2%)           \n##     No                     334 (77.5%) 515 (66.8%) 715 (66.8%)           \n## History of hypertension:                                        &lt;0.001   \n##     Yes                    111 (25.8%) 233 (29.6%) 379 (35.5%)           \n##     No                     320 (74.2%) 553 (70.4%) 690 (64.5%)           \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n上面介绍的所有参数descrTable都是支持的。\n支持的格式非常丰富：\n\nexport2csv(restab, file='table1.csv'), 导出为CSV\nexport2html(restab, file='table1.html'), 导出为HTML\nexport2latex(restab, file='table1.tex'), 导出为LaTeX\nexport2pdf(restab, file='table1.pdf'), 导出为PDF\nexport2md(restab, file='table1.md'), 导出为Markdown\nexport2word(restab, file='table1.docx'), 导出为Word\nexport2xls(restab, file='table1.xlsx'), 导出为Excel\n\n导出时还支持各种格式调整，比如添加阴影等。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>三线表和统计绘图</span>"
    ]
  },
  {
    "objectID": "统计图表.html#统计图",
    "href": "统计图表.html#统计图",
    "title": "7  三线表和统计绘图",
    "section": "7.2 统计图",
    "text": "7.2 统计图\n统计绘图是R语言的拿手好戏，下面将会给大家介绍常见的统计图表的绘制。以下图形的各种细节都可以根据自己的需要进行个性化的修改。\n例10-4。条形图。\n\ndata10_4 &lt;- data.frame(`灌注方法`=c(\"方法1\",\"方法2\",\"方法3\"),\n                       rate = c(17.9,20.8,33.3))\ndata10_4\n##   灌注方法 rate\n## 1    方法1 17.9\n## 2    方法2 20.8\n## 3    方法3 33.3\n\n\nlibrary(ggplot2)\nlibrary(ggprism)\n\nggplot(data10_4, aes(`灌注方法`,rate))+\n  geom_bar(stat = \"identity\",fill=\"white\",color=\"black\",width = 0.4)+\n  ylab(\"再发率（%）\")+\n  scale_y_continuous(expand = c(0,0))+\n  theme_classic()+\n  theme(axis.title = element_text(color = \"black\",size = 15))\n\n\n\n\n\n\n\n\n例10-5。分组条形图。\n\nlibrary(haven)\ndata10_5 &lt;- haven::read_sav(\"datasets/例10-05.sav\")\ndata10_5 &lt;- as_factor(data10_5)\ndata10_5\n## # A tibble: 4 × 3\n##   year  agent  rate\n##   &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;\n## 1 2005  男       75\n## 2 2005  女       60\n## 3 2010  男       56\n## 4 2010  女       53\n\n\nlibrary(ggplot2)\n\nggplot(data10_5, aes(agent,rate))+\n  geom_bar(stat = \"identity\",aes(fill=year),position = \"dodge\")+\n  labs(x=\"性别\",y=\"患龋率（%）\",fill=\"年份\")+\n  scale_y_continuous(expand = c(0,0))+\n  theme_classic()+\n  theme(axis.title = element_text(color = \"black\",size = 15))\n\n\n\n\n\n\n\n\n例10-6。饼图。\n\ndata10_6 &lt;- data.frame(`失败原因`=c(\"无菌性松动\",\"感染\",\"假体周围骨折\",\"假体不稳定\",\"其他\"),\n                       `数量`=c(226,52,22,17,10))\n\nlibrary(dplyr)\n\ndata10_6 &lt;- data10_6 %&gt;% \n  arrange(desc(`数量`)) %&gt;% \n  mutate(`失败原因`=factor(`失败原因`,levels=c(\"无菌性松动\",\"感染\",\n                                       \"假体周围骨折\",\"假体不稳定\",\"其他\"))) %&gt;% \n  mutate(prop = round(`数量` / sum(`数量`),2),\n         prop = scales::percent(prop))\n\ndata10_6\n##       失败原因 数量 prop\n## 1   无菌性松动  226  69%\n## 2         感染   52  16%\n## 3 假体周围骨折   22   7%\n## 4   假体不稳定   17   5%\n## 5         其他   10   3%\n\n由于ggplot2的大佬们普遍认为饼图是一种很差劲的图形，所以ggplot2对饼图的支持并不好。\n\n# 默认的大概是这种程度\nggplot(data10_6, aes(x=\"\",y=`数量`,fill=`失败原因`))+\n  geom_bar(stat = \"identity\",width = 1,color=\"white\")+\n  geom_text(aes(label = prop),position = position_stack(vjust = 0.5))+\n  coord_polar(\"y\", start=0)+\n  theme_void()\n\n\n\n\n\n\n\n\n例10-7。百分比条形图。\n\nlibrary(haven)\ndata10_7 &lt;- haven::read_sav(\"datasets/例10-07.sav\",encoding = \"GBK\")\ndata10_7 &lt;- as_factor(data10_7)\ndata10_7\n## # A tibble: 12 × 3\n##    year   reason   percent\n##    &lt;fct&gt;  &lt;fct&gt;      &lt;dbl&gt;\n##  1 1996年 肺炎        23.4\n##  2 1996年 早产        14.2\n##  3 1996年 出生窒息    14.1\n##  4 1996年 腹泻         5.6\n##  5 1996年 意外窒息     4.1\n##  6 1996年 其它        38.6\n##  7 2000年 肺炎        19.5\n##  8 2000年 早产        17  \n##  9 2000年 出生窒息    15.9\n## 10 2000年 腹泻         4.9\n## 11 2000年 意外窒息     3.7\n## 12 2000年 其它        39\n\n\nlibrary(scales)\n\nggplot(data10_7, aes(year, percent, fill=reason))+ \n  geom_bar(stat = \"identity\",position = \"stack\",width = 0.5,color=\"black\")+\n  labs(fill=\"\",x=\"\",y=\"\")+\n  scale_y_continuous(labels = percent_format(scale = 1))+\n  guides(fill=guide_legend(reverse = T))+\n  theme_bw()+\n  theme(axis.text = element_text(size = 18, colour = \"black\"),\n        legend.text = element_text(size = 18, colour = \"black\"),\n        legend.position = \"bottom\")+\n  coord_flip()\n\n\n\n\n\n\n\n#ggsave(\"xxxx.png\",width=10,height=5,dpi=300)\n\n例10-8。折线图。\n\nlibrary(haven)\ndata10_8 &lt;- haven::read_sav(\"datasets/例10-08.sav\",encoding = \"GBK\")\ndata10_8 &lt;- as_factor(data10_8)\ndata10_8\n## # A tibble: 10 × 3\n##    year  agent counts\n##    &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n##  1 2006  男      5500\n##  2 2006  女      2500\n##  3 2007  男      6000\n##  4 2007  女      3000\n##  5 2008  男      9000\n##  6 2008  女      3500\n##  7 2009  男     11000\n##  8 2009  女      5000\n##  9 2010  男     11000\n## 10 2010  女      5000\n\n\nggplot(data10_8, aes(year,counts))+\n  geom_line(aes(group = agent,linetype=agent))+\n  labs(x=\"年份\",y=\"布氏菌病发病人数\",linetype=\"性别\")+\n  theme_classic()+\n  theme(axis.text = element_text(size = 18, colour = \"black\"),\n        axis.title = element_text(color = \"black\",size = 18),\n        legend.text = element_text(size = 18, colour = \"black\"),\n        legend.title = element_text(size = 18, colour = \"black\"))\n\n\n\n\n\n\n\n\n例10-9。点线图。\n\nlibrary(haven)\ndata10_9 &lt;- haven::read_sav(\"datasets/例10-09.sav\",encoding = \"GBK\")\ndata10_9 &lt;- as_factor(data10_9)\ndata10_9\n## # A tibble: 10 × 3\n##     year 病型   发病率\n##    &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;\n##  1  1997 艾滋病 0.0069\n##  2  1998 艾滋病 0.0177\n##  3  1999 艾滋病 0.0187\n##  4  2000 艾滋病 0.0312\n##  5  2001 艾滋病 0.0468\n##  6  1997 梅毒   3.76  \n##  7  1998 梅毒   4.58  \n##  8  1999 梅毒   5.72  \n##  9  2000 梅毒   6.09  \n## 10  2001 梅毒   6.27\n\n\np1 &lt;- ggplot(data10_9, aes(year,`发病率`))+\n  geom_line(aes(group = `病型`,linetype=`病型`))+\n  geom_point(aes(group = `病型`,shape=`病型`),size=4)+\n  labs(x=\"年份\",y=\"发病率（1/10万）\")+\n  theme_classic()+\n  theme(axis.text = element_text(size = 14, colour = \"black\"),\n        axis.title = element_text(color = \"black\",size = 14),\n        legend.text = element_text(size = 14, colour = \"black\"),\n        legend.title = element_text(size = 14, colour = \"black\"))\n  \np2 &lt;- ggplot(data10_9, aes(year,log10(`发病率`)))+ # 不知道课本取的log几\n  geom_line(aes(group = `病型`,linetype=`病型`))+\n  geom_point(aes(group = `病型`,shape=`病型`),size=4)+\n  labs(x=\"年份\",y=\"发病率（1/10万）\")+\n  theme_classic()+\n  theme(axis.text = element_text(size = 14, colour = \"black\"),\n        axis.title = element_text(color = \"black\",size = 14),\n        legend.text = element_text(size = 14, colour = \"black\"),\n        legend.title = element_text(size = 14, colour = \"black\"))\n\nlibrary(patchwork)\np1+p2+plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n例10-10。直方图。\n\nlibrary(haven)\ndata10_10 &lt;- haven::read_sav(\"datasets/例10-10.sav\",encoding = \"GBK\")\ndata10_10 &lt;- as_factor(data10_10)\ndata10_10\n## # A tibble: 16 × 2\n##    age   count\n##    &lt;fct&gt; &lt;dbl&gt;\n##  1 0-        7\n##  2 1-       15\n##  3 2-        8\n##  4 3-       11\n##  5 4-       14\n##  6 5-       14\n##  7 6-        8\n##  8 7-        6\n##  9 8-        3\n## 10 9-        2\n## 11 10-       8\n## 12 15-       3\n## 13 20-       1\n## 14 25-       2\n## 15 30-       1\n## 16 35-40     1\n\n下面这个其实假的直方图（虽然和课本中的看起来差不多），因为没给原始数据，给的是计数好的数据，所以是用条形图伪装的直方图：\n\nggplot(data10_10, aes(age,count))+\n  geom_bar(stat = \"identity\",fill=\"white\",color=\"black\",\n           width = 1,position = position_dodge(width = 1))+\n  labs(x=\"年龄（岁）\",y=\"每岁病例数\")+\n  scale_y_continuous(expand = c(0,0))+\n  theme_classic()+\n  theme(axis.title = element_text(color = \"black\",size = 15))\n\n\n\n\n\n\n\n\n例10-11。地图。没给数据，直接自己编一个。\n首先下载中国地图。中国地图下载地址：地图选择器\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(dplyr)\n\nchina_map &lt;- st_read(\"datasets/中华人民共和国.json\")\n## Reading layer `中华人民共和国' from data source \n##   `F:\\R_books\\medstat_quartobook\\datasets\\中华人民共和国.json' \n##   using driver `GeoJSON'\n## Simple feature collection with 35 features and 10 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 73.50235 ymin: 3.397162 xmax: 135.0957 ymax: 53.56327\n## Geodetic CRS:  WGS 84\n\n然后给每个省编点数据：\n\nset.seed(123)\nchina_map &lt;- china_map %&gt;% \n  mutate(name_short=substr(name,1,2),\n         number = sample(10:100,35,replace=F),\n         group = sample(paste0(\"group\",1:5),35,replace=T))\n\nchina_map$name_short[c(5,8)] &lt;- c(\"内蒙古\",\"黑龙江\")\n\n画图即可,ggplot2画地图非常厉害，下面这个只是非常基础的，可以进行非常多的修改。\n\nggplot(data = china_map) + \n  geom_sf(aes(fill=group)) + \n  geom_sf_text(aes(label = name_short),nudge_y = 0,size=2)+\n  geom_sf_text(aes(label = number),nudge_y = -1,size=2)+\n  theme_minimal()\n## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\n## give correct results for longitude/latitude data\n## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\n## give correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n例10-12。箱线图。没给数据，直接用R语言自带的iris数据演示一下。\n\nlibrary(ggplot2)\n\nggplot(iris, aes(Species,Sepal.Length))+\n  stat_boxplot(geom = \"errorbar\",width = 0.2)+\n  geom_boxplot()\n\n\n\n\n\n\n\n\n例10-13。茎叶图。\n\nlibrary(haven)\ndata10_13 &lt;- haven::read_sav(\"datasets/例10-13.sav\",encoding = \"GBK\")\ndata10_13 &lt;- as_factor(data10_13)\ndata10_13\n## # A tibble: 138 × 1\n##      rbc\n##    &lt;dbl&gt;\n##  1  3.96\n##  2  3.77\n##  3  4.63\n##  4  4.56\n##  5  4.66\n##  6  4.61\n##  7  4.98\n##  8  5.28\n##  9  5.11\n## 10  4.92\n## # ℹ 128 more rows\n\nR自带函数就可以画（但是这个图很少用）：\n\nstem(data10_13$rbc,scale = 1)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##   30 | 7\n##   31 | \n##   32 | 17\n##   33 | 9\n##   34 | 2\n##   35 | 299\n##   36 | 0124467789\n##   37 | 12266679\n##   38 | 3399\n##   39 | 166667778\n##   40 | 11122223344\n##   41 | 2234666789\n##   42 | 000011133345566666667888999\n##   43 | 01223466666\n##   44 | 12279\n##   45 | 4566677\n##   46 | 111366899\n##   47 | 15666\n##   48 | 139\n##   49 | 258\n##   50 | 13\n##   51 | 12\n##   52 | 348\n##   53 | \n##   54 | 6\n\n例10-14。误差条图。\n\nlibrary(haven)\ndata10_14 &lt;- haven::read_sav(\"datasets/例10-14.sav\",encoding = \"GBK\")\ndata10_14 &lt;- as_factor(data10_14)\ndata10_14\n## # A tibble: 120 × 2\n##    group     dmdz\n##    &lt;fct&gt;    &lt;dbl&gt;\n##  1 安慰剂组  3.53\n##  2 安慰剂组  4.59\n##  3 安慰剂组  4.34\n##  4 安慰剂组  2.66\n##  5 安慰剂组  3.59\n##  6 安慰剂组  3.13\n##  7 安慰剂组  2.64\n##  8 安慰剂组  2.56\n##  9 安慰剂组  3.5 \n## 10 安慰剂组  3.25\n## # ℹ 110 more rows\n\n先计算每个组的均值和可信区间：\n\n95%可信区间的计算：均值±1.96*标准误，见课本第一章第三节：总体均数的估计\n\n\nlibrary(dplyr)\n\ndata10_14_1 &lt;- data10_14 %&gt;% \n  group_by(group) %&gt;% \n  summarise(mm = mean(dmdz),\n            lower = mm - 1.96*(sd(dmdz)/sqrt(30)),\n            upper = mm + 1.96*(sd(dmdz)/sqrt(30)))\ndata10_14_1\n## # A tibble: 4 × 4\n##   group       mm lower upper\n##   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 安慰剂组  3.43  3.17  3.69\n## 2 新药2.4   2.72  2.49  2.94\n## 3 新药4.8   2.70  2.52  2.88\n## 4 新药7.2   1.97  1.70  2.23\n\n\nggplot(data10_14_1)+\n  geom_point(aes(group,mm),size=4,shape=0)+\n  geom_errorbar(aes(x=group,ymin=lower,ymax=upper),\n                width=0.1)+\n  theme_classic()+\n  labs(x=\"分组\",y=\"95%CI\")+\n  theme_classic()+\n  theme(axis.title = element_text(color = \"black\",size = 15))",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>三线表和统计绘图</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html",
    "href": "1011-samplesize.html",
    "title": "8  样本量计算",
    "section": "",
    "text": "8.1 t检验的样本量计算\n对于t检验，可以使用pwr.t.test(n= , d= , sig.level= , power= , type= , alternative= )计算样本量，其中： - n：样本量 - d：效应值，即标准化的均值之差，d = (μ1 - μ2) / σ，也就是（组1均值 - 组2均值）/ 标准差 - sig.level：显著性水平，默认值0.05 - power：功效 - type：检验类型：两样本t检验（two.sample），单样本t检验（one.sample），配对t检验（paired），默认两样本t检验 - alternative：双侧检验还是单侧检验，双侧（two.sided），单侧（less或者greater），默认双侧检验",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#t检验的样本量计算",
    "href": "1011-samplesize.html#t检验的样本量计算",
    "title": "8  样本量计算",
    "section": "",
    "text": "8.1.1 单样本t检验（样本均数和已知总体均数比较）\n使用课本例36-3的例子。\n用某药治疗矽肺患者，估计可增加尿矽排出量，其标准差为25mg/L，若要求以α=0.05，β=0.1的概率，能辨别出尿矽排出量平均增加10mg/L，问需要多少矽肺患者做实验？\n\nlibrary(pwr)\n\npwr.t.test(d = 10/25, \n           sig.level = 0.05,\n           power = 1-0.1,\n           type = \"one.sample\",\n           alternative = \"greater\"\n           )\n## \n##      One-sample t test power calculation \n## \n##               n = 54.90553\n##               d = 0.4\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = greater\n\nn = 54.90553，结果和课本一模一样！是不是非常简单？\n单样本t检验也可以使用R自带的函数进行计算：\n\npower.t.test(delta = 10,\n             sd = 25,\n             sig.level = 0.05,\n             power = 1-0.1,\n             type = \"one.sample\",\n             alternative = \"one.sided\"\n             )\n## \n##      One-sample t test power calculation \n## \n##               n = 54.90553\n##           delta = 10\n##              sd = 25\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = one.sided\n\n结果都是一样的~\n\n\n8.1.2 两样本t检验（两样本均数比较）\n使用课本例36-4的例子\n在做两种处理动物冠状静脉窦的血流量实验时，比较A处理动物和B处理动物的平均血流量增加，设两处理的标准差相等。若要求以α=0.05，β=0.1的概率，达到能辨别出两者增加的差别是其标准差的60%，需要多少实验动物？\n感觉和小学做应用题差不多… 两者增加的差别是其标准差的60%，也就是 (μ1 - μ2) / σ = 0.6。\n\nlibrary(pwr)\n\npwr.t.test(d = 0.6,\n           sig.level = 0.05,\n           power = 1 - 0.1,\n           type = \"two.sample\",\n           alternative = \"two.sided\"\n           )\n## \n##      Two-sample t test power calculation \n## \n##               n = 59.35155\n##               d = 0.6\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\n\nn = 59.35155，和课本结果一模一样，每组需要大约60例！\n两样本t检验也可以使用R自带的函数power.t.test()进行计算，但是例题中的这种情况刚好没有给出具体的两组间差值和标准差，所以就不能用了。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#多样本均数比较",
    "href": "1011-samplesize.html#多样本均数比较",
    "title": "8  样本量计算",
    "section": "8.2 多样本均数比较",
    "text": "8.2 多样本均数比较\n使用课本例36-5的例子。\n拟用4种方法治疗贫血患者，估计治疗后血红蛋白增加的均数分别为18,13,16,10，标准差分别为10,9,9,8，设α=0.05，β=0.1，若要得出有差别的结论，每组需要多少例？\n这是一个完全随机设计多样本比较的方差分析的例子，相信大家都能看出来！\n但是，在R里面计算种类型的样本量非常困难，原因在于效应量effect size很难计算出来，最终结果也和课本上面的公式计算出来的样本量不一样，所以我推荐用PASS软件，点点点即可！\n这种情况使用函数pwr.anova.test(k= , n= , f= , sig.level= , power= )计算，其中 f是效应量effect size，计算方法如下：\n\n\n\n\n\n\n\n\n\nk是组数，其余的和t检验的相同。\n首先我们要计算f值，但是根据这个公式，很明显是计算不出来的！\n如果使用R自带函数power.anova.test(groups = NULL, n = NULL, between.var = NULL, within.var = NULL,sig.level = 0.05, power = NULL)函数计算，因为无法计算within.var，所以也是行不通的。\n还是乖乖用PASS吧…\n如果有大佬知道怎么计算，欢迎留言告知~",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#样本率和已知总体率的比较",
    "href": "1011-samplesize.html#样本率和已知总体率的比较",
    "title": "8  样本量计算",
    "section": "8.3 样本率和已知总体率的比较",
    "text": "8.3 样本率和已知总体率的比较\n使用课本例36-6的例子。\n已知常规方法治疗某种病的有效率是80%，现试验一种新的额治疗方法，预计有效率是90%，设α=0.05，β=0.1，问需要多少病例才能发现两种方法的有效率有10%的差别？\n\n# 首先计算h值（effect size），pwr包自带了函数，根据两个率可计算，\n# h的计算使用的是这个公式：2*asin(sqrt(0.9))-2*asin(sqrt(0.8))\nES.h(0.9,0.8)\n## [1] 0.2837941\n\n# 然后进行样本量计算\npwr.p.test(h = ES.h(0.9,0.8),\n           sig.level = 0.05,\n           power = 1-0.1,\n           alternative = \"greater\"\n           )\n## \n##      proportion power calculation for binomial distribution (arcsine transformation) \n## \n##               h = 0.2837941\n##               n = 106.3315\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = greater\n\n得到的结果和课本差别有点大，课本是137.1，而我们的结果是106，主要是由于计算方法不同，建议对于此类设计的样本量计算，还是直接套课本公式或者使用PASS软件。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#两独立样本率的比较",
    "href": "1011-samplesize.html#两独立样本率的比较",
    "title": "8  样本量计算",
    "section": "8.4 两独立样本率的比较",
    "text": "8.4 两独立样本率的比较\n使用课本例36-7的例子。\n初步观察甲乙两药对某病的疗效，甲药有效率为60%。乙药有效率为85%，现拟进一步做治疗实验，设α=0.05,1-β=0.9，问每组需要多少病例？\n下面演示使用pwr包计算：\n\n# 首先计算h值，pwr包自带了函数，根据两个率可计算\nES.h(0.85,0.60)\n## [1] 0.5740396\n\n# 然后进行样本量计算\npwr.2p.test(h = ES.h(0.85,0.60),\n           sig.level = 0.05,\n           power = 1-0.1,\n           alternative = \"two.sided\"\n           )\n## \n##      Difference of proportion power calculation for binomial distribution (arcsine transformation) \n## \n##               h = 0.5740396\n##               n = 63.77382\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = two.sided\n## \n## NOTE: same sample sizes\n\nn = 63.77382，和课本是一模一样的结果！\n这种情况下用R自带的``也是很好用的：\n\npower.prop.test(p1 = 0.85,\n                p2 = 0.6,\n                sig.level = 0.05,\n                power = 1-0.1,\n                alternative = \"two.sided\"\n                )\n## \n##      Two-sample comparison of proportions power calculation \n## \n##               n = 64.93465\n##              p1 = 0.85\n##              p2 = 0.6\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\n\n算出来结果是64.93465，和课本差别不大~",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#多样本率的比较",
    "href": "1011-samplesize.html#多样本率的比较",
    "title": "8  样本量计算",
    "section": "8.5 多样本率的比较",
    "text": "8.5 多样本率的比较\n使用课本例36-8的例子。\n拟观察3种方法治疗消化性溃疡的效果，初步估计甲法有效率为40%，乙法50%，丙法65%，设α=0.05，β=0.1，试估计样本量？\n很明显属于行x列表资料的卡方检验！所以我们使用pwr.chisq.test()函数进行计算样本量。\n首先我们要计算effect size：\n\n#               甲法 乙法 丙法\nprob &lt;- rbind(c(0.4, 0.5, 0.65), # 有效率\n              c(0.6, 0.5, 0.35)) # 无效率\n\n# pwr包自带的这个函数专门用于此种情况的effect size计算\nES.w2(prob/3) # 有几组就除以几，这里需要理解列联表资料的一些指标计算\n## [1] 0.2055947\n\n这样我们就得到effect size了，然后就可以计算样本量了。\n\npwr.chisq.test(w = ES.w2(prob/3), # effect size\n               df = 2, #（3-1）*（2-1）= 2\n               sig.level = 0.05,\n               power = 1-0.1\n               )\n## \n##      Chi squared power calculation \n## \n##               w = 0.2055947\n##               N = 299.3655\n##              df = 2\n##       sig.level = 0.05\n##           power = 0.9\n## \n## NOTE: N is the number of observations\n\n最终得到的结果是一共需要299例，课本是297例，基本一样~",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#直线相关分析",
    "href": "1011-samplesize.html#直线相关分析",
    "title": "8  样本量计算",
    "section": "8.6 直线相关分析",
    "text": "8.6 直线相关分析\n使用课本例36-9的例子。\n根据以往经验，血硒与发硒含量间直线相关系数为0.8，若想在α=0.05，β=0.1的水平上得到相关系数有统计学意义的结论，应调查多少人？\n\npwr.r.test(r=0.8,\n           sig.level = 0.05,\n           power = 1-0.1,\n           alternative = \"two.sided\")\n## \n##      approximate correlation power calculation (arctangh transformation) \n## \n##               n = 11.16238\n##               r = 0.8\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = two.sided\n\nn = 11.16238，结果和课本一样~\nOK，以上就是使用R语言计算样本量的例子。可以看到大部分都是可以很简单的计算出来，但是在方便快捷性上还是差PASS软件太远了，对于此类样本量计算的问题，可能PASS是更好的选择。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1012-randomgroup.html",
    "href": "1012-randomgroup.html",
    "title": "9  随机分组",
    "section": "",
    "text": "9.1 简单随机\n比如30个人，按照完全随机化的方法分为2组，一组试验组，一组对照组，每组15人。\nid &lt;- c(1:30)\ngroup &lt;- rep(c(\"试验组\",\"对照组\"),15)\nrand &lt;- sample(group, 30, replace = T)\n(res &lt;- cbind(id, group))\n##       id   group   \n##  [1,] \"1\"  \"试验组\"\n##  [2,] \"2\"  \"对照组\"\n##  [3,] \"3\"  \"试验组\"\n##  [4,] \"4\"  \"对照组\"\n##  [5,] \"5\"  \"试验组\"\n##  [6,] \"6\"  \"对照组\"\n##  [7,] \"7\"  \"试验组\"\n##  [8,] \"8\"  \"对照组\"\n##  [9,] \"9\"  \"试验组\"\n## [10,] \"10\" \"对照组\"\n## [11,] \"11\" \"试验组\"\n## [12,] \"12\" \"对照组\"\n## [13,] \"13\" \"试验组\"\n## [14,] \"14\" \"对照组\"\n## [15,] \"15\" \"试验组\"\n## [16,] \"16\" \"对照组\"\n## [17,] \"17\" \"试验组\"\n## [18,] \"18\" \"对照组\"\n## [19,] \"19\" \"试验组\"\n## [20,] \"20\" \"对照组\"\n## [21,] \"21\" \"试验组\"\n## [22,] \"22\" \"对照组\"\n## [23,] \"23\" \"试验组\"\n## [24,] \"24\" \"对照组\"\n## [25,] \"25\" \"试验组\"\n## [26,] \"26\" \"对照组\"\n## [27,] \"27\" \"试验组\"\n## [28,] \"28\" \"对照组\"\n## [29,] \"29\" \"试验组\"\n## [30,] \"30\" \"对照组\"\n我们也可以通过randomizr这个包实现，没安装的需要先安装。\ninstall.packages(\"randomizr\")\n“抛硬币”式的简单随机分组通过simple_ra()函数实现：\nlibrary(randomizr)\n\n# 100人分2组\nsim &lt;- simple_ra(100, num_arms = 2, conditions = c(\"试验组\",\"对照组\"))\n\nsim\n##   [1] 试验组 对照组 对照组 对照组 对照组 对照组 试验组 试验组 试验组 对照组\n##  [11] 对照组 试验组 对照组 对照组 试验组 试验组 试验组 对照组 对照组 对照组\n##  [21] 试验组 对照组 对照组 对照组 对照组 对照组 对照组 试验组 试验组 对照组\n##  [31] 对照组 对照组 对照组 对照组 试验组 试验组 试验组 对照组 试验组 对照组\n##  [41] 试验组 试验组 对照组 对照组 试验组 对照组 对照组 试验组 对照组 对照组\n##  [51] 对照组 对照组 对照组 对照组 试验组 试验组 对照组 对照组 对照组 对照组\n##  [61] 对照组 试验组 试验组 对照组 试验组 试验组 试验组 试验组 试验组 对照组\n##  [71] 试验组 对照组 对照组 对照组 对照组 对照组 试验组 对照组 对照组 对照组\n##  [81] 试验组 试验组 对照组 试验组 对照组 对照组 试验组 对照组 试验组 试验组\n##  [91] 试验组 对照组 试验组 试验组 对照组 对照组 试验组 试验组 对照组 试验组\n## Levels: 试验组 对照组\ntable(sim)\n## sim\n## 试验组 对照组 \n##     42     58\n但是这种分组最大的问题是组间人数不一样，通常在临床研究设计中都是1:1的，我们可以使用另一个函数解决这个问题。\ncom &lt;- complete_ra(100, num_arms = 2, conditions = c(\"试验组\",\"对照组\"))\n\ncom\n##   [1] 对照组 对照组 试验组 对照组 试验组 对照组 试验组 试验组 试验组 试验组\n##  [11] 对照组 对照组 试验组 对照组 试验组 对照组 对照组 试验组 试验组 对照组\n##  [21] 试验组 对照组 对照组 试验组 对照组 对照组 对照组 对照组 试验组 试验组\n##  [31] 试验组 对照组 试验组 试验组 对照组 对照组 对照组 对照组 对照组 对照组\n##  [41] 试验组 试验组 对照组 对照组 试验组 试验组 对照组 试验组 对照组 对照组\n##  [51] 对照组 试验组 对照组 试验组 对照组 试验组 对照组 对照组 对照组 试验组\n##  [61] 试验组 试验组 试验组 试验组 试验组 对照组 试验组 对照组 试验组 对照组\n##  [71] 对照组 试验组 对照组 对照组 试验组 试验组 对照组 试验组 试验组 试验组\n##  [81] 对照组 试验组 对照组 试验组 对照组 试验组 试验组 试验组 试验组 试验组\n##  [91] 试验组 对照组 对照组 试验组 对照组 对照组 对照组 试验组 试验组 对照组\n## Levels: 试验组 对照组\ntable(com)\n## com\n## 试验组 对照组 \n##     50     50\n完美解决组间人数不相等问题。\n网络上的大神也给出了自己编写的函数：https://shumchi.github.io/Randomization/\nsimple_random &lt;- function(size, grp = 2, T_2_C = \"1:1\"){\n    set.seed(20210412)\n    id_num &lt;- seq(1, size, 1)\n    random_seq &lt;- runif(n = size, min = 0, max = 1)\n    int_rank &lt;- rank(random_seq)\n    \n    ratio_T &lt;- as.numeric(substr(T_2_C, 1, 1))\n    ratio_C &lt;- as.numeric(substr(T_2_C, 3, 3))\n    \n    if (grp == 2) {\n        group &lt;- ifelse(int_rank &lt;= size/(ratio_T + ratio_C), \"T\", \"C\")\n    } else if (grp &gt; 3) {\n        group &lt;- cut(int_rank, breaks = grp, labels = paste(\"Group\", 1:grp))\n    }\n    \n    df &lt;- data.frame(\"ID\" = id_num, \"RandomNum\" = random_seq, \n                        \"Rank\" = int_rank, \"Group\" = group)\n    \n    #write.csv(df, \"simple randomization table.csv\", row.names = FALSE)\n    \n    return(df)\n}\n20个人随机分组：\nsimple_random(20)\n##    ID  RandomNum Rank Group\n## 1   1 0.83237492   19     C\n## 2   2 0.95522177   20     C\n## 3   3 0.59787880   10     T\n## 4   4 0.35076793    4     T\n## 5   5 0.43157421    6     T\n## 6   6 0.63326323   13     C\n## 7   7 0.78015581   17     C\n## 8   8 0.46990952    7     T\n## 9   9 0.38535395    5     T\n## 10 10 0.63361183   14     C\n## 11 11 0.73655082   15     C\n## 12 12 0.49675139    8     T\n## 13 13 0.61201021   11     C\n## 14 14 0.23511285    3     T\n## 15 15 0.52894214    9     T\n## 16 16 0.04290597    1     T\n## 17 17 0.74367251   16     C\n## 18 18 0.79647582   18     C\n## 19 19 0.62653890   12     C\n## 20 20 0.22537775    2     T\n除此之外，还有非常多的R包可以实现随机分组，包括但不限于简单随机分组/区组随机/分层随机等。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>随机分组</span>"
    ]
  },
  {
    "objectID": "1012-randomgroup.html#区组随机",
    "href": "1012-randomgroup.html#区组随机",
    "title": "9  随机分组",
    "section": "9.2 区组随机",
    "text": "9.2 区组随机\n可以用randomizr实现区组随机：\n\n# Load built-in dataset\ndata(HairEyeColor)\nHairEyeColor &lt;- data.frame(HairEyeColor)\n\n# Transform so each row is a subject\n# Columns describe subject's hair color, eye color, and gender\nhec &lt;- HairEyeColor[rep(1:nrow(HairEyeColor),\n                        times = HairEyeColor$Freq), 1:3]\n\nN &lt;- nrow(hec)\n\n# Fix the rownames\nrownames(hec) &lt;- NULL\ndim(hec)\n## [1] 592   3\nhead(hec)\n##    Hair   Eye  Sex\n## 1 Black Brown Male\n## 2 Black Brown Male\n## 3 Black Brown Male\n## 4 Black Brown Male\n## 5 Black Brown Male\n## 6 Black Brown Male\n\n根据毛发颜色进入不同的区组，然后再分为3组：\n\nZ &lt;- block_ra(blocks = hec$Hair, conditions = c(\"Control\", \"Placebo\", \"Treatment\"))\ntable(Z, hec$Hair)\n##            \n## Z           Black Brown Red Blond\n##   Control      36    95  24    42\n##   Placebo      36    96  23    42\n##   Treatment    36    95  24    43\n\n可以自由控制入组数量：\n\nsort(unique(hec$Hair))\n## [1] Black Brown Red   Blond\n## Levels: Black Brown Red Blond\nblock_m_each &lt;- rbind(c(78, 30),\n                      c(186, 100),\n                      c(51, 20),\n                      c(87,40))\n\nblock_m_each\n##      [,1] [,2]\n## [1,]   78   30\n## [2,]  186  100\n## [3,]   51   20\n## [4,]   87   40\nZ &lt;- block_ra(blocks = hec$Hair, block_m_each = block_m_each)\ntable(Z, hec$Hair)\n##    \n## Z   Black Brown Red Blond\n##   0    78   186  51    87\n##   1    30   100  20    40\n\n但是这个区组随机对于临床研究来说不是很适用，因为这里的情况需要提前准备好所有的受试者，然后在进行分组，而对于临床研究来说，受试者是一个一个来的，不是一下子全部到齐的，所以block_ra()可能更适合动物实验或者基础研究的分组。更多关于区组随机的知识，大家可以参考医咖会的这篇文章：一文详解区组随机化，包教包懂！\n临床研究的随机分组可以通过blockrand包实现，特别适合一次招募1人的临床研究！\n比如100人随机分为2组每组50人：\n\nlibrary(blockrand)\n\nset.seed(111)\nres &lt;- blockrand(n=100, num.levels = 2, levels = c(\"试验组\",\"对照组\"))\nhead(res)\n##   id block.id block.size treatment\n## 1  1        1          4    试验组\n## 2  2        1          4    对照组\n## 3  3        1          4    试验组\n## 4  4        1          4    对照组\n## 5  5        2          6    试验组\n## 6  6        2          6    试验组\n\ntable(res$treatment)\n## \n## 对照组 试验组 \n##     51     51\n\n还可以顺便帮我们生成PDF文件，方便装入信封。\n\nshowtext::showtext_auto(enable = T)\n\nplotblockrand(res,file = \"res.pdf\",\n              top = list(text=c(\"xxx临床研究\",\"受试者编号：%ID%\",\"入组：%TREAT%\"),\n                         col=c(\"black\",\"blue\",\"red\"),\n                         font=c(2,2,4)\n                         ),\n              middle=list(text=c(\"xxx临床研究\",\"受试者编号：%ID%\"),\n                          col=c(\"black\",\"blue\",\"red\"),\n                          font=c(2,2,4)\n                          ),\n              bottom=\"联系电话：123456789\",\n              cut.marks=TRUE # 裁剪标记\n              )",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>随机分组</span>"
    ]
  },
  {
    "objectID": "1012-randomgroup.html#分层随机",
    "href": "1012-randomgroup.html#分层随机",
    "title": "9  随机分组",
    "section": "9.3 分层随机",
    "text": "9.3 分层随机\n比如120个受试者分成4组，试验组1，试验组2，阳性对照组，阴性对照组，每组30人，并且根据性别进行分层（男性和女性），要求男性60例，女性60例。\n使用blockrand实现。\n\nlibrary(blockrand)\n\n# 男性60例随机分组\nset.seed(123)\nres.M &lt;- blockrand(n = 60,\n                 num.levels = 4,\n                 levels = c(\"试验组1\",\"试验组2\",\"阳性对照组\",\"阴性对照组\"),\n                 stratum = \"男性\",\n                 id.prefix = \"男\", # id前缀\n                 block.sizes = c(3), \n                 block.prefix = \"男\" # 前缀\n                 )\ntable(res.M$treatment)\n## \n##    试验组1    试验组2 阳性对照组 阴性对照组 \n##         15         15         15         15\n\n# 女性60例随机分组\nset.seed(456)\nres.F &lt;- blockrand(n = 60,\n                 num.levels = 4,\n                 levels = c(\"试验组1\",\"试验组2\",\"阳性对照组\",\"阴性对照组\"),\n                 stratum = \"女性\",\n                 id.prefix = \"女\", # id前缀\n                 block.sizes = c(3), \n                 block.prefix = \"女\" # 前缀\n                 )\ntable(res.F$treatment)\n## \n##    试验组1    试验组2 阳性对照组 阴性对照组 \n##         15         15         15         15\n\n# 结果合并即可\nres &lt;- cbind(res.M,res.F)\ndim(res)\n## [1] 60 10\nhead(res)\n##     id stratum block.id block.size  treatment   id stratum block.id block.size\n## 1 男01    男性      男1         12 阳性对照组 女01    女性      女1         12\n## 2 男02    男性      男1         12 阴性对照组 女02    女性      女1         12\n## 3 男03    男性      男1         12    试验组2 女03    女性      女1         12\n## 4 男04    男性      男1         12    试验组2 女04    女性      女1         12\n## 5 男05    男性      男1         12    试验组2 女05    女性      女1         12\n## 6 男06    男性      男1         12 阳性对照组 女06    女性      女1         12\n##    treatment\n## 1    试验组1\n## 2 阳性对照组\n## 3    试验组2\n## 4 阴性对照组\n## 5 阴性对照组\n## 6 阳性对照组\n\n写入PDF文件，方便制作信封，分组隐匿：\n\nshowtext::showtext_auto(enable = T)\n\nplotblockrand(res, file = \"res1.pdf\",\n              top=list(text=c(\"不得了临床试验\",\"受试者编号: %ID%\",\"组别: %TREAT%\"),\n                       col=c('black','blue','red'),font=c(2,2,4)),\n              middle=list(text=c(\"不得了临床试验\",\"性别: %STRAT%\",\"受试者编号: %ID%\"),\n                          col=c('black','blue','red'),font=c(1,2,3)),\n              bottom=\"联系电话：123456789\",\n              cut.marks=TRUE\n              )",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>随机分组</span>"
    ]
  },
  {
    "objectID": "1014-batchttest.html",
    "href": "1014-batchttest.html",
    "title": "10  R语言“tidy”流统计分析",
    "section": "",
    "text": "10.1 简介\n虽然说是同时进行t检验，但是这是一种通用的方法，当然也可以同时进行方差分析、正态性检验、方差齐性检验、秩和检验等等。\n前面的介绍多数是基于R语言自带的函数进行医学统计学分析，下面介绍一个比较现代化、用法也更加简单直接的方法，也就是基于rstatix这个包实现。\nrstatix提供一个简单直观的管道友好的框架，与整洁的设计理念一致，用于执行基本的统计检验，包括t检验，Wilcoxon检验，方差分析，Kruskal-Wallis和相关性分析等。每个分析的输出会自动转换成一个整洁的数据框架，以方便可视化。\n附加功能可用于重塑，重新排序，操作和可视化相关矩阵。功能还包括析因实验的分析，包括重复测量设计、析因设计、正交设计等。\n还可以计算几个效应大小指标，包括方差分析eta平方，t检验的Cohen’s-d和分类变量之间的关联的Cramer’s-v。 该软件包包含用于识别单变量和多变量异常值、评估正态性和方差齐性的辅助函数。\n在spss中进行这些检验时，不管有多少列变量，只要都选中，就可以一次全部进行检验，在R语言里当然也可以！\n主要是通过rstatix这个包完成，数据格式要求是长数据。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R语言“tidy”流统计分析</span>"
    ]
  },
  {
    "objectID": "1014-batchttest.html#实战",
    "href": "1014-batchttest.html#实战",
    "title": "10  R语言“tidy”流统计分析",
    "section": "10.2 实战",
    "text": "10.2 实战\n下面通过一个例子进行说明，可能有些不太适当，只是演示使用方法。\n60个病人随机分为实验组和对照组，每组30人，记录患者的4项评分，现在需要对这4项评分同时进行正态性和方差齐性检验和t检验（可能不符合t检验的条件，这里只是演示方法）。\n\ndf &lt;- read.csv(\"datasets/20210801.csv\",header = T)\n\nstr(df)\n## 'data.frame':    60 obs. of  5 variables:\n##  $ 组别    : chr  \"实验组\" \"实验组\" \"实验组\" \"实验组\" ...\n##  $ 排便困难: int  12 11 15 14 11 13 11 14 13 15 ...\n##  $ 生活质量: int  87 94 95 85 101 91 84 89 84 92 ...\n##  $ 粪便性状: int  2 3 2 2 3 3 3 3 3 3 ...\n##  $ 排便时间: int  2 2 2 2 2 2 2 3 2 2 ...\nhead(df)\n##     组别 排便困难 生活质量 粪便性状 排便时间\n## 1 实验组       12       87        2        2\n## 2 实验组       11       94        3        2\n## 3 实验组       15       95        2        2\n## 4 实验组       14       85        2        2\n## 5 实验组       11      101        3        2\n## 6 实验组       13       91        3        2\n\ndf这样的数据是宽数据，首先变成长数据：\n\nsuppressMessages(library(tidyverse))\n\ndf_l &lt;- df %&gt;% \n  pivot_longer(cols = 2:5, names_to = \"变量\", values_to = \"积分\") %&gt;% \n  dplyr::mutate_if(is.character, as.factor)\n\nstr(df_l)\n## tibble [240 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ 组别: Factor w/ 2 levels \"对照组\",\"实验组\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ 变量: Factor w/ 4 levels \"粪便性状\",\"排便困难\",..: 2 4 1 3 2 4 1 3 2 4 ...\n##  $ 积分: int [1:240] 12 87 2 2 11 94 3 2 15 95 ...\nhead(df_l)\n## # A tibble: 6 × 3\n##   组别   变量      积分\n##   &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;\n## 1 实验组 排便困难    12\n## 2 实验组 生活质量    87\n## 3 实验组 粪便性状     2\n## 4 实验组 排便时间     2\n## 5 实验组 排便困难    11\n## 6 实验组 生活质量    94\n\n同时进行正态性检验：\n\nlibrary(rstatix)\n\ndf_l %&gt;% group_by(变量,组别) %&gt;% shapiro_test(积分)\n## # A tibble: 8 × 5\n##   组别   变量     variable statistic        p\n##   &lt;fct&gt;  &lt;fct&gt;    &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n## 1 对照组 粪便性状 积分         0.452 1.73e- 9\n## 2 实验组 粪便性状 积分         0.404 5.98e-10\n## 3 对照组 排便困难 积分         0.871 1.78e- 3\n## 4 实验组 排便困难 积分         0.915 1.95e- 2\n## 5 对照组 排便时间 积分         0.577 3.91e- 8\n## 6 实验组 排便时间 积分         0.597 6.64e- 8\n## 7 对照组 生活质量 积分         0.974 6.47e- 1\n## 8 实验组 生活质量 积分         0.962 3.46e- 1\n\n方差齐性检验：\n\ndf_l %&gt;% group_by(变量) %&gt;% levene_test(积分 ~ 组别)\n## # A tibble: 4 × 5\n##   变量       df1   df2 statistic      p\n##   &lt;fct&gt;    &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n## 1 粪便性状     1    58    0.127  0.723 \n## 2 排便困难     1    58    0.226  0.636 \n## 3 排便时间     1    58    0.0746 0.786 \n## 4 生活质量     1    58    3.96   0.0514\n\nt检验：\n\ndf_l %&gt;% group_by(变量) %&gt;% t_test(积分 ~ 组别)\n## # A tibble: 4 × 9\n##   变量     .y.   group1 group2    n1    n2 statistic    df     p\n## * &lt;fct&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 粪便性状 积分  对照组 实验组    30    30   -0.356   57.5 0.723\n## 2 排便困难 积分  对照组 实验组    30    30    0.0890  57.4 0.929\n## 3 排便时间 积分  对照组 实验组    30    30   -0.273   58.0 0.786\n## 4 生活质量 积分  对照组 实验组    30    30   -0.101   52.4 0.92\n\n非常方便，结果也是一目了然，再也不用羡慕SPSS的这一点了！\n你又多了一个用R语言进行医学统计学的理由！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R语言“tidy”流统计分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html",
    "href": "1003-dysanova.html",
    "title": "11  多因素方差分析",
    "section": "",
    "text": "11.1 2 x 2 两因素析因设计资料的方差分析\n使用课本例11-1的数据，自己手动摘录：\ndf11_1 &lt;- data.frame(\n  x1 = rep(c(\"外膜缝合\",\"束膜缝合\"), each = 10),\n  x2 = rep(c(\"缝合1个月\",\"缝合2个月\"), each = 5),\n  y = c(10,10,40,50,10,30,30,70,60,30,10,20,30,50,30,50,50,70,60,30)\n)\n\nstr(df11_1)\n## 'data.frame':    20 obs. of  3 variables:\n##  $ x1: chr  \"外膜缝合\" \"外膜缝合\" \"外膜缝合\" \"外膜缝合\" ...\n##  $ x2: chr  \"缝合1个月\" \"缝合1个月\" \"缝合1个月\" \"缝合1个月\" ...\n##  $ y : num  10 10 40 50 10 30 30 70 60 30 ...\n数据一共3列，第1列是缝合方法，第2列是时间，第3列是轴突通过率。试比较不同缝合方法和缝合后时间对轴突通过率的影响，做析因设计的方差分析。\n进行析因设计资料的方差分析（考虑所有因素的主效应和交互作用）：\nf1 &lt;- aov(y ~ x1 * x2, data = df11_1)\n\nsummary(f1)\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## x1           1    180     180   0.600 0.4499  \n## x2           1   2420    2420   8.067 0.0118 *\n## x1:x2        1     20      20   0.067 0.7995  \n## Residuals   16   4800     300                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n结果是一个方差分析表。分别给出了A因素、B因素、AB交互作用、个体间的自由度、离均差平方和、均方误差、F值、P值，可以看到结果和课本是一致的！\n简单介绍一下可视化两因素析因设计的方法：\ninteraction.plot(df11_1$x2, df11_1$x1, df11_1$y, type = \"b\", \n                 col = c(\"red\",\"blue\"), pch = c(12,15),\n                 xlab = \"缝合时间\", ylab = \"轴突通过率\")\n另外一种可视化方法：\nlibrary(gplots)\n\nattach(df11_1)\n\nplotmeans(y ~ interaction(x1,x2),\n          connect = list(c(1,3), c(2,4)),\n          col = c(\"red\",\"darkgreen\"),\n          main = \"两因素析因设计\",\n          xlab = \"时间和方法的交互\")\n再介绍一种方法：\nlibrary(HH)\n\ninteraction2wt(y ~ x1 * x2)\n\n\n\n\n\n\n\ndetach(df11_1)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#x-2-两因素析因设计资料的方差分析",
    "href": "1003-dysanova.html#x-2-两因素析因设计资料的方差分析",
    "title": "11  多因素方差分析",
    "section": "",
    "text": "注释\n\n\n\naov(y ~ x1 * x2, data = df11_1)等价于aov(y ~ x1 + x2 + x1:x2, data = df11_1)，表示x1的主效应、x2的主效应、x1和x2的交互效应。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#i-x-j-两因素析因设计资料的方差分析",
    "href": "1003-dysanova.html#i-x-j-两因素析因设计资料的方差分析",
    "title": "11  多因素方差分析",
    "section": "11.2 I x J 两因素析因设计资料的方差分析",
    "text": "11.2 I x J 两因素析因设计资料的方差分析\n使用课本例11-2的数据，自己手动摘录：\n\ndf11_2 &lt;- data.frame(\n  druga = rep(c(\"1mg\",\"2.5mg\",\"5mg\"), each = 3),\n  drugb = rep(c(\"5微克\",\"15微克\",\"30微克\"),each = 9),\n  y = c(105,80,65,75,115,80,85,120,125,115,105,80,125,130,90,65,\n        120,100,75,95,85,135,120,150,180,190,160)\n)\n\nstr(df11_2)\n## 'data.frame':    27 obs. of  3 variables:\n##  $ druga: chr  \"1mg\" \"1mg\" \"1mg\" \"2.5mg\" ...\n##  $ drugb: chr  \"5微克\" \"5微克\" \"5微克\" \"5微克\" ...\n##  $ y    : num  105 80 65 75 115 80 85 120 125 115 ...\n\n数据一共3列，第1列是a药物的剂量（3种剂量，代表3个水平），第2列是b药物的剂量（3种剂量），第3列是镇痛时间。\n进行两因素三水平的析因设计资料方差分析（考虑所有因素的主效应和交互作用）：\n\nf2 &lt;- aov(y ~ druga * drugb, data = df11_2)\n\nsummary(f2)\n##             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \n## druga        2   6572    3286   8.470 0.00256 **\n## drugb        2   7022    3511   9.050 0.00190 **\n## druga:drugb  4   7872    1968   5.073 0.00647 **\n## Residuals   18   6983     388                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果和课本也是一模一样的哦！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#i-x-j-x-k-三因素析因设计资料的方差分析",
    "href": "1003-dysanova.html#i-x-j-x-k-三因素析因设计资料的方差分析",
    "title": "11  多因素方差分析",
    "section": "11.3 I x J x K 三因素析因设计资料的方差分析",
    "text": "11.3 I x J x K 三因素析因设计资料的方差分析\n使用课本例11-3的数据，\n\ndf11_3 &lt;- foreign::read.spss(\"datasets/例11-03-5种军装热感觉5-2-2.sav\", \n                             to.data.frame = T,reencode=\"UTF-8\")\n\ndf11_3$a &lt;- factor(df11_3$a)\n\nstr(df11_3)\n## 'data.frame':    100 obs. of  4 variables:\n##  $ b: Factor w/ 2 levels \"干燥\",\"潮湿\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ c: Factor w/ 2 levels \"静坐\",\"活动\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ a: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 2 2 2 2 2 ...\n##  $ x: num  0.25 -0.25 1.25 -0.75 0.4 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:4] \"活动环境\" \"活动状态\" \"军装类型\" \"主观热感觉\"\n##   ..- attr(*, \"names\")= chr [1:4] \"b\" \"c\" \"a\" \"x\"\n##  - attr(*, \"codepage\")= int 65001\n\n进行3因素析因设计资料的方差分析（考虑所有的主效应和交互作用）：\n\nf3 &lt;- aov(x ~ a * b * c, data = df11_3)\n\nsummary(f3)\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## a            4   5.20    1.30   3.024   0.0224 *  \n## b            1   9.94    9.94  23.138 6.98e-06 ***\n## c            1 283.35  283.35 659.485  &lt; 2e-16 ***\n## a:b          4   1.94    0.48   1.128   0.3491    \n## a:c          4   1.48    0.37   0.862   0.4905    \n## b:c          1  12.68   12.68  29.514 5.82e-07 ***\n## a:b:c        4   1.61    0.40   0.937   0.4472    \n## Residuals   80  34.37    0.43                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果也是和课本一模一样。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#正交设计资料的方差分析",
    "href": "1003-dysanova.html#正交设计资料的方差分析",
    "title": "11  多因素方差分析",
    "section": "11.4 正交设计资料的方差分析",
    "text": "11.4 正交设计资料的方差分析\n正交设计是残缺不全版本的析因设计，注意指定交互作用即可。\n使用课本例11-4的数据。\n\ndf11_4 &lt;- data.frame(\n  a = rep(c(\"5度\",\"25度\"),each = 4),\n  b = rep(c(0.5, 5.0), each = 2),\n  c = c(10, 30),\n  d = c(6.0, 8.0,8.0,6.0,8.0,6.0,6.0,8.0),\n  x = c(86,95,91,94,91,96,83,88)\n)\n\ndf11_4$a &lt;- factor(df11_4$a)\ndf11_4$b &lt;- factor(df11_4$b)\ndf11_4$c &lt;- factor(df11_4$c)\ndf11_4$d &lt;- factor(df11_4$d)\n\nstr(df11_4)\n## 'data.frame':    8 obs. of  5 variables:\n##  $ a: Factor w/ 2 levels \"25度\",\"5度\": 2 2 2 2 1 1 1 1\n##  $ b: Factor w/ 2 levels \"0.5\",\"5\": 1 1 2 2 1 1 2 2\n##  $ c: Factor w/ 2 levels \"10\",\"30\": 1 2 1 2 1 2 1 2\n##  $ d: Factor w/ 2 levels \"6\",\"8\": 1 2 2 1 2 1 1 2\n##  $ x: num  86 95 91 94 91 96 83 88\n\n进行正交设计资料的方差分析，只考虑4个因素的主效应以及a和b的一阶交互作用：\n\nf4 &lt;- aov(x ~ a + b + c + d + a:b, data = df11_4)\n\nsummary(f4)\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## a            1    8.0     8.0     3.2 0.2155  \n## b            1   18.0    18.0     7.2 0.1153  \n## c            1   60.5    60.5    24.2 0.0389 *\n## d            1    4.5     4.5     1.8 0.3118  \n## a:b          1   50.0    50.0    20.0 0.0465 *\n## Residuals    2    5.0     2.5                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果和课本一模一样，用R语言进行方差分析真是太简单了！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#嵌套设计资料的方差分析",
    "href": "1003-dysanova.html#嵌套设计资料的方差分析",
    "title": "11  多因素方差分析",
    "section": "11.5 嵌套设计资料的方差分析",
    "text": "11.5 嵌套设计资料的方差分析\n嵌套设计也是残缺不全版本的析因设计，使用也是要注意指定主效应和交互效应。\n使用课本例11-6的数据。\n\ndf11_6 &lt;- data.frame(\n  factor1 = factor(rep(c(\"A\",\"B\",\"C\"),each=6)),\n  factor2 = factor(rep(c(70,80,90,55,65,75,90,95,100),each=2)),\n  y = c(82,84,91,88,85,83,65,61,62,59,56,60,71,67,75,78,85,89)\n  )\nstr(df11_6)\n## 'data.frame':    18 obs. of  3 variables:\n##  $ factor1: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 1 2 2 2 2 ...\n##  $ factor2: Factor w/ 8 levels \"55\",\"65\",\"70\",..: 3 3 5 5 6 6 1 1 2 2 ...\n##  $ y      : num  82 84 91 88 85 83 65 61 62 59 ...\n\ndf11_6\n##    factor1 factor2  y\n## 1        A      70 82\n## 2        A      70 84\n## 3        A      80 91\n## 4        A      80 88\n## 5        A      90 85\n## 6        A      90 83\n## 7        B      55 65\n## 8        B      55 61\n## 9        B      65 62\n## 10       B      65 59\n## 11       B      75 56\n## 12       B      75 60\n## 13       C      90 71\n## 14       C      90 67\n## 15       C      95 75\n## 16       C      95 78\n## 17       C     100 85\n## 18       C     100 89\n\nfactor1是一级实验因素（不同的催化剂），factor2是二级实验因素（不同的温度），y是因变量。\n进行嵌套实验设计的方差分析：\n\n# “/”表示factor2嵌套在factor1里\nf &lt;- aov(y ~ factor1 / factor2, data = df11_6)\n\n# 等价于以下写法，所以“/”在R中的公式中也是有特殊含义的！\n#f &lt;- aov(y ~ factor1 + factor1:factor2, data = df11_6)\n\nsummary(f)\n##                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## factor1          2 1956.0   978.0  177.82 5.83e-08 ***\n## factor1:factor2  6  401.0    66.8   12.15 0.000716 ***\n## Residuals        9   49.5     5.5                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果和课本相同。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#裂区设计资料的方差分析",
    "href": "1003-dysanova.html#裂区设计资料的方差分析",
    "title": "11  多因素方差分析",
    "section": "11.6 裂区设计资料的方差分析",
    "text": "11.6 裂区设计资料的方差分析\n使用课本例11-7的数据。这是一个完全随机的2*2裂区设计，家兔为一级实验单位，注射部位为二级实验单位。\n\ndf11_7 &lt;- data.frame(\n  factorA = factor(rep(c(\"a1\",\"a2\"),each=10)),\n  factorB = factor(rep(c(\"b1\",\"b2\"),10)),\n  id = factor(rep(c(1:10),each=2)),\n  y = c(15.75,19.00,15.50,20.75,15.50,18.50,17.00,20.50,16.50,20.00,\n        18.25,22.25,18.50,21.50,19.75,23.50,21.50,24.75,20.75,23.75)\n  )\nstr(df11_7)\n## 'data.frame':    20 obs. of  4 variables:\n##  $ factorA: Factor w/ 2 levels \"a1\",\"a2\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ factorB: Factor w/ 2 levels \"b1\",\"b2\": 1 2 1 2 1 2 1 2 1 2 ...\n##  $ id     : Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 2 3 3 4 4 5 5 ...\n##  $ y      : num  15.8 19 15.5 20.8 15.5 ...\n\ndf11_7\n##    factorA factorB id     y\n## 1       a1      b1  1 15.75\n## 2       a1      b2  1 19.00\n## 3       a1      b1  2 15.50\n## 4       a1      b2  2 20.75\n## 5       a1      b1  3 15.50\n## 6       a1      b2  3 18.50\n## 7       a1      b1  4 17.00\n## 8       a1      b2  4 20.50\n## 9       a1      b1  5 16.50\n## 10      a1      b2  5 20.00\n## 11      a2      b1  6 18.25\n## 12      a2      b2  6 22.25\n## 13      a2      b1  7 18.50\n## 14      a2      b2  7 21.50\n## 15      a2      b1  8 19.75\n## 16      a2      b2  8 23.50\n## 17      a2      b1  9 21.50\n## 18      a2      b2  9 24.75\n## 19      a2      b1 10 20.75\n## 20      a2      b2 10 23.75\n\n裂区设计的A因素只作用于一级实验单位，B因素只作用于二级实验单位，所以其方差分析也是由两部分组成（课本P183）。\n该例题中每个家兔对应着B因素（毒素浓度）的两个水平（每只家兔会注射两种浓度的毒素），但每只家兔只对应A因素的1个水平（每只家兔只会注射一种药物，不会同时注射两种药物），所以需要为B因素指定误差项。\n\n# factorB is nested in id，每个id对应多个factorB\n# factorA和factorB有交叉，但是id只和factorB有交叉\nf &lt;- aov(y ~ factorA * factorB + Error(id/factorB), data = df11_7)\nsummary(f)\n## \n## Error: id\n##           Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## factorA    1  63.01   63.01   28.01 0.000735 ***\n## Residuals  8  18.00    2.25                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: id:factorB\n##                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## factorB          1  63.01   63.01  252.05 2.48e-07 ***\n## factorA:factorB  1   0.11    0.11    0.45    0.521    \n## Residuals        8   2.00    0.25                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果同课本相同。第一部分是A因素主效应和误差，第二部分是：B因素主效应、A和B的交互效应、误差。\n裂区设计和嵌套设计R方差分析实现的参考链接：\n\nCrossed and Nested Factors\naov() error term in R: what’s the difference bw Error(id) and Error(id/timevar) specification?\nFormulae in R\nR and Analysis of Variance",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1008-mauchly.html",
    "href": "1008-mauchly.html",
    "title": "12  球对称检验",
    "section": "",
    "text": "12.1 课本表12-3的数据\n这是一个只有1组的！\n读取数据：\ndf &lt;- foreign::read.spss(\"datasets/表12-3重复测量ANOVA.sav\",\n                         to.data.frame = T, reencode = \"utf-8\")\n\nstr(df)\n## 'data.frame':    8 obs. of  4 variables:\n##  $ t0  : num  5.32 5.32 5.94 5.49 5.71 6.27 5.88 5.32\n##  $ t45 : num  5.32 5.26 5.88 5.43 5.49 6.27 5.77 5.15\n##  $ t90 : num  4.98 4.93 5.43 5.32 5.43 5.66 5.43 5.04\n##  $ t135: num  4.65 4.7 5.04 5.04 4.93 5.26 4.93 4.48\n##  - attr(*, \"variable.labels\")= Named chr(0) \n##   ..- attr(*, \"names\")= chr(0) \n##  - attr(*, \"codepage\")= int 936\nhead(df)\n##     t0  t45  t90 t135\n## 1 5.32 5.32 4.98 4.65\n## 2 5.32 5.26 4.93 4.70\n## 3 5.94 5.88 5.43 5.04\n## 4 5.49 5.43 5.32 5.04\n## 5 5.71 5.49 5.43 4.93\n## 6 6.27 6.27 5.66 5.26\n数据一共4列，就是4个时间点的血糖值。\n首先将数据变为矩阵：\ndf &lt;- as.matrix(df)\n然后进行球对称检验（球形检验）：\nmauchly.test(lm(df ~ 1), X = ~ 1)\n## \n##  Mauchly's test of sphericity\n##  Contrasts orthogonal to\n##  ~1\n## \n## \n## data:  SSD matrix from lm(formula = df ~ 1)\n## W = 0.06273, p-value = 0.008207\n结果就有了，就是这么简单直接，网上很多资料都是直接复制粘贴帮助文档里的内容，非常费脑子！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>球对称检验</span>"
    ]
  },
  {
    "objectID": "1008-mauchly.html#课本例12-3的数据",
    "href": "1008-mauchly.html#课本例12-3的数据",
    "title": "12  球对称检验",
    "section": "12.2 课本例12-3的数据",
    "text": "12.2 课本例12-3的数据\n这个数据有2组！\n直接读取：\n\ndf1 &lt;- foreign::read.spss(\"datasets/例12-03.sav\",to.data.frame = T)\n\nstr(df1)\n## 'data.frame':    15 obs. of  7 variables:\n##  $ No   : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ group: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 2 2 2 2 2 ...\n##  $ t0   : num  120 118 119 121 127 121 122 128 117 118 ...\n##  $ t1   : num  108 109 112 112 121 120 121 129 115 114 ...\n##  $ t2   : num  112 115 119 119 127 118 119 126 111 116 ...\n##  $ t3   : num  120 126 124 126 133 131 129 135 123 123 ...\n##  $ t4   : num  117 123 118 120 126 137 133 142 131 133 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:7] \"\\xd0\\xf2\\xba\\xc5\" \"\\xd7\\xe9\\xb1\\xf0\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:7] \"No\" \"group\" \"t0\" \"t1\" ...\nhead(df1)\n##   No group  t0  t1  t2  t3  t4\n## 1  1     A 120 108 112 120 117\n## 2  2     A 118 109 115 126 123\n## 3  3     A 119 112 119 124 118\n## 4  4     A 121 112 119 126 120\n## 5  5     A 127 121 127 133 126\n## 6  6     B 121 120 118 131 137\n\n数据一共7列，第1列是患者编号，第2列是诱导方法（3种），第3-7列是5个时间点的血压。\n首先将数据变为矩阵，转换数据格式：\n\ndf2 &lt;- as.matrix(cbind(df1[1:5,3:7], df1[6:10,3:7], df1[11:15,3:7]))\n\n把测量点和分组单独建立，注意要和上面的顺序一致：\n\ntimes = ordered(rep(1:5,3))\ngroup = factor(rep(c(\"A\",\"B\",\"C\"),each = 5))\n\n然后进行球对称检验（球形检验）：\n\nmauchly.test(lm(df2 ~ 1), M = ~ group + times, X = ~ times)\n## \n##  Mauchly's test of sphericity\n##  Contrasts orthogonal to\n##  ~times\n## \n##  Contrasts spanned by\n##  ~group + times\n## \n## \n## data:  SSD matrix from lm(formula = df2 ~ 1)\n## W = 0.427, p-value = 0.279\n\n真的是有点费事儿！不过现在很多R包都可以在进行重复测量方差分析时自动给出球形检验的结果，已经方便多了。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>球对称检验</span>"
    ]
  },
  {
    "objectID": "1004-repeatedanova.html",
    "href": "1004-repeatedanova.html",
    "title": "13  重复测量方差分析",
    "section": "",
    "text": "13.1 重复测量数据两因素两水平的方差分析\n使用课本例12-1的数据，直接读取：\ndf12_1 &lt;- foreign::read.spss(\"datasets/12-1.sav\", to.data.frame = T)\n\nstr(df12_1)\n## 'data.frame':    20 obs. of  5 variables:\n##  $ n    : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : num  130 124 136 128 122 118 116 138 126 124 ...\n##  $ x2   : num  114 110 126 116 102 100 98 122 108 106 ...\n##  $ group: Factor w/ 2 levels \"处理组\",\"对照组\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ d    : num  16 14 10 12 20 18 18 16 18 18 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:5] \"编号\" \"治疗前血压\" \"治疗后血压\" \"组别\" ...\n##   ..- attr(*, \"names\")= chr [1:5] \"n\" \"x1\" \"x2\" \"group\" ...\n##  - attr(*, \"codepage\")= int 936\n数据一共5列（第5列是自己算出来的，其实原始数据只有4列），第1列是编号，第2列是治疗前血压，第3例是治疗后血压，第4列是分组，第5列是血压前后差值。\n进行重复测量数据两因素两水平的方差分析前，先把数据转换一下格式：\nlibrary(tidyverse)\n\n# 变成长数据\ndf12_11 &lt;- \n  df12_1[,1:4] %&gt;% \n  pivot_longer(cols = 2:3,names_to = \"time\",values_to = \"hp\") %&gt;% \n  mutate_if(is.character, as.factor)\n\ndf12_11$n &lt;- factor(df12_11$n)\n\nstr(df12_11)\n## tibble [40 × 4] (S3: tbl_df/tbl/data.frame)\n##  $ n    : Factor w/ 20 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 2 3 3 4 4 5 5 ...\n##  $ group: Factor w/ 2 levels \"处理组\",\"对照组\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ time : Factor w/ 2 levels \"x1\",\"x2\": 1 2 1 2 1 2 1 2 1 2 ...\n##  $ hp   : num [1:40] 130 114 124 110 136 126 128 116 122 102 ...\n转换后的数据格式如上。\n进行重复测量数据两因素两水平的方差分析:\nhp是因变量，time是测量时间（治疗前和治疗后各测量一次），group是分组因素（两种治疗方法），n是受试者编号。\n# time和group是有交叉的，每个受试者（n）只和time有交叉，和group没有交叉\nf1 &lt;- aov(hp ~ time * group + Error(n/time), data = df12_11)\n\nsummary(f1)\n## \n## Error: n\n##           Df Sum Sq Mean Sq F value Pr(&gt;F)\n## group      1  202.5   202.5   1.574  0.226\n## Residuals 18 2315.4   128.6               \n## \n## Error: n:time\n##            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## time        1 1020.1  1020.1   55.01 7.08e-07 ***\n## time:group  1  348.1   348.1   18.77 0.000401 ***\n## Residuals  18  333.8    18.5                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n结果输出了两张表，第二个是测量前后比较与交互作用的方差分析表，第一个是处理组与对照组比较的方差分析表，可以看到结果和课本是一样的！\n用图形方式展示重复测量的结果：\nwith(df12_11,\n     interaction.plot(time, group, hp, type = \"b\", col = c(\"red\",\"blue\"), \n                      pch = c(12,16), main = \"两因素两水平重复测量方差分析\"))\n或者用箱线图展示结果：\nboxplot(hp ~ group*time, data = df12_11, col = c(\"gold\",\"green\"),\n        main = \"两因素两水平重复测量方差分析\")",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>重复测量方差分析</span>"
    ]
  },
  {
    "objectID": "1004-repeatedanova.html#重复测量数据两因素多水平的分析",
    "href": "1004-repeatedanova.html#重复测量数据两因素多水平的分析",
    "title": "13  重复测量方差分析",
    "section": "13.2 重复测量数据两因素多水平的分析",
    "text": "13.2 重复测量数据两因素多水平的分析\n使用课本例12-3的数据，直接读取：\n\ndf12_3 &lt;- foreign::read.spss(\"datasets/例12-03.sav\",to.data.frame = T,\n                             reencode = \"utf-8\"\n                             )\n\nstr(df12_3)\n## 'data.frame':    15 obs. of  7 variables:\n##  $ No   : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ group: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 2 2 2 2 2 ...\n##  $ t0   : num  120 118 119 121 127 121 122 128 117 118 ...\n##  $ t1   : num  108 109 112 112 121 120 121 129 115 114 ...\n##  $ t2   : num  112 115 119 119 127 118 119 126 111 116 ...\n##  $ t3   : num  120 126 124 126 133 131 129 135 123 123 ...\n##  $ t4   : num  117 123 118 120 126 137 133 142 131 133 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:7] \"....\" \"....\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:7] \"No\" \"group\" \"t0\" \"t1\" ...\n\nhead(df12_3)\n##   No group  t0  t1  t2  t3  t4\n## 1  1     A 120 108 112 120 117\n## 2  2     A 118 109 115 126 123\n## 3  3     A 119 112 119 124 118\n## 4  4     A 121 112 119 126 120\n## 5  5     A 127 121 127 133 126\n## 6  6     B 121 120 118 131 137\n\n数据一共7列，第1列是患者编号，第2列是诱导方法（3种），第3-7列是5个时间点的血压。\n首先转换数据格式：\n\nlibrary(tidyverse)\n\n# 变为长数据\ndf12_31 &lt;- df12_3 %&gt;% \n  pivot_longer(cols = 3:7, names_to = \"times\", values_to = \"hp\")\n\ndf12_31$No &lt;- factor(df12_31$No)\ndf12_31$times &lt;- factor(df12_31$times)\n\nstr(df12_31)\n## tibble [75 × 4] (S3: tbl_df/tbl/data.frame)\n##  $ No   : Factor w/ 15 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 2 2 2 2 2 ...\n##  $ group: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ times: Factor w/ 5 levels \"t0\",\"t1\",\"t2\",..: 1 2 3 4 5 1 2 3 4 5 ...\n##  $ hp   : num [1:75] 120 108 112 120 117 118 109 115 126 123 ...\n\n转换后的格式见上图。\n进行方差分析（和两因素两水平没有任何区别）：\n\nf2 &lt;- aov(hp ~ times * group + Error(No/(times)), data = df12_31)\n\nsummary(f2)\n## \n## Error: No\n##           Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## group      2  912.2   456.1   5.783 0.0174 *\n## Residuals 12  946.5    78.9                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: No:times\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## times        4 2336.5   584.1   106.6  &lt; 2e-16 ***\n## times:group  8  837.6   104.7    19.1 1.62e-12 ***\n## Residuals   48  263.1     5.5                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n输出结果是两张表格，第1个是不同诱导方法患者血压比较的方差分析表，第2个是麻醉诱导时相及其与诱导方法交互作用的方差分析表。\n结果和课本是一样的！具体意义解读请认真学习医学统计学相关知识。\n用图形方式展示重复测量的结果：\n\nwith(df12_31,\n     interaction.plot(times, group, hp, type = \"b\", \n                      col = c(\"red\",\"blue\",\"green\"), \n                      pch = c(12,16,20), \n                      main = \"两因素多水平重复测量方差分析\"))\n\n\n\n\n\n\n\n\n或者用箱线图展示结果：\n\nboxplot(hp ~ group*times, data = df12_31, col = c(\"gold\",\"green\",\"black\"),\n        main = \"两因素多水平重复测量方差分析\")",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>重复测量方差分析</span>"
    ]
  },
  {
    "objectID": "1004-repeatedanova.html#重复测量数据的多重比较",
    "href": "1004-repeatedanova.html#重复测量数据的多重比较",
    "title": "13  重复测量方差分析",
    "section": "13.3 重复测量数据的多重比较",
    "text": "13.3 重复测量数据的多重比较\n使用课本例12-1的数据，直接读取：\n\ndf12_3 &lt;- foreign::read.spss(\"datasets/例12-03.sav\",to.data.frame = T)\n\nstr(df12_3)\n## 'data.frame':    15 obs. of  7 variables:\n##  $ No   : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ group: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 2 2 2 2 2 ...\n##  $ t0   : num  120 118 119 121 127 121 122 128 117 118 ...\n##  $ t1   : num  108 109 112 112 121 120 121 129 115 114 ...\n##  $ t2   : num  112 115 119 119 127 118 119 126 111 116 ...\n##  $ t3   : num  120 126 124 126 133 131 129 135 123 123 ...\n##  $ t4   : num  117 123 118 120 126 137 133 142 131 133 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:7] \"\\xd0\\xf2\\xba\\xc5\" \"\\xd7\\xe9\\xb1\\xf0\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:7] \"No\" \"group\" \"t0\" \"t1\" ...\n\n数据一共7列，第1列是患者编号，第2列是诱导方法（3种），第3-7列是5个时间点的血压。\n首先转换数据格式：\n\nlibrary(reshape2)\n\ndf.l &lt;- melt(df12_3, id.vars = c(\"No\",\"group\"), \n             variable.name = \"times\", \n             value.name = \"hp\")\ndf.l$No &lt;- factor(df.l$No)\n\nstr(df.l)\n## 'data.frame':    75 obs. of  4 variables:\n##  $ No   : Factor w/ 15 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  $ group: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 2 2 2 2 2 ...\n##  $ times: Factor w/ 5 levels \"t0\",\"t1\",\"t2\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ hp   : num  120 118 119 121 127 121 122 128 117 118 ...\nhead(df.l)\n##   No group times  hp\n## 1  1     A    t0 120\n## 2  2     A    t0 118\n## 3  3     A    t0 119\n## 4  4     A    t0 121\n## 5  5     A    t0 127\n## 6  6     B    t0 121\n\n进行重复测量方差分析，默认方法不能输出球形检验的结果，所以我更推荐rstatix提供的方法：\n\n# 默认\nf &lt;- aov(hp ~ group*times + Error(No/times), data = df.l)\nsummary(f)\n## \n## Error: No\n##           Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## group      2  912.2   456.1   5.783 0.0174 *\n## Residuals 12  946.5    78.9                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: No:times\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## times        4 2336.5   584.1   106.6  &lt; 2e-16 ***\n## group:times  8  837.6   104.7    19.1 1.62e-12 ***\n## Residuals   48  263.1     5.5                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n# rstatix\nlibrary(rstatix)\n\nanova_test(data = df.l,\n           dv = hp,\n           wid = No,\n           within = times,\n           between = group\n           )\n## ANOVA Table (type II tests)\n## \n## $ANOVA\n##        Effect DFn DFd       F        p p&lt;.05   ges\n## 1       group   2  12   5.783 1.70e-02     * 0.430\n## 2       times   4  48 106.558 3.02e-23     * 0.659\n## 3 group:times   8  48  19.101 1.62e-12     * 0.409\n## \n## $`Mauchly's Test for Sphericity`\n##        Effect     W     p p&lt;.05\n## 1       times 0.293 0.178      \n## 2 group:times 0.293 0.178      \n## \n## $`Sphericity Corrections`\n##        Effect   GGe      DF[GG]    p[GG] p[GG]&lt;.05   HFe      DF[HF]    p[HF]\n## 1       times 0.679 2.71, 32.58 1.87e-16         * 0.896 3.59, 43.03 4.65e-21\n## 2 group:times 0.679 5.43, 32.58 4.26e-09         * 0.896 7.17, 43.03 2.04e-11\n##   p[HF]&lt;.05\n## 1         *\n## 2         *\n\n画图展示：\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndf.l |&gt; \n  group_by(times,group) |&gt; \n  summarise(mm=mean(hp)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(times,mm))+\n  geom_line(aes(group=group,color=group),linewidth=1.2)+\n  theme_bw()\n\n\n\n\n\n\n\n\n接下来是重复测量数据的多重比较，课本中分成了3个方面。\n\n13.3.1 组间差别多重比较\n首先也计算下各组的均值，和课本对比下（一样的）：\n\ndf.l |&gt; \n  group_by(group) |&gt; \n  summarise(mm=mean(hp))\n## # A tibble: 3 × 2\n##   group    mm\n##   &lt;fct&gt; &lt;dbl&gt;\n## 1 A      120.\n## 2 B      124.\n## 3 C      128.\n\nLSD/SNK/Tukey/Dunnett/Bonferroni等方法都可以，和多个均数比较的多重检验一样。\n\nlibrary(PMCMRplus)\n\nsummary(lsdTest(hp ~ group, data = df.l))\n##            t value  Pr(&gt;|t|)    \n## B - A == 0   2.175 0.0329218   *\n## C - A == 0   3.860 0.0002446 ***\n## C - B == 0   1.686 0.0962097   .\n\nP值和课本不太一样，但是结论是一样的，A组和B组之间，A组和C组之间有差别，B组和C组之间没有差别。\n\n\n13.3.2 时间趋势比较\n重复测量方差分析可以采取正交多项式来探索时间变化趋势，具体的内涵解读可以参考冯国双老师的这篇文章：重复测量数据探索时间变化趋势\n在R里面进行正交多项式的探索略显复杂，需要对时间变量（这里是times）进行正交多项式转换，我们这里有5个时间点，所以是1次方到4次方：\n\n# 给大家展示下正交多项式转换\ncontrasts(df.l$times) &lt;- contr.poly(5)\ncontrasts(df.l$times)\n##               .L         .Q            .C         ^4\n## t0 -6.324555e-01  0.5345225 -3.162278e-01  0.1195229\n## t1 -3.162278e-01 -0.2672612  6.324555e-01 -0.4780914\n## t2 -3.510833e-17 -0.5345225  1.755417e-16  0.7171372\n## t3  3.162278e-01 -0.2672612 -6.324555e-01 -0.4780914\n## t4  6.324555e-01  0.5345225  3.162278e-01  0.1195229\n\n下面进行方差分析，此时是单纯探索时间对因变量的影响，所以注意formula的形式：\n\n# A组\nf1 &lt;- aov(hp ~ times, data = df.l[df.l$group==\"A\",])\n\n# 分别看不同次方的结果\nsummary(f1, \n        split=list(times=list(liner=1,quadratic=2,cubic=3,biquadrate=4)))\n##                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## times                4  475.4   118.9   5.580 0.003486 ** \n##   times: liner       1   84.5    84.5   3.967 0.060229 .  \n##   times: quadratic   1   26.4    26.4   1.240 0.278655    \n##   times: cubic       1  364.5   364.5  17.113 0.000511 ***\n##   times: biquadrate  1    0.0     0.0   0.001 0.972627    \n## Residuals           20  426.0    21.3                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n# B组\nf2 &lt;- aov(hp ~ times, data = df.l[df.l$group==\"B\",])\nsummary(f2, split=list(times=list(liner=1,quadratic=2,cubic=3,biquadrate=4)))\n##                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## times                4 1017.0   254.3   9.757 0.000152 ***\n##   times: liner       1  662.5   662.5  25.421 6.24e-05 ***\n##   times: quadratic   1  296.2   296.2  11.367 0.003034 ** \n##   times: cubic       1    3.9     3.9   0.150 0.702229    \n##   times: biquadrate  1   54.4    54.4   2.088 0.163954    \n## Residuals           20  521.2    26.1                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n# C组\nf3 &lt;- aov(hp ~ times+Error(No/times), data = df.l[df.l$group==\"C\",])\nsummary(f3, split=list(times=list(liner=1,quadratic=2,cubic=3,biquadrate=4)))\n## \n## Error: No\n##           Df Sum Sq Mean Sq F value Pr(&gt;F)\n## Residuals  4     98    24.5               \n## \n## Error: No:times\n##                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## times                4 1681.6   420.4  40.915 3.28e-08 ***\n##   times: liner       1  403.3   403.3  39.249 1.13e-05 ***\n##   times: quadratic   1   41.7    41.7   4.054   0.0612 .  \n##   times: cubic       1  605.5   605.5  58.931 9.43e-07 ***\n##   times: biquadrate  1  631.1   631.1  61.425 7.23e-07 ***\n## Residuals           16  164.4    10.3                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n以上3组结果中，第2组和第3组结果的的离均差平方和、自由度、均方都是和课本一样的，但是F值和P值相差很多。第1组的所有结果都和课本相差很大。关于这方面的资料较少，如果有大神知道，欢迎指教！\n\n\n13.3.3 时间点多重比较\n课本说因为事后检验重复次数太多难以承受，但是我们用计算机很快，所以用事后检验也没什么问题。\n事后检验可以参考组间比较，根据组别进行分组，分组比较不同时间点的差别。\n事前检验课本采用配对t检验，全都和t0的数据进行比较。\n事前检验使用rstatix包解决:\n\nlibrary(rstatix)\n\ndf.l |&gt; \n  group_by(group) |&gt; \n  t_test(hp ~ times, ref.group = \"t0\",paired = T)\n## # A tibble: 12 × 11\n##    group .y.   group1 group2    n1    n2 statistic    df         p    p.adj\n##  * &lt;fct&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n##  1 A     hp    t0     t1         5     5     8.35      4 0.001     0.004   \n##  2 A     hp    t0     t2         5     5     1.77      4 0.152     0.304   \n##  3 A     hp    t0     t3         5     5    -3.64      4 0.022     0.066   \n##  4 A     hp    t0     t4         5     5     0.147     4 0.89      0.89    \n##  5 B     hp    t0     t1         5     5     1.72      4 0.16      0.16    \n##  6 B     hp    t0     t2         5     5     4.35      4 0.012     0.024   \n##  7 B     hp    t0     t3         5     5    -8.37      4 0.001     0.003   \n##  8 B     hp    t0     t4         5     5   -16.7       4 0.0000747 0.000299\n##  9 C     hp    t0     t1         5     5     1.44      4 0.223     0.292   \n## 10 C     hp    t0     t2         5     5     4.75      4 0.009     0.028   \n## 11 C     hp    t0     t3         5     5    -5.12      4 0.007     0.028   \n## 12 C     hp    t0     t4         5     5    -1.80      4 0.146     0.292   \n## # ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n直接给出3组的结果，和课本一模一样~",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>重复测量方差分析</span>"
    ]
  },
  {
    "objectID": "1005-ancova.html",
    "href": "1005-ancova.html",
    "title": "14  协方差分析",
    "section": "",
    "text": "14.1 完全随机设计资料的协方差分析\n使用课本例13-1的例子。\n首先是读取数据，本次数据手动录入：\ndf13_1 &lt;- data.frame(x1=c(10.8,11.6,10.6,9.0,11.2,9.9,10.6,10.4,9.6,10.5,\n                          10.6,9.9,9.5,9.7,10.7,9.2,10.5,11.0,10.1,10.7,8.5,\n                          10.0, 10.4,9.7,9.4,9.2,10.5,11.2,9.6,8.0),\n                     y1=c(9.4,9.7,8.7,7.2,10.0,8.5,8.3,8.1,8.5,9.1,9.2,8.4,\n                          7.6,7.9,8.8,7.4,8.6,9.2,8.0,8.5,7.3,8.3,\n                          8.6,8.7,7.6,8.0,8.8,9.5,8.2,7.2),\n                     x2=c(10.4,9.7,9.9,9.8,11.1,8.2,8.8,10.0,9.0,9.4,8.9,\n                          10.3,9.3,9.2,10.9,9.2,9.2,10.4,11.2,11.1,11.0,\n                          8.6,9.3,10.3,10.3,9.8,10.5,10.7,10.4,9.4),\n                     y2=c(9.2,9.1,8.9,8.6,9.9,7.1,7.8,7.9,8.0,9.0,7.9,8.9,\n                          8.9,8.1,10.2,8.5,9.0,8.9,9.8,10.1,8.5,8.1,8.6,\n                          8.9,9.6,8.1,9.9,9.3,8.7,8.7),\n                     x3=c(9.8,11.2,10.7,9.6,10.1,9.8,10.1,10.3,11.0,10.5,\n                          9.2,10.1,10.4,10.0,8.4,10.1,9.3,10.5,11.1,10.5,\n                          9.7,9.2,9.3,10.4,10.0,10.3,9.9,9.4,8.3,9.2),\n                     y3=c(7.6,7.9,9.0,7.8,8.5,7.5,8.3,8.2,8.4,8.1,7.0,7.7,\n                          8.0,6.6,6.1,8.1,7.8,8.4,8.2,8.0,7.6,6.9,6.7,\n                          8.1,7.4,8.2,7.6,7.8,6.6,7.2)\n                     )\n看一下数据结构：\nstr(df13_1)\n## 'data.frame':    30 obs. of  6 variables:\n##  $ x1: num  10.8 11.6 10.6 9 11.2 9.9 10.6 10.4 9.6 10.5 ...\n##  $ y1: num  9.4 9.7 8.7 7.2 10 8.5 8.3 8.1 8.5 9.1 ...\n##  $ x2: num  10.4 9.7 9.9 9.8 11.1 8.2 8.8 10 9 9.4 ...\n##  $ y2: num  9.2 9.1 8.9 8.6 9.9 7.1 7.8 7.9 8 9 ...\n##  $ x3: num  9.8 11.2 10.7 9.6 10.1 9.8 10.1 10.3 11 10.5 ...\n##  $ y3: num  7.6 7.9 9 7.8 8.5 7.5 8.3 8.2 8.4 8.1 ...\n可以看到一共6列，和课本上面的一模一样，分别是x1,y1,x2,y2,x3,y3。\n接下来为了进行方差分析，需要变为长数据，把所有的x放在1列，所有的y放在1列，还有一列是组别：\n如果大家还对长宽数据转换不了解的，可以翻看之前的历史推文：\n这是一个非常重要且使用频率极高的技能！\nsuppressPackageStartupMessages(library(tidyverse))\n\ndf13_11 &lt;- df13_1 %&gt;% \n  pivot_longer(cols = everything(), # 变长\n               names_to = c(\".value\",\"group\"),\n               names_pattern = \"(.)(.)\"\n               ) %&gt;% \n  mutate(group = as.factor(group)) # 组别变为因子型\n\nglimpse(df13_11) # 查看数据结构，神奇！\n## Rows: 90\n## Columns: 3\n## $ group &lt;fct&gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1…\n## $ x     &lt;dbl&gt; 10.8, 10.4, 9.8, 11.6, 9.7, 11.2, 10.6, 9.9, 10.7, 9.0, 9.8, 9.6…\n## $ y     &lt;dbl&gt; 9.4, 9.2, 7.6, 9.7, 9.1, 7.9, 8.7, 8.9, 9.0, 7.2, 8.6, 7.8, 10.0…\n所有的x放在1列，所有的y放在1列，还有一列是组别！\n然后就是进行单因素协方差分析：\n# 注意公式的写法，一定是把协变量放在主变量前面！\nfit &lt;- aov(y ~ x + group, data = df13_11) \nsummary(fit)\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## x            1  29.06  29.057  171.20 &lt;2e-16 ***\n## group        2  19.85   9.925   58.48 &lt;2e-16 ***\n## Residuals   86  14.60   0.170                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n得到的结果和课本是一模一样的，组内ss=14.60, ms=0.170, v=86, 修正均数ss=19.85, ms=9.925, v=2, F=58.48，拒绝H0，接受H1，可以认为在扣除初始（基线）糖化血红蛋白含量的影响后，3组患者的总体降糖均数有差别。\n结果的可视化可以使用HH包：\nlibrary(HH)\n一行代码即可：\nancovaplot(y ~ x + group, data = df13_11)\n但其实我们也可以用ggplot2来画，可能更好看一点：\ntheme_set(theme_bw())\n\np1 &lt;- ggplot(df13_11, aes(x=x,y=y))+\n  geom_point(aes(color=group,shape=group))+\n  geom_smooth(method = \"lm\",se=F,aes(color=group))+\n  labs(y=NULL)\n\np2 &lt;- ggplot(df13_11, aes(x=x,y=y))+\n  geom_point(aes(color=group,shape=group))+\n  geom_smooth(method = \"lm\",se=F,aes(color=group))+\n  facet_wrap(~group)\n\nlibrary(patchwork)\np2 + p1 + plot_layout(guides = 'collect',widths = c(3, 1))\n好看是好看，但是很明显不如HH简洁啊！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>协方差分析</span>"
    ]
  },
  {
    "objectID": "1005-ancova.html#完全随机设计资料的协方差分析",
    "href": "1005-ancova.html#完全随机设计资料的协方差分析",
    "title": "14  协方差分析",
    "section": "",
    "text": "宽数据变为长数据的5种情况！\n长数据变为宽数据的7种情况！\n长宽数据转换的特殊情况",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>协方差分析</span>"
    ]
  },
  {
    "objectID": "1005-ancova.html#使用rstatix进行优雅的协方差分析",
    "href": "1005-ancova.html#使用rstatix进行优雅的协方差分析",
    "title": "14  协方差分析",
    "section": "14.2 使用rstatix进行优雅的协方差分析",
    "text": "14.2 使用rstatix进行优雅的协方差分析\n\nlibrary(rstatix)\n\nres &lt;- anova_test(y ~ x + group, data = df13_11, type = 1)#不同类型可选\nget_anova_table(res)\n## ANOVA Table (type I tests)\n## \n##   Effect DFn DFd       F        p p&lt;.05   ges\n## 1      x   1  86 171.199 3.64e-22     * 0.666\n## 2  group   2  86  58.480 9.22e-17     * 0.576\n\n结果也是一样的！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>协方差分析</span>"
    ]
  },
  {
    "objectID": "1005-ancova.html#随机区组设计资料的协方差分析",
    "href": "1005-ancova.html#随机区组设计资料的协方差分析",
    "title": "14  协方差分析",
    "section": "14.3 随机区组设计资料的协方差分析",
    "text": "14.3 随机区组设计资料的协方差分析\n使用课本例13-2的数据。\n\ndf &lt;- foreign::read.spss(\"datasets/例13-02.sav\",to.data.frame = T,\n                         reencode = \"utf-8\")\ndf$block &lt;- factor(df$block)\n\nstr(df)\n## 'data.frame':    36 obs. of  4 variables:\n##  $ x    : num  257 272 210 300 262 ...\n##  $ y    : num  27 41.7 25 52 14.5 48.8 48 9.5 37 56.5 ...\n##  $ group: Factor w/ 3 levels \"A....\",\"B....\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ block: Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:4] \"..ʳ..\" \"..........\" \"........\" \"......\"\n##   ..- attr(*, \"names\")= chr [1:4] \"x\" \"y\" \"group\" \"block\"\nhead(df)\n##       x    y group block\n## 1 256.9 27.0 A....     1\n## 2 271.6 41.7 A....     2\n## 3 210.2 25.0 A....     3\n## 4 300.1 52.0 A....     4\n## 5 262.2 14.5 A....     5\n## 6 304.4 48.8 A....     6\n\n进行随机区组设计的协方差分析：\n\nfit &lt;- aov(y ~ x + block + group, data = df) # 注意顺序\nsummary(fit)\n##             Df Sum Sq Mean Sq F value  Pr(&gt;F)    \n## x            1  69073   69073 651.823 &lt; 2e-16 ***\n## block       11   4024     366   3.452 0.00711 ** \n## group        2    464     232   2.189 0.13692    \n## Residuals   21   2225     106                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(fit)\n## Anova Table (Type II tests)\n## \n## Response: y\n##           Sum Sq Df F value    Pr(&gt;F)    \n## x         6174.2  1 58.2643 1.733e-07 ***\n## block     3765.3 11  3.2302   0.01009 *  \n## group      463.9  2  2.1891   0.13692    \n## Residuals 2225.4 21                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果和课本一致。\n或者用rstatix：\n\nfit &lt;- anova_test(y ~ x + block + group, data = df, type = 3)\nget_anova_table(fit)\n## ANOVA Table (type III tests)\n## \n##   Effect DFn DFd      F        p p&lt;.05   ges\n## 1      x   1  21 58.264 1.73e-07     * 0.735\n## 2  block  11  21  3.230 1.00e-02     * 0.629\n## 3  group   2  21  2.189 1.37e-01       0.173",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>协方差分析</span>"
    ]
  },
  {
    "objectID": "1010-anovaattention.html",
    "href": "1010-anovaattention.html",
    "title": "15  R语言方差分析注意事项",
    "section": "",
    "text": "15.1 均衡设计和非均衡设计\n均衡设计是指不同组别之间的样本量相等，非均衡设计自然就是指不同组别之间样本量不相同。\n医学研究中大部分都是均衡设计。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>R语言方差分析注意事项</span>"
    ]
  },
  {
    "objectID": "1010-anovaattention.html#方差分析的3种类型",
    "href": "1010-anovaattention.html#方差分析的3种类型",
    "title": "15  R语言方差分析注意事项",
    "section": "15.2 方差分析的3种类型",
    "text": "15.2 方差分析的3种类型\n在计算方差分析中的平方和时，有3种类型（你可以简单理解为方差分析有3种类型），SPSS/SAS在做方差分析的时候，默认是类型Ⅲ，但是R语言中的aov()函数做方差分析时，默认是类型Ⅰ。\nR语言中做方差分析是用公式表示的，比如：aov(y ~ A + B + A:B, data = df)。\n\n表达式中效应的顺序在两种情况下会造成影响：(a)因子不止一个，并且是非平衡设计；(b)存在协变量。出现任意一种情况时，等式右边的变量都与其他每个变量相关。此时，我们无法清晰地划分它们对因变量的影响。一般来说，越基础性的效应越需要放在表达式前面。具体来讲，首先是协变量，然后是主效应，接着是双因素的交互项，再接着是三因素的交互项，以此类推。对于主效应，越基础性的变量越应放在表达式前面，因此性别要放在处理方式之前。有一个基本的准则：若研究设计不是正交的（也就是说，因子和/或协变量相关），一定要谨慎设置效应的顺序。–《R语言实战》\n\n也就是说：\n\n如果是均衡设计，3种类型的方差分析没有差别，这也是为什么之前的演示全都和SPSS结果一样的原因！\n如果是非均衡设计，但是只存在组别因素（比如完全随机设计的方差分析），结果也是没有差别的！\n如果是非均衡设计并且有多个因素，或者存在协变量时，3种类型方差分析的结果是不一样的！\n\n3种类型的区别可以参考下面这张图：\n\n\n\n\n\n\n\n\n\nR语言的aov()函数不能更改类型，但是我们通过其他R包实现更改类型。比如car::Anova()或者rstatix包。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>R语言方差分析注意事项</span>"
    ]
  },
  {
    "objectID": "1010-anovaattention.html#示例",
    "href": "1010-anovaattention.html#示例",
    "title": "15  R语言方差分析注意事项",
    "section": "15.3 示例",
    "text": "15.3 示例\n使用3个简单的小例子进行演示。\n\n15.3.1 示例一\n首先是一个单因素均衡设计的例子，来自课本例4-2。\n\ntrt&lt;-c(rep(\"group1\",30),rep(\"group2\",30),rep(\"group3\",30),rep(\"group4\",30))\n\nweight&lt;-c(3.53,4.59,4.34,2.66,3.59,3.13,3.30,4.04,3.53,3.56,3.85,4.07,1.37,\n          3.93,2.33,2.98,4.00,3.55,2.64,2.56,3.50,3.25,2.96,4.30,3.52,3.93,\n          4.19,2.96,4.16,2.59,2.42,3.36,4.32,2.34,2.68,2.95,2.36,2.56,2.52,\n          2.27,2.98,3.72,2.65,2.22,2.90,1.98,2.63,2.86,2.93,2.17,2.72,1.56,\n          3.11,1.81,1.77,2.80,3.57,2.97,4.02,2.31,2.86,2.28,2.39,2.28,2.48,\n          2.28,3.48,2.42,2.41,2.66,3.29,2.70,2.66,3.68,2.65,2.66,2.32,2.61,\n          3.64,2.58,3.65,3.21,2.23,2.32,2.68,3.04,2.81,3.02,1.97,1.68,0.89,\n          1.06,1.08,1.27,1.63,1.89,1.31,2.51,1.88,1.41,3.19,1.92,0.94,2.11,\n          2.81,1.98,1.74,2.16,3.37,2.97,1.69,1.19,2.17,2.28,1.72,2.47,1.02,\n          2.52,2.10,3.71)\n\ndf &lt;- data.frame(trt,weight)\n\nstr(df)\n## 'data.frame':    120 obs. of  2 variables:\n##  $ trt   : chr  \"group1\" \"group1\" \"group1\" \"group1\" ...\n##  $ weight: num  3.53 4.59 4.34 2.66 3.59 3.13 3.3 4.04 3.53 3.56 ...\n\nR语言中的方差分析结果比较：\n\n# 1型\nfit &lt;- aov(weight ~ trt, data = df)\nsummary(fit)\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## trt           3  32.16  10.719   24.88 1.67e-12 ***\n## Residuals   116  49.97   0.431                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2型\ncar::Anova(fit, type=2)\n## Anova Table (Type II tests)\n## \n## Response: weight\n##           Sum Sq  Df F value    Pr(&gt;F)    \n## trt       32.156   3  24.884 1.674e-12 ***\n## Residuals 49.967 116                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 3型\ncar::Anova(fit, type=3)\n## Anova Table (Type III tests)\n## \n## Response: weight\n##             Sum Sq  Df F value    Pr(&gt;F)    \n## (Intercept) 353.02   1 819.537 &lt; 2.2e-16 ***\n## trt          32.16   3  24.884 1.674e-12 ***\n## Residuals    49.97 116                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n可以看到3种类型的结果是完全一样的，没有差别~\n下面我们更改一下样本数量，使其变成非均衡设计：\n\n# 每组样本数量改一下\ntrt&lt;-c(rep(\"group1\",10),rep(\"group2\",50),rep(\"group3\",35),rep(\"group4\",25))\n\nweight&lt;-c(3.53,4.59,4.34,2.66,3.59,3.13,3.30,4.04,3.53,3.56,3.85,4.07,1.37,\n          3.93,2.33,2.98,4.00,3.55,2.64,2.56,3.50,3.25,2.96,4.30,3.52,3.93,\n          4.19,2.96,4.16,2.59,2.42,3.36,4.32,2.34,2.68,2.95,2.36,2.56,2.52,\n          2.27,2.98,3.72,2.65,2.22,2.90,1.98,2.63,2.86,2.93,2.17,2.72,1.56,\n          3.11,1.81,1.77,2.80,3.57,2.97,4.02,2.31,2.86,2.28,2.39,2.28,2.48,\n          2.28,3.48,2.42,2.41,2.66,3.29,2.70,2.66,3.68,2.65,2.66,2.32,2.61,\n          3.64,2.58,3.65,3.21,2.23,2.32,2.68,3.04,2.81,3.02,1.97,1.68,0.89,\n          1.06,1.08,1.27,1.63,1.89,1.31,2.51,1.88,1.41,3.19,1.92,0.94,2.11,\n          2.81,1.98,1.74,2.16,3.37,2.97,1.69,1.19,2.17,2.28,1.72,2.47,1.02,\n          2.52,2.10,3.71)\n\ndf1 &lt;- data.frame(trt,weight)\n\n然后再来看一下3种类型方差分析的结果：\n\n# 1型\nfit1 &lt;- aov(weight ~ trt, data = df1)\nsummary(fit1)\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## trt           3  22.03   7.343   14.17 6.23e-08 ***\n## Residuals   116  60.09   0.518                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2型\ncar::Anova(fit1, type = 2)\n## Anova Table (Type II tests)\n## \n## Response: weight\n##           Sum Sq  Df F value    Pr(&gt;F)    \n## trt       22.029   3  14.174 6.228e-08 ***\n## Residuals 60.094 116                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 3型\ncar::Anova(fit1, type = 3)\n## Anova Table (Type III tests)\n## \n## Response: weight\n##              Sum Sq  Df F value    Pr(&gt;F)    \n## (Intercept) 131.551   1 253.933 &lt; 2.2e-16 ***\n## trt          22.029   3  14.174 6.228e-08 ***\n## Residuals    60.094 116                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n可以看到3种类型的结果也是一样的哦！\n\n\n15.3.2 示例二\n使用一个随机区组设计的方差分析进行演示，示例数据来自课本例4-3的数据。\n首先也是均衡设计的情况：\n\nweight &lt;- c(0.82,0.65,0.51,0.73,0.54,0.23,0.43,0.34,0.28,0.41,0.21,\n            0.31,0.68,0.43,0.24)\nblock &lt;- c(rep(c(\"1\",\"2\",\"3\",\"4\",\"5\"),each=3))\ngroup &lt;- c(rep(c(\"A\",\"B\",\"C\"),5))\n\ndata4_4 &lt;- data.frame(weight,block,group)\n\nstr(data4_4)\n## 'data.frame':    15 obs. of  3 variables:\n##  $ weight: num  0.82 0.65 0.51 0.73 0.54 0.23 0.43 0.34 0.28 0.41 ...\n##  $ block : chr  \"1\" \"1\" \"1\" \"2\" ...\n##  $ group : chr  \"A\" \"B\" \"C\" \"A\" ...\n\n下面是3种类型方差分析的结果，由于是均衡设计，3种类型没有任何区别，并且即使你把区组因素和分组因素的位置互换，也不会有任何差别哦！\n\n# 1型\nfit &lt;- aov(weight ~ block + group, data = data4_4)\nsummary(fit)\n##             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \n## block        4 0.2284 0.05709   5.978 0.01579 * \n## group        2 0.2280 0.11400  11.937 0.00397 **\n## Residuals    8 0.0764 0.00955                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2型\ncar::Anova(fit, type = 2)\n## Anova Table (Type II tests)\n## \n## Response: weight\n##            Sum Sq Df F value   Pr(&gt;F)   \n## block     0.22836  4   5.978 0.015787 * \n## group     0.22800  2  11.937 0.003968 **\n## Residuals 0.07640  8                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 3型\ncar::Anova(fit, type = 3)\n## Anova Table (Type III tests)\n## \n## Response: weight\n##              Sum Sq Df F value    Pr(&gt;F)    \n## (Intercept) 1.44086  1 150.875 1.794e-06 ***\n## block       0.22836  4   5.978  0.015787 *  \n## group       0.22800  2  11.937  0.003968 ** \n## Residuals   0.07640  8                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n下面给它改成非均衡设计：\n\nweight &lt;- c(0.82,0.65,0.51,0.73,0.54,0.23,0.43,0.34,0.28,0.41,0.21,\n            0.31,0.68,0.43,0.24)\nblock &lt;- c(rep(c(\"1\",\"2\",\"3\",\"4\",\"5\"),each=3))\n\n# 每组样本量不一样\ngroup &lt;- c(rep(c(\"A\"),2),rep(\"B\",9),rep(\"C\",4))\n\ndata4_4 &lt;- data.frame(weight,block,group)\n\n下面再看一下3种类型方差分析的结果：\n\n# 1型\nfit1 &lt;- aov(weight ~ block + group, data = data4_4)\nsummary(fit)\n##             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \n## block        4 0.2284 0.05709   5.978 0.01579 * \n## group        2 0.2280 0.11400  11.937 0.00397 **\n## Residuals    8 0.0764 0.00955                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2型\ncar::Anova(fit1, type = 2)\n## Anova Table (Type II tests)\n## \n## Response: weight\n##             Sum Sq Df F value Pr(&gt;F)\n## block     0.079789  4  0.5896 0.6798\n## group     0.033750  2  0.4988 0.6250\n## Residuals 0.270650  8\n\n# 3型\ncar::Anova(fit1, type = 3)\n## Anova Table (Type III tests)\n## \n## Response: weight\n##              Sum Sq Df F value    Pr(&gt;F)    \n## (Intercept) 1.08045  1 31.9364 0.0004807 ***\n## block       0.07979  4  0.5896 0.6798021    \n## group       0.03375  2  0.4988 0.6249619    \n## Residuals   0.27065  8                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n可以看到结果完全不一样了哦！\n\n\n15.3.3 协方差分析\n就用一个简单的完全随机设计资料的协方差分析进行演示，示例数据来自课本例13-1。\n\ndf13_1 &lt;- data.frame(x1=c(10.8,11.6,10.6,9.0,11.2,9.9,10.6,10.4,9.6,10.5,\n                          10.6,9.9,9.5,9.7,10.7,9.2,10.5,11.0,10.1,10.7,8.5,10.0,\n                          10.4,9.7,9.4,9.2,10.5,11.2,9.6,8.0),\n                     y1=c(9.4,9.7,8.7,7.2,10.0,8.5,8.3,8.1,8.5,9.1,9.2,8.4,\n                          7.6,7.9,8.8,7.4,8.6,9.2,8.0,8.5,7.3,\n                          8.3,8.6,8.7,7.6,8.0,8.8,9.5,8.2,7.2),\n                     x2=c(10.4,9.7,9.9,9.8,11.1,8.2,8.8,10.0,9.0,9.4,\n                          8.9,10.3,9.3,9.2,10.9,9.2,9.2,10.4,11.2,\n                          11.1,11.0,8.6,9.3,10.3,10.3,9.8,10.5,10.7,10.4,9.4),\n                     y2=c(9.2,9.1,8.9,8.6,9.9,7.1,7.8,7.9,8.0,9.0,7.9,8.9,\n                          8.9,8.1,10.2,8.5,9.0,8.9,9.8,10.1,8.5,\n                          8.1,8.6,8.9,9.6,8.1,9.9,9.3,8.7,8.7),\n                     x3=c(9.8,11.2,10.7,9.6,10.1,9.8,10.1,10.3,11.0,10.5,9.2,\n                          10.1,10.4,10.0,8.4,10.1,9.3,10.5,11.1,10.5,\n                          9.7,9.2,9.3,10.4,10.0,10.3,9.9,9.4,8.3,9.2),\n                     y3=c(7.6,7.9,9.0,7.8,8.5,7.5,8.3,8.2,8.4,8.1,7.0,7.7,8.0,\n                          6.6,6.1,8.1,7.8,8.4,8.2,8.0,7.6,6.9,\n                          6.7,8.1,7.4,8.2,7.6,7.8,6.6,7.2)\n                     )\n\nsuppressPackageStartupMessages(library(tidyverse))\n\ndf13_11 &lt;- df13_1 %&gt;% \n  pivot_longer(cols = everything(), # 变长\n               names_to = c(\".value\",\"group\"),\n               names_pattern = \"(.)(.)\"\n               ) %&gt;% \n  mutate(group = as.factor(group)) # 组别变为因子型\n\nglimpse(df13_11)\n## Rows: 90\n## Columns: 3\n## $ group &lt;fct&gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1…\n## $ x     &lt;dbl&gt; 10.8, 10.4, 9.8, 11.6, 9.7, 11.2, 10.6, 9.9, 10.7, 9.0, 9.8, 9.6…\n## $ y     &lt;dbl&gt; 9.4, 9.2, 7.6, 9.7, 9.1, 7.9, 8.7, 8.9, 9.0, 7.2, 8.6, 7.8, 10.0…\n\ngroup是分组因素，x是协变量，y是因变量。\n下面进行3种类型的协方差分析：\n\n# 1型\nfit &lt;- aov(y ~ x + group, data = df13_11) \nsummary(fit)\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## x            1  29.06  29.057  171.20 &lt;2e-16 ***\n## group        2  19.85   9.925   58.48 &lt;2e-16 ***\n## Residuals   86  14.60   0.170                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2型\ncar::Anova(fit, type = 2)\n## Anova Table (Type II tests)\n## \n## Response: y\n##           Sum Sq Df F value    Pr(&gt;F)    \n## x         30.183  1  177.83 &lt; 2.2e-16 ***\n## group     19.851  2   58.48 &lt; 2.2e-16 ***\n## Residuals 14.596 86                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 3型\ncar::Anova(fit, type = 3)\n## Anova Table (Type III tests)\n## \n## Response: y\n##              Sum Sq Df  F value Pr(&gt;F)    \n## (Intercept)  0.3818  1   2.2493 0.1373    \n## x           30.1830  1 177.8346 &lt;2e-16 ***\n## group       19.8510  2  58.4798 &lt;2e-16 ***\n## Residuals   14.5963 86                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n可以看到在有协变量的情况下，即使是均衡设计，1型和2型、3型之间的平方和及F值也是有差异的。\n如果你很细心，你可能会发现R中进行单因素协方差分析（ANCOVA）的公式写法和随机区组的anova一模一样！那R语言怎么知道我们是要进行ancova还是随机区组的anova呢？\n很简单，在这里x作为协变量，是数值型，所以R默认会进行ancova，如果是因子型或者字符型，R会默认进行随机区组的anova，比如上面那个随机区组的例子！你会不会觉得这种识别方法过于粗糙了？还真不是，因为协方差分析的应用条件中也有规定：\n\n协方差分析要求协变量是连续变量，而且不能是影响处理的变量。–《医学统计学》P201\n\n所以，一切规定，皆有缘由，都是细节！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>R语言方差分析注意事项</span>"
    ]
  },
  {
    "objectID": "1017-multireg.html",
    "href": "1017-multireg.html",
    "title": "16  多元线性回归",
    "section": "",
    "text": "16.1 多元线性回归\n使用课本例15-1的数据，手动摘录：\ndf15_1 &lt;- data.frame(\n  cho = c(5.68,3.79,6.02,4.85,4.60,6.05,4.90,7.08,3.85,4.65,4.59,4.29,7.97,\n      6.19,6.13,5.71,6.40,6.06,5.09,6.13,5.78,5.43,6.50,7.98,11.54,5.84,\n      3.84),\n  tg = c(1.90,1.64,3.56,1.07,2.32,0.64,8.50,3.00,2.11,0.63,1.97,1.97,1.93,\n      1.18,2.06,1.78,2.40,3.67,1.03,1.71,3.36,1.13,6.21,7.92,10.89,0.92,\n      1.20),\n  ri = c(4.53, 7.32,6.95,5.88,4.05,1.42,12.60,6.75,16.28,6.59,3.61,6.61,7.57,\n      1.42,10.35,8.53,4.53,12.79,2.53,5.28,2.96,4.31,3.47,3.37,1.20,8.61,\n      6.45),\n  hba = c(8.2,6.9,10.8,8.3,7.5,13.6,8.5,11.5,7.9,7.1,8.7,7.8,9.9,6.9,10.5,8.0,\n      10.3,7.1,8.9,9.9,8.0,11.3,12.3,9.8,10.5,6.4,9.6),\n  fpg = c(11.2,8.8,12.3,11.6,13.4,18.3,11.1,12.1,9.6,8.4,9.3,10.6,8.4,9.6,10.9,\n     10.1,14.8,9.1,10.8,10.2,13.6,14.9,16.0,13.2,20.0,13.3,10.4)\n  )\n\nstr(df15_1)\n## 'data.frame':    27 obs. of  5 variables:\n##  $ cho: num  5.68 3.79 6.02 4.85 4.6 6.05 4.9 7.08 3.85 4.65 ...\n##  $ tg : num  1.9 1.64 3.56 1.07 2.32 0.64 8.5 3 2.11 0.63 ...\n##  $ ri : num  4.53 7.32 6.95 5.88 4.05 ...\n##  $ hba: num  8.2 6.9 10.8 8.3 7.5 13.6 8.5 11.5 7.9 7.1 ...\n##  $ fpg: num  11.2 8.8 12.3 11.6 13.4 18.3 11.1 12.1 9.6 8.4 ...\nhead(df15_1)\n##    cho   tg   ri  hba  fpg\n## 1 5.68 1.90 4.53  8.2 11.2\n## 2 3.79 1.64 7.32  6.9  8.8\n## 3 6.02 3.56 6.95 10.8 12.3\n## 4 4.85 1.07 5.88  8.3 11.6\n## 5 4.60 2.32 4.05  7.5 13.4\n## 6 6.05 0.64 1.42 13.6 18.3\n数据一共5列，第1列是总胆固醇，第2列是甘油三酯，第3列是胰岛素，第4列是糖化血红蛋白，第5列是空腹血糖（因变量）。\n在建立回归方程前，先简单探索下数据：\nlibrary(GGally)\n\nggpairs(df15_1) + theme_bw()\n从这幅图来看，血糖和糖化血红蛋白相关性最大，和甘油三酯关系最小。\n接下来建立回归方程：\nf &lt;- lm(fpg ~ cho + tg + ri + hba, data = df15_1)\n\nsummary(f)\n## \n## Call:\n## lm(formula = fpg ~ cho + tg + ri + hba, data = df15_1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.6268 -1.2004 -0.2276  1.5389  4.4467 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept)   5.9433     2.8286   2.101   0.0473 *\n## cho           0.1424     0.3657   0.390   0.7006  \n## tg            0.3515     0.2042   1.721   0.0993 .\n## ri           -0.2706     0.1214  -2.229   0.0363 *\n## hba           0.6382     0.2433   2.623   0.0155 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.01 on 22 degrees of freedom\n## Multiple R-squared:  0.6008, Adjusted R-squared:  0.5282 \n## F-statistic: 8.278 on 4 and 22 DF,  p-value: 0.0003121\n这个结果信息很丰富，给出了截距，各自变量的系数以及标准误、t值、P值，最下方给出了决定系数R2，调整后的R2，F值，总体方程的P值等。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "1017-multireg.html#模型评价",
    "href": "1017-multireg.html#模型评价",
    "title": "16  多元线性回归",
    "section": "16.2 模型评价",
    "text": "16.2 模型评价\n回归模型可以通过R2、AIC、BIC、RMSE等评价，R2范围在0~1之间，越接近1说明结果越好。AIC、BIC、RMSE是越小越好。\n\nlibrary(performance)\nr2(f)\n## # R2 for Linear Regression\n##        R2: 0.601\n##   adj. R2: 0.528\nAIC(f)\n## [1] 120.78\nBIC(f)\n## [1] 128.5551\nrmse(f)\n## [1] 1.81395\n\n或者直接输出所有结果：\n\nmodel_performance(f)\n## # Indices of model performance\n## \n## AIC     |    AICc |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n## ---------------------------------------------------------------\n## 120.780 | 124.980 | 128.555 | 0.601 |     0.528 | 1.814 | 2.010",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "1017-multireg.html#回归诊断",
    "href": "1017-multireg.html#回归诊断",
    "title": "16  多元线性回归",
    "section": "16.3 回归诊断",
    "text": "16.3 回归诊断\n判断数据是否满足多元线性回归的条件，也就是4个条件：\n\n正态性\n独立性\n等方差性\n线性\n\n\n16.3.1 看图判断\n\nopar &lt;- par(mfrow = c(2,2))\nplot(f)\n\n\n\n\n\n\n\npar(opar)\n\n\n第1幅图（左上）是残差拟合图，展示真实残差和拟合残差的关系，判读是否满足线性这个条件，如果满足，则应该为一条直线，但是本图明显是一条曲线，说明不是很满足线性这个条件，可能需要加二次项。\n第2幅图（右上）是正态Q-Q图，判断是否满足正态性这个条件，通过这个图来看，基本满足。\n第3幅图（左下）是位置尺度图，判读是否满足同方差性，如果满足，水平线两侧的点应该随机分布，从此图来看基本满足。\n第4幅图（右下）是残差杠杆图，用于识别离群点等。\n\n上面是比较原始的方法，下面介绍一个非常现代化的R包，用于实现以上图形：\n\nlibrary(performance)\ncheck_model(f)\n\n\n\n\n\n\n\n\n是不是更加好看了呢？\n这几个图也可以单独画出来，使用以下代码即可：\n\ndiagnostic_plots &lt;- plot(check_model(f, panel = FALSE))\n\n首先看第一个图。这个图是基于check_predictions()函数的，属于事后检验，是检查真实数据和模型数据的拟合情况的。下图中绿色粗线是真实的预测变量的分布情况，蓝色线条表示模拟的分布，理想的情况应该是完全重合的。从下图来看，其实是有些问题的，这说明我们用的模型可能不太合适。\n\ndiagnostic_plots[[1]]\n\n\n\n\n\n\n\n\n下面看第2张图。这张图是检查预测变量和结果变量是否符合线性关系的。合理的情况是残差完全随机地分布在参考线两侧。从这张图来看我们的数据其实不太完美。\n\ndiagnostic_plots[[2]]\n\n\n\n\n\n\n\n\n下面是第3幅图，是用来检查方差齐性的，同上面介绍过的位置尺度图。\n\ndiagnostic_plots[[3]]\n\n\n\n\n\n\n\n\n第4幅图是用来观察强影响点或者离群值、异常值的。使用的是库克距离（cook’s-distance）来计算的，图中在虚线（库克距离）外的点可被认为是异常值。\n\ndiagnostic_plots[[4]]\n\n\n\n\n\n\n\n\n第5幅图是关于多重共线性的。是通过方差膨胀因子来评价的，下图中展示了4个变量的VIF，基本都在3以下，可认为不存在多重共线性：\n\ndiagnostic_plots[[5]]\n\n\n\n\n\n\n\n\n第6幅图是看正态性的。理想情况下数据点应该均匀的分布在横线上，最好是和横线重合，尤其是尾部，我们这个数据还算可以。\n\ndiagnostic_plots[[6]]\n\n\n\n\n\n\n\n\n\n\n16.3.2 统计方法验证\n也可以通过统计方法判断，比如gvlma包可以实现对线性模型的综合判断：\n\nlibrary(gvlma)\ngvmodel&lt;-gvlma(f)\nsummary(gvmodel)\n## \n## Call:\n## lm(formula = fpg ~ cho + tg + ri + hba, data = df15_1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.6268 -1.2004 -0.2276  1.5389  4.4467 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept)   5.9433     2.8286   2.101   0.0473 *\n## cho           0.1424     0.3657   0.390   0.7006  \n## tg            0.3515     0.2042   1.721   0.0993 .\n## ri           -0.2706     0.1214  -2.229   0.0363 *\n## hba           0.6382     0.2433   2.623   0.0155 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.01 on 22 degrees of freedom\n## Multiple R-squared:  0.6008, Adjusted R-squared:  0.5282 \n## F-statistic: 8.278 on 4 and 22 DF,  p-value: 0.0003121\n## \n## \n## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\n## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\n## Level of Significance =  0.05 \n## \n## Call:\n##  gvlma(x = f) \n## \n##                      Value  p-value                   Decision\n## Global Stat        9.68910 0.046003 Assumptions NOT satisfied!\n## Skewness           0.65344 0.418886    Assumptions acceptable.\n## Kurtosis           0.04015 0.841193    Assumptions acceptable.\n## Link Function      7.68064 0.005582 Assumptions NOT satisfied!\n## Heteroscedasticity 1.31487 0.251515    Assumptions acceptable.\n\n\n全局统计量：粗略估计结果变量和预测变量是否符合线性关系，结果是不符合\n偏度和峰度：检验残差分布，结果是符合\n连接函数：检测结果变量是连续型还是二分类，结果是不符合\n异方差：检验方差齐性，结果是符合\n\n以上是多个条件一起输出判断，也可以针对单独的条件进行判断。\n首先看下正态性的判断。\n\nlibrary(car)\n\n# 验证正态性\nqqPlot(f,labels = row.names(df15_1), id.method = \"identify\",simulate = T,\n       main = \"Q-Q plot\")   \n\n\n\n\n\n\n\n## [1] 13 26\n\n从图中可看出正态性基本满足。\n当然也可以使用非常好用的performance包实现：\n\ncheck_normality(f)\n## OK: residuals appear as normally distributed (p = 0.671).\n\n检测离群值，基于cook距离：\n\ncheck_outliers(f)\n## 1 outlier detected: case 25.\n## - Based on the following method and threshold: cook (0.9).\n## - For variable: (Whole model).\n\n检测残差（或者因变量）独立性：\n\nset.seed(123)\ncheck_autocorrelation(f)\n## OK: Residuals appear to be independent and not autocorrelated (p = 0.296).\n\n或者通过car包：\n\nset.seed(123)\n# 验证因变量独立性\ndurbinWatsonTest(f)   \n##  lag Autocorrelation D-W Statistic p-value\n##    1       0.1778885      1.634654   0.296\n##  Alternative hypothesis: rho != 0\n\nP值大于0.05，满足条件。\n\n# 验证线性\ncrPlots(f)\n\n\n\n\n\n\n\n\n通过观察成分残差图，线性基本满足。\n下面是检测方差齐性：\n\n# 验证方差齐性\nncvTest(f)   \n## Non-constant Variance Score Test \n## Variance formula: ~ fitted.values \n## Chisquare = 0.0004274839, Df = 1, p = 0.9835\n\nP值大于0.05，方差齐性满足。\n方差齐性检验也可以通过performance包实现：\n\n# performance检测方差齐性\ncheck_heteroscedasticity(f)\n## OK: Error variance appears to be homoscedastic (p = 0.984).\n\n\n\n16.3.3 多重共线性的检验\n下面是多重共线性的检验，通过计算方差膨胀因子检验。\n\nvif(f)\n##      cho       tg       ri      hba \n## 2.185539 1.779862 1.278364 1.266730\nvif(f)&gt;4\n##   cho    tg    ri   hba \n## FALSE FALSE FALSE FALSE\n\n都小于4（标准有争议），基本不存在多重共线性。\n或者通过performance实现：\n\ncheck_collinearity(f)\n## # Check for Multicollinearity\n## \n## Low Correlation\n## \n##  Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n##   cho 2.19 [1.54, 3.62]         1.48      0.46     [0.28, 0.65]\n##    tg 1.78 [1.31, 2.95]         1.33      0.56     [0.34, 0.76]\n##    ri 1.28 [1.06, 2.32]         1.13      0.78     [0.43, 0.94]\n##   hba 1.27 [1.05, 2.32]         1.13      0.79     [0.43, 0.95]\n\n下面是两种自变量选择的方法，介绍的较为简单，更加详细的介绍请参考R语言实战临床预测模型中的变量选择合集，其中详细介绍了多种方法，如：\n\n逐步回归法\n最优子集法\n先单后多法\n……",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "1017-multireg.html#逐步选择法",
    "href": "1017-multireg.html#逐步选择法",
    "title": "16  多元线性回归",
    "section": "16.4 逐步选择法",
    "text": "16.4 逐步选择法\n向后回归：\n\nlibrary(MASS)\nstepAIC(f, direction = \"backward\")\n## Start:  AIC=42.16\n## fpg ~ cho + tg + ri + hba\n## \n##        Df Sum of Sq     RSS    AIC\n## - cho   1    0.6129  89.454 40.343\n## &lt;none&gt;               88.841 42.157\n## - tg    1   11.9627 100.804 43.568\n## - ri    1   20.0635 108.905 45.655\n## - hba   1   27.7939 116.635 47.507\n## \n## Step:  AIC=40.34\n## fpg ~ tg + ri + hba\n## \n##        Df Sum of Sq     RSS    AIC\n## &lt;none&gt;               89.454 40.343\n## - ri    1    25.690 115.144 45.159\n## - tg    1    26.530 115.984 45.356\n## - hba   1    32.269 121.723 46.660\n## \n## Call:\n## lm(formula = fpg ~ tg + ri + hba, data = df15_1)\n## \n## Coefficients:\n## (Intercept)           tg           ri          hba  \n##      6.4996       0.4023      -0.2870       0.6632\n\n向前回归：\n\nstepAIC(f, direction = \"forward\")\n## Start:  AIC=42.16\n## fpg ~ cho + tg + ri + hba\n## \n## Call:\n## lm(formula = fpg ~ cho + tg + ri + hba, data = df15_1)\n## \n## Coefficients:\n## (Intercept)          cho           tg           ri          hba  \n##      5.9433       0.1424       0.3515      -0.2706       0.6382\n\n逐步回归：\n\nstepAIC(f, direction = \"both\")\n## Start:  AIC=42.16\n## fpg ~ cho + tg + ri + hba\n## \n##        Df Sum of Sq     RSS    AIC\n## - cho   1    0.6129  89.454 40.343\n## &lt;none&gt;               88.841 42.157\n## - tg    1   11.9627 100.804 43.568\n## - ri    1   20.0635 108.905 45.655\n## - hba   1   27.7939 116.635 47.507\n## \n## Step:  AIC=40.34\n## fpg ~ tg + ri + hba\n## \n##        Df Sum of Sq     RSS    AIC\n## &lt;none&gt;               89.454 40.343\n## + cho   1     0.613  88.841 42.157\n## - ri    1    25.690 115.144 45.159\n## - tg    1    26.530 115.984 45.356\n## - hba   1    32.269 121.723 46.660\n## \n## Call:\n## lm(formula = fpg ~ tg + ri + hba, data = df15_1)\n## \n## Coefficients:\n## (Intercept)           tg           ri          hba  \n##      6.4996       0.4023      -0.2870       0.6632",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "1017-multireg.html#全局择优法",
    "href": "1017-multireg.html#全局择优法",
    "title": "16  多元线性回归",
    "section": "16.5 全局择优法",
    "text": "16.5 全局择优法\n也就是全子集回归法。\n\nlibrary(leaps)\nleaps &lt;- regsubsets(fpg ~ cho + tg + ri + hba, data = df15_1, nbest=4)\n\nplot(leaps, scale = \"Cp\") # 通过Cp判断\n\n\n\n\n\n\n\n\nCp是越小越好的，从上面这幅图来看，纳入3个自变量（tg/ri/hba）时最好。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "1018-logistic.html",
    "href": "1018-logistic.html",
    "title": "17  Logistic回归",
    "section": "",
    "text": "17.1 二项logistic回归\nR语言中的factor()函数可以把变量变为因子类型，默认是没有等级之分的（可以理解为无序分类变量nominal）！当然也可以通过添加参数ordered=T变成有序因子（等级资料，有序分类ordinal）。\n因变量是二分类变量时，可以使用二项逻辑回归（binomial logistic regression），自变量可以是数值变量、无序多分类变量、有序多分类变量。\n本次数据使用孙振球版《医学统计学》第4版例16-2的数据，直接读取。\n为了探讨冠心病发生的危险因素，对26例冠心病患者和28例对照者进行病例-对照研究，试用逻辑回归筛选危险因素。\ndf16_2 &lt;- foreign::read.spss(\"datasets/例16-02.sav\", \n                             to.data.frame = T,\n                             use.value.labels = F,\n                             reencode  = \"utf-8\")\n\nstr(df16_2)\n## 'data.frame':    54 obs. of  11 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : num  3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : num  1 0 1 0 0 0 0 0 0 0 ...\n##  $ x3   : num  0 1 0 0 0 1 1 1 0 0 ...\n##  $ x4   : num  1 1 1 1 1 1 0 1 0 1 ...\n##  $ x5   : num  0 0 0 0 0 0 0 1 0 0 ...\n##  $ x6   : num  0 0 0 0 1 0 0 0 0 0 ...\n##  $ x7   : num  1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : num  1 0 0 0 1 1 0 0 1 0 ...\n##  $ y    : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n数据一共11列，第1列是编号，第2-9列是自变量，第10列是因变量。\n具体说明： - x1：年龄，小于45岁是1,45-55是2,55-65是3,65以上是4； - x2：高血压病史，1代表有，0代表无； - x3：高血压家族史，1代表有，0代表无； - x4：吸烟，1代表吸烟，0代表不吸烟； - x5：高血脂病史，1代表有，0代表无； - x6：动物脂肪摄入，0表示低，1表示高 - x7：BMI，小于24是1,24-26是2，大于26是3； - x8：A型性格，1代表是，0代表否； - y：是否是冠心病，1代表是，0代表否\n这里的x1~y虽然是数值型，但并不是真的代表数字大小，只是为了方便标识，进行了转换，因此在进行logistic回归之前，我们要把数值型变量变成无序分类或有序分类变量，在R语言中可以通过factor()函数变成因子型实现。\n# 变成因子型\ndf16_2[,c(2:10)] &lt;- lapply(df16_2[,c(2:10)], factor)\nstr(df16_2)\n## 'data.frame':    54 obs. of  11 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 1 1 1 ...\n##  $ x3   : Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 2 2 2 1 1 ...\n##  $ x4   : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 1 2 1 2 ...\n##  $ x5   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n##  $ x6   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 1 1 1 ...\n##  $ x7   : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 2 1 1 2 1 ...\n##  $ y    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n需要注意的是自变量x1和x7，这两个应该是有序分类变量，这种自变量在进行逻辑回归时，可以进行哑变量设置，即给定一个参考，让其他所有组都和参考相比，比如这里，我们把x1变成因子型后，R语言在进行logistic回归时，会默认选择第一个为参考。\n接下来进行二项逻辑回归，在R语言中，默认是以因子的第一个为参考的，不仅是自变量，因变量也是如此！ 和SPSS的默认方式不太一样。\nf &lt;- glm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, \n         data = df16_2, \n         family = binomial())\n\nsummary(f)\n## \n## Call:\n## glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, family = binomial(), \n##     data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -5.46026    2.07370  -2.633  0.00846 **\n## x12          0.85285    1.54399   0.552  0.58070   \n## x13          0.47754    1.59320   0.300  0.76438   \n## x14          3.44227    2.10985   1.632  0.10278   \n## x21          1.14905    0.93176   1.233  0.21750   \n## x31          1.66039    1.16857   1.421  0.15535   \n## x41          0.85994    1.32437   0.649  0.51613   \n## x51          0.73600    0.97088   0.758  0.44840   \n## x61          3.92067    1.57004   2.497  0.01252 * \n## x72         -0.03467    1.13363  -0.031  0.97560   \n## x73         -0.38230    1.61710  -0.236  0.81311   \n## x81          2.46322    1.10484   2.229  0.02578 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 40.028  on 42  degrees of freedom\n## AIC: 64.028\n## \n## Number of Fisher Scoring iterations: 6\n结果详解：\nDeviance Residuals:表示偏差残差统计量。在理想情况下应服从正态分布，均值应为0。\n在此例中，中位数的符号为负（-0.01406），表明整体向左偏移，中位数的大小表明偏移的程度。第一个四分位数（1Q）和第三个四分位数（3Q）为两侧“尾巴”分布的幅度。这里3Q大于1Q（绝对值），表明这个曲线是向右倾斜的。最大和最小残差可用来检验数据中的离群值。\n结果中Estimate是回归系数和截距，Std. Error表示回归系数的标准误，z value是统计量值（z的平方就是Wald值），Pr(&gt;|z|)是P值。\nβ值（这里就是Estimate）是指回归系数和截距（常数项），可以是负数（负相关时回归系数出现负值）；\nOR是比值比（odds ratio），OR = exp(β)，其取值范围是0至正无穷，不可能是负数；\nWald是一个卡方值，等于β除以它的标准误（这里是Std. Error），然后取平方（也就是z值的平方），因此也不可能是负数。Wald用于对β值进行检验，考察β值是否等于0。若β值等于0，其对应的OR值，也就是Exp(β)为1，表明两组没有显著差异。Wald值越大，β值越不可能等于0。\n结果中出现了x12/x13/x14这种，这是因为R语言在做回归时，如果设置了哑变量，默认是以第一个为参考的，其余都是和第一个进行比较，这也是R中自动进行哑变量编码的方式。\nNull deviance:无效偏差（零偏差），Residual deviance:残差偏差，无效偏差和残差偏差之间的差异越大越好，用来评价模型与实际数据的吻合情况。\nAIC：赤池信息准则，表示模型拟合程度的好坏，AIC越低表示模型拟合越好。\n最后还有一个Fisher Scoring的迭代次数，这个和最大似然函数的计算有关。\n我们可以通过函数的方式分别获取模型信息。\n# β值\ncoefficients(f)\n## (Intercept)         x12         x13         x14         x21         x31 \n## -5.46025547  0.85285212  0.47754497  3.44227124  1.14905003  1.66039360 \n##         x41         x51         x61         x72         x73         x81 \n##  0.85994185  0.73600239  3.92067487 -0.03467497 -0.38230011  2.46321944\n\n# β值\ncoef(f)\n## (Intercept)         x12         x13         x14         x21         x31 \n## -5.46025547  0.85285212  0.47754497  3.44227124  1.14905003  1.66039360 \n##         x41         x51         x61         x72         x73         x81 \n##  0.85994185  0.73600239  3.92067487 -0.03467497 -0.38230011  2.46321944\n\n# β值的95%可信区间\nconfint(f)\n##                   2.5 %    97.5 %\n## (Intercept) -10.3696980 -1.983104\n## x12          -2.0317236  4.405067\n## x13          -2.5429244  4.085370\n## x14          -0.2319302  8.343123\n## x21          -0.6458070  3.099838\n## x31          -0.5431686  4.205175\n## x41          -1.6713365  3.801261\n## x51          -1.1846658  2.725051\n## x61           1.3290533  7.677657\n## x72          -2.3618580  2.224863\n## x73          -3.8303437  2.725470\n## x81           0.5105394  4.997352\n\n# OR值\nexp(coef(f))\n##  (Intercept)          x12          x13          x14          x21          x31 \n##  0.004252469  2.346329320  1.612111759 31.257871683  3.155194147  5.261381340 \n##          x41          x51          x61          x72          x73          x81 \n##  2.363023282  2.087573511 50.434470096  0.965919321  0.682290259 11.742555242\n\n# OR值的95%的可信区间\nexp(confint(f))\n##                    2.5 %       97.5 %\n## (Intercept) 3.136876e-05    0.1376413\n## x12         1.311093e-01   81.8646261\n## x13         7.863610e-02   59.4639513\n## x14         7.930015e-01 4201.1887167\n## x21         5.242393e-01   22.1943589\n## x31         5.809047e-01   67.0323349\n## x41         1.879956e-01   44.7576059\n## x51         3.058484e-01   15.2571993\n## x61         3.777465e+00 2159.5535363\n## x72         9.424495e-02    9.2522177\n## x73         2.170216e-02   15.2635868\n## x81         1.666190e+00  148.0206875\n这里x21的OR值是2.346329320，代表：45~55岁的人群患冠心病的风险是小于45岁人群的2.346329320倍，但是这个结果并没有统计学意义！\n# Wald值\nsummary(f)$coefficients[,3]^2\n## (Intercept)         x12         x13         x14         x21         x31 \n## 6.933188870 0.305111544 0.089843733 2.661883233 1.520790277 2.018903576 \n##         x41         x51         x61         x72         x73         x81 \n## 0.421615676 0.574682148 6.235929079 0.000935592 0.055890396 4.970577395\n\n# P值\nsummary(f)$coefficients[,4]\n## (Intercept)         x12         x13         x14         x21         x31 \n##  0.00846107  0.58069555  0.76437591  0.10277898  0.21749994  0.15535128 \n##         x41         x51         x61         x72         x73         x81 \n##  0.51613195  0.44840433  0.01251839  0.97559855  0.81311338  0.02578204\n\n# 预测值\nfitted(f) # 或者 predict(f,type = \"response\")\n##           1           2           3           4           5           6 \n## 0.375076515 0.110360122 0.069240725 0.023034427 0.905605901 0.491543165 \n##           7           8           9          10          11          12 \n## 0.049878030 0.151052146 0.104875967 0.009948712 0.208062753 0.046013662 \n##          13          14          15          16          17          18 \n## 0.009879122 0.497751927 0.500211100 0.074509703 0.023034427 0.105543397 \n##          19          20          21          22          23          24 \n## 0.359548891 0.441102099 0.048627400 0.734770361 0.366272916 0.009879122 \n##          25          26          27          28          29          30 \n## 0.049878030 0.366272916 0.009879122 0.101665098 0.995553588 0.950848767 \n##          31          32          33          34          35          36 \n## 0.712839656 0.995611072 0.216828996 0.984826081 0.543195397 0.905612594 \n##          37          38          39          40          41          42 \n## 0.868286980 0.993760333 0.868286980 0.034813473 0.902606657 0.966930037 \n##          43          44          45          46          47          48 \n## 0.375076515 0.964725296 0.840087511 0.818110300 0.881331876 0.676305952 \n##          49          50          51          52          53          54 \n## 0.780828686 0.555921773 0.986103872 0.816157300 0.466253375 0.655579178\n\n# 偏差\ndeviance(f)\n## [1] 40.02758\n\n# 残差自由度\ndf.residual(f)\n## [1] 42\n\n# 伪R^2\nDescTools::PseudoR2(f, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n##   McFadden   CoxSnell Nagelkerke \n##  0.4647704  0.4746397  0.6331426\n这里需要说明以下fitted(f)，也就是predict(f,type = \"response\")得到的结果是预测概率，范围是0-1之间的。\n对于logistic回归来说，如果不使用type函数，默认是type = \"link\"，返回的是logit(P)的值。\n# 默认返回logit(P)的值\npredict(f)\n\n# 返回概率\npredict(f, type = \"response\")\n模型整体的假设检验：\n# 先构建一个只有截距的模型\nf0 &lt;- glm(y ~ 1, data = df16_2, family = binomial())\n\n# 原模型和这个空模型进行比较\nanova(f0,f,test=\"Chisq\")\n## Analysis of Deviance Table\n## \n## Model 1: y ~ 1\n## Model 2: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\n##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n## 1        53     74.786                          \n## 2        42     40.028 11   34.758 0.0002716 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nP&lt;0.001，说明我们的模型是有意义的。\n逐步回归法的logistic回归，可以使用step()函数：\n# 向前\nf1 &lt;- step(f, direction = \"forward\")\n## Start:  AIC=64.03\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\nsummary(f1)\n## \n## Call:\n## glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, family = binomial(), \n##     data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -5.46026    2.07370  -2.633  0.00846 **\n## x12          0.85285    1.54399   0.552  0.58070   \n## x13          0.47754    1.59320   0.300  0.76438   \n## x14          3.44227    2.10985   1.632  0.10278   \n## x21          1.14905    0.93176   1.233  0.21750   \n## x31          1.66039    1.16857   1.421  0.15535   \n## x41          0.85994    1.32437   0.649  0.51613   \n## x51          0.73600    0.97088   0.758  0.44840   \n## x61          3.92067    1.57004   2.497  0.01252 * \n## x72         -0.03467    1.13363  -0.031  0.97560   \n## x73         -0.38230    1.61710  -0.236  0.81311   \n## x81          2.46322    1.10484   2.229  0.02578 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 40.028  on 42  degrees of freedom\n## AIC: 64.028\n## \n## Number of Fisher Scoring iterations: 6\n\n# 向后\nf2 &lt;- step(f, direction = \"backward\")\n## Start:  AIC=64.03\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\n## \n##        Df Deviance    AIC\n## - x7    2   40.086 60.086\n## - x1    3   43.933 61.933\n## - x4    1   40.466 62.466\n## - x5    1   40.605 62.605\n## - x2    1   41.600 63.600\n## &lt;none&gt;      40.028 64.028\n## - x3    1   42.196 64.196\n## - x8    1   46.365 68.365\n## - x6    1   50.469 72.469\n## \n## Step:  AIC=60.09\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x1    3   44.541 58.541\n## - x5    1   40.611 58.611\n## - x4    1   40.814 58.814\n## - x2    1   41.616 59.616\n## &lt;none&gt;      40.086 60.086\n## - x3    1   42.747 60.747\n## - x8    1   47.255 65.255\n## - x6    1   51.415 69.415\n## \n## Step:  AIC=58.54\n## y ~ x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x5    1   45.746 57.746\n## - x4    1   45.779 57.779\n## - x3    1   45.853 57.853\n## &lt;none&gt;      44.541 58.541\n## - x2    1   46.763 58.763\n## - x8    1   50.136 62.136\n## - x6    1   54.588 66.588\n## \n## Step:  AIC=57.75\n## y ~ x2 + x3 + x4 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x4    1   47.537 57.537\n## &lt;none&gt;      45.746 57.746\n## - x2    1   48.470 58.470\n## - x3    1   49.083 59.083\n## - x8    1   51.976 61.976\n## - x6    1   56.634 66.634\n## \n## Step:  AIC=57.54\n## y ~ x2 + x3 + x6 + x8\n## \n##        Df Deviance    AIC\n## &lt;none&gt;      47.537 57.537\n## - x3    1   50.276 58.276\n## - x2    1   51.418 59.418\n## - x8    1   53.869 61.869\n## - x6    1   59.649 67.649\nsummary(f2)\n## \n## Call:\n## glm(formula = y ~ x2 + x3 + x6 + x8, family = binomial(), data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -3.0314     0.8965  -3.381 0.000722 ***\n## x21           1.4715     0.7656   1.922 0.054617 .  \n## x31           1.2251     0.7543   1.624 0.104359    \n## x61           3.6124     1.3391   2.698 0.006985 ** \n## x81           1.8639     0.8045   2.317 0.020505 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 47.537  on 49  degrees of freedom\n## AIC: 57.537\n## \n## Number of Fisher Scoring iterations: 5\n\n# 步进法\nf3 &lt;- step(f, direction = \"both\")\n## Start:  AIC=64.03\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\n## \n##        Df Deviance    AIC\n## - x7    2   40.086 60.086\n## - x1    3   43.933 61.933\n## - x4    1   40.466 62.466\n## - x5    1   40.605 62.605\n## - x2    1   41.600 63.600\n## &lt;none&gt;      40.028 64.028\n## - x3    1   42.196 64.196\n## - x8    1   46.365 68.365\n## - x6    1   50.469 72.469\n## \n## Step:  AIC=60.09\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x1    3   44.541 58.541\n## - x5    1   40.611 58.611\n## - x4    1   40.814 58.814\n## - x2    1   41.616 59.616\n## &lt;none&gt;      40.086 60.086\n## - x3    1   42.747 60.747\n## + x7    2   40.028 64.028\n## - x8    1   47.255 65.255\n## - x6    1   51.415 69.415\n## \n## Step:  AIC=58.54\n## y ~ x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x5    1   45.746 57.746\n## - x4    1   45.779 57.779\n## - x3    1   45.853 57.853\n## &lt;none&gt;      44.541 58.541\n## - x2    1   46.763 58.763\n## + x1    3   40.086 60.086\n## + x7    2   43.933 61.933\n## - x8    1   50.136 62.136\n## - x6    1   54.588 66.588\n## \n## Step:  AIC=57.75\n## y ~ x2 + x3 + x4 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x4    1   47.537 57.537\n## &lt;none&gt;      45.746 57.746\n## - x2    1   48.470 58.470\n## + x5    1   44.541 58.541\n## + x1    3   40.611 58.611\n## - x3    1   49.083 59.083\n## + x7    2   44.697 60.697\n## - x8    1   51.976 61.976\n## - x6    1   56.634 66.634\n## \n## Step:  AIC=57.54\n## y ~ x2 + x3 + x6 + x8\n## \n##        Df Deviance    AIC\n## &lt;none&gt;      47.537 57.537\n## + x1    3   41.625 57.625\n## + x4    1   45.746 57.746\n## + x5    1   45.779 57.779\n## - x3    1   50.276 58.276\n## - x2    1   51.418 59.418\n## + x7    2   46.792 60.792\n## - x8    1   53.869 61.869\n## - x6    1   59.649 67.649\nsummary(f3)\n## \n## Call:\n## glm(formula = y ~ x2 + x3 + x6 + x8, family = binomial(), data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -3.0314     0.8965  -3.381 0.000722 ***\n## x21           1.4715     0.7656   1.922 0.054617 .  \n## x31           1.2251     0.7543   1.624 0.104359    \n## x61           3.6124     1.3391   2.698 0.006985 ** \n## x81           1.8639     0.8045   2.317 0.020505 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 47.537  on 49  degrees of freedom\n## AIC: 57.537\n## \n## Number of Fisher Scoring iterations: 5\n按照步进法最终纳入的自变量是x2,x6,x8。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Logistic回归</span>"
    ]
  },
  {
    "objectID": "1018-logistic.html#多项逻辑回归",
    "href": "1018-logistic.html#多项逻辑回归",
    "title": "17  Logistic回归",
    "section": "17.2 多项逻辑回归",
    "text": "17.2 多项逻辑回归\n因变量是无序多分类资料（＞2）时，可使用多分类逻辑回归（multinomial logistic regression）。\n使用课本例16-5的数据，课本电子版及数据已上传到QQ群，自行下载即可。\n某研究人员欲了解不同社区和性别之间居民获取健康知识的途径是否相同，对2个社区的314名成人进行了调查，其中X1是社区，社区1用0表示，社区2用1表示；X2是性别，0是男，1是女，Y是获取健康知识途径，1是传统大众传媒，2是网络，3是社区宣传。\n\ndf &lt;- read.csv(\"datasets/例16-05.csv\",header = T)\n\npsych::headtail(df)\n## Warning in psych::headtail(df): headtail is deprecated.  Please use the\n## headTail function\n##      X1  X2   Y\n## 1     0   0   1\n## 2     0   0   1\n## 3     0   0   1\n## 4     0   0   1\n## ... ... ... ...\n## 311   1   1   3\n## 312   1   1   3\n## 313   1   1   3\n## 314   1   1   3\n\n首先变为因子型，无需多分类的logistic回归需要对因变量设置参考，我们这里直接用factor()函数变为因子，这样在进行无序多分类的logistic时默认是以第一个为参考。也可以使用relevel()重新设置参考。\n\ndf$X1 &lt;- factor(df$X1,levels = c(0,1),labels = c(\"社区1\",\"社区2\"))\ndf$X2 &lt;- factor(df$X2,levels = c(0,1),labels = c(\"男\",\"女\"))\n\n# 因变量设置参考，这里选择第1个（传统大众传媒）为参考\ndf$Y &lt;- factor(df$Y,levels = c(1,2,3),labels = c(\"传统大众传媒\",\"网络\",\"社区宣传\"))\n\nstr(df)\n## 'data.frame':    314 obs. of  3 variables:\n##  $ X1: Factor w/ 2 levels \"社区1\",\"社区2\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ X2: Factor w/ 2 levels \"男\",\"女\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Y : Factor w/ 3 levels \"传统大众传媒\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n使用nnet::multinom进行无序多分类的logistic回归：\n\nlibrary(nnet)\n\nfit &lt;- multinom(Y ~ X1 + X2, data = df, model = T)\n## # weights:  12 (6 variable)\n## initial  value 344.964259 \n## iter  10 value 316.575399\n## iter  10 value 316.575399\n## iter  10 value 316.575399\n## final  value 316.575399 \n## converged\n\nsummary(fit)\n## Call:\n## multinom(formula = Y ~ X1 + X2, data = df, model = T)\n## \n## Coefficients:\n##          (Intercept)    X1社区2      X2女\n## 网络       0.5484998 -1.3743147 0.4321069\n## 社区宣传   0.3940422 -0.9933526 1.2266459\n## \n## Std. Errors:\n##          (Intercept)   X1社区2      X2女\n## 网络       0.2583299 0.3201514 0.3265384\n## 社区宣传   0.2574175 0.2952083 0.2991714\n## \n## Residual Deviance: 633.1508 \n## AIC: 645.1508\n\n可以看到结果比二项逻辑回归的结果简洁多了，少了很多信息，只给出了Coefficients/Std. Errors/Residual Deviance/AIC。\n不过也是两个模型的结果，分别是 社区宣传 和 传统大众传媒 比，网络 和 传统大众传媒 比。\n自变量的Z值（wald Z, Z-score）和P值需要手动计算:\n\nz_stats &lt;- summary(fit)$coefficients/summary(fit)$standard.errors\n\np_values &lt;- (1 - pnorm(abs(z_stats)))*2\n\np_values\n##          (Intercept)      X1社区2         X2女\n## 网络      0.03373263 1.765117e-05 1.857371e-01\n## 社区宣传  0.12583082 7.656564e-04 4.128929e-05\n\n但其实可以调包解决：\n\nres &lt;- broom::tidy(fit)\nres\n## # A tibble: 6 × 6\n##   y.level  term        estimate std.error statistic   p.value\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 网络     (Intercept)    0.548     0.258      2.12 0.0337   \n## 2 网络     X1社区2       -1.37      0.320     -4.29 0.0000177\n## 3 网络     X2女           0.432     0.327      1.32 0.186    \n## 4 社区宣传 (Intercept)    0.394     0.257      1.53 0.126    \n## 5 社区宣传 X1社区2       -0.993     0.295     -3.36 0.000766 \n## 6 社区宣传 X2女           1.23      0.299      4.10 0.0000413\n\nOR值及其95%的可信区间也没有给出来，需要手动计算OR值和可信区间：\n\n# 计算OR值\nOR &lt;- exp(coef(fit))\nOR\n##          (Intercept)   X1社区2     X2女\n## 网络        1.730655 0.2530129 1.540500\n## 社区宣传    1.482963 0.3703330 3.409774\n\n# 计算OR值的95%的可信区间\nOR.confi &lt;- exp(confint(fit))\nOR.confi\n## , , 网络\n## \n##                 2.5 %    97.5 %\n## (Intercept) 1.0430848 2.8714501\n## X1社区2     0.1350919 0.4738666\n## X2女        0.8122910 2.9215386\n## \n## , , 社区宣传\n## \n##                 2.5 %    97.5 %\n## (Intercept) 0.8953982 2.4560912\n## X1社区2     0.2076398 0.6605021\n## X2女        1.8970133 6.1288742\n\n模型整体的假设检验：\n\n# 先构建一个只有截距的模型\nfit0 &lt;- multinom(Y ~ 1, data = df, model = T)\n## # weights:  6 (2 variable)\n## initial  value 344.964259 \n## final  value 338.603448 \n## converged\n\n# 两个模型比较,Likelihood ratio tests\nanova(fit0, fit)\n## Likelihood ratio tests of Multinomial Models\n## \n## Response: Y\n##     Model Resid. df Resid. Dev   Test    Df LR stat.      Pr(Chi)\n## 1       1       626   677.2069                                   \n## 2 X1 + X2       622   633.1508 1 vs 2     4  44.0561 6.245931e-09\n\nP&lt;0.001，模型具有统计学意义。\n获取模型预测的类别：\n\npred &lt;- predict(fit, df, type = \"class\")\nhead(pred)\n## [1] 网络 网络 网络 网络 网络 网络\n## Levels: 传统大众传媒 网络 社区宣传\n\n获取模型预测的概率：\n\nprob &lt;- predict(fit, df, type = \"probs\") # 或者使用 fitted(fit)\nhead(prob)\n##   传统大众传媒      网络  社区宣传\n## 1    0.2373257 0.4107289 0.3519453\n## 2    0.2373257 0.4107289 0.3519453\n## 3    0.2373257 0.4107289 0.3519453\n## 4    0.2373257 0.4107289 0.3519453\n## 5    0.2373257 0.4107289 0.3519453\n## 6    0.2373257 0.4107289 0.3519453\n\n模型拟合优度的检验，这里使用卡方检验：\n\nchisq.test(df$Y, pred)\n## \n##  Pearson's Chi-squared test\n## \n## data:  df$Y and pred\n## X-squared = 39.521, df = 4, p-value = 5.436e-08\n\n计算伪R2：\n\nDescTools::PseudoR2(fit, which = \"all\")\n##        McFadden     McFaddenAdj        CoxSnell      Nagelkerke   AldrichNelson \n##      0.06505559      0.04733575      0.13090778      0.14803636              NA \n## VeallZimmermann           Efron McKelveyZavoina            Tjur             AIC \n##              NA              NA              NA              NA    645.15079819 \n##             BIC          logLik         logLik0              G2 \n##    667.64715610   -316.57539909   -338.60344772     44.05609725\n\n不仅给出了伪R^2，还给出了超多的值，每一项的意义可以参考下面这张图：\n\n\n\n\n\n\n\n\n\n结果解读可以参考二项逻辑回归。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Logistic回归</span>"
    ]
  },
  {
    "objectID": "1018-logistic.html#有序逻辑回归",
    "href": "1018-logistic.html#有序逻辑回归",
    "title": "17  Logistic回归",
    "section": "17.3 有序逻辑回归",
    "text": "17.3 有序逻辑回归\nordinal logistic regression适用于因变量为等级资料。使用课本例16-4的数据。\n随机选取84例患者做临床试验，探讨性别和治疗方法对该病的影响。变量赋值为：性别（X1，男=0，女=1），治疗方法（X2，传统疗法=0，新型疗法=1），疗效（Y，无效=1，有效=2，痊愈=3）。\n\ndf &lt;- read.csv(\"datasets/例16-04.csv\",header = T)\npsych::headtail(df)\n## Warning in psych::headtail(df): headtail is deprecated.  Please use the\n## headTail function\n##      X1  X2   Y\n## 1     0   0   1\n## 2     0   0   1\n## 3     0   0   1\n## 4     0   0   1\n## ... ... ... ...\n## 81    1   1   3\n## 82    1   1   3\n## 83    1   1   3\n## 84    1   1   3\n\n变为因子型：\n\n# 因变量变为有序因子\ndf$Y &lt;- factor(df$Y, levels = c(1,2,3),\n               labels = c(\"无效\",\"有效\",\"痊愈\"),\n               ordered = T)\n\n# 自变量变为无序因子\ndf$X1 &lt;- factor(df$X1,levels = c(0,1),labels = c(\"男\",\"女\"))\ndf$X2 &lt;- factor(df$X2,levels = c(0,1),labels = c(\"传统疗法\",\"新型疗法\"))\n\nstr(df)\n## 'data.frame':    84 obs. of  3 variables:\n##  $ X1: Factor w/ 2 levels \"男\",\"女\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ X2: Factor w/ 2 levels \"传统疗法\",\"新型疗法\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Y : Ord.factor w/ 3 levels \"无效\"&lt;\"有效\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n\n使用MASS::polr拟合有序逻辑回归：\n\nlibrary(MASS)\n\nfit &lt;- polr(Y ~ X1 + X2, data = df,Hess = TRUE,method = \"logistic\")\nsummary(fit)\n## Call:\n## polr(formula = Y ~ X1 + X2, data = df, Hess = TRUE, method = \"logistic\")\n## \n## Coefficients:\n##            Value Std. Error t value\n## X1女       1.319     0.5381   2.451\n## X2新型疗法 1.797     0.4718   3.809\n## \n## Intercepts:\n##           Value  Std. Error t value\n## 无效|有效 1.8128 0.5654     3.2061 \n## 有效|痊愈 2.6672 0.6065     4.3979 \n## \n## Residual Deviance: 150.0294 \n## AIC: 158.0294\n\n结果也是没有给出P值，手动计算P值：\n\np &lt;- pnorm(abs(coef(summary(fit))[, \"t value\"]),lower.tail = F)*2\np\n##         X1女   X2新型疗法    无效|有效    有效|痊愈 \n## 1.425572e-02 1.392807e-04 1.345300e-03 1.092866e-05\n\n这次broom::tidy(fit)并没有直接给出P值：\n\nbroom::tidy(fit)\n## # A tibble: 4 × 5\n##   term       estimate std.error statistic coef.type  \n##   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n## 1 X1女           1.32     0.538      2.45 coefficient\n## 2 X2新型疗法     1.80     0.472      3.81 coefficient\n## 3 无效|有效      1.81     0.565      3.21 scale      \n## 4 有效|痊愈      2.67     0.606      4.40 scale\n\nOR值：\n\nOR &lt;- exp(coef(fit))\nOR\n##       X1女 X2新型疗法 \n##   3.738765   6.033338\n\n平行线检验(Brant-Wald test)：\n\nbrant::brant(fit)\n## -------------------------------------------- \n## Test for X2  df  probability \n## -------------------------------------------- \n## Omnibus      1.83    2   0.4\n## X1女      1.59    1   0.21\n## X2新型疗法       0.01    1   0.94\n## -------------------------------------------- \n## \n## H0: Parallel Regression Assumption holds\n## Warning in brant::brant(fit): 1 combinations in table(dv,ivs) do not occur.\n## Because of that, the test results might be invalid.\n\nP值&gt;0.05，平行线检验通过，可以使用有序逻辑回归。\n模型整体的显著性检验：\n\n# 先构建一个只有截距的模型\nfit0 &lt;- polr(Y ~ 1, data = df,Hess = TRUE,method = \"logistic\")\n\n# 两个模型比较\nanova(fit0, fit)\n## Likelihood ratio tests of ordinal regression models\n## \n## Response: Y\n##     Model Resid. df Resid. Dev   Test    Df LR stat.     Pr(Chi)\n## 1       1        82   169.9159                                  \n## 2 X1 + X2        80   150.0294 1 vs 2     2  19.8865 4.80508e-05\n\nP值＜0.01，模型是有意义的。\n获取模型预测的类别：\n\npred &lt;- predict(fit, df, type = \"class\")\nhead(pred)\n## [1] 无效 无效 无效 无效 无效 无效\n## Levels: 无效 有效 痊愈\n\n获取模型预测的概率：\n\nprob &lt;- predict(fit, df, type = \"probs\") # 或者使用 fitted(fit)\nhead(prob)\n##        无效       有效       痊愈\n## 1 0.8597003 0.07536263 0.06493706\n## 2 0.8597003 0.07536263 0.06493706\n## 3 0.8597003 0.07536263 0.06493706\n## 4 0.8597003 0.07536263 0.06493706\n## 5 0.8597003 0.07536263 0.06493706\n## 6 0.8597003 0.07536263 0.06493706\n\n模型拟合优度的检验，这里使用卡方检验：\n\nchisq.test(df$Y, pred)\n## Warning in chisq.test(df$Y, pred): Chi-squared approximation may be incorrect\n## \n##  Pearson's Chi-squared test\n## \n## data:  df$Y and pred\n## X-squared = 14.246, df = 2, p-value = 0.0008065\n\n计算伪R2：\n\nDescTools::PseudoR2(fit, which = \"all\")\n##        McFadden        CoxSnell      Nagelkerke   AldrichNelson VeallZimmermann \n##       0.1170373       0.2108068       0.2429443              NA              NA \n##           Efron McKelveyZavoina            Tjur             AIC             BIC \n##              NA              NA              NA     158.0294131     167.7526803 \n##          logLik         logLik0              G2 \n##     -75.0147065     -84.9579583      19.8865036",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Logistic回归</span>"
    ]
  },
  {
    "objectID": "1018-logistic.html#条件逻辑回归",
    "href": "1018-logistic.html#条件逻辑回归",
    "title": "17  Logistic回归",
    "section": "17.4 条件逻辑回归",
    "text": "17.4 条件逻辑回归\nconditional logistic regression是针对配对数据资料分析的一种方法。在一些病例-对照研究中，把病例和对照按照年龄、性别等进行配对，形成多个匹配组，各匹配组的病例数和对照数是任意的，并不是1个对1个，常用的是每组中有一个病例和多个对照，即1：M配对研究。\n使用课本例16-3的数据。某北方城市研究喉癌发病的危险因素，用1:2配对研究，现选取了6个可能的危险因素并记录了25对数据，试做条件logistic回归。\n\n\n\n\n\n\n\n\n\n\ndf &lt;- foreign::read.spss(\"datasets/例16-03.sav\",to.data.frame = T)\npsych::headtail(df)\n## Warning in psych::headtail(df): headtail is deprecated.  Please use the\n## headTail function\n##       i   y  x1  x2  x3  x4  x5  x6\n## 1     1   1   3   5   1   1   1   0\n## 2     1   0   1   1   1   3   3   0\n## 3     1   0   1   1   1   3   3   0\n## 4     2   1   1   3   1   1   3   0\n## ... ... ... ... ... ... ... ... ...\n## 72   24   0   1   1   2   3   2   0\n## 73   25   1   1   4   1   1   1   1\n## 74   25   0   1   1   1   3   2   0\n## 75   25   0   1   1   1   3   3   0\nstr(df)\n## 'data.frame':    75 obs. of  8 variables:\n##  $ i : num  1 1 1 2 2 2 3 3 3 4 ...\n##  $ y : num  1 0 0 1 0 0 1 0 0 1 ...\n##  $ x1: num  3 1 1 1 1 1 1 1 1 1 ...\n##  $ x2: num  5 1 1 3 1 2 4 5 4 4 ...\n##  $ x3: num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ x4: num  1 3 3 1 3 3 3 3 3 2 ...\n##  $ x5: num  1 3 3 3 2 2 2 2 2 1 ...\n##  $ x6: num  0 0 0 0 0 0 0 0 0 1 ...\n\ni是配对的对子数。\n不需要变成因子型。\n使用survival::clogit进行条件逻辑回归：\n\nlibrary(survival)\n\nfit &lt;- clogit(y ~ x1+x2+x3+x4+x5+x6+strata(i), data = df, method = \"exact\")\n\nsummary(fit)\n## Call:\n## coxph(formula = Surv(rep(1, 75L), y) ~ x1 + x2 + x3 + x4 + x5 + \n##     x6 + strata(i), data = df, method = \"exact\")\n## \n##   n= 75, number of events= 25 \n## \n##        coef exp(coef) se(coef)      z Pr(&gt;|z|)  \n## x1  2.58880  13.31380  2.50172  1.035   0.3008  \n## x2  1.68796   5.40843  0.68545  2.463   0.0138 *\n## x3  2.31944  10.16995  1.26096  1.839   0.0659 .\n## x4 -3.88886   0.02047  1.90656 -2.040   0.0414 *\n## x5 -0.49102   0.61200  1.19020 -0.413   0.6799  \n## x6  3.50899  33.41447  2.13723  1.642   0.1006  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##    exp(coef) exp(-coef) lower .95 upper .95\n## x1  13.31380    0.07511 0.0988170 1793.7921\n## x2   5.40843    0.18490 1.4112830   20.7266\n## x3  10.16995    0.09833 0.8589963  120.4056\n## x4   0.02047   48.85506 0.0004878    0.8589\n## x5   0.61200    1.63399 0.0593818    6.3074\n## x6  33.41447    0.02993 0.5066653 2203.6771\n## \n## Concordance= 0.91  (se = 0.064 )\n## Likelihood ratio test= 42.21  on 6 df,   p=2e-07\n## Wald test            = 7.71  on 6 df,   p=0.3\n## Score (logrank) test = 29.13  on 6 df,   p=6e-05\n\n结果非常齐全，β值，OR值，P值等信息都有了。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Logistic回归</span>"
    ]
  },
  {
    "objectID": "1018-logistic.html#参考资料",
    "href": "1018-logistic.html#参考资料",
    "title": "17  Logistic回归",
    "section": "17.5 参考资料",
    "text": "17.5 参考资料\n\nhttps://blog.csdn.net/weixin_41744624/article/details/105506951\nhttps://zhuanlan.zhihu.com/p/113403422\nhttps://duanku.pai-hang-bang.cn/kuzi_1046977453210716059\nhttps://bookdown.org/chua/ber642_advanced_regression/\nhttps://peopleanalytics-regression-book.org/",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Logistic回归</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html",
    "href": "1019-codescheme.html",
    "title": "18  分类变量进行回归分析时的编码方案",
    "section": "",
    "text": "18.1 演示数据\n使用hsb2数据集进行演示。其中write是数值型因变量，race是其中一个自变量，是无序分类变量，有4个类别：1 = Hispanic, 2 = Asian, 3 = African American and 4 = Caucasian。\nload(file = \"datasets/codingSchemes.rdata\")\n\n# 把race变为因子型，并放到新的一列中\nhsb2$race.f &lt;- factor(hsb2$race, labels=c(\"Hispanic\", \"Asian\", \"African-Am\", \"Caucasian\"))\n\n# 根据race.f进行分组，计算分组后因变量write的均值\ntapply(hsb2$write, hsb2$race.f, mean)\n##   Hispanic      Asian African-Am  Caucasian \n##   46.45833   58.00000   48.20000   54.05517\n记住这个均值，很重要，后面要用的！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>分类变量进行回归分析时的编码方案</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#dummy-coding",
    "href": "1019-codescheme.html#dummy-coding",
    "title": "18  分类变量进行回归分析时的编码方案",
    "section": "18.2 Dummy Coding",
    "text": "18.2 Dummy Coding\n哑变量是最常见的分类变量编码方式，它以其中一个类别为参考，其他所有类别都和参考进行比较。比如，我们设定race中的1为参考，2,3,4都和1进行比较，也就是race=1时，write的均值。\n\n\n\n\n\n\n\n\n\n这也是R语言中数值型和无序因子型变量的默认编码方式，如果要手动设置，可以使用函数contr.treatment()，这个函数就是手动进行哑变量设置的函数。\n\ncontr.treatment(4)\n##   2 3 4\n## 1 0 0 0\n## 2 1 0 0\n## 3 0 1 0\n## 4 0 0 1\n\n这是一个比较矩阵，如果有4个类别就是4行，每行代表一个类别，比如第一行，2,3,4都是0，那么就代表类别1,；第二行，2是1,3,4都是0，就是代表类别2，以此类推。\n如果有K个类别，就会有K-1个哑变量，比如我们这个例子，有4个类别，就有3个哑变量，在R中的其他编码方式也是这样的。\nR语言中对不同类型的变量都会有默认的编码方式，可以通过contrasts()函数查看，比如对于race.f默认的就是哑变量编码：\n\n# 和上面的比较矩阵一模一样的形式\ncontrasts(hsb2$race.f)\n##            Asian African-Am Caucasian\n## Hispanic       0          0         0\n## Asian          1          0         0\n## African-Am     0          1         0\n## Caucasian      0          0         1\n\n在前面介绍logistic回归时，多次使用过哑变量编码的方式，因为是默认的，所以并不用手动设置！\n手动设置也可以，和默认是一样的结果，现在我们定义race.f这个变量进入回归模型后使用哑变量编码的方式：\n\n# 手动定义变量的编码方式\ncontrasts(hsb2$race.f) &lt;- contr.treatment(4)\n\n# 进行回归\nsummary(lm(write ~ race.f, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   46.458      1.842  25.218  &lt; 2e-16 ***\n## race.f2       11.542      3.286   3.512 0.000552 ***\n## race.f3        1.742      2.732   0.637 0.524613    \n## race.f4        7.597      1.989   3.820 0.000179 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\n可以看到结果中race.f2/race.f3/race.f4，都是和race.f1进行比较！和不设置的是一模一样的结果。\n\n请注意它们的系数，比如race.f2的系数是11.542，这个系数就是根据race.f进行分组后，race.f=2时write的均值 减去 race.f=1时write的均值，也就是58-46.45833！intercept（截距）的系数就是参考组write的平均值46.45833！\n\n哑变量编码后的数据进入回归分析时的具体操作可以这么理解，比如现在是race.f这个变量设置了哑变量编码的方式，那当它进入回归分析时，这一列就被我们设置的另外3列替代了，也就是原数据中的race.f这一列被另外3列哑变量替代了，当race.f这列的值是Hispanic时，3列哑变量就分别是0,0,0，如果race.f这列的值是Asian时，3列哑变量就分别是1,0,0，不知道大家理解了没有。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>分类变量进行回归分析时的编码方案</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#simple-coding",
    "href": "1019-codescheme.html#simple-coding",
    "title": "18  分类变量进行回归分析时的编码方案",
    "section": "18.3 simple coding",
    "text": "18.3 simple coding\n简单编码和哑变量编码的唯一不同之处是截距！哑变量的截距是参考组的因变量平均值，简单编码的截距是总的平均值。\n下面这张图是简单编码的方式。简单编码的所有类别编码相加需要等于1。\n\n\n\n\n\n\n\n\n\nlevel 1还是参考组，对于race.f1，是类别2比类别1，类别2就被编码为3/4，其他类别都是 -1/4，对于race.f2是类别3比类别1，类别3就别编码为为3/4，其他类别被编码为 -1/4，对于race.f3是类别4比类别1，类别4别编码为3/4，其他类别被编码为 -1/4。\n也就是说，参考组肯定是被编码为 -1/4（和类别个数有关，这里race.f是4个类别），谁和参考组比谁就被编码为 3/4。\n如果变量有K个类别的话，那就是参考组被编码为 -1/k，谁和参考组比谁就被编码为(k-1)/k。\nsimple coding在R中并没有提供直接的函数，但是可以通过哑变量进行转换：\n\n# 设置simple coding\nc&lt;-contr.treatment(4)\nmy.coding&lt;-matrix(rep(1/4, 12), ncol=3)\nmy.simple&lt;-c-my.coding\nmy.simple\n##       2     3     4\n## 1 -0.25 -0.25 -0.25\n## 2  0.75 -0.25 -0.25\n## 3 -0.25  0.75 -0.25\n## 4 -0.25 -0.25  0.75\n\n\n# 把race.f变成simple coding\ncontrasts(hsb2$race.f) &lt;- my.simple\n\n# 进行回归\nsummary(lm(write ~ race.f, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f2      11.5417     3.2861   3.512 0.000552 ***\n## race.f3       1.7417     2.7325   0.637 0.524613    \n## race.f4       7.5968     1.9889   3.820 0.000179 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\n可以看到除了截距不一样，其他的系数都是一样的，这里的截距51.6784 = (46.45833 + 58 + 48.2 + 54.05517)/4 。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>分类变量进行回归分析时的编码方案</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#deviation-coding",
    "href": "1019-codescheme.html#deviation-coding",
    "title": "18  分类变量进行回归分析时的编码方案",
    "section": "18.4 Deviation coding",
    "text": "18.4 Deviation coding\n这种编码方式比较的是某个类别的因变量的均值和其他所有类别的因变量的均值。它的比较矩阵是下面这种的。\n\n\n\n\n\n\n\n\n\n第1组是比较类别1和其它所有类别，第2组是比较类别2和其它所有类别，第3组是比较类别3和其它所有类别。\n这种比较矩阵（编码方式）通过赋值1，0，-1实现。第1组中，类别1被设为1（因为是类别1和其他所有类别比较），第2组是类别2被设为1，第3组是类别3倍设为1，而类别4一直是 -1，因为它没和别人比过。\n在R中通过函数contr.sum()实现这种编码方式（比较矩阵）：\n\ncontr.sum(4)\n##   [,1] [,2] [,3]\n## 1    1    0    0\n## 2    0    1    0\n## 3    0    0    1\n## 4   -1   -1   -1\n\n现在我们把race.f这个变量设置为deviation coding，再次进行回归分析：\n\ncontrasts(hsb2$race.f) &lt;- contr.sum(4)\n\nsummary(lm(write ~ race.f, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f1      -5.2200     1.6314  -3.200  0.00160 ** \n## race.f2       6.3216     2.1603   2.926  0.00384 ** \n## race.f3      -3.4784     1.7323  -2.008  0.04602 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\n此时的截距是总的均值，也就是(46.4583 + 58 + 48.2 + 54.0552) / 4 = 51.678375，回归系数是相应类别的均值减去总的均值，比如race.f1的回归系数 -5.2200 = 46.4583 – 51.678375。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>分类变量进行回归分析时的编码方案</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#orthogonal-polynomial-coding",
    "href": "1019-codescheme.html#orthogonal-polynomial-coding",
    "title": "18  分类变量进行回归分析时的编码方案",
    "section": "18.5 Orthogonal Polynomial Coding",
    "text": "18.5 Orthogonal Polynomial Coding\n正交多项式编码，在重复测量方差分析的多重比较中用过，这种编码方式常用于线性趋势检验，检测某自变量的1次项、2次项、3次项等和因变量之间有无线性关系。只用在有序分类变量（有序因子）且不同类别间对因变量影响相同的情况下。\n在R语言中中通过函数contr.poly()实现对某个变量的正交多项式编码，对于有序因子变量来说，这种编码方式是默认的，不需要手动指定。\n我们使用hsb2数据集中的read这一个变量，把它变成一个有序因子：\n\n# 新建一列，根据read转换为有序因子\nhsb2$readcat &lt;- cut(hsb2$read, 4, ordered=T)\ntable(hsb2$readcat)\n## \n## (28,40] (40,52] (52,64] (64,76] \n##      22      93      55      30\n\ntapply(hsb2$write, hsb2$readcat, mean)\n##  (28,40]  (40,52]  (52,64]  (64,76] \n## 42.77273 49.97849 56.56364 61.83333\n\n\n\n\n\n\n\n\n\n\n\ncontr.poly(4)\n##              .L   .Q         .C\n## [1,] -0.6708204  0.5 -0.2236068\n## [2,] -0.2236068 -0.5  0.6708204\n## [3,]  0.2236068 -0.5 -0.6708204\n## [4,]  0.6708204  0.5  0.2236068\n\n目前我们还未对readcat这个变量进行任何编码设置，但是R语言是有默认编码方式的，可以通过contrasts()查看：\n\n# 发现默认就是正交多项式编码\ncontrasts(hsb2$readcat)\n##              .L   .Q         .C\n## [1,] -0.6708204  0.5 -0.2236068\n## [2,] -0.2236068 -0.5  0.6708204\n## [3,]  0.2236068 -0.5 -0.6708204\n## [4,]  0.6708204  0.5  0.2236068\n\n当然也可以手动设置：\n\ncontrasts(hsb2$readcat) &lt;- contr.poly(4)\n\nsummary(lm(write ~ readcat, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ readcat, data = hsb2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -18.979  -5.824   1.227   5.436  17.021 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  52.7870     0.6339  83.268   &lt;2e-16 ***\n## readcat.L    14.2587     1.4841   9.607   &lt;2e-16 ***\n## readcat.Q    -0.9680     1.2679  -0.764    0.446    \n## readcat.C    -0.1554     1.0062  -0.154    0.877    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.726 on 196 degrees of freedom\n## Multiple R-squared:  0.3456, Adjusted R-squared:  0.3356 \n## F-statistic: 34.51 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n回归分析结果显示readcat的一次项和因变量有很强的的线性关系，二次项和三次项没有明显的线性关系。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>分类变量进行回归分析时的编码方案</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#helmert-coding",
    "href": "1019-codescheme.html#helmert-coding",
    "title": "18  分类变量进行回归分析时的编码方案",
    "section": "18.6 Helmert Coding",
    "text": "18.6 Helmert Coding\nHelmert Coding比较的是当前类别下的因变量平均值和后面的类别的因变量平均值。\n\n\n\n\n\n\n\n\n\n如上图所示，race.f1比较的是类别1的因变量平均值和类别2,3,4的因变量平均值，race.f2比较的是类别2的因变量平均值和类别3,4的因变量平均值，race.f3比较的是类别3的因变量平均值和类别4的因变量平均值。\nR中提供了contr.helmert，但是对应的是 反helmert编码方式。所以如果想要对一个变量使用Helmert coding，需要手动设置：\n\nmy.helmert &lt;- matrix(c(3/4, -1/4, -1/4, -1/4, 0, 2/3, -1/3, -1/3, 0, 0, 1/\n    2, -1/2), ncol = 3)\nmy.helmert\n##       [,1]       [,2] [,3]\n## [1,]  0.75  0.0000000  0.0\n## [2,] -0.25  0.6666667  0.0\n## [3,] -0.25 -0.3333333  0.5\n## [4,] -0.25 -0.3333333 -0.5\n\n现在把race.f设置为Helmert coding：\n\ncontrasts(hsb2$race.f) &lt;- my.helmert\n\nsummary(lm(write ~ race.f, hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f1      -6.9601     2.1752  -3.200  0.00160 ** \n## race.f2       6.8724     2.9263   2.348  0.01985 *  \n## race.f3      -5.8552     2.1528  -2.720  0.00712 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\nrace.f1的回归系数 -6.9601 = 46.4583 – [(58 + 48.2 + 54.0552) / 3]，也就是类别1的因变量平均值减去类别2,3,4的因变量平均值；race.f2和race.f3的系数同理。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>分类变量进行回归分析时的编码方案</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#reverse-helmert-coding",
    "href": "1019-codescheme.html#reverse-helmert-coding",
    "title": "18  分类变量进行回归分析时的编码方案",
    "section": "18.7 Reverse Helmert Coding",
    "text": "18.7 Reverse Helmert Coding\n和Helmert Coding刚好完全相反，每一个类别和它前面的类别比较。\n如下图所示，race.f1比较的是类别2的因变量平均值和类别1的因变量平均值，race.f2比较的是类别3的因变量平均值和类别1,2的因变量平均值，race.f3比较的是类别4的因变量平均值和类别1,2,3的因变量平均值。\n\n\n\n\n\n\n\n\n\nR中提供了contr.helmert，可以进行Reverse Helmert Coding：\n\ncontr.helmert(4)\n##   [,1] [,2] [,3]\n## 1   -1   -1   -1\n## 2    1   -1   -1\n## 3    0    2   -1\n## 4    0    0    3\n\n把race.f设置为这种编码方式再次进行回归分析：\n\ncontrasts(hsb2$race.f) &lt;- contr.helmert(4)\n\nsummary(lm(write ~ race.f, hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f1       5.7708     1.6431   3.512 0.000552 ***\n## race.f2      -1.3431     0.8675  -1.548 0.123170    \n## race.f3       0.7923     0.3720   2.130 0.034439 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\n看完了上面那么多例子，这个回归系数的解读大家应该明白了吧，这里就不再赘述了。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>分类变量进行回归分析时的编码方案</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#forward-difference-coding",
    "href": "1019-codescheme.html#forward-difference-coding",
    "title": "18  分类变量进行回归分析时的编码方案",
    "section": "18.8 Forward Difference Coding",
    "text": "18.8 Forward Difference Coding\n这种编码方式比较的是当前类别的因变量均值和它相邻的下一个类别的因变量均值，注意是下一个，Helmert coding是多个。\n如下图所示，race.f1比较的是类别1和类别2，race.f2比较的是类别2和3，race.f3`比较的是类别3和类别4。\n\n\n\n\n\n\n\n\n\nR语言中并没有默认函数提供此类编码方式，需要手动设置。\n\nmy.forward.diff = matrix(c(3/4, -1/4, -1/4, -1/4, 1/2, 1/2, -1/2, -1/2, 1/\n4, 1/4, 1/4, -3/4), ncol = 3)\nmy.forward.diff\n##       [,1] [,2]  [,3]\n## [1,]  0.75  0.5  0.25\n## [2,] -0.25  0.5  0.25\n## [3,] -0.25 -0.5  0.25\n## [4,] -0.25 -0.5 -0.75\n\n把race.f设置为这种编码方式：\n\ncontrasts(hsb2$race.f) &lt;- my.forward.diff\n\nsummary(lm(write ~ race.f, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f1     -11.5417     3.2861  -3.512 0.000552 ***\n## race.f2       9.8000     3.3878   2.893 0.004251 ** \n## race.f3      -5.8552     2.1528  -2.720 0.007118 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\nrace.f1的截距是类别2和类别1的因变量差值， -11.5417 = 46.4583 – 58，race.f2的截距 是类别3和类别2的因变量差值，9.9 = 58 – 48.2，race.f3的截距是类别4和类别3的因变量差值，-5.8552 = 48.2 – 54.0552。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>分类变量进行回归分析时的编码方案</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#backward-difference-coding",
    "href": "1019-codescheme.html#backward-difference-coding",
    "title": "18  分类变量进行回归分析时的编码方案",
    "section": "18.9 Backward Difference Coding",
    "text": "18.9 Backward Difference Coding\n这种编码方式和前一种刚好相反，比较的是当前类别和它相邻的前一个类别的因变量均值。\n\n\n\n\n\n\n\n\n\n在R语言中实现这种编码方式也是需要手动设置的：\n\nmy.backward.diff = matrix(c(-3/4, 1/4, 1/4, 1/4, -1/2, -1/2, 1/2, 1/2, -1/4, -1/4, -1/4, 3/4), ncol = 3)\nmy.backward.diff\n##       [,1] [,2]  [,3]\n## [1,] -0.75 -0.5 -0.25\n## [2,]  0.25 -0.5 -0.25\n## [3,]  0.25  0.5 -0.25\n## [4,]  0.25  0.5  0.75\n\n\ncontrasts(hsb2$race.f) &lt;- my.backward.diff\n\nsummary(lm(write ~ race.f, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f1      11.5417     3.2861   3.512 0.000552 ***\n## race.f2      -9.8000     3.3878  -2.893 0.004251 ** \n## race.f3       5.8552     2.1528   2.720 0.007118 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\n此时的截距正负号刚好和上面一种情况相反。\n这几种就是常见的R语言中分类变量的编码方式，除了这几个，大家还可以根据自己需要灵活手动设置。\n大家以为这套规则只是R语言中独有的吗？并不是，在SPSS、SAS等软件中，分类变量的编码方式也是类似的！\n这里只演示了线性回归的，logistic回归、cox回归也是类似的编码方案！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>分类变量进行回归分析时的编码方案</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#参考资料",
    "href": "1019-codescheme.html#参考资料",
    "title": "18  分类变量进行回归分析时的编码方案",
    "section": "18.10 参考资料",
    "text": "18.10 参考资料\n\nhttps://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>分类变量进行回归分析时的编码方案</span>"
    ]
  },
  {
    "objectID": "1032-survival.html",
    "href": "1032-survival.html",
    "title": "19  R语言生存分析",
    "section": "",
    "text": "19.1 生存过程的描述\nlibrary(survival)\nlibrary(survminer)\n使用survival包中的lung数据集用于演示，这是一份关于肺癌患者的生存数据。time是生存时间，以天为单位，status是生存状态，1代表删失，2代表死亡。但是一般在生存分析中我们喜欢用1代表死亡，用0代表删失，所以我们更改一下（其实不改也可以，你记住就行）。\ndf &lt;- lung\ndf$status &lt;- ifelse(df$status == 2,1,0)\nstr(df)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  1 1 0 1 1 0 1 1 1 1 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n首先把生存时间和生存状态用Surv()放到一起，可以看到有+的就是截尾数据。\nSurv(time = lung$time, event = lung$status)\n##   [1]  306   455  1010+  210   883  1022+  310   361   218   166   170   654 \n##  [13]  728    71   567   144   613   707    61    88   301    81   624   371 \n##  [25]  394   520   574   118   390    12   473    26   533   107    53   122 \n##  [37]  814   965+   93   731   460   153   433   145   583    95   303   519 \n##  [49]  643   765   735   189    53   246   689    65     5   132   687   345 \n##  [61]  444   223   175    60   163    65   208   821+  428   230   840+  305 \n##  [73]   11   132   226   426   705   363    11   176   791    95   196+  167 \n##  [85]  806+  284   641   147   740+  163   655   239    88   245   588+   30 \n##  [97]  179   310   477   166   559+  450   364   107   177   156   529+   11 \n## [109]  429   351    15   181   283   201   524    13   212   524   288   363 \n## [121]  442   199   550    54   558   207    92    60   551+  543+  293   202 \n## [133]  353   511+  267   511+  371   387   457   337   201   404+  222    62 \n## [145]  458+  356+  353   163    31   340   229   444+  315+  182   156   329 \n## [157]  364+  291   179   376+  384+  268   292+  142   413+  266+  194   320 \n## [169]  181   285   301+  348   197   382+  303+  296+  180   186   145   269+\n## [181]  300+  284+  350   272+  292+  332+  285   259+  110   286   270    81 \n## [193]  131   225+  269   225+  243+  279+  276+  135    79    59   240+  202+\n## [205]  235+  105   224+  239   237+  173+  252+  221+  185+   92+   13   222+\n## [217]  192+  183   211+  175+  197+  203+  116   188+  191+  105+  174+  177+\n如果只是想要描述一下这份生存数据，可以使用寿命表法或者K-M曲线，在R中可以通过survfit()实现。\n# 构建生存曲线\nfit &lt;- survfit(Surv(time, status) ~ 1, data = df)\n\n# 寿命表，surv_summary比默认的summary()更好\nsurv_summary(fit)\n##     time n.risk n.event n.censor       surv     std.err     upper      lower\n## 1      5    228       1        0 0.99561404 0.004395615 1.0000000 0.98707342\n## 2     11    227       3        0 0.98245614 0.008849904 0.9996460 0.96556190\n## 3     12    224       1        0 0.97807018 0.009916654 0.9972662 0.95924368\n## 4     13    223       2        0 0.96929825 0.011786516 0.9919508 0.94716300\n## 5     15    221       1        0 0.96491228 0.012628921 0.9890941 0.94132171\n## 6     26    220       1        0 0.96052632 0.013425540 0.9861367 0.93558107\n## 7     30    219       1        0 0.95614035 0.014184183 0.9830945 0.92992527\n## 8     31    218       1        0 0.95175439 0.014910735 0.9799794 0.92434234\n## 9     53    217       2        0 0.94298246 0.016284897 0.9735659 0.91335978\n## 10    54    215       1        0 0.93859649 0.016939076 0.9702809 0.90794671\n## 11    59    214       1        0 0.93421053 0.017574720 0.9669508 0.90257880\n## 12    60    213       2        0 0.92543860 0.018798180 0.9601711 0.89196244\n## 13    61    211       1        0 0.92105263 0.019389168 0.9567281 0.88670745\n## 14    62    210       1        0 0.91666667 0.019968077 0.9532533 0.88148430\n## 15    65    209       2        0 0.90789474 0.021093908 0.9462168 0.87112471\n## 16    71    207       1        0 0.90350877 0.021642644 0.9426590 0.86598451\n## 17    79    206       1        0 0.89912281 0.022182963 0.9390770 0.86086855\n## 18    81    205       2        0 0.89035088 0.023240987 0.9318456 0.85070391\n## 19    88    203       2        0 0.88157895 0.024272607 0.9245323 0.84062118\n## 20    92    201       1        1 0.87719298 0.024779731 0.9208475 0.83560803\n## 21    93    199       1        0 0.87278498 0.025286647 0.9171308 0.83058337\n## 22    95    198       2        0 0.86396897 0.026285933 0.9096467 0.82058489\n## 23   105    196       1        1 0.85956096 0.026778995 0.9058807 0.81560966\n## 24   107    194       2        0 0.85069951 0.027763443 0.8982733 0.80564534\n## 25   110    192       1        0 0.84626878 0.028250266 0.8944478 0.80068492\n## 26   116    191       1        0 0.84183806 0.028733836 0.8906085 0.79573831\n## 27   118    190       1        0 0.83740733 0.029214392 0.8867559 0.79080503\n## 28   122    189       1        0 0.83297660 0.029692160 0.8828904 0.78588462\n## 29   131    188       1        0 0.82854588 0.030167350 0.8790125 0.78097668\n## 30   132    187       2        0 0.81968442 0.031110783 0.8712208 0.77119665\n## 31   135    185       1        0 0.81525370 0.031579392 0.8673077 0.76632386\n## 32   142    184       1        0 0.81082297 0.032046159 0.8633836 0.76146212\n## 33   144    183       1        0 0.80639224 0.032511243 0.8594487 0.75661112\n## 34   145    182       2        0 0.79753079 0.033436970 0.8515479 0.74694024\n## 35   147    180       1        0 0.79310006 0.033897900 0.8475824 0.74211984\n## 36   153    179       1        0 0.78866934 0.034357720 0.8436072 0.73730913\n## 37   156    178       2        0 0.77980788 0.035274546 0.8356287 0.72771592\n## 38   163    176       3        0 0.76651570 0.036644539 0.8235936 0.71339353\n## 39   166    173       2        0 0.75765425 0.037555674 0.8155273 0.70388809\n## 40   167    171       1        0 0.75322352 0.038010898 0.8114818 0.69914771\n## 41   170    170       1        0 0.74879280 0.038466026 0.8074284 0.69441536\n## 42   173    169       0        1 0.74879280 0.038466026 0.8074284 0.69441536\n## 43   174    168       0        1 0.74879280 0.038466026 0.8074284 0.69441536\n## 44   175    167       1        1 0.74430901 0.038932090 0.8033269 0.68962694\n## 45   176    165       1        0 0.73979804 0.039403839 0.7991969 0.68481391\n## 46   177    164       1        1 0.73528708 0.039875693 0.7950587 0.68000904\n## 47   179    162       2        0 0.72620946 0.040831745 0.7867159 0.67035655\n## 48   180    160       1        0 0.72167065 0.041310284 0.7825326 0.66554231\n## 49   181    159       2        0 0.71259304 0.042268879 0.7741425 0.65593716\n## 50   182    157       1        0 0.70805423 0.042749126 0.7699360 0.65114603\n## 51   183    156       1        0 0.70351542 0.043230132 0.7657221 0.64636237\n## 52   185    155       0        1 0.70351542 0.043230132 0.7657221 0.64636237\n## 53   186    154       1        0 0.69894713 0.043718251 0.7614780 0.64155115\n## 54   188    153       0        1 0.69894713 0.043718251 0.7614780 0.64155115\n## 55   189    152       1        0 0.69434880 0.044213739 0.7572033 0.63671178\n## 56   191    151       0        1 0.69434880 0.044213739 0.7572033 0.63671178\n## 57   192    150       0        1 0.69434880 0.044213739 0.7572033 0.63671178\n## 58   194    149       1        0 0.68968874 0.044723618 0.7528734 0.63180684\n## 59   196    148       0        1 0.68968874 0.044723618 0.7528734 0.63180684\n## 60   197    147       1        1 0.68499698 0.045241530 0.7485112 0.62687218\n## 61   199    145       1        0 0.68027286 0.045767770 0.7441162 0.62190715\n## 62   201    144       2        0 0.67082463 0.046824116 0.7353020 0.61200115\n## 63   202    142       1        1 0.66610051 0.047354439 0.7308831 0.60705997\n## 64   203    140       0        1 0.66610051 0.047354439 0.7308831 0.60705997\n## 65   207    139       1        0 0.66130842 0.047901723 0.7264037 0.60204649\n## 66   208    138       1        0 0.65651633 0.048450680 0.7219163 0.59704112\n## 67   210    137       1        0 0.65172424 0.049001423 0.7174208 0.59204373\n## 68   211    136       0        1 0.65172424 0.049001423 0.7174208 0.59204373\n## 69   212    135       1        0 0.64689665 0.049562270 0.7128898 0.58701260\n## 70   218    134       1        0 0.64206907 0.050125134 0.7083507 0.58198951\n## 71   221    133       0        1 0.64206907 0.050125134 0.7083507 0.58198951\n## 72   222    132       1        1 0.63720491 0.050698710 0.7037752 0.57693155\n## 73   223    130       1        0 0.63230333 0.051283424 0.6991623 0.57183791\n## 74   224    129       0        1 0.63230333 0.051283424 0.6991623 0.57183791\n## 75   225    128       0        2 0.63230333 0.051283424 0.6991623 0.57183791\n## 76   226    126       1        0 0.62728505 0.051898763 0.6944504 0.56661573\n## 77   229    125       1        0 0.62226677 0.052516642 0.6897296 0.56140253\n## 78   230    124       1        0 0.61724849 0.053137208 0.6849999 0.55619818\n## 79   235    123       0        1 0.61724849 0.053137208 0.6849999 0.55619818\n## 80   237    122       0        1 0.61724849 0.053137208 0.6849999 0.55619818\n## 81   239    121       2        0 0.60704604 0.054428498 0.6753847 0.54562217\n## 82   240    119       0        1 0.60704604 0.054428498 0.6753847 0.54562217\n## 83   243    118       0        1 0.60704604 0.054428498 0.6753847 0.54562217\n## 84   245    117       1        0 0.60185761 0.055101203 0.6704957 0.54024596\n## 85   246    116       1        0 0.59666918 0.055777281 0.6655969 0.53487943\n## 86   252    115       0        1 0.59666918 0.055777281 0.6655969 0.53487943\n## 87   259    114       0        1 0.59666918 0.055777281 0.6655969 0.53487943\n## 88   266    113       0        1 0.59666918 0.055777281 0.6655969 0.53487943\n## 89   267    112       1        0 0.59134178 0.056493740 0.6605811 0.52935986\n## 90   268    111       1        0 0.58601437 0.057214008 0.6555547 0.52385081\n## 91   269    110       1        1 0.58068697 0.057938291 0.6505179 0.51835217\n## 92   270    108       1        0 0.57531024 0.058680326 0.6454326 0.51280626\n## 93   272    107       0        1 0.57531024 0.058680326 0.6454326 0.51280626\n## 94   276    106       0        1 0.57531024 0.058680326 0.6454326 0.51280626\n## 95   279    105       0        1 0.57531024 0.058680326 0.6454326 0.51280626\n## 96   283    104       1        0 0.56977841 0.059470446 0.6402172 0.50708954\n## 97   284    103       1        1 0.56424658 0.060265393 0.6349901 0.50138454\n## 98   285    101       2        0 0.55307338 0.061902647 0.6244165 0.48988160\n## 99   286     99       1        0 0.54748678 0.062729652 0.6191120 0.48414791\n## 100  288     98       1        0 0.54190018 0.063562614 0.6137958 0.47842592\n## 101  291     97       1        0 0.53631358 0.064401818 0.6084680 0.47271553\n## 102  292     96       0        2 0.53631358 0.064401818 0.6084680 0.47271553\n## 103  293     94       1        0 0.53060812 0.065283876 0.6030365 0.46687880\n## 104  296     93       0        1 0.53060812 0.065283876 0.6030365 0.46687880\n## 105  300     92       0        1 0.53060812 0.065283876 0.6030365 0.46687880\n## 106  301     91       1        1 0.52477726 0.066212421 0.5974962 0.46090869\n## 107  303     89       1        1 0.51888089 0.067169680 0.5918922 0.45487570\n## 108  305     87       1        0 0.51291674 0.068157318 0.5862225 0.44877769\n## 109  306     86       1        0 0.50695259 0.069153590 0.5805384 0.44269407\n## 110  310     85       2        0 0.49502429 0.071173772 0.5691277 0.43056952\n## 111  315     83       0        1 0.49502429 0.071173772 0.5691277 0.43056952\n## 112  320     82       1        0 0.48898741 0.072223700 0.5633452 0.42444435\n## 113  329     81       1        0 0.48295053 0.073284268 0.5575481 0.41833381\n## 114  332     80       0        1 0.48295053 0.073284268 0.5575481 0.41833381\n## 115  337     79       1        0 0.47683723 0.074383257 0.5516775 0.41214973\n## 116  340     78       1        0 0.47072393 0.075494166 0.5457918 0.40598083\n## 117  345     77       1        0 0.46461064 0.076617562 0.5398911 0.39982704\n## 118  348     76       1        0 0.45849734 0.077754031 0.5339753 0.39368826\n## 119  350     75       1        0 0.45238404 0.078904180 0.5280446 0.38756443\n## 120  351     74       1        0 0.44627074 0.080068634 0.5220991 0.38145549\n## 121  353     73       2        0 0.43404415 0.082443090 0.5101637 0.36928207\n## 122  356     71       0        1 0.43404415 0.082443090 0.5101637 0.36928207\n## 123  361     70       1        0 0.42784352 0.083689321 0.5041055 0.36311858\n## 124  363     69       2        0 0.41544226 0.086235271 0.4919424 0.35083836\n## 125  364     67       1        1 0.40924162 0.087536643 0.4858376 0.34472158\n## 126  371     65       2        0 0.39664957 0.090283246 0.4734305 0.33232098\n## 127  376     63       0        1 0.39664957 0.090283246 0.4734305 0.33232098\n## 128  382     62       0        1 0.39664957 0.090283246 0.4734305 0.33232098\n## 129  384     61       0        1 0.39664957 0.090283246 0.4734305 0.33232098\n## 130  387     60       1        0 0.39003875 0.091834363 0.4669574 0.32579034\n## 131  390     59       1        0 0.38342792 0.093411868 0.4604644 0.31927978\n## 132  394     58       1        0 0.37681710 0.095017143 0.4539514 0.31278928\n## 133  404     57       0        1 0.37681710 0.095017143 0.4539514 0.31278928\n## 134  413     56       0        1 0.37681710 0.095017143 0.4539514 0.31278928\n## 135  426     55       1        0 0.36996588 0.096772712 0.4472339 0.30604732\n## 136  428     54       1        0 0.36311466 0.098561472 0.4404935 0.29932852\n## 137  429     53       1        0 0.35626344 0.100385300 0.4337299 0.29263289\n## 138  433     52       1        0 0.34941222 0.102246185 0.4269433 0.28596045\n## 139  442     51       1        0 0.34256100 0.104146240 0.4201335 0.27931128\n## 140  444     50       1        1 0.33570978 0.106087711 0.4133006 0.27268545\n## 141  450     48       1        0 0.32871582 0.108156668 0.4063345 0.26592397\n## 142  455     47       1        0 0.32172187 0.110274202 0.3993431 0.25918807\n## 143  457     46       1        0 0.31472792 0.112443281 0.3923261 0.25247790\n## 144  458     45       0        1 0.31472792 0.112443281 0.3923261 0.25247790\n## 145  460     44       1        0 0.30757501 0.114769476 0.3851616 0.24561738\n## 146  473     43       1        0 0.30042210 0.117156914 0.3779689 0.23878538\n## 147  477     42       1        0 0.29326919 0.119609626 0.3707476 0.23198214\n## 148  511     41       0        2 0.29326919 0.119609626 0.3707476 0.23198214\n## 149  519     39       1        0 0.28574947 0.122397820 0.3632207 0.22480203\n## 150  520     38       1        0 0.27822975 0.125269565 0.3556585 0.21765764\n## 151  524     37       2        0 0.26319030 0.131289244 0.3404266 0.20347744\n## 152  529     35       0        1 0.26319030 0.131289244 0.3404266 0.20347744\n## 153  533     34       1        0 0.25544941 0.134640748 0.3325916 0.19619977\n## 154  543     33       0        1 0.25544941 0.134640748 0.3325916 0.19619977\n## 155  550     32       1        0 0.24746662 0.138333639 0.3245387 0.18869779\n## 156  551     31       0        1 0.24746662 0.138333639 0.3245387 0.18869779\n## 157  558     30       1        0 0.23921773 0.142427599 0.3162481 0.18095008\n## 158  559     29       0        1 0.23921773 0.142427599 0.3162481 0.18095008\n## 159  567     28       1        0 0.23067424 0.146997865 0.3076975 0.17293157\n## 160  574     27       1        0 0.22213075 0.151765851 0.2990832 0.16497774\n## 161  583     26       1        0 0.21358726 0.156752465 0.2904045 0.15708959\n## 162  588     25       0        1 0.21358726 0.156752465 0.2904045 0.15708959\n## 163  613     24       1        0 0.20468779 0.162428228 0.2814175 0.14887877\n## 164  624     23       1        0 0.19578832 0.168401942 0.2723521 0.14074818\n## 165  641     22       1        0 0.18688885 0.174710378 0.2632068 0.13269961\n## 166  643     21       1        0 0.17798938 0.181396440 0.2539797 0.12473524\n## 167  654     20       1        0 0.16908991 0.188510603 0.2446686 0.11685766\n## 168  655     19       1        0 0.16019044 0.196112784 0.2352708 0.10906995\n## 169  687     18       1        0 0.15129097 0.204274810 0.2257834 0.10137573\n## 170  689     17       1        0 0.14239151 0.213083712 0.2162028 0.09377928\n## 171  705     16       1        0 0.13349204 0.222646211 0.2065248 0.08628565\n## 172  707     15       1        0 0.12459257 0.233094916 0.1967446 0.07890080\n## 173  728     14       1        0 0.11569310 0.244597108 0.1868568 0.07163182\n## 174  731     13       1        0 0.10679363 0.257367445 0.1768548 0.06448724\n## 175  735     12       1        0 0.09789416 0.271686878 0.1667313 0.05747732\n## 176  740     11       0        1 0.09789416 0.271686878 0.1667313 0.05747732\n## 177  765     10       1        0 0.08810474 0.291418720 0.1559751 0.04976720\n## 178  791      9       1        0 0.07831533 0.314346559 0.1450170 0.04229358\n## 179  806      8       0        1 0.07831533 0.314346559 0.1450170 0.04229358\n## 180  814      7       1        0 0.06712742 0.350176075 0.1333431 0.03379322\n## 181  821      6       0        1 0.06712742 0.350176075 0.1333431 0.03379322\n## 182  840      5       0        1 0.06712742 0.350176075 0.1333431 0.03379322\n## 183  883      4       1        0 0.05034557 0.453824434 0.1225342 0.02068546\n## 184  965      3       0        1 0.05034557 0.453824434 0.1225342 0.02068546\n## 185 1010      2       0        1 0.05034557 0.453824434 0.1225342 0.02068546\n## 186 1022      1       0        1 0.05034557 0.453824434 0.1225342 0.02068546\n画出生存曲线，横坐标是生存时间，纵坐标是生存率。\nggsurvplot(fit,\n           conf.int = TRUE, # 可信区间\n           palette= 'blue', # 更改配色\n           surv.median.line = \"hv\", # 中位生存时间\n           ggtheme = theme_bw() # 更改主题\n           \n)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>R语言生存分析</span>"
    ]
  },
  {
    "objectID": "1032-survival.html#生存过程的比较",
    "href": "1032-survival.html#生存过程的比较",
    "title": "19  R语言生存分析",
    "section": "19.2 生存过程的比较",
    "text": "19.2 生存过程的比较\n如果通过某个变量把数据分为多组，然后检验不同组别之间的生存时间（生存曲线）有无差别，则可以通过logrank检验或者breslow检验。\n在R语言中通过survdiff()实现logrank检验。\n\nfit &lt;- survdiff(Surv(time, status) ~ sex, data = df)\nfit\n## Call:\n## survdiff(formula = Surv(time, status) ~ sex, data = df)\n## \n##         N Observed Expected (O-E)^2/E (O-E)^2/V\n## sex=1 138      112     91.6      4.55      10.3\n## sex=2  90       53     73.4      5.68      10.3\n## \n##  Chisq= 10.3  on 1 degrees of freedom, p= 0.001\n\n可以用神包broom提取数据：\n\nbroom::tidy(fit)\n## # A tibble: 2 × 4\n##   sex       N   obs   exp\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 1       138   112  91.6\n## 2 2        90    53  73.4\nbroom::glance(fit)\n## # A tibble: 1 × 3\n##   statistic    df p.value\n##       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n## 1      10.3     1 0.00131\n\n对于不同组别之间生存曲线的检验，也可以通过K-M图示的方法：\n\nfit.logrank &lt;- survfit(Surv(time, status) ~ sex, data = df)\n\n# 这一步输出太多，我注释掉了，可以自己运行看看\n# surv_summary(fit.logrank) # 可以查看寿命表\n\n通过ggsurvplot()进行可视化，非常多的细节可以修改，超级详细的教程可以参考我的另一篇推文：R语言生存曲线的可视化(超详细)\n\nggsurvplot(fit.logrank, \n           data = df,\n           surv.median.line = \"hv\", # Add medians survival\n           \n           # Change legends: title & labels\n           legend.title = \"Sex\",\n           legend.labs = c(\"Male\", \"Female\"),\n           \n           # Add p-value and tervals\n           pval = TRUE, # 这里P值直接写数字也行\n           conf.int = TRUE,\n           \n           # Add risk table\n           risk.table = TRUE, \n           tables.height = 0.2,\n           tables.theme = theme_cleantable(),\n           \n           ncensor.plot = TRUE,\n           \n           # Color palettes. Use custom color: c(\"#E7B800\", \"#2E9FDF\"),\n           # or brewer color (e.g.: \"Dark2\"), or ggsci color (e.g.: \"jco\")\n           palette = c(\"#E7B800\", \"#2E9FDF\"),\n           ggtheme = theme_bw(), # Change ggplot2 theme\n           \n           # Change font size, style and color\n           main = \"Survival curve\",\n           font.main = c(16, \"bold\", \"darkblue\"),\n           font.x = c(14, \"bold.italic\", \"red\"),\n           font.y = c(14, \"bold.italic\", \"darkred\"),\n           font.tickslab = c(12, \"plain\", \"darkgreen\")\n)\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n自带的surv_cutpoint()可用于寻找最佳切点，但是只能用于连续性数据。\n使用myeloma数据进行演示。\n\nrm(list = ls())\n\n# 0. Load some data\ndata(myeloma)\nhead(myeloma)\n##          molecular_group chr1q21_status treatment event  time   CCND1 CRIM1\n## GSM50986      Cyclin D-1       3 copies       TT2     0 69.24  9908.4 420.9\n## GSM50988      Cyclin D-2       2 copies       TT2     0 66.43 16698.8  52.0\n## GSM50989           MMSET       2 copies       TT2     0 66.50   294.5 617.9\n## GSM50990           MMSET       3 copies       TT2     1 42.67   241.9  11.9\n## GSM50991             MAF           &lt;NA&gt;       TT2     0 65.00   472.6  38.8\n## GSM50992    Hyperdiploid       2 copies       TT2     0 65.20   664.1  16.9\n##          DEPDC1    IRF4   TP53   WHSC1\n## GSM50986  523.5 16156.5   10.0   261.9\n## GSM50988   21.1 16946.2 1056.9   363.8\n## GSM50989  192.9  8903.9 1762.8 10042.9\n## GSM50990  184.7 11894.7  946.8  4931.0\n## GSM50991  212.0  7563.1  361.4   165.0\n## GSM50992  341.6 16023.4 2096.3   569.2\n\n寻找最佳切点：\n\n# 1. Determine the optimal cutpoint of variables\nres.cut &lt;- surv_cutpoint(myeloma, time = \"time\", event = \"event\",\n                         variables = c(\"DEPDC1\", \"WHSC1\", \"CRIM1\") # 找这3个变量的最佳切点\n                         )\n\nsummary(res.cut)\n##        cutpoint statistic\n## DEPDC1    279.8  4.275452\n## WHSC1    3205.6  3.361330\n## CRIM1      82.3  1.968317\n\n查看根据最佳切点进行分组后的数据分布情况：\n\n# 2. Plot cutpoint for DEPDC1\nplot(res.cut, \"DEPDC1\", palette = \"npg\")\n## $DEPDC1\n\n\n\n\n\n\n\n\n根据最佳切点重新划分数据，这样数据就根据最佳切点变成了高表达/低表达组。\n\n# 3. Categorize variables\nres.cat &lt;- surv_categorize(res.cut)\nhead(res.cat)\n##           time event DEPDC1 WHSC1 CRIM1\n## GSM50986 69.24     0   high   low  high\n## GSM50988 66.43     0    low   low   low\n## GSM50989 66.50     0    low  high  high\n## GSM50990 42.67     1    low  high   low\n## GSM50991 65.00     0    low   low   low\n## GSM50992 65.20     0   high   low   low\n\n根据最佳切点绘制生存曲线：\n\n# 4. Fit survival curves and visualize\nlibrary(\"survival\")\nfit &lt;- survfit(Surv(time, event) ~DEPDC1, data = res.cat)\nggsurvplot(fit, data = res.cat, risk.table = TRUE, conf.int = TRUE)\n\n\n\n\n\n\n\n\n确定最佳切点的R包还有非常多，其他的后续会再介绍。\n下次继续介绍Cox回归。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>R语言生存分析</span>"
    ]
  },
  {
    "objectID": "1032-survival.html#cox回归",
    "href": "1032-survival.html#cox回归",
    "title": "19  R语言生存分析",
    "section": "19.3 Cox回归",
    "text": "19.3 Cox回归\n上次介绍了生存分析中的寿命表、K-M曲线、logrank检验、最佳切点的寻找等，本次主要介绍Cox回归。\n本推文不涉及理论，只有实操，想要了解生存分析的理论的请自行学习。\n使用survival包中的lung数据集用于演示，这是一份关于肺癌患者的生存数据。time是生存时间，以天为单位，status是生存状态，1代表删失，2代表死亡。\n\nrm(list = ls())\nlibrary(survival)\nlibrary(survminer)\n\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\n可以使用cox回归探索危险因素。分类变量需要变为因子型，这样在进行回归时会自动进行哑变量设置。\n\nlung$sex &lt;- factor(lung$sex, labels = c(\"female\",\"male\"))\nlung$ph.ecog &lt;- factor(lung$ph.ecog, labels = c(\"asymptomatic\", \"symptomatic\",\n                                                'in bed &lt;50%','in bed &gt;50%'))\n\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : Factor w/ 4 levels \"asymptomatic\",..: 2 1 1 2 1 2 3 3 2 3 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\n拟合多因素Cox回归模型，这里我们只用sex/age两个变量做演示：\n\nfit.cox &lt;- coxph(Surv(time, status) ~ sex + age + ph.karno, data = lung)\n\n# 查看结果\nsummary(fit.cox)\n## Call:\n## coxph(formula = Surv(time, status) ~ sex + age + ph.karno, data = lung)\n## \n##   n= 227, number of events= 164 \n##    (1 observation deleted due to missingness)\n## \n##               coef exp(coef)  se(coef)      z Pr(&gt;|z|)   \n## sexmale  -0.497170  0.608249  0.167713 -2.964  0.00303 **\n## age       0.012375  1.012452  0.009405  1.316  0.18821   \n## ph.karno -0.013322  0.986767  0.005880 -2.266  0.02348 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##          exp(coef) exp(-coef) lower .95 upper .95\n## sexmale     0.6082     1.6441    0.4378    0.8450\n## age         1.0125     0.9877    0.9940    1.0313\n## ph.karno    0.9868     1.0134    0.9755    0.9982\n## \n## Concordance= 0.637  (se = 0.025 )\n## Likelihood ratio test= 18.81  on 3 df,   p=3e-04\n## Wald test            = 18.73  on 3 df,   p=3e-04\n## Score (logrank) test = 19.05  on 3 df,   p=3e-04\n\n结果解读和logistic回归的结果解读类似：R语言logistic回归的细节解读\n\ncoef是回归系数，\nexp(coef)是HR值，\nse(coef)是回归系数的标准误，\nz是Wald检验的z值，\nPr(&gt;|z|)是回归系数的P值，\nlower .95/upper .95是HR值的95%可信区间。\n\nConcordance= 0.645是Cox回归的C-index，最后给出了Likelihood ratio test似然比检验的统计量、自由度、P值；Wald test的统计量、自由度、P值；Score (logrank) test的统计量、自由度、P值。\n想获得整洁的结果不需要自己提取，只要用神包broom即可：\n\nbroom::tidy(fit.cox, exponentiate = T, conf.int = T)\n## # A tibble: 3 × 7\n##   term     estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 sexmale     0.608   0.168       -2.96 0.00303    0.438     0.845\n## 2 age         1.01    0.00940      1.32 0.188      0.994     1.03 \n## 3 ph.karno    0.987   0.00588     -2.27 0.0235     0.975     0.998\n\n\nestimate：HR值（exp(coef)）\nstd.error：回归系数的标准误（se(coef)）\nstatistic：Wald检验的z值\np.value：回归系数的P值\nconf.low/conf.high：HR的95%的可信区间\n\n构建好Cox回归后，也可以用函数单独提取想要的结果，以下图片展示了可用于提取模型信息的函数，和logistic回归差不多：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n进行Cox回归必须要符合等比例风险假设，关于什么是等比例风险假设，可以参考郑老师的这篇文章：生存分析COX回归，小心你的数据不符合应用条件\n等比例风险的检验可以通过很多方法进行，比如K-M曲线，一般如果有交叉，那么可能不符合等比例风险假设，还可以通过各种残差分布来检验。\n下面是Cox回归的等比例风险假设检验，检验方法是基于Schoenfeld残差：\n\nftest &lt;- cox.zph(fit.cox)\nftest\n##           chisq df      p\n## sex       3.085  1 0.0790\n## age       0.478  1 0.4892\n## ph.karno  8.017  1 0.0046\n## GLOBAL   10.359  3 0.0157\n\n可以看到ph.karno的P值是小于0.05的，其实是不满足等比例风险假设的，下一篇推文会说到不符合等比例风险假设时该怎么办。\n这种方法是基于Schoenfeld残差，检验结果可以通过图示画出来：\n\nlibrary(survminer)\n\nggcoxzph(ftest)\n\n\n\n\n\n\n\n\n可以看到sex和age的回归系数随着时间变化基本没啥变化，稳定在0水平线上，和上面的检验结果一样。\n还可以通过以下方式查看残差的变化：\n\nggcoxdiagnostics(fit.cox, type = \"schoenfeld\")\n## Warning: `gather_()` was deprecated in tidyr 1.2.0.\n## ℹ Please use `gather()` instead.\n## ℹ The deprecated feature was likely used in the survminer package.\n##   Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n\n\n\n\n\n\n\n\n这张图反映的也是回归系数随时间的变化趋势，和上面的图意思一样，如果符合比例风险假设，那么结果应该是一条水平线，从图示来看，这3个变量都是有点问题的，但是真实数据往往不可能是完美的，很少有完全符合要求的数据。\n除了Schoenfeld残差外，ggcoxdiagnostics()还支持其他类型，比如：“martingale”, “deviance”, “score”,“dfbetas” and “scaledsch”在，只需要在type参数中提供合适的类型即可。\ncox回归也是回归分析的一种，可以计算出回归系数和95%的可信区间，因此结果可以通过森林图展示：\n\n# 为了森林图好看点，多选几个变量\nfit.cox &lt;- coxph(Surv(time, status) ~ . , data = lung)\n\nggforest(fit.cox, data = lung,\n         main = \"Hazard ratio\",\n         cpositions = c(0.01, 0.15, 0.35), # 更改前三列的相对位置\n         fontsize = 0.7,\n         refLabel = \"reference\",\n         noDigits = 2\n         )\n\n\n\n\n\n\n\n\n这个结果如果你觉得不好看，或者你还有其他的森林图想做到统一的样式，可以考虑我公众号中介绍的画森林图的方法进行个性化定制:\n\n画一个好看的森林图\n用更简单的方式画森林图\nR语言画森林图系列3\nR语言画森林图系列4\nggplot2绘制森林图(有亚组和没亚组)\n\n以上是Cox回归的主要内容，大家有问题可以加群或者评论区留言。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>R语言生存分析</span>"
    ]
  },
  {
    "objectID": "1032-survival.html#时间依存协变量的cox回归和时间依存系数cox回归",
    "href": "1032-survival.html#时间依存协变量的cox回归和时间依存系数cox回归",
    "title": "19  R语言生存分析",
    "section": "19.4 时间依存协变量的Cox回归和时间依存系数Cox回归",
    "text": "19.4 时间依存协变量的Cox回归和时间依存系数Cox回归\n之前分别介绍了生存分析中的寿命表法、K-M曲线、logrank检验，以及Cox回归的构建、可视化以及比例风险检验的内容。\n本次主要介绍如果数据不符合PH假设时采取的方法。\n关于时依协变量、时依系数的基础知识，大家可以参考这几篇文章：\n\nsurvival包的案例介绍：Using Time Dependent Covariates and Time Dependent Coefcients in the Cox Model\n医咖会：一文详解时依协变量\n7code：含时依协变量的Cox回归\n\n如果不能满足PH假设，可以考虑使用时依协变量或者时依系数Cox回归，时依协变量和时依系数是两个概念，简单来说就是如果一个协变量本身会随着时间而改变，这种叫时依协变量，如果是协变量的系数随着时间改变，这种叫时依系数。\n这里以survival包的veteran数据集为例，演示如何处理此类不符合PH检验的情况。\n\nrm(list = ls())\nlibrary(survival)\nstr(veteran)\n## 'data.frame':    137 obs. of  8 variables:\n##  $ trt     : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ celltype: Factor w/ 4 levels \"squamous\",\"smallcell\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ time    : num  72 411 228 126 118 10 82 110 314 100 ...\n##  $ status  : num  1 1 1 1 1 1 1 1 1 0 ...\n##  $ karno   : num  60 70 60 60 70 20 40 80 50 70 ...\n##  $ diagtime: num  7 5 3 9 11 5 10 29 18 6 ...\n##  $ age     : num  69 64 38 63 65 49 69 68 43 70 ...\n##  $ prior   : num  0 10 0 10 10 0 10 0 0 0 ...\n\n这个数据集中的变量解释如下图：\n\n\n\n\n\n\n\n\n\n首先构建普通的Cox回归，进行等比例风险假设，这里只选择了trt/prior/karno3个变量，而且trt/prior作为分类变量并没有转换为因子型，因为二分类变量数值型和因子型的结果是一样的，转不转换没啥影响！\n\nfit &lt;- coxph(Surv(time, status) ~ trt + prior + karno, data = veteran)\n\n# 进行PH检验\nzp &lt;- cox.zph(fit)\nzp\n##         chisq df       p\n## trt     0.288  1 0.59125\n## prior   2.168  1 0.14087\n## karno  12.138  1 0.00049\n## GLOBAL 18.073  3 0.00042\n\n可以看到变量karno的P值小于0.05，是不满足PH假设的。\n通过图形化方法查看PH检验的结果：\n\n#op &lt;- par(mfrow=c(1,3))\n#plot(zp)\n#par(op)\n#ggcoxdiagnostics(fit, type = \"schoenfeld\")\nplot(zp[3])\nabline(0,0, col=\"red\") # 0水平线\nabline(h=fit$coef[3], col=\"green\", lwd=2, lty=2) \n\n\n\n\n\n\n\n\n黑色实线以及两侧的虚线是karno的系数随着时间变化的曲线，绿色虚线是假设karno符合PH检验时的总体估计线，红色实线是参考线。\n这张图反映了karno变量的系数随着时间的改变，karno偏离的比较厉害（上面注释掉的代码可以都运行看看其他变量的情况），系数最开始接近-0.05，然后逐渐趋于0，最后又开始趋向-0.05，所以它的系数是一致在随着时间改变的，不符合比例风险假设。\n\n19.4.1 对时间分层\n这种情况下一个比较简单的解决方式是对时间使用分层函数。根据上面的图示我们知道karno的系数大概分为3层（3段），可以根据两个拐点进行分层，通过survival中的survSplit()实现。\n\nvet2 &lt;- survSplit(Surv(time, status) ~ ., data= veteran, \n                  cut=c(90, 180), # 两个拐点把时间分为3层（3段）\n                  episode= \"tgroup\", \n                  id=\"id\")\nvet2[1:7, c(\"id\", \"tstart\", \"time\", \"status\", \"tgroup\", \"age\", \"karno\")]\n##   id tstart time status tgroup age karno\n## 1  1      0   72      1      1  69    60\n## 2  2      0   90      0      1  64    70\n## 3  2     90  180      0      2  64    70\n## 4  2    180  411      1      3  64    70\n## 5  3      0   90      0      1  38    60\n## 6  3     90  180      0      2  38    60\n## 7  3    180  228      1      3  38    60\n\n结果多了两列：tstart/tgroup。\n受试者1（id编号为1）在第72天的时候死了，所以数据和之前一样。受试者2和3（id为2和3）虽然时间在变，但是直到第3层才死去，karno的值没有变化。\n重新拟合Cox模型，此时tgroup是分好的层，所以要用strata()，另外karno会随着时间变化，和时间有交互，所以用karno:strata(tgroup)。\n\n# 注意此时Surv()的用法！\nfit2 &lt;- coxph(Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), data = vet2)\nfit2\n## Call:\n## coxph(formula = Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), \n##     data = vet2)\n## \n##                                   coef exp(coef)  se(coef)      z        p\n## trt                          -0.011025  0.989035  0.189062 -0.058    0.953\n## prior                        -0.006107  0.993912  0.020355 -0.300    0.764\n## karno:strata(tgroup)tgroup=1 -0.048755  0.952414  0.006222 -7.836 4.64e-15\n## karno:strata(tgroup)tgroup=2  0.008050  1.008083  0.012823  0.628    0.530\n## karno:strata(tgroup)tgroup=3 -0.008349  0.991686  0.014620 -0.571    0.568\n## \n## Likelihood ratio test=63.04  on 5 df, p=2.857e-12\n## n= 225, number of events= 128\n\n结果表明karno这个变量只有在tgroup=1（第1层，前3个月）才有意义，后面两层是没有意义的。\n再次进行PH检验：\n\ncox.zph(fit2)\n##                      chisq df     p\n## trt                   1.72  1 0.189\n## prior                 3.81  1 0.051\n## karno:strata(tgroup)  3.04  3 0.385\n## GLOBAL                8.03  5 0.154\n\n这时karno:strata(tgroup)就满足了等比例风险假设。\n\n\n19.4.2 连续性时依系数变换\n除了对时间进行分层外，还有一种解决方法。\n上面的图中我们可以看出karno系数随时间变化的曲线明显不是线性的，我们可以通过数据变换把它变成类似线性的，比如取log，这种变换通过tt(time transform)函数实现。\n这种方法实际上是通过tt()函数构建了一个时依协变量，但是这样做是为了解决系数随着时间改变的问题（也就是为了解决时依系数的问题）。\n\nfit3 &lt;- coxph(Surv(time, status) ~ trt + prior + karno + tt(karno), # 对karno进行变换\n              data = veteran, \n              tt = function(x, t, ...) x * log(t+20) # 具体变换方式\n              )\nfit3\n## Call:\n## coxph(formula = Surv(time, status) ~ trt + prior + karno + tt(karno), \n##     data = veteran, tt = function(x, t, ...) x * log(t + 20))\n## \n##                coef exp(coef)  se(coef)      z        p\n## trt        0.016478  1.016614  0.190707  0.086  0.93115\n## prior     -0.009317  0.990726  0.020296 -0.459  0.64619\n## karno     -0.124662  0.882795  0.028785 -4.331 1.49e-05\n## tt(karno)  0.021310  1.021538  0.006607  3.225  0.00126\n## \n## Likelihood ratio test=53.84  on 4 df, p=5.698e-11\n## n= 137, number of events= 128\n\n此时karno的时依系数估计为：-0.124662 * log(t + 20)。\n在构建时依协变量时，可以选择x * t、x * log(t)、x * log(t + 20)、x * log(t + 200)等等，没有明确的规定，要结合结果和图示进行选择，可以参考冯国双老师的文章：一文详解时依协变量。\n我们可以把现在的时依系数估计和经过变换后的的PH检验画在一起，看看变换后的效果：\n\n# 变换后的PH检验\nzp &lt;- cox.zph(fit, transform = function(time) log(time + 20))\n\n# 画图\nplot(zp[3])\nabline(0,0, col=\"red\") # 0水平线\nabline(h=fit$coef[3], col=\"green\", lwd=2, lty=2) # 整体估计\nabline(coef(fit3)[3:4],lwd=2,lty=3,col=\"blue\") # 现在的估计\n\n\n\n\n\n\n\n\n可以看到变换后结果好多了（蓝色虚线，和黑色曲线相比较），虽然还是有一点倾斜。\n以上是两种处理不满足PH假设的方法，实际还有很多种方法，比较常用的是对时间进行分层，其他方法有机会继续介绍。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>R语言生存分析</span>"
    ]
  },
  {
    "objectID": "1032-survival.html#参考资料",
    "href": "1032-survival.html#参考资料",
    "title": "19  R语言生存分析",
    "section": "19.5 参考资料",
    "text": "19.5 参考资料\n\nhttp://www.sthda.com/english/wiki/survival-analysis-basics\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nsurvival包帮助文档\nhttps://mp.weixin.qq.com/s/2rwxeaF_M0UnqPi2F9JNxA",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>R语言生存分析</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html",
    "href": "1033-survivalvis.html",
    "title": "20  R语言生存曲线可视化",
    "section": "",
    "text": "20.1 演示数据\n使用survival包中的lung数据集用于演示，这是一份关于肺癌患者的生存数据。time是生存时间，以天为单位，status是生存状态，1代表删失，2代表死亡。\nlibrary(survival)\nlibrary(survminer)\n\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\nfit &lt;- survfit(Surv(time, status) ~ sex, data = lung)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#基本的生存曲线",
    "href": "1033-survivalvis.html#基本的生存曲线",
    "title": "20  R语言生存曲线可视化",
    "section": "20.2 基本的生存曲线",
    "text": "20.2 基本的生存曲线\n最基本的生存曲线：\n\nggsurvplot(fit, data = lung)\n\n\n\n\n\n\n\n\n删失数据的形状可以更改，默认是+，我们可以改成自己喜欢的：\n\n# 更改删失数据的形状、大小\nggsurvplot(fit, data = lung, censor.shape=\"|\", censor.size = 4)\n\n\n\n\n\n\n\n\n字体都是可以进行更改的！\n\nggsurvplot(fit, data = lung,\n           surv.median.line = \"hv\", # 中位生存时间\n   title = \"Survival curves\", \n   subtitle = \"Based on Kaplan-Meier estimates\",\n   caption = \"created with survminer\",\n   font.title = c(16, \"bold\", \"darkblue\"), # 大小、粗细、颜色\n   font.subtitle = c(15, \"bold.italic\", \"purple\"),\n   font.caption = c(14, \"plain\", \"orange\"),\n   font.x = c(14, \"bold.italic\", \"red\"),\n   font.y = c(14, \"bold.italic\", \"darkred\"),\n   font.tickslab = c(12, \"plain\", \"darkgreen\"))\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n累积风险曲线：\n\nggsurvplot(fit,\n           fun = \"cumhaz\", \n           conf.int = TRUE, # 可信区间\n           palette = \"lancet\", # 支持ggsci配色，自定义颜色，brewer palettes中的配色，等\n           ggtheme = theme_bw() # 支持ggplot2及其扩展包的主题\n)\n\n\n\n\n\n\n\n\n累积事件曲线：\n\nggsurvplot(fit,\n           fun = \"event\", \n           conf.int = TRUE, # 可信区间\n           palette = \"grey\",\n           ggtheme = theme_pubclean() \n)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#增加-risk-table",
    "href": "1033-survivalvis.html#增加-risk-table",
    "title": "20  R语言生存曲线可视化",
    "section": "20.3 增加 risk table",
    "text": "20.3 增加 risk table\n增加多种自定义选项：\n\nggsurvplot(\n  fit,\n  data = lung,\n  size = 1,                 # 更改线条粗细\n  # 配色方案，支持ggsci配色，自定义颜色，brewer palettes中的配色，等\n  palette = \"lancet\",\n  conf.int = TRUE,          # 可信区间\n  pval = TRUE,              # log-rank P值，也可以提供一个数值\n  #计算P值的方法，\n  pval.method = TRUE,       \n  log.rank.weights = \"1\",\n  risk.table = TRUE,        # 增加risk table\n  risk.table.col = \"strata\",# risk table根据分组使用不同颜色\n  legend.labs = c(\"Male\", \"Female\"),    # 图例标签\n  risk.table.height = 0.25, # risk table高度\n  ggtheme = theme_classic2()      # 主题，支持ggplot2及其扩展包的主题\n)\n\n\n\n\n\n\n\n\n计算P值的方法，可参考https://rpkgs.datanovia.com/survminer/articles/Specifiying_weights_in_log-rank_comparisons.html\n\nggsurvplot(\n   fit,                     \n   data = lung,             \n   risk.table = TRUE,       \n   pval = TRUE,             \n   conf.int = TRUE,         \n   xlim = c(0,500),         # 横坐标轴范围，相当于局部放大\n   xlab = \"Time in days\",   # 横坐标标题\n   break.time.by = 100,     # 横坐标刻度\n   ggtheme = theme_light(), \n   risk.table.y.text.col = T, # risk table文字注释颜色\n   risk.table.y.text = FALSE # risk table显示条形而不是文字\n)\n\n\n\n\n\n\n\n\nrisk table的各种字体也都是可以更改的！\n\nggsurvplot(fit, data = lung,\n   title = \"Survival curves\", subtitle = \"Based on Kaplan-Meier estimates\",\n   caption = \"created with survminer\",\n   font.title = c(16, \"bold\", \"darkblue\"),\n   font.subtitle = c(15, \"bold.italic\", \"purple\"),\n   font.caption = c(14, \"plain\", \"orange\"),\n   font.x = c(14, \"bold.italic\", \"red\"),\n   font.y = c(14, \"bold.italic\", \"darkred\"),\n   font.tickslab = c(12, \"plain\", \"darkgreen\"),\n   ########## risk table #########,\n   risk.table = TRUE,\n   risk.table.title = \"Note the risk set sizes\",\n   risk.table.subtitle = \"and remember about censoring.\",\n   risk.table.caption = \"source code: website.com\",\n   risk.table.height = 0.45)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#增加删失时间表ncensor-plot",
    "href": "1033-survivalvis.html#增加删失时间表ncensor-plot",
    "title": "20  R语言生存曲线可视化",
    "section": "20.4 增加删失时间表ncensor plot",
    "text": "20.4 增加删失时间表ncensor plot\n\nggsurvplot(fit, data = lung, risk.table = TRUE, ncensor.plot = TRUE)\n\n\n\n\n\n\n\n\nncensor plot的字体也是支持各种设置的。\n\nggsurvplot(fit, data = lung,\n   title = \"Survival curves\", subtitle = \"Based on Kaplan-Meier estimates\",\n   caption = \"created with survminer\",\n   font.title = c(16, \"bold\", \"darkblue\"),\n   font.subtitle = c(15, \"bold.italic\", \"purple\"),\n   font.caption = c(14, \"plain\", \"orange\"),\n   font.x = c(14, \"bold.italic\", \"red\"),\n   font.y = c(14, \"bold.italic\", \"darkred\"),\n   font.tickslab = c(12, \"plain\", \"darkgreen\"),\n   ########## risk table #########,\n   risk.table = TRUE,\n   risk.table.title = \"Note the risk set sizes\",\n   risk.table.subtitle = \"and remember about censoring.\",\n   risk.table.caption = \"source code: website.com\",\n   risk.table.height = 0.2,\n   ## ncensor plot ##\n   ncensor.plot = TRUE,\n   ncensor.plot.title = \"Number of censorings\",\n   ncensor.plot.subtitle = \"over the time.\",\n   ncensor.plot.caption = \"data available at data.com\",\n   ncensor.plot.height = 0.25)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#超级无敌精细化自定设置",
    "href": "1033-survivalvis.html#超级无敌精细化自定设置",
    "title": "20  R语言生存曲线可视化",
    "section": "20.5 超级无敌精细化自定设置",
    "text": "20.5 超级无敌精细化自定设置\n首先设置好自己的默认样式：\n\nggsurv &lt;- ggsurvplot(\n           fit,                     \n           data = lung,             \n           risk.table = TRUE,       \n           pval = TRUE,             \n           conf.int = TRUE,         \n           palette = c(\"#E7B800\", \"#2E9FDF\"),\n           xlim = c(0,500),         \n           xlab = \"Time in days\",   \n           break.time.by = 100,     \n           ggtheme = theme_light(), \n          risk.table.y.text.col = T,\n          risk.table.height = 0.25, \n          risk.table.y.text = FALSE,\n          ncensor.plot = TRUE,      \n          ncensor.plot.height = 0.25,\n          conf.int.style = \"step\",  # customize style of confidence intervals\n          surv.median.line = \"hv\",  \n          legend.labs = c(\"Male\", \"Female\")    \n        )\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\nggsurv\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n自定义一个函数，用来更改各种样式：\n\ncustomize_labels &lt;- function (p, font.title = NULL,\n                              font.subtitle = NULL, font.caption = NULL,\n                              font.x = NULL, font.y = NULL, font.xtickslab = NULL, font.ytickslab = NULL)\n{\n  original.p &lt;- p\n  if(is.ggplot(original.p)) list.plots &lt;- list(original.p)\n  else if(is.list(original.p)) list.plots &lt;- original.p\n  else stop(\"Can't handle an object of class \", class (original.p))\n  .set_font &lt;- function(font){\n    font &lt;- ggpubr:::.parse_font(font)\n    ggtext::element_markdown (size = font$size, face = font$face, colour = font$color)\n  }\n  for(i in 1:length(list.plots)){\n    p &lt;- list.plots[[i]]\n    if(is.ggplot(p)){\n      if (!is.null(font.title)) p &lt;- p + theme(plot.title = .set_font(font.title))\n      if (!is.null(font.subtitle)) p &lt;- p + theme(plot.subtitle = .set_font(font.subtitle))\n      if (!is.null(font.caption)) p &lt;- p + theme(plot.caption = .set_font(font.caption))\n      if (!is.null(font.x)) p &lt;- p + theme(axis.title.x = .set_font(font.x))\n      if (!is.null(font.y)) p &lt;- p + theme(axis.title.y = .set_font(font.y))\n      if (!is.null(font.xtickslab)) p &lt;- p + theme(axis.text.x = .set_font(font.xtickslab))\n      if (!is.null(font.ytickslab)) p &lt;- p + theme(axis.text.y = .set_font(font.ytickslab))\n      list.plots[[i]] &lt;- p\n    }\n  }\n  if(is.ggplot(original.p)) list.plots[[1]]\n  else list.plots\n}\n\n然后分别对上面图形的3个部分（生存曲线、risk table、ncensor plot）进行个性化自定义\n\n# 更改生存曲线的标签\nggsurv$plot &lt;- ggsurv$plot + labs(\n  title    = \"Survival curves\",\n  subtitle = \"Based on Kaplan-Meier estimates\",\n  caption  = \"created with survminer\"\n  )\n\n# 更改risk table的标签\nggsurv$table &lt;- ggsurv$table + labs(\n  title    = \"Note the risk set sizes\",\n  subtitle = \"and remember about censoring.\",\n  caption  = \"source code: website.com\"\n  )\n\n# 更改ncensor plot的标签 \nggsurv$ncensor.plot &lt;- ggsurv$ncensor.plot + labs(\n  title    = \"Number of censorings\",\n  subtitle = \"over the time.\",\n  caption  = \"source code: website.com\"\n  )\n\n# 更改生存曲线，risk table，ncensor plot的字体大小、类型、颜色\n\nggsurv &lt;- customize_labels(\n  ggsurv,\n  font.title    = c(16, \"bold\", \"darkblue\"),\n  font.subtitle = c(15, \"bold.italic\", \"purple\"),\n  font.caption  = c(14, \"plain\", \"orange\"),\n  font.x        = c(14, \"bold.italic\", \"red\"),\n  font.y        = c(14, \"bold.italic\", \"darkred\"),\n  font.xtickslab = c(12, \"plain\", \"darkgreen\")\n)\n\nggsurv\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#多个组的生存曲线",
    "href": "1033-survivalvis.html#多个组的生存曲线",
    "title": "20  R语言生存曲线可视化",
    "section": "20.6 多个组的生存曲线",
    "text": "20.6 多个组的生存曲线\n如果你的分类变量是多个组别的（常见的都是两组比较的），会自动画出多条生存曲线。如果你有多个分类自变量，会自动画出所有组合的生存曲线。\n使用colon数据集，其中time是时间，status是生存状态，1为发生终点事件，0为删失，rx是治疗方式，有三种：observation、Levamisole、Levamisole+5-FU，obstruct是肿瘤是否阻塞结肠，有为1，无为0，adhere是肿瘤是否粘附附近器官，有为1，无为0。\n\nrm(list = ls())\nlibrary(survival)\nlibrary(survminer)\n\npsych::headTail(colon)\n##       id study      rx sex age obstruct perfor adhere nodes status differ\n## 1      1     1 Lev+5FU   1  43        0      0      0     5      1      2\n## 2      1     1 Lev+5FU   1  43        0      0      0     5      1      2\n## 3      2     1 Lev+5FU   1  63        0      0      0     1      0      2\n## 4      2     1 Lev+5FU   1  63        0      0      0     1      0      2\n## ...  ...   ...    &lt;NA&gt; ... ...      ...    ...    ...   ...    ...    ...\n## 1855 928     1 Lev+5FU   0  48        1      0      0     4      0      2\n## 1856 928     1 Lev+5FU   0  48        1      0      0     4      0      2\n## 1857 929     1     Lev   0  66        1      0      0     1      0      2\n## 1858 929     1     Lev   0  66        1      0      0     1      0      2\n##      extent surg node4 time etype\n## 1         3    0     1 1521     2\n## 2         3    0     1  968     1\n## 3         3    0     0 3087     2\n## 4         3    0     0 3087     1\n## ...     ...  ...   ...  ...   ...\n## 1855      3    1     1 2072     2\n## 1856      3    1     1 2072     1\n## 1857      3    0     0 1820     2\n## 1858      3    0     0 1820     1\n\n# 两个分类变量\nfit2 &lt;- survfit( Surv(time, status) ~ rx + obstruct, data = colon )\n\n# 结果会给出所有组合的生存曲线\nggsurvplot(fit2, pval = TRUE, \n           risk.table = TRUE,\n           risk.table.height = 0.3\n           )",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#多个分类变量分面绘制",
    "href": "1033-survivalvis.html#多个分类变量分面绘制",
    "title": "20  R语言生存曲线可视化",
    "section": "20.7 多个分类变量分面绘制",
    "text": "20.7 多个分类变量分面绘制\n还是以colon数据集为例，这次我们用3个变量：sex/rx/adhere，这3个都是分类变量。\n首先构建生存函数：\n\nfit3 &lt;- survfit(Surv(time, status) ~ sex + rx + adhere, data = colon )\n\n然后把生存曲线保存为一个对象：\n\nggsurv &lt;- ggsurvplot(fit3, data = colon,\n  fun = \"cumhaz\", conf.int = TRUE,\n  risk.table = TRUE, risk.table.col=\"strata\",\n  ggtheme = theme_bw())\n\n接下来就可以分别提取生存曲线（这里是cumhaz，累积风险曲线）、risk table、删失事件表，根据不同的变量进行分面即可：\n\n# 分面累积风险曲线\ncurv_facet &lt;- ggsurv$plot + facet_grid(rx ~ adhere)\ncurv_facet\n\n\n\n\n\n\n\n\n\n# 分面risk table，和上面的累积风险曲线分面方法一样\nggsurv$table + facet_grid(rx ~ adhere, scales = \"free\")+\n theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n# risk table另一种分面方法，由于有3个分类变量，可以选择自己需要的分面方法\ntbl_facet &lt;- ggsurv$table + facet_grid(.~ adhere, scales = \"free\")\ntbl_facet + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n# 重新安排下布局，把生存曲线和risk table画在一起\ng2 &lt;- ggplotGrob(curv_facet)\ng3 &lt;- ggplotGrob(tbl_facet)\nmin_ncol &lt;- min(ncol(g2), ncol(g3))\ng &lt;- gridExtra::gtable_rbind(g2[, 1:min_ncol], g3[, 1:min_ncol], size=\"last\")\ng$widths &lt;- grid::unit.pmax(g2$widths, g3$widths)\ngrid::grid.newpage()\ngrid::grid.draw(g)\n\n\n\n\n\n\n\n\n如果想根据某个变量进行分组绘制生存曲线，然后分面展示，也可以用ggsurvplot_facet()实现：\n\nfit &lt;- survfit( Surv(time, status) ~ sex, data = colon )\n\n# 根据rx进行分组，展示每个组内的生存曲线\nggsurvplot_facet(fit, colon, \n                 facet.by = \"rx\",\n                 palette = \"jco\", \n                 pval = TRUE)\n## Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n## ℹ Please use `as_tibble()` instead.\n## ℹ The signature and semantics have changed, see `?as_tibble`.\n## ℹ The deprecated feature was likely used in the survminer package.\n##   Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n## Warning: `select_()` was deprecated in dplyr 0.7.0.\n## ℹ Please use `select()` instead.\n## ℹ The deprecated feature was likely used in the survminer package.\n##   Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n\n\n\n\n\n\n\n\n还可以根据多个变量进行分面展示：\n\nggsurvplot_facet(fit, colon, facet.by = c(\"rx\", \"adhere\"),\n                palette = \"jco\", pval = TRUE)\n\n\n\n\n\n\n\n\n\nfit2 &lt;- survfit( Surv(time, status) ~ sex + rx, data = colon )\nggsurvplot_facet(fit2, colon, facet.by = \"adhere\",\n                palette = \"jco\", pval = TRUE)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#同时绘制多个生存函数",
    "href": "1033-survivalvis.html#同时绘制多个生存函数",
    "title": "20  R语言生存曲线可视化",
    "section": "20.8 同时绘制多个生存函数",
    "text": "20.8 同时绘制多个生存函数\n\ndata(colon)\n## Warning in data(colon): data set 'colon' not found\nf1 &lt;- survfit(Surv(time, status) ~ adhere, data = colon)\nf2 &lt;- survfit(Surv(time, status) ~ rx, data = colon)\nfits &lt;- list(sex = f1, rx = f2)\n\n# 一下子画好！在循环出图时有用处\nlegend.title &lt;- list(\"sex\", \"rx\")\nggsurvplot_list(fits, colon, legend.title = legend.title)\n## $sex\n\n\n\n\n\n\n\n## \n## $rx\n\n\n\n\n\n\n\n## \n## attr(,\"class\")\n## [1] \"list\"            \"ggsurvplot_list\"",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#根据某一个变量分组绘制",
    "href": "1033-survivalvis.html#根据某一个变量分组绘制",
    "title": "20  R语言生存曲线可视化",
    "section": "20.9 根据某一个变量分组绘制",
    "text": "20.9 根据某一个变量分组绘制\n比如以colon数据为例，我们想以rx（治疗方式）进行分组，然后看每个组内的生存曲线，可以通过ggsurvplot_group_by()实现。\n\nrm(list = ls())\n\nfit &lt;- survfit( Surv(time, status) ~ sex, data = colon )\n\n# Visualize: grouped by treatment rx\n#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\nggsurv.list &lt;- ggsurvplot_group_by(fit, colon, group.by = \"rx\", risk.table = TRUE,\n                                 pval = TRUE, conf.int = TRUE, palette = \"jco\")\nnames(ggsurv.list)\n## [1] \"rx.Obs::sex\"     \"rx.Lev::sex\"     \"rx.Lev+5FU::sex\"\n\n这个图形和上面的分面展示中的ggsurvplot_facet画出来的图形是一样的，区别就是一个是分面，这个是分开多个图形！\n可以根据多个变量进行分组，比如下面这个情况，会分别绘制6张生存曲线图：\n\n# Visualize: grouped by treatment rx and adhere\nggsurv.list &lt;- ggsurvplot_group_by(fit, colon, group.by = c(\"rx\", \"adhere\"),\n                                 risk.table = TRUE,\n                                 pval = TRUE, conf.int = TRUE, palette = \"jco\")\n\n# 6张图的名字，图没有画出来，感兴趣的可以自己试试看\nnames(ggsurv.list)\n## [1] \"rx:Obs, adhere:0::sex\"     \"rx:Obs, adhere:1::sex\"    \n## [3] \"rx:Lev, adhere:0::sex\"     \"rx:Lev, adhere:1::sex\"    \n## [5] \"rx:Lev+5FU, adhere:0::sex\" \"rx:Lev+5FU, adhere:1::sex\"",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#在原有生存曲线的基础上增加",
    "href": "1033-survivalvis.html#在原有生存曲线的基础上增加",
    "title": "20  R语言生存曲线可视化",
    "section": "20.10 在原有生存曲线的基础上增加",
    "text": "20.10 在原有生存曲线的基础上增加\n先画好一个生存曲线图，然后在原图的基础上添加新的生存曲线图，类似于base r中常用的add = T，比如在这篇推文中介绍的：多个时间点和多指标生存曲线\n\nlibrary(survival)\n\n# 注意这里的surv_fit，是survfit的封装\nfit &lt;- surv_fit(Surv(time, status) ~ sex, data = lung)\n\n# Visualize survival curves\nggsurvplot(fit, data = lung,\n          risk.table = TRUE, pval = TRUE,\n          surv.median.line = \"hv\", palette = \"jco\")\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n在上面图形的基础上添加所有人的总的生存曲线：\n\n# Add survival curves of pooled patients (Null model)\n# Use add.all = TRUE option\nggsurvplot(fit, data = lung,\n          risk.table = TRUE, pval = TRUE,\n          surv.median.line = \"hv\", palette = \"jco\",\n          add.all = TRUE)\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 3 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 3 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 3 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 3 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#多个生存函数画在一起",
    "href": "1033-survivalvis.html#多个生存函数画在一起",
    "title": "20  R语言生存曲线可视化",
    "section": "20.11 多个生存函数画在一起",
    "text": "20.11 多个生存函数画在一起\n比如把PFS和OS的生存曲线画在一张图上。\n\nrm(list = ls())\n# 构建一个示例数据集\nset.seed(123)\ndemo.data &lt;- data.frame(\n   os.time = colon$time,\n   os.status = colon$status,\n   pfs.time = sample(colon$time),\n   pfs.status = colon$status,\n   sex = colon$sex, rx = colon$rx, adhere = colon$adhere\n )\n\n# 总体的PFS和OS生存曲线\npfs &lt;- survfit( Surv(pfs.time, pfs.status) ~ 1, data = demo.data)\nos &lt;- survfit( Surv(os.time, os.status) ~ 1, data = demo.data)\n\n# Combine on the same plot\nfit &lt;- list(PFS = pfs, OS = os)\nggsurvplot_combine(fit, demo.data)\n\n\n\n\n\n\n\n\n这个情况你用ggsurvplot_list也能画，不过就是分开的两个图形了！\n如果是分类变量会自动画出多条生存曲线：\n\npfs &lt;- survfit( Surv(pfs.time, pfs.status) ~ rx, data = demo.data)\nos &lt;- survfit( Surv(os.time, os.status) ~ rx, data = demo.data)\n# Combine on the same plot\nfit &lt;- list(PFS = pfs, OS = os)\nggsurvplot_combine(fit, demo.data)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#参考资料",
    "href": "1033-survivalvis.html#参考资料",
    "title": "20  R语言生存曲线可视化",
    "section": "20.12 参考资料",
    "text": "20.12 参考资料\n\nsurvminer包帮助文档",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>R语言生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1020-discriminant.html",
    "href": "1020-discriminant.html",
    "title": "21  R语言判别分析",
    "section": "",
    "text": "21.1 Fisher判别分析\nFisher判别又称为典型判别（canonical discriminant）分析，适用于两类和多分类判别。\nFisher判别使用贝叶斯定理确定每个观测属于某个类别的概率。如果你有两个类别，比如良性和恶性，判别分析会分别计算属于两个类别的概率，然后选择概率大的类别作为正确的类别。\n线性判别分析假设每个类中的观测服从多元正态分布，并且不同类别之间的协方差相等。二次判别假设观测服从正态分布，每种类别都有自己的协方差。\n使用孙振球版《医学统计学》第4版例20-1的数据。电子版及配套数据已上传到QQ群，需要的加群下载即可。\n收集了22例肝硬化患者的3个指标，其中早期患者（用1表示）12例，晚期患者（用2表示），试做判别分析。\ndf &lt;- read.csv(\"datasets/例20-1.csv\")\n\npsych::headTail(df)\n##      id  x1  x2  x3   y\n## 1     1  23   8   0   1\n## 2     2  -1   9  -2   1\n## 3     3 -10   5   0   1\n## 4     4  -7  -2   1   1\n## ... ... ... ... ... ...\n## 19   19  -9 -20   3   2\n## 20   20  -7  -2   3   2\n## 21   21  -9   6   0   2\n## 22   22  12   0   0   2\n这个数据集中id是编号，x1,x2,x3是自变量，y是因变量。\n线性判别分析可以通过MASS包中的lda函数实现：\nlibrary(MASS)\n\nfit &lt;- lda(y ~ x1+x2+x3, data = df)\nfit\n## Call:\n## lda(y ~ x1 + x2 + x3, data = df)\n## \n## Prior probabilities of groups:\n##         1         2 \n## 0.5454545 0.4545455 \n## \n## Group means:\n##   x1 x2 x3\n## 1 -3  4 -1\n## 2  4 -5  1\n## \n## Coefficients of linear discriminants:\n##           LD1\n## x1  0.0395150\n## x2 -0.1265698\n## x3  0.1792631\nPrior probabilities of groups是先验概率，类别1的概率是0.5454545，类别2是0.4545455。\n然后给出了每个组在不同类别中的均值。\n最下面给出了线性判别系数，如果你的结果变量是3个类别，会给出两组判别系数，这里我的结果变量只有2分类，所以结果只有1组。\n结果可以画出来：\nplot(fit,type=\"both\")\n上图是判别分析结果的直方图和密度图，可以看出组间有重合，说明有些分组分错了。\n下面用predict提取判别分析的分类结果。\npredict用于判别分析可以得到3种类型的结果，class是类别，posterior是概率，x是线性判别评分。\npred &lt;- predict(fit)$class\ntable(df$y, pred)\n##    pred\n##      1  2\n##   1 11  1\n##   2  2  8\n可以看到有3个分类分错了，结果还是可以的。\n可以查看每个患者的后验概率：\n# 查看概率\npredict(fit)$posterior\n##             1           2\n## 1  0.62566758 0.374332416\n## 2  0.95508370 0.044916302\n## 3  0.89600449 0.103995511\n## 4  0.51330556 0.486694443\n## 5  0.95464457 0.045355435\n## 6  0.88314148 0.116858515\n## 7  0.77454260 0.225457398\n## 8  0.99508599 0.004914013\n## 9  0.89391137 0.106088634\n## 10 0.84899794 0.151002059\n## 11 0.31960372 0.680396284\n## 12 0.64144092 0.358559076\n## 13 0.14903037 0.850969632\n## 14 0.57026493 0.429735074\n## 15 0.13106732 0.868932682\n## 16 0.26925350 0.730746503\n## 17 0.03911397 0.960886034\n## 18 0.04332382 0.956676176\n## 19 0.01115243 0.988847571\n## 20 0.35826933 0.641730670\n## 21 0.90954200 0.090457999\n## 22 0.37480490 0.625195100\n上面的图我们也可以用ggplot2画出来。\ndf.plot &lt;- data.frame(LD1 = predict(fit)$x[,1],\n                      y = factor(df$y,labels = c(\"早期患者\",\"晚期患者\"))\n                      )\n\nlibrary(ggplot2)\n\nggplot(df.plot, aes(x=LD1, fill=y))+\n  geom_histogram()+\n  facet_wrap(~ y, ncol = 1)\n如果你想用这个模型预测新的数据，只需要predict(fit, newdata = xxx)即可。比如我们新建一个数据：\ntmp &lt;- data.frame(x1 = c(-9,-7,-9),\n                  x2 = c(-18,-2,6),\n                  x3 = c(3,3,1)\n                  )\n\npredict(fit, newdata = tmp)\n## $class\n## [1] 2 2 1\n## Levels: 1 2\n## \n## $posterior\n##            1         2\n## 1 0.01736557 0.9826344\n## 2 0.35826933 0.6417307\n## 3 0.87974275 0.1202573\n## \n## $x\n##          LD1\n## 1  2.4580167\n## 2  0.5119296\n## 3 -0.9381851\n这样就得到新的结果。\n我们再用一个iris鸢尾花数据集演示下线性判别分析的结果可视化，这个结果变量是3分类的。\nstr(iris)\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n拟合模型：\nlibrary(MASS)\n\nfit &lt;- lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris)\nfit\n## Call:\n## lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, \n##     data = iris)\n## \n## Prior probabilities of groups:\n##     setosa versicolor  virginica \n##  0.3333333  0.3333333  0.3333333 \n## \n## Group means:\n##            Sepal.Length Sepal.Width Petal.Length Petal.Width\n## setosa            5.006       3.428        1.462       0.246\n## versicolor        5.936       2.770        4.260       1.326\n## virginica         6.588       2.974        5.552       2.026\n## \n## Coefficients of linear discriminants:\n##                     LD1         LD2\n## Sepal.Length  0.8293776 -0.02410215\n## Sepal.Width   1.5344731 -2.16452123\n## Petal.Length -2.2012117  0.93192121\n## Petal.Width  -2.8104603 -2.83918785\n## \n## Proportion of trace:\n##    LD1    LD2 \n## 0.9912 0.0088\n可视化结果：\niris$LD1 &lt;- predict(fit)$x[,1]\niris$LD2 &lt;- predict(fit)$x[,2]\n\nlibrary(ggplot2)\n\nggplot(iris, aes(LD1,LD2))+\n  geom_point(aes(color=Species),size=3)\nggplot(iris, aes(x=LD1, fill=Species))+\n  geom_histogram()+\n  facet_wrap(~ Species, ncol = 1)\n二次判别分析和线性判别分析用法一样。\nfit &lt;- qda(y ~ x1+x2+x3, data = df)\nfit\n## Call:\n## qda(y ~ x1 + x2 + x3, data = df)\n## \n## Prior probabilities of groups:\n##         1         2 \n## 0.5454545 0.4545455 \n## \n## Group means:\n##   x1 x2 x3\n## 1 -3  4 -1\n## 2  4 -5  1\n结果不含判别系数，查看分类结果：\npred &lt;- predict(fit)$class\ntable(df$y, pred)\n##    pred\n##      1  2\n##   1 10  2\n##   2  1  9\n也是3个分错了。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>R语言判别分析</span>"
    ]
  },
  {
    "objectID": "1020-discriminant.html#bayes判别分析",
    "href": "1020-discriminant.html#bayes判别分析",
    "title": "21  R语言判别分析",
    "section": "21.2 Bayes判别分析",
    "text": "21.2 Bayes判别分析\n贝叶斯判别也是根据概率大小进行判别，要求各类近似服从多元正态分布。当各类的协方差相等时，可得到线性贝叶斯判别函数，当各类的协方差不相等时，可得到二次贝叶斯判别函数。\n欲用4个标化后的影像学指标鉴别脑囊肿（1）、胶质瘤（2）、转移瘤（3），收集了17个病例，试建立判别贝叶斯函数。\n\ndf &lt;- read.csv(\"datasets/例20-4.csv\")\n\ndf$y &lt;- factor(df$y)\n\npsych::headTail(df)\n##       x1    x2  x3  x4    y\n## 1      6 -11.5  19  90    1\n## 2    -11 -18.5  25 -36    3\n## 3   90.2   -17  17   3    2\n## 4     -4   -15  13  54    1\n## ...  ...   ... ... ... &lt;NA&gt;\n## 14    10   -18  14  50    1\n## 15    -8   -14  16  56    1\n## 16   0.6   -13  26  21    3\n## 17   -40   -20  22 -50    3\n\n使用klaR包实现贝叶斯判别分析：\n\nlibrary(klaR)\n\nfit &lt;- NaiveBayes(y ~ ., data = df)\nfit\n## $apriori\n## grouping\n##         1         2         3 \n## 0.4117647 0.2352941 0.3529412 \n## \n## $tables\n## $tables$x1\n##        [,1]     [,2]\n## 1 -14.42857 38.26163\n## 2   0.80000 78.10779\n## 3  -6.65000 19.78017\n## \n## $tables$x2\n##        [,1]     [,2]\n## 1 -17.34286 4.103599\n## 2 -17.42500 3.085855\n## 3 -17.33333 4.143268\n## \n## $tables$x3\n##       [,1]     [,2]\n## 1 12.71429 4.990467\n## 2 17.50000 2.081666\n## 3 20.16667 6.493587\n## \n## $tables$x4\n##        [,1]     [,2]\n## 1  31.14286 44.03948\n## 2   0.00000 30.75711\n## 3 -15.00000 35.83295\n## \n## \n## $levels\n## [1] \"1\" \"2\" \"3\"\n## \n## $call\n## NaiveBayes.default(x = X, grouping = Y)\n## \n## $x\n##        x1    x2 x3  x4\n## 1     6.0 -11.5 19  90\n## 2   -11.0 -18.5 25 -36\n## 3    90.2 -17.0 17   3\n## 4    -4.0 -15.0 13  54\n## 5     0.0 -14.0 20  35\n## 6     0.5 -11.5 19  37\n## 7   -10.0 -19.0 21 -42\n## 8     0.0 -23.0  5 -35\n## 9    20.0 -22.0  8 -20\n## 10 -100.0 -21.4  7 -15\n## 11 -100.0 -21.5 15 -40\n## 12   13.0 -17.2 18   2\n## 13   -5.0 -18.5 15  18\n## 14   10.0 -18.0 14  50\n## 15   -8.0 -14.0 16  56\n## 16    0.6 -13.0 26  21\n## 17  -40.0 -20.0 22 -50\n## \n## $usekernel\n## [1] FALSE\n## \n## $varnames\n## [1] \"x1\" \"x2\" \"x3\" \"x4\"\n## \n## attr(,\"class\")\n## [1] \"NaiveBayes\"\n\n获取预测结果，并查看混淆矩阵：\n\npred &lt;- predict(fit)$class\n## Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n## observation 10\ntable(pred, df$y)\n##     \n## pred 1 2 3\n##    1 7 0 1\n##    2 0 3 0\n##    3 0 1 5\n\n只有两个分错了。\n如果要预测新的数据，只需要predict(fit, newdata = xxx)即可。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>R语言判别分析</span>"
    ]
  },
  {
    "objectID": "1021-cluster.html",
    "href": "1021-cluster.html",
    "title": "22  R语言聚类分析",
    "section": "",
    "text": "22.1 系统聚类（层次聚类,Hierarchical clustering）\n# 没安装flexclust包的需要先安装\ndata(nutrient, package = \"flexclust\")\nrow.names(nutrient) &lt;- tolower(row.names(nutrient))\n\ndim(nutrient) # 27行5列\n## [1] 27  5\n\npsych::headTail(nutrient)\n##                 energy protein fat calcium iron\n## beef braised       340      20  28       9  2.6\n## hamburger          245      21  17       9  2.7\n## beef roast         420      15  39       7    2\n## beef steak         375      19  32       9  2.6\n## ...                ...     ... ...     ...  ...\n## salmon canned      120      17   5     159  0.7\n## sardines canned    180      22   9     367  2.5\n## tuna canned        170      25   7       7  1.2\n## shrimp canned      110      23   1      98  2.6\n层次聚类在R语言中非常简单，通过hclust实现。\n# 聚类前先进行标准化\nnutrient.scaled &lt;- scale(nutrient)\nh.clust &lt;- hclust(dist(nutrient.scaled,method = \"euclidean\"), # 计算距离有不同方法\n                  method = \"average\" # 层次聚类有不同方法\n                  )\n下面就是画图，简单点可以直接用plot()。\nplot(h.clust,hang = -1,main = \"层次聚类\", sub=\"\", \n     xlab=\"\", cex.lab = 1.0, cex.axis = 1.0, cex.main = 2)\n关于更加精细化的细节修改，下面会介绍。或者可以借助其他R包快速绘制好看的聚类分析图形。\n如何选择聚类的个数呢？\n可以通过R包NbClust实现。\nlibrary(NbClust)\n\nnc &lt;- NbClust(nutrient.scaled, distance = \"euclidean\",\n              min.nc = 2, # 最小聚类数\n              max.nc = 10, # 最大聚类树\n              method = \"average\"\n              )\n\n\n\n\n\n\n\n## *** : The Hubert index is a graphical method of determining the number of clusters.\n##                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n##                 significant increase of the value of the measure i.e the significant peak in Hubert\n##                 index second differences plot. \n## \n\n\n\n\n\n\n\n## *** : The D index is a graphical method of determining the number of clusters. \n##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n##                 second differences plot) that corresponds to a significant increase of the value of\n##                 the measure. \n##  \n## ******************************************************************* \n## * Among all indices:                                                \n## * 5 proposed 2 as the best number of clusters \n## * 5 proposed 3 as the best number of clusters \n## * 2 proposed 4 as the best number of clusters \n## * 4 proposed 5 as the best number of clusters \n## * 1 proposed 8 as the best number of clusters \n## * 1 proposed 9 as the best number of clusters \n## * 5 proposed 10 as the best number of clusters \n## \n##                    ***** Conclusion *****                            \n##  \n## * According to the majority rule, the best number of clusters is  2 \n##  \n##  \n## *******************************************************************\n输出日志里给出了评判准则以及最终结果：Hubert index和D index使用图形的方式判断最佳聚类个数，拐点明显的可视作最佳聚类个数。\n它给出的结论是最佳聚类数是2。我们也可以通过条形图查看这些评判准则的具体数量。\nbarplot(table(nc$Best.nc[1,]),\n        xlab = \"聚类数目\",\n        ylab = \"评判准则个数\"\n        )\n从条形图中可以看出，聚类数目为2,3,5,10时，评判准则个数最多，为5个，这里我们可以选择5个。\n# 把聚类树划分为5类\ncluster &lt;- cutree(h.clust, k=5)\n\n# 查看每一类有多少例\ntable(cluster)\n## cluster\n##  1  2  3  4  5 \n##  7 16  1  2  1\n把最终结果画出来：\nplot(h.clust, hang = -1,main = \"\",xlab = \"\")\nrect.hclust(h.clust, k=5) # 添加矩形，方便观察",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>R语言聚类分析</span>"
    ]
  },
  {
    "objectID": "1021-cluster.html#聚类分析可视化",
    "href": "1021-cluster.html#聚类分析可视化",
    "title": "22  R语言聚类分析",
    "section": "22.2 聚类分析可视化",
    "text": "22.2 聚类分析可视化\n上面使用默认的plot函数进行聚类树的可视化，下面继续扩展聚类树的可视化。\n默认的聚类树可视化函数已经非常好用，有非常多的自定义设置，可以轻松实现好看的聚类树可视化。\n\nop &lt;- par(bg = \"grey90\")\n\nplot(h.clust, main = \"层次聚类\", sub=\"\", xlab = \"\",\n     col = \"#487AA1\", col.main = \"#45ADA8\", col.lab = \"#7C8071\",\n     col.axis = \"#F38630\", lwd = 2, lty = 1, hang = -1, axes = FALSE)\n# add axis\naxis(side = 2, at = 0:5, col = \"#F38630\",\n     labels = FALSE, lwd = 2)\n# add text in margin\nmtext(0:5, side = 2, at = 0:5,\n      line = 1, col = \"#A38630\", las = 2)\n\n\n\n\n\n\n\n\npar(op)\n\n如果对默认的可视化效果不满意，可以先用as.dendrogram()转化一下，再画图可以指定更多细节。\n\ndhc &lt;- as.dendrogram(h.clust)\nplot(dhc,type = \"triangle\") # 比如换个类型\n\n\n\n\n\n\n\n\n可以提取部分树进行查看，使用cut指定某个高度以上或以下的树进行查看。\n\nop &lt;- par(mfrow = c(2, 1))\n\n# 高度在3以上的树\nplot(cut(dhc, h = 3)$upper, main = \"Upper tree of cut at h=3\")\n\n# 高度在3以下的树\nplot(cut(dhc, h = 3)$lower[[2]],\n     main = \"Second branch of lower tree with cut at h=3\")\n\n\n\n\n\n\n\n\npar(op)\n\n每一个节点都有不同的属性，比如颜色、形状等，我们可以用函数修改每个节点的属性。\n比如修改标签的颜色。\n\n# 按照上面画出来的结果，我们可以分为5类，所以准备好5个颜色\nlabelColors = c(\"#CDB380\", \"#036564\", \"#EB6841\", \"#EDC951\", \"#487AA1\")\n\n# 把聚类树分为5个类\nclusMember &lt;- cutree(h.clust,k=5)\n\n# 给标签增加不同的颜色\ncolLab &lt;- function(n) {\n  if (is.leaf(n)) {\n    a &lt;- attributes(n)\n    labCol &lt;- labelColors[clusMember[which(names(clusMember) == a$label)]]\n    attr(n, \"nodePar\") &lt;- c(a$nodePar,\n                            list(cex=1.5, # 节点形状大小\n                                 pch=20, # 节点形状\n                                 col=labCol, # 节点颜色\n                                 lab.col=labCol, # 标签颜色\n                                 lab.font=2, # 标签字体，粗体斜体粗斜体\n                                 lab.cex=1 # 标签大小\n                                 )\n                            )\n  }\n  n\n}\n\n# 把自定义标签颜色应用到聚类树中\ndiyDendro = dendrapply(dhc, colLab)    \n\n# 画图\nplot(diyDendro, main = \"DIY Dendrogram\")  \n\n# 加图例\nlegend(\"topright\", \n     legend = c(\"Cluster 1\",\"Cluster 2\",\"Cluster 3\",\"Cluster 4\",\"Cluster 5\"), \n     col = c(\"#CDB380\", \"#036564\", \"#EB6841\", \"#EDC951\", \"#487AA1\"), \n     pch = c(20,20,20,20,20), bty = \"n\", pt.cex = 2, cex = 1 , \n     text.col = \"black\", horiz = FALSE, inset = c(0, 0.1))\n\n\n\n\n\n\n\n\n如果想要更加精美的聚类分析可视化，可以参考之前的几篇推文：\n\n又是聚类分析可视化\nR语言可视化聚类树\nR语言画好看的聚类树\n\n参考资料：\n\nR帮助文档\nhttps://r-graph-gallery.com/31-custom-colors-in-dendrogram.html\nhttps://www.gastonsanchez.com/visually-enforced/how-to/2012/10/0/Dendrograms/",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>R语言聚类分析</span>"
    ]
  },
  {
    "objectID": "1021-cluster.html#快速聚类划分聚类partitioning-clustering",
    "href": "1021-cluster.html#快速聚类划分聚类partitioning-clustering",
    "title": "22  R语言聚类分析",
    "section": "22.3 快速聚类（划分聚类,partitioning clustering）",
    "text": "22.3 快速聚类（划分聚类,partitioning clustering）\n\n22.3.1 K-means聚类\nK-means聚类，K均值聚类，是快速聚类的一种。比层次聚类更适合大样本的数据。在R语言中可以通过kmeans()实现K均值聚类。\n使用K均值聚类处理178种葡萄酒中13种化学成分的数据集。\n\ndata(wine, package = \"rattle\")\ndf &lt;- scale(wine[,-1])\n\npsych::headTail(df)\n##     Alcohol Malic   Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids\n## 1      1.51 -0.56  0.23      -1.17      1.91    0.81       1.03         -0.66\n## 2      0.25  -0.5 -0.83      -2.48      0.02    0.57       0.73         -0.82\n## 3       0.2  0.02  1.11      -0.27      0.09    0.81       1.21          -0.5\n## 4      1.69 -0.35  0.49      -0.81      0.93    2.48       1.46         -0.98\n## ...     ...   ...   ...        ...       ...     ...        ...           ...\n## 175    0.49  1.41  0.41       1.05      0.16   -0.79      -1.28          0.55\n## 176    0.33  1.74 -0.39       0.15      1.42   -1.13      -1.34          0.55\n## 177    0.21  0.23  0.01       0.15      1.42   -1.03      -1.35          1.35\n## 178    1.39  1.58  1.36        1.5     -0.26   -0.39      -1.27          1.59\n##     Proanthocyanins Color   Hue Dilution Proline\n## 1              1.22  0.25  0.36     1.84    1.01\n## 2             -0.54 -0.29   0.4     1.11    0.96\n## 3              2.13  0.27  0.32     0.79    1.39\n## 4              1.03  1.18 -0.43     1.18    2.33\n## ...             ...   ...   ...      ...     ...\n## 175           -0.32  0.97 -1.13    -1.48    0.01\n## 176           -0.42  2.22 -1.61    -1.48    0.28\n## 177           -0.23  1.83 -1.56     -1.4     0.3\n## 178           -0.42  1.79 -1.52    -1.42   -0.59\n\n进行K均值聚类时，需要在一开始就指定聚类的个数，我们也可以通过NbClust包实现这个过程。\n\nlibrary(NbClust)\n\nset.seed(123)\nnc &lt;- NbClust(df, min.nc = 2, max.nc = 15, method = \"kmeans\")# 方法选择kmeans\n\n\n\n\n\n\n\n## *** : The Hubert index is a graphical method of determining the number of clusters.\n##                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n##                 significant increase of the value of the measure i.e the significant peak in Hubert\n##                 index second differences plot. \n## \n\n\n\n\n\n\n\n## *** : The D index is a graphical method of determining the number of clusters. \n##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n##                 second differences plot) that corresponds to a significant increase of the value of\n##                 the measure. \n##  \n## ******************************************************************* \n## * Among all indices:                                                \n## * 2 proposed 2 as the best number of clusters \n## * 19 proposed 3 as the best number of clusters \n## * 1 proposed 14 as the best number of clusters \n## * 1 proposed 15 as the best number of clusters \n## \n##                    ***** Conclusion *****                            \n##  \n## * According to the majority rule, the best number of clusters is  3 \n##  \n##  \n## *******************************************************************\n\n结果中给出了划分依据以及最佳的聚类数目为3个，可以画图查看结果：\n\ntable(nc$Best.nc[1,])\n## \n##  0  1  2  3 14 15 \n##  2  1  2 19  1  1\n\nbarplot(table(nc$Best.nc[1,]),\n        xlab = \"聚类数目\",\n        ylab = \"评判准则个数\"\n        )\n\n\n\n\n\n\n\n\n可以看到聚类数目为3是最佳的选择。\n确定最佳聚类个数过程也可以通过非常好用的R包factoextra实现。\n\nlibrary(factoextra)\n\nset.seed(123)\nfviz_nbclust(df, kmeans, k.max = 15)\n\n\n\n\n\n\n\n\n这个结果给出的最佳聚类个数也是3个。\n下面进行K均值聚类，聚类数目设为3.\n\nset.seed(123)\nfit.km &lt;- kmeans(df, centers = 3, nstart = 25)\nfit.km\n## K-means clustering with 3 clusters of sizes 51, 62, 65\n## \n## Cluster means:\n##      Alcohol      Malic        Ash Alcalinity   Magnesium     Phenols\n## 1  0.1644436  0.8690954  0.1863726  0.5228924 -0.07526047 -0.97657548\n## 2  0.8328826 -0.3029551  0.3636801 -0.6084749  0.57596208  0.88274724\n## 3 -0.9234669 -0.3929331 -0.4931257  0.1701220 -0.49032869 -0.07576891\n##    Flavanoids Nonflavanoids Proanthocyanins      Color        Hue   Dilution\n## 1 -1.21182921    0.72402116     -0.77751312  0.9388902 -1.1615122 -1.2887761\n## 2  0.97506900   -0.56050853      0.57865427  0.1705823  0.4726504  0.7770551\n## 3  0.02075402   -0.03343924      0.05810161 -0.8993770  0.4605046  0.2700025\n##      Proline\n## 1 -0.4059428\n## 2  1.1220202\n## 3 -0.7517257\n## \n## Clustering vector:\n##   [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [38] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 1 3 3 3 3 3 3 3 3 3 3 3 2\n##  [75] 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 1 3 3 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n## [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n## \n## Within cluster sum of squares by cluster:\n## [1] 326.3537 385.6983 558.6971\n##  (between_SS / total_SS =  44.8 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n\n结果很详细，K均值聚类聚为3类，每一类数量分别是51,62,65。然后还给出了聚类中心，每一个观测分别属于哪一个类。\n不管是哪一种聚类方法，factoextra配合factomineR都可以给出非常好看的可视化结果。\n\nfviz_cluster(fit.km, data = df)\n\n\n\n\n\n\n\n\n有非常多的细节可以调整，大家在使用的时候可以自己尝试，和之前推文中介绍的PCA美化一样，也是支持ggplot2语法的。\n\nfviz_cluster(fit.km, data = df, \n             ellipse = T, # 增加椭圆\n             ellipse.type = \"t\", # 椭圆类型\n             geom = \"point\", # 只显示点不要文字\n             palette = \"lancet\", # 支持超多配色方案\n             ggtheme = theme_bw() # 支持更换主题\n             )\n\n\n\n\n\n\n\n\n\n\n22.3.2 围绕中心点的划分PAM\nK均值聚类是基于均值的，所以对异常值很敏感。一个更稳健的方法是围绕中心点的划分（PAM）。用一个最有代表性的观测值代表这一类(有点类似于主成分)。K均值聚类一般使用欧几里得距离，而PAM可以使用任意的距离来计算。因此，PAM可以容纳混合数据类型，并且不仅限于连续变量。\n我们还是用葡萄酒数据进行演示。PAM聚类可以通过cluster包中的pam()实现。\n\nlibrary(cluster)\n\nset.seed(123)\nfit.pam &lt;- pam(wine[-1,], k=3 # 聚为3类\n               , stand = T # 聚类前进行标准化\n               )\nfit.pam\n## Medoids:\n##      ID Type Alcohol Malic  Ash Alcalinity Magnesium Phenols Flavanoids\n## 36   35    1   13.48  1.81 2.41       20.5       100    2.70       2.98\n## 107 106    2   12.25  1.73 2.12       19.0        80    1.65       2.03\n## 149 148    3   13.32  3.24 2.38       21.5        92    1.93       0.76\n##     Nonflavanoids Proanthocyanins Color  Hue Dilution Proline\n## 36           0.26            1.86  5.10 1.04     3.47     920\n## 107          0.37            1.63  3.40 1.00     3.17     510\n## 149          0.45            1.25  8.42 0.55     1.62     650\n## Clustering vector:\n##   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21 \n##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n##  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41 \n##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n##  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61 \n##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   2 \n##  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81 \n##   2   2   1   2   1   2   2   2   1   2   1   2   1   1   2   2   2   2   1   2 \n##  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 \n##   2   2   3   2   2   2   2   2   2   2   2   2   2   2   1   1   2   1   2   2 \n## 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 \n##   2   2   2   2   2   2   2   2   1   2   2   2   2   2   2   2   2   2   2   2 \n## 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 \n##   1   2   2   2   2   2   2   2   2   3   3   3   3   3   3   3   3   3   3   3 \n## 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 \n##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n## 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 \n##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n## Objective function:\n##    build     swap \n## 3.537365 3.504175 \n## \n## Available components:\n##  [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n##  [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"\n\nMedoids给出了中心点，用第35个观测代表第1类，第107个观测代表第2类，第149个观测代表第3类。Clustering vector给出了每一个观测分别属于哪一个类。结果可以画出来：\n\nclusplot(fit.pam, main = \"PAM cluster\")\n\n\n\n\n\n\n\n\n同样也可以用factoextra包实现可视化。\n\nfviz_cluster(fit.pam, \n             ellipse = T, # 增加椭圆\n             ellipse.type = \"t\", # 椭圆类型\n             geom = \"point\", # 只显示点不要文字\n             palette = \"aaas\", # 支持超多配色方案\n             ggtheme = theme_bw() # 支持更换主题\n             )\n\n\n\n\n\n\n\n\n以后会给大家带来factoextra和factomineR包的详细介绍。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>R语言聚类分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html",
    "href": "1022-pca.html",
    "title": "23  R语言主成分分析",
    "section": "",
    "text": "23.1 加载数据\n使用R语言自带的iris鸢尾花数据进行演示。\nstr(iris)\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\npsych::headTail(iris)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 1            5.1         3.5          1.4         0.2    setosa\n## 2            4.9           3          1.4         0.2    setosa\n## 3            4.7         3.2          1.3         0.2    setosa\n## 4            4.6         3.1          1.5         0.2    setosa\n## ...          ...         ...          ...         ...      &lt;NA&gt;\n## 147          6.3         2.5            5         1.9 virginica\n## 148          6.5           3          5.2           2 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9           3          5.1         1.8 virginica\n首先给大家介绍下R自带的主成分分析函数。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>R语言主成分分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html#相关性检验",
    "href": "1022-pca.html#相关性检验",
    "title": "23  R语言主成分分析",
    "section": "23.2 相关性检验",
    "text": "23.2 相关性检验\n在进行PCA之前可以先进行相关性分析，看看相关系数：\n\ncor(iris[,-5])\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\n## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\n## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\n## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>R语言主成分分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html#kmo和bartlett球形检验",
    "href": "1022-pca.html#kmo和bartlett球形检验",
    "title": "23  R语言主成分分析",
    "section": "23.3 KMO和Bartlett球形检验",
    "text": "23.3 KMO和Bartlett球形检验\n使用psych实现，关于这两个检验的解读大家自行学习~\n\npsych::KMO(iris[,-5])\n## Kaiser-Meyer-Olkin factor adequacy\n## Call: psych::KMO(r = iris[, -5])\n## Overall MSA =  0.54\n## MSA for each item = \n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##         0.58         0.27         0.53         0.63\n\n这个检验主要反应样本量够不够，Overall MSA是总体的检验统计量，然后是每个变量的检验统计量。 MSA越大越好。一般要求大于0.5才可以（没有绝对标准，根据实际情况来）。\n\npsych::cortest.bartlett(iris[,-5])\n## $chisq\n## [1] 706.9592\n## \n## $p.value\n## [1] 1.92268e-149\n## \n## $df\n## [1] 6\n\np.value小于0.05，表明数据可以进行主成分分析。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>R语言主成分分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html#r自带的pca",
    "href": "1022-pca.html#r自带的pca",
    "title": "23  R语言主成分分析",
    "section": "23.4 R自带的PCA",
    "text": "23.4 R自带的PCA\n主成分的实现可以通过分步计算，主要就是标准化-求相关矩阵-计算特征值和特征向量。\nR中自带了prcomp()进行主成分分析，这就是工具的魅力，一次完成多步需求。\n使用prcomp()进行主成分分析：\n\n# R自带函数\npca.res &lt;- prcomp(iris[,-5], scale. = T, # 标准化\n                  center = T # 中心化\n                  )\n\n# 查看标准差、特征向量（回归系数）\npca.res\n## Standard deviations (1, .., p=4):\n## [1] 1.7083611 0.9560494 0.3830886 0.1439265\n## \n## Rotation (n x k) = (4 x 4):\n##                     PC1         PC2        PC3        PC4\n## Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\n## Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\n## Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\n## Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n主成分就是根据这几个系数(这几个系数也叫主成分载荷)算出来的：\nPC1 = 0.5210659Sepal.Length - 0.2693474Sepal.Width + 0.5804131Petal.Length + 0.5648565Petal.Width\n后面的主成分计算方法以此类推。\n\n# 样本得分score\nhead(pca.res$x)\n##            PC1        PC2         PC3          PC4\n## [1,] -2.257141 -0.4784238  0.12727962  0.024087508\n## [2,] -2.074013  0.6718827  0.23382552  0.102662845\n## [3,] -2.356335  0.3407664 -0.04405390  0.028282305\n## [4,] -2.291707  0.5953999 -0.09098530 -0.065735340\n## [5,] -2.381863 -0.6446757 -0.01568565 -0.035802870\n## [6,] -2.068701 -1.4842053 -0.02687825  0.006586116\n\n\n# 查看标准差、方差贡献率、累积方差贡献率\nsummary(pca.res)\n## Importance of components:\n##                           PC1    PC2     PC3     PC4\n## Standard deviation     1.7084 0.9560 0.38309 0.14393\n## Proportion of Variance 0.7296 0.2285 0.03669 0.00518\n## Cumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\nStandard deviation:标准差\nProportion of Variance:方差贡献率\nCumulative Proportion:累积方差贡献率\n\n关于主成分分析中的各种术语解读，我推荐知乎上的一篇文章：主成分分析各类术语的白话解读",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>R语言主成分分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html#结果可视化",
    "href": "1022-pca.html#结果可视化",
    "title": "23  R语言主成分分析",
    "section": "23.5 结果可视化",
    "text": "23.5 结果可视化\n默认的主成分分析结果可视化：\n\nbiplot(pca.res)\n\n\n\n\n\n\n\n\n碎石图可以帮助确认最佳的主成分个数，可以使用默认的screeplot()实现：\n\n# 默认是条形图，我们改为折线图，其实就是方差贡献度的可视化\nscreeplot(pca.res, type = \"lines\")\n\n\n\n\n\n\n\n\n可以看到用2-3个主成分就挺好了。\n\n一般来说，主成分的保留个数可以按照以下原则确定： 1. 以累积贡献率确定，当前K个主成分的累积贡献率达到某一特定值（一般选70%或者80%都行）时，则保留前K个主成分； 2. 以特征值大小来确定：如果主成分的特征值大于1，就保留这个主成分。\n\n但是保留几个主成分并没有绝对的标准，大家根据自己的实际情况来！\n今天只是小试牛刀，后面会为大家带来更加详细的主成分分析可视化。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>R语言主成分分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html#主成分分析可视化超详细",
    "href": "1022-pca.html#主成分分析可视化超详细",
    "title": "23  R语言主成分分析",
    "section": "23.6 主成分分析可视化(超详细)",
    "text": "23.6 主成分分析可视化(超详细)\n网络上很多R语言教程都是基于R语言实战进行修改，今天为大家介绍更好用的R包，在之前聚类分析中也经常用到：factoextra和factoMineR，关于主成分分析的可视化，大家比较常见的可能是ggbiplot，这几个R包都挺不错，大家可以比较下。\n这两个R包的函数可以直接使用prcomp()函数的结果，也可以使用FactoMineR的PCA()函数进行，结果更加详细。\n\n23.6.1 进行PCA分析\n使用R语言自带的iris鸢尾花数据进行演示。\n\nrm(list = ls())\nlibrary(factoextra)\nlibrary(FactoMineR)\n\npca.res &lt;- PCA(iris[,-5], graph = F, scale.unit = T) # 简简单单1行代码实现主成分分析\npca.res\n## **Results for the Principal Component Analysis (PCA)**\n## The analysis was performed on 150 individuals, described by 4 variables\n## *The results are available in the following objects:\n## \n##    name               description                          \n## 1  \"$eig\"             \"eigenvalues\"                        \n## 2  \"$var\"             \"results for the variables\"          \n## 3  \"$var$coord\"       \"coord. for the variables\"           \n## 4  \"$var$cor\"         \"correlations variables - dimensions\"\n## 5  \"$var$cos2\"        \"cos2 for the variables\"             \n## 6  \"$var$contrib\"     \"contributions of the variables\"     \n## 7  \"$ind\"             \"results for the individuals\"        \n## 8  \"$ind$coord\"       \"coord. for the individuals\"         \n## 9  \"$ind$cos2\"        \"cos2 for the individuals\"           \n## 10 \"$ind$contrib\"     \"contributions of the individuals\"   \n## 11 \"$call\"            \"summary statistics\"                 \n## 12 \"$call$centre\"     \"mean of the variables\"              \n## 13 \"$call$ecart.type\" \"standard error of the variables\"    \n## 14 \"$call$row.w\"      \"weights for the individuals\"        \n## 15 \"$call$col.w\"      \"weights for the variables\"\n\n结果信息丰富，可以通过不断的$获取，也可以通过特定函数提取，下面介绍。\n\n\n23.6.2 特征值可视化\n获取特征值、方差贡献率和累积方差贡献率，可以看到和上一篇的结果是一样的：\n\nget_eigenvalue(pca.res)\n##       eigenvalue variance.percent cumulative.variance.percent\n## Dim.1 2.91849782       72.9624454                    72.96245\n## Dim.2 0.91403047       22.8507618                    95.81321\n## Dim.3 0.14675688        3.6689219                    99.48213\n## Dim.4 0.02071484        0.5178709                   100.00000\n\n结果中的这几个概念在上一篇已经解释过了：R语言主成分分析\n通过这几个值，可以确定主成分个数，当然也可以通过碎石图（就是方差解释度的可视化）直观的观察：\n\nfviz_eig(pca.res,addlabels = T,ylim=c(0,100))\n\n\n\n\n\n\n\n\n\n\n23.6.3 提取变量结果\n通过get_pca_var()`函数实现：\n\nres.var &lt;- get_pca_var(pca.res)\nres.var$cor\n##                   Dim.1      Dim.2       Dim.3       Dim.4\n## Sepal.Length  0.8901688 0.36082989 -0.27565767 -0.03760602\n## Sepal.Width  -0.4601427 0.88271627  0.09361987  0.01777631\n## Petal.Length  0.9915552 0.02341519  0.05444699  0.11534978\n## Petal.Width   0.9649790 0.06399985  0.24298265 -0.07535950\nres.var$coord          \n##                   Dim.1      Dim.2       Dim.3       Dim.4\n## Sepal.Length  0.8901688 0.36082989 -0.27565767 -0.03760602\n## Sepal.Width  -0.4601427 0.88271627  0.09361987  0.01777631\n## Petal.Length  0.9915552 0.02341519  0.05444699  0.11534978\n## Petal.Width   0.9649790 0.06399985  0.24298265 -0.07535950\nres.var$contrib       \n##                  Dim.1       Dim.2     Dim.3     Dim.4\n## Sepal.Length 27.150969 14.24440565 51.777574  6.827052\n## Sepal.Width   7.254804 85.24748749  5.972245  1.525463\n## Petal.Length 33.687936  0.05998389  2.019990 64.232089\n## Petal.Width  31.906291  0.44812296 40.230191 27.415396\nres.var$cos2        \n##                  Dim.1       Dim.2       Dim.3        Dim.4\n## Sepal.Length 0.7924004 0.130198208 0.075987149 0.0014142127\n## Sepal.Width  0.2117313 0.779188012 0.008764681 0.0003159971\n## Petal.Length 0.9831817 0.000548271 0.002964475 0.0133055723\n## Petal.Width  0.9311844 0.004095980 0.059040571 0.0056790544\n\n\nres.var$cor:变量和主成分的相关系数\nres.var$coord: 变量在主成分投影上的坐标，下面会结合图说明，因为进行了标准化，所以和相关系数结果一样，其数值代表了主成分和变量之间的相关性\nres.var$cos2: 是coord的平方，也是表示主成分和变量间的相关性，同一个变量所有cos2的总和是1\nres.var$contrib: 变量对主成分的贡献\n\n这几个结果都可以进行可视化。\n\n\n23.6.4 变量结果可视化\n使用fviz_pca_var()对变量结果进行可视化：\n\nfviz_pca_var(pca.res)\n\n\n\n\n\n\n\n\nres.var$coord是变量在主成分投影上的坐标，Sepal.Width在Dim.1的坐标是-0.4601427，在Dim.2的坐标是0.88271627，根据这两个坐标就画出来Sepal.Width那根线了，以此类推~\n\n23.6.4.1 变量和主成分的cos2可视化\ncos2是coord的平方，也是表示主成分和变量间的相关性，所以首先可以画相关图：\n\nlibrary(\"corrplot\")\ncorrplot(res.var$cos2, is.corr = F)\n\n\n\n\n\n\n\n\n可以看到Petal.Length、Petal.Width和Dim1的相关性比较强，Sepal.Width和Dim2的相关性比较强。\n通过fviz_cos2()查看变量在不同主成分的总和，以下是不同变量在第1和第2主成分的加和，如果把axes = 1:2改成axes = 1:4，就会变成都是1（这个数据最多4个主成分，同一变量的cos2在所有主成分的总和是1）。\n\nfviz_cos2(pca.res, choice = \"var\", axes = 1:2)\n\n\n\n\n\n\n\n\n可以通过col.var = \"cos2\"参数给不同变量按照cos2的数值大小上色：\n\nfviz_pca_var(pca.res, col.var = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE \n             )\n\n\n\n\n\n\n\n\n\n# 黑白版本\nfviz_pca_var(pca.res, alpha.var = \"cos2\")\n\n\n\n\n\n\n\n\n\n\n23.6.4.2 变量对主成分的贡献可视化\n\nres.var$contrib\n##                  Dim.1       Dim.2     Dim.3     Dim.4\n## Sepal.Length 27.150969 14.24440565 51.777574  6.827052\n## Sepal.Width   7.254804 85.24748749  5.972245  1.525463\n## Petal.Length 33.687936  0.05998389  2.019990 64.232089\n## Petal.Width  31.906291  0.44812296 40.230191 27.415396\n\n首先也是可以通过画相关性图进行可视化：\n\nlibrary(\"corrplot\")\ncorrplot(res.var$contrib, is.corr=FALSE) \n\n\n\n\n\n\n\n\n通过fviz_contrib()可视化变量对不同主成分的贡献：\n\n# 对第1主成分的贡献\nfviz_contrib(pca.res, choice = \"var\", axes = 1)\n\n\n\n\n\n\n\n\n\n# 对第1和第2主成分的贡献\nfviz_contrib(pca.res, choice = \"var\", axes = 1:2)\n\n\n\n\n\n\n\n\n通过col.var = \"contrib\"参数给不同变量按照contrib的数值大小上色：\n\nfviz_pca_var(pca.res, col.var = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")\n             )\n\n\n\n\n\n\n\n\n\n\n\n23.6.5 Dimension description\n\nres.desc &lt;- dimdesc(pca.res, axes = c(1,2), proba = 0.05)\n# Description of dimension 1\nres.desc$Dim.1\n## \n## Link between the variable and the continuous variables (R-square)\n## =================================================================================\n##              correlation       p.value\n## Petal.Length   0.9915552 3.369916e-133\n## Petal.Width    0.9649790  6.609632e-88\n## Sepal.Length   0.8901688  2.190813e-52\n## Sepal.Width   -0.4601427  3.139724e-09\n\n\n\n23.6.6 提取样本结果\n使用get_pca_ind()提取样本结果，和变量结果类似：\n\nres.ind &lt;- get_pca_ind(pca.res)\n\nhead(res.ind$coord)          \n##       Dim.1      Dim.2       Dim.3       Dim.4\n## 1 -2.264703  0.4800266 -0.12770602 -0.02416820\n## 2 -2.080961 -0.6741336 -0.23460885 -0.10300677\n## 3 -2.364229 -0.3419080  0.04420148 -0.02837705\n## 4 -2.299384 -0.5973945  0.09129011  0.06595556\n## 5 -2.389842  0.6468354  0.01573820  0.03592281\n## 6 -2.075631  1.4891775  0.02696829 -0.00660818\nhead(res.ind$contrib)      \n##       Dim.1      Dim.2       Dim.3       Dim.4\n## 1 1.1715796 0.16806554 0.074085470 0.018798188\n## 2 0.9891845 0.33146674 0.250034006 0.341474919\n## 3 1.2768164 0.08526419 0.008875320 0.025915633\n## 4 1.2077372 0.26029781 0.037858004 0.140000650\n## 5 1.3046313 0.30516562 0.001125175 0.041530572\n## 6 0.9841236 1.61748779 0.003303827 0.001405371\nhead(res.ind$cos2)          \n##       Dim.1      Dim.2        Dim.3        Dim.4\n## 1 0.9539975 0.04286032 0.0030335249 1.086460e-04\n## 2 0.8927725 0.09369248 0.0113475382 2.187482e-03\n## 3 0.9790410 0.02047578 0.0003422122 1.410446e-04\n## 4 0.9346682 0.06308947 0.0014732682 7.690193e-04\n## 5 0.9315095 0.06823959 0.0000403979 2.104697e-04\n## 6 0.6600989 0.33978301 0.0001114335 6.690714e-06\n\n3个概念和变量的解释也是类似的，只不过上面是变量（列）和主成分的关系，现在是样本（观测，行）和主成分的关系。\n\n\n23.6.7 样本结果可视化\n样本的结果可视化可能是更常见的PCA图形，通过fviz_pca_ind()实现：\n\nfviz_pca_ind(pca.res)\n\n\n\n\n\n\n\n\n这个图是通过res.ind$coord里面的坐标实现的，其实就是不同样本在不同主成分的上面的得分score。\n默认的可视化比较简陋，但是可以通过超多参数实现各种精细化的控制，比如把不同的属性映射给点的大小和颜色，实现各种花里胡哨的效果。\n比如通过组别上色，就是大家最常见的PCA可视化图形：\n\n# 经典图形，是不是很熟悉？\nfviz_pca_ind(pca.res,\n             geom.ind = \"point\", # 只显示点，不要文字\n             col.ind = iris$Species, # 按照组别上色\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), # 自己提供颜色，或者使用主题\n             addEllipses = TRUE, # 添加置信椭圆\n             legend.title = \"Groups\"\n             )\n\n\n\n\n\n\n\n\n\n23.6.7.1 样本的cos2可视化\n使用方法和变量的cos2可视化基本一样，通过更改参数值即可实现：\n\nfviz_pca_ind(pca.res,\n             col.ind = \"cos2\", # 按照cos2上色\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE    \n             )\n\n\n\n\n\n\n\n\n可以更改点的大小、颜色等，只要设置合适的参数即可：\n\nfviz_pca_ind(pca.res, \n             pointsize = \"cos2\", # 把cos2的大小映射给点的大小\n             pointshape = 21, \n             fill = \"#E7B800\",\n             repel = TRUE \n             )\n\n\n\n\n\n\n\n\n同时更改点的大小和颜色当然也是支持的：\n\nfviz_pca_ind(pca.res, \n             col.ind = \"cos2\", # 控制颜色\n             pointsize = \"contrib\", # 控制大小\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE \n             )\n\n\n\n\n\n\n\n\n使用参数choice = \"ind\"可视化样本对不同主成分的cos2：\n\n# axes选择主成分\nfviz_cos2(pca.res, choice = \"ind\", axes = 1:2)\n\n\n\n\n\n\n\n\n\n\n23.6.7.2 样本对主成分的贡献可视化\n和变量对主成分的贡献可视化非常类似，简单演示下：\n\nfviz_contrib(pca.res, choice = \"ind\", axes = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n23.6.8 biplot\n双标图…\n同时展示变量和样本和主成分的关系，超级多的自定义可视化细节。\n\n# 同时有箭头和椭圆\nfviz_pca_biplot(pca.res, \n                col.ind = iris$Species, \n                palette = \"jco\", \n                addEllipses = TRUE, \n                label = \"var\",\n                col.var = \"black\", \n                repel = TRUE,\n                legend.title = \"Species\"\n                ) \n\n\n\n\n\n\n\n\n\nfviz_pca_biplot(pca.res, \n                # 组别映射给点的填充色\n                geom.ind = \"point\",\n                pointshape = 21,\n                pointsize = 2.5,\n                fill.ind = iris$Species,\n                col.ind = \"black\",\n                # 通过自定义分组给变量上色\n                col.var = factor(c(\"sepal\", \"sepal\", \"petal\", \"petal\")),\n                # 自定义图例标题\n                legend.title = list(fill = \"Species\", color = \"Clusters\"),\n                repel = TRUE        \n             )+\n  ggpubr::fill_palette(\"jco\")+ # 选择点的填充色的配色\n  ggpubr::color_palette(\"npg\") # 选择变量颜色的配色\n\n\n\n\n\n\n\n\n\nfviz_pca_biplot(pca.res, \n                # 自定义样本部分\n                geom.ind = \"point\",\n                fill.ind = iris$Species, # 填充色\n                col.ind = \"black\", # 边框色\n                pointshape = 21, # 点的形状\n                pointsize = 2, \n                palette = \"jco\",\n                addEllipses = TRUE,\n                # 自定义变量部分\n                alpha.var =\"contrib\", col.var = \"contrib\",\n                gradient.cols = \"RdYlBu\",\n                \n                # 自定义图例标题\n                legend.title = list(fill = \"Species\", color = \"Contrib\",\n                                    alpha = \"Contrib\")\n                )\n\n\n\n\n\n\n\n\nfviz_xxx系列可视化函数底层是ggscatter的封装，这个函数来自ggpubr包，所有ggpubr支持的特性都可以给fviz_xxx函数使用，这也是这几个函数功能强大的原因，毕竟底层都是ggplot2!\n下载会继续给大家介绍如何提取PCA的数据，并使用ggplot2可视化，以及三维PCA图的实现。\nfactoextra和factoMineR在聚类分析、主成分分析、因子分析等方面都可以使用。\n参考资料：http://www.sthda.com/",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>R语言主成分分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html#主成分分析可视化3d版",
    "href": "1022-pca.html#主成分分析可视化3d版",
    "title": "23  R语言主成分分析",
    "section": "23.7 主成分分析可视化3d版",
    "text": "23.7 主成分分析可视化3d版\n之前详细介绍了R语言中的主成分分析，以及超级详细的主成分分析可视化方法，主要是基于factoextra和factoMineR两个神包。\n今天说一下如何提取数据用ggplot2画PCA图，以及三维PCA图。\n\n23.7.1 提取数据\n还是使用鸢尾花数据集。\n\nrm(list = ls())\n\npca.res &lt;- prcomp(iris[,-5], scale. = T, center = T)\npca.res\n## Standard deviations (1, .., p=4):\n## [1] 1.7083611 0.9560494 0.3830886 0.1439265\n## \n## Rotation (n x k) = (4 x 4):\n##                     PC1         PC2        PC3        PC4\n## Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\n## Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\n## Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\n## Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n在上一篇中提到过，经典的PCA图的横纵坐标其实就是不同样本在不同主成分中的得分，只要提取出来就可以用gplot2画了。\n\n# 提取得分\ntmp &lt;- as.data.frame(pca.res$x)\nhead(tmp)\n##         PC1        PC2         PC3          PC4\n## 1 -2.257141 -0.4784238  0.12727962  0.024087508\n## 2 -2.074013  0.6718827  0.23382552  0.102662845\n## 3 -2.356335  0.3407664 -0.04405390  0.028282305\n## 4 -2.291707  0.5953999 -0.09098530 -0.065735340\n## 5 -2.381863 -0.6446757 -0.01568565 -0.035802870\n## 6 -2.068701 -1.4842053 -0.02687825  0.006586116\n\n和原数据拼到一起就可以画图了：\n\ntmp$species &lt;- iris$Species\nhead(tmp)\n##         PC1        PC2         PC3          PC4 species\n## 1 -2.257141 -0.4784238  0.12727962  0.024087508  setosa\n## 2 -2.074013  0.6718827  0.23382552  0.102662845  setosa\n## 3 -2.356335  0.3407664 -0.04405390  0.028282305  setosa\n## 4 -2.291707  0.5953999 -0.09098530 -0.065735340  setosa\n## 5 -2.381863 -0.6446757 -0.01568565 -0.035802870  setosa\n## 6 -2.068701 -1.4842053 -0.02687825  0.006586116  setosa\n\n\nlibrary(ggplot2)\nlibrary(ggsci)\n\nggplot(tmp, aes(PC1, PC2))+\n  geom_point(aes(color = species))+\n  stat_ellipse(aes(fill=species), alpha = 0.2,\n               geom =\"polygon\",type = \"norm\")+\n  scale_fill_aaas()+\n  scale_color_aaas()+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n23.7.2 3d版\n其实就是使用3个主成分，之前介绍过一种：使用R语言美化PCA图，使用方法非常简单，也是在文献中学习到的。\n\n\n\n\n\n\n\n\n\n今天再介绍下scatterplot3d包。\n\nlibrary(scatterplot3d)\n\nscatterplot3d(tmp[,1:3], # 第1-3主成分\n              # 颜色长度要和样本长度一样，且对应！\n              color = rep(c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),each=50),\n              pch = 15,\n              lty.hide = 2\n              )\nlegend(\"topleft\",c('Setosa','Versicolor','Virginica'),\nfill=c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),box.col=NA)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>R语言主成分分析</span>"
    ]
  },
  {
    "objectID": "1023-factoranalysis.html",
    "href": "1023-factoranalysis.html",
    "title": "24  R语言因子分析",
    "section": "",
    "text": "24.1 演示数据\n数据来自于孙振球医学统计学第4版例22-2.\n某医院为了评价医疗工作质量，收集了三年的门诊人次、出院人数、病床利用率、病床周转次数、平均住院天数、治愈好转率、病死率、诊断符合率、抢救成功率9个指标，采用因子分析方法，探讨其综合评价体系。\ndf &lt;- foreign::read.spss(\"datasets/例22-02.sav\",to.data.frame = T,\n                         reencode = \"utf-8\")\nnames(df) &lt;- c(\"年\",\"月\",\"门诊人次\",\"出院人数\",\"病床利用率\",\"病床周转次数\",\n               \"平均住院天数\",\"治愈好转率\",\"病死率\",\"诊断符合率\",\"抢救成功率\")\n\nstr(df)\n## 'data.frame':    36 obs. of  11 variables:\n##  $ 年          : num  1991 1991 1991 1991 1991 ...\n##  $ 月          : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ 门诊人次    : num  4.34 3.45 4.38 4.18 4.32 4.13 4.57 4.31 4.06 4.43 ...\n##  $ 出院人数    : num  389 271 385 377 378 349 361 209 425 458 ...\n##  $ 病床利用率  : num  99.1 88.3 104 99.5 102 ...\n##  $ 病床周转次数: num  1.23 0.85 1.21 1.19 1.19 1.1 1.14 0.52 0.93 0.95 ...\n##  $ 平均住院天数: num  25.5 23.6 26.5 26.9 27.6 ...\n##  $ 治愈好转率  : num  93.2 94.3 92.5 93.9 93.2 ...\n##  $ 病死率      : num  3.56 2.44 4.02 2.92 1.99 4.38 2.73 3.65 3.09 4.21 ...\n##  $ 诊断符合率  : num  97.5 97.9 98.5 99.4 99.7 ...\n##  $ 抢救成功率  : num  61.7 73.3 76.8 63.2 80 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \".·.\" \".....˴.\" \"..Ժ....\" ...\npsych::headTail(df)\n##       年  月 门诊人次 出院人数 病床利用率 病床周转次数 平均住院天数 治愈好转率\n## 1   1991   1     4.34      389      99.06         1.23        25.46      93.15\n## 2   1991   2     3.45      271      88.28         0.85        23.55      94.31\n## 3   1991   3     4.38      385     103.97         1.21        26.54      92.53\n## 4   1991   4     4.18      377      99.48         1.19        26.89      93.86\n## ...  ... ...      ...      ...        ...          ...          ...        ...\n## 33  1993   9      3.9      555      80.58          1.1        23.08      94.38\n## 34  1993  10     3.62      554      87.21          1.1         22.5      92.43\n## 35  1993  11     3.75      586      90.31         1.12        23.73      92.47\n## 36  1993  12     3.77      627      86.47         1.24        23.22      91.17\n##     病死率 诊断符合率 抢救成功率\n## 1     3.56      97.51      61.66\n## 2     2.44      97.94      73.33\n## 3     4.02      98.48      76.79\n## 4     2.92      99.41      63.16\n## ...    ...        ...        ...\n## 33    2.06      96.82      91.79\n## 34    3.22      97.16      87.77\n## 35    2.07      97.74      93.89\n## 36     3.4      98.98       89.8",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>R语言因子分析</span>"
    ]
  },
  {
    "objectID": "1023-factoranalysis.html#判断需要提取的因子个数",
    "href": "1023-factoranalysis.html#判断需要提取的因子个数",
    "title": "24  R语言因子分析",
    "section": "24.2 判断需要提取的因子个数",
    "text": "24.2 判断需要提取的因子个数\nR中自带了factanal()进行因子分析，不过不如psych包好用，我们这里使用psych包演示。\n\n# 只用后面9列数据\ndf.use &lt;- df[,-c(1,2)]\n\nlibrary(psych)\n\n# 碎石图\nfa.parallel(df.use, fa = \"both\",fm=\"ml\")\n\n\n\n\n\n\n\n## Parallel analysis suggests that the number of factors =  2  and the number of components =  3\n\n通过参数fa = \"both\"同时给出了PCA和因子分析的碎石图，根据因子分析碎石图的结果，建议我们提取3个因子。\n但是提取几个因子并没有绝对的标准，我们可以结合多种方法或专业知识，和提取主成分的方法类似，可以参考之前的介绍的方法：R语言主成分分析\n除此之外，还可以结合特征值大小、累计贡献率来确定使用几个因子。\n下面我们首先用9个因子进行因子分析，看看结果再说。\nrotate参数确定旋转方法，有多种不同的选择，比如不旋转、正交旋转法（比如最大方差法）、斜交旋转法等，\nfm参数选择因子计算方法，比如最大似然法ml、主轴迭代法pa、加权最小二乘wls、广义加权最小二乘gls、最小残差minres，等。\n\n# 进行因子分析，首先9个因子用一下看看结果再说，最大似然法，不旋转\nfa.res &lt;- fa(df.use, nfactors = 9, rotate = \"none\", fm=\"ml\")\nfa.res\n## Factor Analysis using method =  ml\n## Call: fa(r = df.use, nfactors = 9, rotate = \"none\", fm = \"ml\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                ML1   ML2   ML3   ML4   ML5   ML6 ML7 ML8 ML9   h2   u2 com\n## 门诊人次      0.14  0.68  0.09  0.37 -0.06 -0.03   0   0   0 0.63 0.37 1.7\n## 出院人数      0.68 -0.30  0.19  0.32  0.06  0.03   0   0   0 0.69 0.31 2.1\n## 病床利用率    0.58  0.52 -0.09 -0.31  0.04  0.05   0   0   0 0.71 0.29 2.6\n## 病床周转次数  0.90  0.20 -0.01 -0.01  0.01 -0.01   0   0   0 0.85 0.15 1.1\n## 平均住院天数 -0.53  0.43  0.38  0.05  0.02  0.09   0   0   0 0.62 0.38 2.9\n## 治愈好转率   -0.03 -0.07  0.69 -0.18  0.13 -0.02   0   0   0 0.53 0.47 1.2\n## 病死率       -0.33  0.08 -0.46  0.18  0.24  0.09   0   0   0 0.42 0.58 2.9\n## 诊断符合率   -0.31  0.54  0.02  0.02 -0.15  0.07   0   0   0 0.41 0.59 1.8\n## 抢救成功率    0.41 -0.64  0.07 -0.03 -0.12  0.10   0   0   0 0.60 0.40 1.9\n## \n##                        ML1  ML2  ML3  ML4  ML5  ML6  ML7  ML8  ML9\n## SS loadings           2.27 1.75 0.89 0.40 0.12 0.04 0.00 0.00 0.00\n## Proportion Var        0.25 0.19 0.10 0.04 0.01 0.00 0.00 0.00 0.00\n## Cumulative Var        0.25 0.45 0.55 0.59 0.60 0.61 0.61 0.61 0.61\n## Proportion Explained  0.41 0.32 0.16 0.07 0.02 0.01 0.00 0.00 0.00\n## Cumulative Proportion 0.41 0.74 0.90 0.97 0.99 1.00 1.00 1.00 1.00\n## \n## Mean item complexity =  2\n## Test of the hypothesis that 9 factors are sufficient.\n## \n## df null model =  36  with the objective function =  3.82 with Chi Square =  119.03\n## df of  the model are -9  and the objective function was  0.46 \n## \n## The root mean square of the residuals (RMSR) is  0.04 \n## The df corrected root mean square of the residuals is  NA \n## \n## The harmonic n.obs is  36 with the empirical chi square  3.42  with prob &lt;  NA \n## The total n.obs was  36  with Likelihood Chi Square =  11.69  with prob &lt;  NA \n## \n## Tucker Lewis Index of factoring reliability =  2.377\n## Fit based upon off diagonal values = 0.99\n## Measures of factor score adequacy             \n##                                                    ML1  ML2  ML3   ML4   ML5\n## Correlation of (regression) scores with factors   0.94 0.90 0.79  0.70  0.42\n## Multiple R square of scores with factors          0.88 0.80 0.63  0.50  0.18\n## Minimum correlation of possible factor scores     0.77 0.61 0.26 -0.01 -0.65\n##                                                     ML6 ML7 ML8 ML9\n## Correlation of (regression) scores with factors    0.26   0   0   0\n## Multiple R square of scores with factors           0.07   0   0   0\n## Minimum correlation of possible factor scores     -0.86  -1  -1  -1\n\nh2是公因子方差，表示因子对每个变量的解释度，u2=1-h2，表示不能被因子解释的比例。\n看结果中的Cumulative Var，累积方差解释，可以看到在使用3个因子时，累计贡献度是0.55,4个因子是0.59，结合碎石图，我们选择用4个因子。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>R语言因子分析</span>"
    ]
  },
  {
    "objectID": "1023-factoranalysis.html#进行因子分析",
    "href": "1023-factoranalysis.html#进行因子分析",
    "title": "24  R语言因子分析",
    "section": "24.3 进行因子分析",
    "text": "24.3 进行因子分析\n选择4个因子\n\n# 选择4个因子，不旋转，最大似然法\nfa.res &lt;- fa(df.use, nfactors = 4, rotate = \"none\", fm=\"ml\")\nfa.res\n## Factor Analysis using method =  ml\n## Call: fa(r = df.use, nfactors = 4, rotate = \"none\", fm = \"ml\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                ML3   ML1   ML2   ML4   h2    u2 com\n## 门诊人次      0.61  0.78  0.11 -0.01 1.00 0.005 1.9\n## 出院人数     -0.40  0.31  0.34 -0.59 0.72 0.276 3.1\n## 病床利用率   -0.30  0.56  0.25  0.49 0.71 0.289 2.9\n## 病床周转次数 -0.55  0.75  0.35  0.01 1.00 0.005 2.3\n## 平均住院天数  0.67 -0.13  0.16  0.26 0.57 0.435 1.5\n## 治愈好转率    0.15 -0.39  0.91  0.00 1.00 0.005 1.4\n## 病死率        0.14 -0.07 -0.47  0.10 0.26 0.743 1.3\n## 诊断符合率    0.45  0.11 -0.10  0.36 0.36 0.642 2.2\n## 抢救成功率   -0.56 -0.12  0.05 -0.46 0.55 0.455 2.1\n## \n##                        ML3  ML1  ML2  ML4\n## SS loadings           1.94 1.79 1.40 1.02\n## Proportion Var        0.22 0.20 0.16 0.11\n## Cumulative Var        0.22 0.41 0.57 0.68\n## Proportion Explained  0.32 0.29 0.23 0.17\n## Cumulative Proportion 0.32 0.61 0.83 1.00\n## \n## Mean item complexity =  2.1\n## Test of the hypothesis that 4 factors are sufficient.\n## \n## df null model =  36  with the objective function =  3.82 with Chi Square =  119.03\n## df of  the model are 6  and the objective function was  0.24 \n## \n## The root mean square of the residuals (RMSR) is  0.04 \n## The df corrected root mean square of the residuals is  0.09 \n## \n## The harmonic n.obs is  36 with the empirical chi square  3.43  with prob &lt;  0.75 \n## The total n.obs was  36  with Likelihood Chi Square =  6.84  with prob &lt;  0.34 \n## \n## Tucker Lewis Index of factoring reliability =  0.931\n## RMSEA index =  0.055  and the 90 % confidence intervals are  0 0.235\n## BIC =  -14.67\n## Fit based upon off diagonal values = 0.99\n## Measures of factor score adequacy             \n##                                                    ML3  ML1  ML2  ML4\n## Correlation of (regression) scores with factors   1.00 1.00 1.00 0.87\n## Multiple R square of scores with factors          0.99 1.00 0.99 0.75\n## Minimum correlation of possible factor scores     0.99 0.99 0.99 0.50\n\n选择4个因子，最终的累积方差解释是0.68，再看因子载荷矩阵，因子1（ML1）在病床周转、门诊人次、病床利用率等具有较高的载荷，因子2在治愈好转率方面具有很大的载荷，因子3在门诊人次，平均住院天数、抢救成功率具有较高的载荷，因子4在出院人数具有较高的载荷。\n从专业角度来看，并没有发现什么规律，好像不能很好的解释专业意义。\n所以我们需要进行因子旋转！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>R语言因子分析</span>"
    ]
  },
  {
    "objectID": "1023-factoranalysis.html#因子旋转",
    "href": "1023-factoranalysis.html#因子旋转",
    "title": "24  R语言因子分析",
    "section": "24.4 因子旋转",
    "text": "24.4 因子旋转\n通过因子旋转我们可以更容易找到内在规律，使得结果更加容易结合专业背景进行解释。\n\n# 选择4个因子，最大方差旋转，最大似然法\nfa.res &lt;- fa(df.use, nfactors = 4, rotate = \"varimax\", fm=\"ml\")\nfa.res\n## Factor Analysis using method =  ml\n## Call: fa(r = df.use, nfactors = 4, rotate = \"varimax\", fm = \"ml\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                ML3   ML1   ML2   ML4   h2    u2 com\n## 门诊人次     -0.31  0.23 -0.03  0.92 1.00 0.005 1.4\n## 出院人数      0.75  0.16  0.24  0.27 0.72 0.276 1.6\n## 病床利用率   -0.10  0.83  0.03  0.07 0.71 0.289 1.0\n## 病床周转次数  0.46  0.84  0.09  0.26 1.00 0.005 1.8\n## 平均住院天数 -0.64 -0.23  0.24  0.21 0.57 0.435 1.8\n## 治愈好转率   -0.09 -0.09  0.98 -0.10 1.00 0.005 1.1\n## 病死率       -0.20 -0.18 -0.42 -0.06 0.26 0.743 1.9\n## 诊断符合率   -0.56  0.02 -0.10  0.18 0.36 0.642 1.3\n## 抢救成功率    0.70 -0.04  0.04 -0.21 0.55 0.455 1.2\n## \n##                        ML3  ML1  ML2  ML4\n## SS loadings           2.15 1.58 1.29 1.12\n## Proportion Var        0.24 0.18 0.14 0.12\n## Cumulative Var        0.24 0.41 0.56 0.68\n## Proportion Explained  0.35 0.26 0.21 0.18\n## Cumulative Proportion 0.35 0.61 0.82 1.00\n## \n## Mean item complexity =  1.4\n## Test of the hypothesis that 4 factors are sufficient.\n## \n## df null model =  36  with the objective function =  3.82 with Chi Square =  119.03\n## df of  the model are 6  and the objective function was  0.24 \n## \n## The root mean square of the residuals (RMSR) is  0.04 \n## The df corrected root mean square of the residuals is  0.09 \n## \n## The harmonic n.obs is  36 with the empirical chi square  3.43  with prob &lt;  0.75 \n## The total n.obs was  36  with Likelihood Chi Square =  6.84  with prob &lt;  0.34 \n## \n## Tucker Lewis Index of factoring reliability =  0.931\n## RMSEA index =  0.055  and the 90 % confidence intervals are  0 0.235\n## BIC =  -14.67\n## Fit based upon off diagonal values = 0.99\n## Measures of factor score adequacy             \n##                                                    ML3  ML1  ML2  ML4\n## Correlation of (regression) scores with factors   0.93 0.96 1.00 0.98\n## Multiple R square of scores with factors          0.86 0.92 0.99 0.96\n## Minimum correlation of possible factor scores     0.72 0.85 0.99 0.91\n\n此时我们再看因子载荷阵，因子3在门诊人次、出院人数、病床周转、平均住院天数、诊断符合率、抢救成功率等多个指标上具有较大的载荷，因子2在治愈好转率、病死率上载荷最大，因子1在病床利用率、病床周转率这两个指标上载荷最高，因子4在门诊人次、出院人数这两个指标的载荷最大。\n因此可以认为因子3反映了反映了医疗工作质量各个方面的情况，称为综合因子；因子1反应病床利用情况，可以成为病床利用因子；因子2反映了医疗水平，称为水平因子；因子4反应了就诊患者数量，称为数量因子。\n可以把结果可视化：\n\nfa.diagram(fa.res)\n\n\n\n\n\n\n\n\n\nfactor.plot(fa.res)\n\n\n\n\n\n\n\n\n关于因子分析，我并没有找到好用的可视化R包，如果大家知道，欢迎评论区留言。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>R语言因子分析</span>"
    ]
  },
  {
    "objectID": "1016-partialcorrelation.html",
    "href": "1016-partialcorrelation.html",
    "title": "25  偏相关和典型相关分析",
    "section": "",
    "text": "25.1 偏相关（partial correlation）\n使用R包ppcor实现。\n首先是加载数据和R包。\nlibrary(ppcor)\n\ndf &lt;- haven::read_sav(\"datasets/data01.sav\")\ndf1 &lt;- df[,2:4]\nnames(df1) &lt;- c(\"height\",\"weight\",\"vc\")\nhead(df1)\n## # A tibble: 6 × 3\n##   height weight    vc\n##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1   139.   30.4  2   \n## 2   164.   46.2  2.75\n## 3   156.   37.1  2.75\n## 4   156.   35.5  2   \n## 5   150.   31    1.5 \n## 6   145    33    2.5\n这个数据有3列，现在我们要探索身高（height）和体重（weight）的关系，其中vc是需要控制的因素。\n首先进行pearson偏相关分析：\np1 &lt;- pcor(df1,method = \"pearson\")\np1\n## $estimate\n##            height    weight         vc\n## height  1.0000000 0.7941292 -0.2022408\n## weight  0.7941292 1.0000000  0.6166786\n## vc     -0.2022408 0.6166786  1.0000000\n## \n## $p.value\n##              height       weight          vc\n## height 0.0000000000 0.0000491115 0.406351395\n## weight 0.0000491115 0.0000000000 0.004920346\n## vc     0.4063513954 0.0049203462 0.000000000\n## \n## $statistic\n##            height   weight         vc\n## height  0.0000000 5.387551 -0.8514549\n## weight  5.3875507 0.000000  3.2299064\n## vc     -0.8514549 3.229906  0.0000000\n## \n## $n\n## [1] 20\n## \n## $gp\n## [1] 1\n## \n## $method\n## [1] \"pearson\"\n结果中$estimate给出了偏相关系数，可以看到在控制了vc后，height和weight的偏相关系数是0.7941292；$p.value给出了相应的P值，$statistic给出了检验统计量。\n上面演示的是pearson偏相关分析，下面展示一个spearman偏相关分析。\n# 加载数据\ndf2 &lt;- haven::read_sav(\"datasets/data02.sav\")\nnames(df2) &lt;- c(\"id\",\"x\",\"y\",\"z\")\n\nhead(df2)\n## # A tibble: 6 × 4\n##      id x         y             z\n##   &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt;\n## 1     7 1 [矮]    1 [轻]     1.25\n## 2    17 1 [矮]    1 [轻]     1.25\n## 3     1 1 [矮]    1 [轻]     2   \n## 4    11 1 [矮]    1 [轻]     2   \n## 5     5 2 [中]    1 [轻]     1.5 \n## 6    15 2 [中]    1 [轻]     1.5\n现在我们要计算x和y的相关性，z是要控制的因素，由于这两个变量是分类变量，所以要用spearman偏相关分析。\n其实用法是一样的，就是改个参数而已：\npcor(df2[,-1],method = \"spearman\")\n## $estimate\n##            x         y          z\n## x  1.0000000 0.6985577 -0.4212568\n## y  0.6985577 1.0000000  0.8486095\n## z -0.4212568 0.8486095  1.0000000\n## \n## $p.value\n##              x            y            z\n## x 0.0000000000 8.779998e-04 7.245901e-02\n## y 0.0008779998 0.000000e+00 4.386687e-06\n## z 0.0724590110 4.386687e-06 0.000000e+00\n## \n## $statistic\n##           x        y         z\n## x  0.000000 4.025172 -1.915103\n## y  4.025172 0.000000  6.613943\n## z -1.915103 6.613943  0.000000\n## \n## $n\n## [1] 20\n## \n## $gp\n## [1] 1\n## \n## $method\n## [1] \"spearman\"\n结果解读同上。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>偏相关和典型相关分析</span>"
    ]
  },
  {
    "objectID": "1016-partialcorrelation.html#偏相关partial-correlation",
    "href": "1016-partialcorrelation.html#偏相关partial-correlation",
    "title": "25  偏相关和典型相关分析",
    "section": "",
    "text": "25.1.1 偏相关散点图\n还是用df1的数据作为演示，现在是研究weight对height的影响，vc是需要控制的变量。\n所以我们可以分别计算残差，用残差的散点图代表偏相关的散点图。\n\n# 首先计算height为因变量，vc是自变量的残差\nresidX &lt;- resid(lm(height~vc,data = df1))\n\n# 再计算weight为因变量，vc是自变量的残差\nresidY &lt;- resid(lm(weight~vc, data = df1))\n\n# 两个残差的相关系数就是weight和height的偏相关系数！\ncor(residX, residY, method = \"pearson\")\n## [1] 0.7941292\n\n# 画图即可\nplot(residX, residY)\n\n\n\n\n\n\n\n\n但是这个图的横纵坐标取值范围对实际来说是不能解释的，所以我们可以分别加上它们各自的平均值，然后再画散点图，方法借鉴了这篇文章：\n\nresidX1 &lt;- residX + mean(df1$height)\nresidY1 &lt;- residY + mean(df1$weight)\n\nplot(residX1, residY1,xlab = \"身高\",ylab = \"体重\")\n\n\n\n\n\n\n\n\n这个就是偏相关散点图了！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>偏相关和典型相关分析</span>"
    ]
  },
  {
    "objectID": "1016-partialcorrelation.html#典型相关canonical-correlation",
    "href": "1016-partialcorrelation.html#典型相关canonical-correlation",
    "title": "25  偏相关和典型相关分析",
    "section": "25.2 典型相关（Canonical Correlation）",
    "text": "25.2 典型相关（Canonical Correlation）\n这个数据来自孙振球《医学统计学》第四版的例23-1，探讨小学生的生长发育指标（肺活量、身高、体重、胸围）和身体素质（短跑、跳高、跳远、实心球）的相互关系。\n\ndf &lt;- read.csv(\"datasets/例23-1.csv\",header = T)\npsych::headtail(df)\n## Warning in psych::headtail(df): headtail is deprecated.  Please use the\n## headTail function\n##     肺活量  身高 体重 胸围 短跑 跳高 跳远 实心球\n## 1     1210 120.1 23.8   61 10.2 66.3 2.01   2.73\n## 2     1210 120.7 23.4 59.8 11.3 67.6 1.92   2.71\n## 3     1040 121.2 22.9   59 10.1 66.5 1.92    2.6\n## 4     1620 121.5 24.6 59.5  9.5 67.8 1.95   2.64\n## ...    ...   ...  ...  ...  ...  ...  ...    ...\n## 81    1310 129.7 24.7 61.7 10.1 69.4 2.03    2.8\n## 82    2280 143.6 37.6   70  9.7 88.8 2.17   4.18\n## 83    1580 136.6 32.3 67.2 10.3 87.1 2.66   4.04\n## 84    2370 147.4 38.8   73 10.8 90.7 2.82   4.38\n\n典型相关分析R语言自带了cancor()函数，无需借助第三方R包：\n\n# 前4个变量和后4个变量做相关性，直接提供2个数据框也可以\ncc1 &lt;- cancor(df[,1:4],df[,5:8])\n\ncc1\n## $cor\n## [1] 0.8858445 0.2791523 0.1940486 0.0379654\n## \n## $xcoef\n##                 [,1]          [,2]         [,3]          [,4]\n## 肺活量 -5.267493e-05 -0.0001955795 -0.000407694  0.0002971469\n## 身高   -7.754975e-03 -0.0086910713  0.021599065  0.0079782016\n## 体重   -3.471120e-03 -0.0180620718 -0.015626841 -0.0522321990\n## 胸围   -1.552353e-02  0.0464952778  0.004886088  0.0178728641\n## \n## $ycoef\n##               [,1]        [,2]        [,3]        [,4]\n## 短跑    0.02340474 -0.08458262  0.07017709 -0.13566387\n## 跳高   -0.01068107 -0.02440377  0.01443519  0.01626168\n## 跳远   -0.02867642  0.92500098  0.23862503 -0.29882238\n## 实心球 -0.06884355 -0.07825414 -0.29442851 -0.19118769\n## \n## $xcenter\n##     肺活量       身高       体重       胸围 \n## 1490.47619  131.52024   26.44405   61.51190 \n## \n## $ycenter\n##      短跑      跳高      跳远    实心球 \n## 10.271429 72.805952  2.109048  2.978929\n\n$cor给出了两组数据之间的典型相关系数，$xcoef是第一组的典型相关系数，可以看到计算出了4个虚拟变量，$ycoef是第二组的典型相关系数。\n下面进行典型相关的显著性检验，使用R包CCP实现。\n\nlibrary(CCP)\n\nrho &lt;- cc1$cor\nn &lt;- dim(df[,1:4])[1]\np &lt;- length(df[,1:4])\nq &lt;- length(df[,5:8])\n\np.asym()函数实现典型相关的显著性检验。需要典型相关系数、观测个数、第一组的变量个数、第二组的变量个数。\n\n# 4种典型相关的结果\np.asym(rho,n,p,q, tstat = \"Wilks\")\n## Wilks' Lambda, using F-approximation (Rao's F):\n##               stat     approx df1      df2   p.value\n## 1 to 4:  0.1907537 10.4765088  16 232.8215 0.0000000\n## 2 to 4:  0.8860745  1.0618303   9 187.5484 0.3930330\n## 3 to 4:  0.9609581  0.7843615   4 156.0000 0.5369444\n## 4 to 4:  0.9985586  0.1140327   1  79.0000 0.7364945\np.asym(rho,n,p,q, tstat = \"Hotelling\")\n##  Hotelling-Lawley Trace, using F-approximation:\n##                 stat     approx df1 df2   p.value\n## 1 to 4:  3.770206950 17.5550261  16 298 0.0000000\n## 2 to 4:  0.125083307  1.0632081   9 306 0.3898996\n## 3 to 4:  0.040571670  0.7962190   4 314 0.5283457\n## 4 to 4:  0.001443452  0.1161979   1 322 0.7334177\np.asym(rho,n,p,q, tstat = \"Pillai\")\n##  Pillai-Bartlett Trace, using F-approximation:\n##                 stat    approx df1 df2      p.value\n## 1 to 4:  0.901742684 5.7482049  16 316 5.963363e-11\n## 2 to 4:  0.117022206 1.0849404   9 324 3.733220e-01\n## 3 to 4:  0.039096223 0.8192541   4 332 5.135803e-01\n## 4 to 4:  0.001441371 0.1225607   1 340 7.264904e-01\np.asym(rho,n,p,q, tstat = \"Roy\")\n##  Roy's Largest Root, using F-approximation:\n##               stat   approx df1 df2 p.value\n## 1 to 1:  0.7847205 71.99119   4  79       0\n## \n##  F statistic for Roy's Greatest Root is an upper bound.\n\n我们就看下Wilks结果，可以看到只有第一个典型相关系数是有意义的，后面3个都没有显著性。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>偏相关和典型相关分析</span>"
    ]
  },
  {
    "objectID": "ROC曲线.html",
    "href": "ROC曲线.html",
    "title": "26  ROC曲线",
    "section": "",
    "text": "26.1 诊断试验\n科学地评价诊断试验是临床医师选择诊断试验的基础。诊断试验在临床上的应用涉及面甚广，包括病因和病原学诊断，疾病病理和功能损害的诊断，疗效的判断，药物毒副作用的监测，疾病预后的判断以及应用于普查、筛检无症状的患者等。用于不同场合的诊断试验有不同的要求，不同的诊断试验本身又有一定的特性。为了合理选用诊断试验以避免盲目性，临床医师就需要对诊断试验进行科学研究并得出科学的评价。\n评价诊断试验的优劣必须以金标准（gold standard）作为参照，没有金标准的诊断试验评价是没有科学性的。所谓诊断试验的金标准，是指当前临床医学界所公认的诊断某病最为可靠的方法。亦即利用金标准能正确地区分某人属”有病”还是”无病”。临床诊断中常用的金标准包括病理学诊断（组织活检和尸检）、外科手术发现、特殊的影像学诊断（如用冠状动脉造影术诊断冠心病等）以及目前尚无特异诊断方法而采用的国际公认的综合诊断标准（如诊断风湿热的Johes标准等）。有时用长期临床随访所获得的肯定诊断，也可作为金标准。必须注意，如果采用的金标准选择不当，就会造成分类错误，从而影响诊断试验正确性的评价。\n用于诊断试验评价的研究对象应包括病例组和对照组。病例组应是按金标准确诊的患者；对照组则应是按金标准证实无该病的患者或正常人群。病例组的选择，应包括各种类型的病例，即典型和不典型，早、中、晚各期，病情轻、中、重，有、无并发症等，这样试验的结果才具有普遍意义；而对照组则可选用经金标准证实无该病的其他病例或正常人，特别应当包括确实无该病，但易与该病相混淆的其他病例，这样选择的对照才具有临床意义，尤其具有鉴别诊断的价值。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ROC曲线</span>"
    ]
  },
  {
    "objectID": "ROC曲线.html#诊断试验的评价",
    "href": "ROC曲线.html#诊断试验的评价",
    "title": "26  ROC曲线",
    "section": "26.2 诊断试验的评价",
    "text": "26.2 诊断试验的评价\n根据诊断试验的结果和金标准的结果，可得到4种情况，将其整理成下面的表格，就得到一个诊断试验的四格表，实际上是一个配对四格表的形式：\n\n用于评价诊断试验的常用指标有：灵敏度和特异度，误诊率和漏诊率。\n\n灵敏度\n\n灵敏度（sensitivity），又称为真阳性率，是实际患病且被试验诊断为患者的概率，即患者被诊断为阳性的概率。灵敏度=a/(a+c)\n\n特异度\n\n特异度（specificity），又被称为真阴性率，是实际未患病而被试验诊断为非患者的概率，即非患者被诊断为阴性的概率。特异度=d/(b+d)\n灵敏度是反映检出能力的指标，而特异度是鉴别非患者能力的指标，二者都是越大越好。\n\n误诊率\n\n误诊率又称为假阳性率，表示实际未患病但被试验诊断为患者的概率，即非患者被诊断为阳性，反映非患者被错误诊断的可能性。误诊率=b/(b+d)\n\n漏诊率\n\n漏诊率又被称为假阳性率，表示实际患病但被试验诊断为非患者的概率，即患者被诊断为阴性，反映患者被遗漏诊断的可能性。漏诊率=c/(a+c)\n显然，灵敏度=1-假阴性率，特异度=1-假阳性率。\n除了以上4个指标外，还有阳性预测值和阴性预测值，用于评价诊断试验预测的准确性。\n\n阳性预测值：positive-predict-value，是试验诊断为阳性者，确为患者的概率。阳性预测值=a/(a+b)。\n阴性预测值：negative-predict-value，是试验诊断为阴性者，确为非患者的概率。阴性预测值=d/(c+d)。\n\n当两个诊断试验进行比较时，单独使用灵敏度与特异度指标，可能出现一个诊断试验的灵敏度高，而另一个诊断试验的特异度高，无法判断哪一个更好。所以发展出了可以将灵敏度和特异度综合起来评价的诊断试验指标，比如：正确率、比数积、阳新似然比、阴性似然比等。\n\n正确率：又称为准确率（机器学习中被称为accuracy），又称为总符合率，表示观察结果和实际结果的符合程度，反应正确诊断患者与非患者的能力。正确率=(a+d)/N，N是总人数。\n约登指数：Youden-index，YI，是反应诊断试验真实性的综合指标，YI=灵敏度+特异度=1。YI的值在-1到1之间，越大说明诊断试验的真实性越好，当YI小于等于0时，该诊断试验无任何临床应用价值。\n比数积：odd-product，OP，表示患者中诊断阳性数、阴性数之比和非患者中诊断阴性数、阳性数之比的乘积。OP=se/(1-se) * sp/(1-sp) = ad/bc。OP越大诊断价值越高。\n阳性似然比：positive-likelihood-ratio，LR+，表示真阳性率与假阳性率之比。LR+=se/(1-sp)\n阴性似然比：negative-likelihood-ratio，LR-，表示假阴性率与真阴性率之比。LR-=(1-se)/sp",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ROC曲线</span>"
    ]
  },
  {
    "objectID": "ROC曲线.html#roc曲线",
    "href": "ROC曲线.html#roc曲线",
    "title": "26  ROC曲线",
    "section": "26.3 ROC曲线",
    "text": "26.3 ROC曲线\n诊断试验把受试者分成患病和非患病，肯定是有一个标准的，高于这个标准就被判断为患病，低于这个标准就被判断为非患病。\n既然有一个标准，那么这个标准就可以有不同的取值，当这个标准变化时，那么被试验判断为患病和非患病的人数自然也就会发生变化。那么灵敏度和特异度这些指标也会发生变化。所以对于不同的诊断取值（被称为截断值），对应着不同的灵敏度和特异度。\n我们以灵敏度(或者1-灵敏度)为纵坐标，特异度为横坐标，把不同截断值下的灵敏度和特异度画在一张图上，并把这些点连接成线，就是ROC曲线。ROC曲线下的面积被称为AUC（area under the curve，AUC）。\nAUC的取值范围在0到1之间，ROC曲线下的面积越大，也就是AUC越大，说明分类越准确，当AUC值为1时，说明是完美的分类，当AUC为0.5时，说明和乱猜差不多。\n下面用一个简单的例子进行说明。\n假如，我想根据ca125的值判定一个人到底有没有肿瘤，找了10个肿瘤患者，20个非肿瘤患者，都给他们测一下ca125，这样就得到了30个ca125的值。\n\nset.seed(20220840)\nca125_1 &lt;- c(rnorm(10,80,20),rnorm(20,50,10))\n\n# 30个人的ca125的值如下\nca125_1\n##  [1]  51.88470  82.45907 113.66834  63.49476  98.29077  63.27374  74.25079\n##  [8]  80.22945  83.01740  99.17105  42.52889  54.56804  48.88383  65.67865\n## [15]  44.73153  45.99028  55.82554  42.79242  60.84917  64.80764  51.11468\n## [22]  43.40118  47.03850  44.75943  68.34163  60.83829  53.32599  59.92225\n## [29]  46.46360  30.02914\n\n假定前10个人是肿瘤，后20个人是非肿瘤。\n\noutcome &lt;- c(rep(c(\"肿瘤\",\"非肿瘤\"),c(10,20)))\noutcome &lt;- factor(outcome,levels = c(\"肿瘤\",\"非肿瘤\"))\noutcome\n##  [1] 肿瘤   肿瘤   肿瘤   肿瘤   肿瘤   肿瘤   肿瘤   肿瘤   肿瘤   肿瘤  \n## [11] 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤\n## [21] 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤\n## Levels: 肿瘤 非肿瘤\n\n放在一个表格中方便观看：\n\ndf &lt;- data.frame(outcome=outcome,\n                 ca125=ca125_1\n                 )\npsych::headTail(df)\n##     outcome  ca125\n## 1      肿瘤  51.88\n## 2      肿瘤  82.46\n## 3      肿瘤 113.67\n## 4      肿瘤  63.49\n## ...    &lt;NA&gt;    ...\n## 27   非肿瘤  53.33\n## 28   非肿瘤  59.92\n## 29   非肿瘤  46.46\n## 30   非肿瘤  30.03\n\n现在如果我们设置ca125&gt;60，判断为肿瘤，ca125≤50判断为非肿瘤，就能得到如下的结果：\n\ndf1 &lt;- transform(df, pred = ifelse(ca125&gt;60,\"猜他是肿瘤\",\"猜他不是肿瘤\"))\ndf1$pred &lt;- factor(df1$pred,levels = c(\"猜他是肿瘤\",\"猜他不是肿瘤\"))\npsych::headTail(df1)\n##     outcome  ca125         pred\n## 1      肿瘤  51.88 猜他不是肿瘤\n## 2      肿瘤  82.46   猜他是肿瘤\n## 3      肿瘤 113.67   猜他是肿瘤\n## 4      肿瘤  63.49   猜他是肿瘤\n## ...    &lt;NA&gt;    ...         &lt;NA&gt;\n## 27   非肿瘤  53.33 猜他不是肿瘤\n## 28   非肿瘤  59.92 猜他不是肿瘤\n## 29   非肿瘤  46.46 猜他不是肿瘤\n## 30   非肿瘤  30.03 猜他不是肿瘤\n\n对以上结果稍加整理，就能得出一个四格表：\n\nxtabs(~pred+outcome,data = df1)\n##               outcome\n## pred           肿瘤 非肿瘤\n##   猜他是肿瘤      9      5\n##   猜他不是肿瘤    1     15\n\n这个表格就是诊断实验的四格表。\n通过这个表格，我们就可以计算灵敏度和特异度等指标了:\n特异度=15/(15+5)=0.75\n灵敏度=9/(1+9)=0.9\n这个表格是以（ca125）60为截断值的，如果我们换一个截断值，那么灵敏度和特异度就会发生变化。\n下面我们编写一个函数，让它帮我们计算在不同的截断值下，灵敏度和特异度的值。\n\ncal_roc &lt;- function(df, cutoff){\n  df &lt;- transform(df, pred = ifelse(ca125&gt;cutoff,\"猜他是肿瘤\",\"猜他不是肿瘤\"))\n  df$pred &lt;- factor(df$pred,levels = c(\"猜他是肿瘤\",\"猜他不是肿瘤\"))\n  tb &lt;- table(df$pred,df$outcome)\n  sens &lt;- tb[1,1]/colSums(tb)[1]\n  spec &lt;- tb[2,2]/colSums(tb)[2]\n  list(sens=sens, spec=spec)\n}\n\n阈值设置为60，看看是不是和我们上面的结果一样：\n\ncal_roc(df,60)\n## $sens\n## 肿瘤 \n##  0.9 \n## \n## $spec\n## 非肿瘤 \n##   0.75\n\n可以看到是一样的。\n下面就是自己选择多个阈值进行计算，先看下ca125的范围，超出这个范围的阈值没有意义。\n\nrange(ca125_1)\n## [1]  30.02914 113.66834\n\n下面我们确定截断值的范围在30到113之间，每次都加1，然后使用我们的函数计算不同截断值下的灵敏度和特异度：\n\n# 确定取哪些截断值\ncutoff &lt;- seq(30,113, 1)\n\n# 计算不同的灵敏度和特异度\nrocs &lt;- purrr::map_dfr(cutoff, cal_roc, df=df)\nrocs$cutoff &lt;- cutoff\npsych::headTail(rocs)\n##   sens spec cutoff\n## 1    1    0     30\n## 2    1 0.05     31\n## 3    1 0.05     32\n## 4    1 0.05     33\n## 5  ...  ...    ...\n## 6  0.1    1    110\n## 7  0.1    1    111\n## 8  0.1    1    112\n## 9  0.1    1    113\n\n这样我们就得到了不同截断值下的灵敏度和特异度。\n有了这个结果后，我们就可以自己画出ROC曲线了，以1-特异度为横坐标，灵敏度为纵坐标：\n\nlibrary(ggplot2)\n\nggplot(rocs, aes(1-spec,sens))+\n  geom_point(size=2,color=\"red\")+\n  geom_path()+\n  coord_fixed()+\n  theme_bw()\n\n\n\n\n\n\n\n\n这就是ROC曲线了。\n因为不同的截断值对应着不同的灵敏度和特异度，那么肯定就存在一个最佳截断值的问题，一般情况下，使约登指数最大的那个截断值，就是最佳截断值。\n\n约登指数=灵敏度+特异度-1\n\n此时的灵敏度+特异度最大，同时也是ROC曲线下的面积（即AUC）最大。\n如何寻找这个最佳的截断值呢？当然是挨个试了！但是借助计算机，这个过程一瞬间就完成了。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ROC曲线</span>"
    ]
  },
  {
    "objectID": "ROC曲线.html#用r包绘制roc",
    "href": "ROC曲线.html#用r包绘制roc",
    "title": "26  ROC曲线",
    "section": "26.4 用R包绘制ROC",
    "text": "26.4 用R包绘制ROC\nR语言中有非常多的方法可以实现ROC曲线，但是基本上都是至少需要2列数据，一列是真实结果，另一列是预测值（或者叫截断值、阈值、指标等），有了这两列数据，就可以轻松使用各种方法画出ROC曲线并计算AUC。这里给大家介绍这个用的最多的pROC包。\npROC包中提供了一个aSAH数据集，这是一个动脉瘤性蛛网膜下腔出血的数据集，一共113行，7列。其中outcome列是结果变量，1代表Good，2代表Poor：\n\ngos6：格拉斯哥量表评分\noutcome：结果变量\ngender：性别\nage：年龄\nwfns：世界神经外科医师联合会公认的神经学量表评分\ns100b：生物标志物\nndka：生物标志物\n\n\nlibrary(pROC)\n\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n假如我们想使用s100b这个指标作为我们的截断值，来判断结局到底是good还是poor，根据前面的ROC曲线的定义，我们可以绘制一个ROC曲线，使用R包实现非常简单：\n\n# 计算\nres &lt;- roc(aSAH$outcome,aSAH$s100b,auc=T)\n\n# 画图\nplot(res,auc.polygon=T,auc.polygon.col=\"steelblue\",\n     print.auc=T,\n     print.auc.x=0.95,print.auc.y=0.9,print.auc.col=\"firebrick\",\n     print.auc.cex=2\n     )\n\n\n\n\n\n\n\n\n给出的信息非常丰富，不仅画出了图，还给出了ROC曲线下面积（即：AUC）。\n\n# 结果已经存储在res中\nres\n## \n## Call:\n## roc.default(response = aSAH$outcome, predictor = aSAH$s100b,     auc = T)\n## \n## Data: aSAH$s100b in 72 controls (aSAH$outcome Good) &lt; 41 cases (aSAH$outcome Poor).\n## Area under the curve: 0.7314\n\n课本中还介绍了一种平滑的ROC曲线，在pROC中实现也很简单，添加一个参数即可：\n\n# 计算\nres_smooth &lt;- roc(aSAH$outcome,aSAH$s100b,auc=T,smooth=T)\n\n# 画图\nplot(res_smooth,auc.polygon=T,auc.polygon.col=\"steelblue\",\n     print.auc=T,\n     print.auc.x=0.95,print.auc.y=0.9,print.auc.col=\"firebrick\",\n     print.auc.cex=2\n     )\n\n\n\n\n\n\n\n\n计算最佳截点也是非常简单的：\n\nplot(res,auc.polygon=T,auc.polygon.col=\"steelblue\",\n     legacy.axes = TRUE, # 让x轴从0开始\n     print.thres=\"best\") # AUC最大的点\n\n\n\n\n\n\n\n\n图中的信息显示：最佳截点是0.205，此时的特异度是0.806，灵敏度是0.634，也就是说，当s100b的值是0.205的时候，约登指数最大，同时也是曲线下面积（AUC）最大。\n\n细心的朋友可以可能已经注意到了，上面的ROC曲线的x轴，有的是0-1，有的是1-0，这两种情况都是对的，没必要纠结哈。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ROC曲线</span>"
    ]
  },
  {
    "objectID": "ROC曲线.html#预测模型和roc",
    "href": "ROC曲线.html#预测模型和roc",
    "title": "26  ROC曲线",
    "section": "26.5 预测模型和ROC",
    "text": "26.5 预测模型和ROC\n了解了ROC曲线的原理后，使用起来就比较简单了。\n以二分类数据为例，如果是在机器学习和临床预测模型中，模型也会对我们的数据作出一个判断，对于每一个样本，模型都会给出两种类型的判断结果，一种是直接给出类别，另一种是给出每种类别的概率，如下所示：\n\n\n\n真实类别\n预测类别\n预测为阳性的概率\n预测为阴性的概率\n\n\n\n\n阳性\n阳性\n0.6\n0.4\n\n\n阳性\n阳性\n0.7\n0.3\n\n\n阴性\n阳性\n0.8\n0.2\n\n\n阳性\n阴性\n0.4\n0.6\n\n\n阴性\n阴性\n0.2\n0.8\n\n\n阴性\n阴性\n0.25\n0.75\n\n\n阴性\n阴性\n0.32\n0.68\n\n\n阳性\n阳性\n0.7\n0.3\n\n\n\n这个概率就是我们的截断值，取不同的概率阈值，就会得到不同的分类结果，我们可以规定当概率大于0.5判断为阳性，也可以规定概率大于0.8判断为阳性。所以不同的概率阈值，也对应着不同的灵敏度和特异度，据此就可以绘制出ROC曲线了。\n在实际使用时，我们可以借助多种R包帮我们实现”根据不同的阈值计算计算灵敏度/特异度并绘图”的过程，并不需要自己计算，常用的R包有：ROCR、pROC、yardstick等。\n除此之外，ROC曲线既然是通过不同的截断值实现的，那么就必然存在最佳截点的问题；对于生存分析来说，也涉及到time-dependent-ROC等问题；为了增加检验可信度，还有bootstrap-ROC等；还有多分类的ROC曲线等，这些问题可参考以下推文：\n\n多时间点和多指标的ROC曲线\n临床预测模型之二分类资料ROC曲线的绘制\n临床预测模型之生存资料ROC曲线的绘制\nROC曲线(AUC)的显著性检验\n生存资料ROC曲线的最佳截点和平滑曲线\nROC曲线纯手工绘制\nR语言计算AUC(ROC曲线)的注意事项\nROC阴性结果还是阳性结果\n多指标联合诊断的ROC曲线\nROC曲线最佳截点\nbootstrap ROC/AUC\nR语言多分类ROC曲线绘制\n\n公众号后台回复ROC即可获取以上合集链接。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ROC曲线</span>"
    ]
  },
  {
    "objectID": "1034-finegray.html",
    "href": "1034-finegray.html",
    "title": "27  Fine-Gray检验和竞争风险模型列线图",
    "section": "",
    "text": "27.1 加载数据和R包\n探讨骨髓移植和血液移植治疗白血病的疗效，结局事件定义为复发，某些患者因为移植不良反应死亡，定义为竞争风险事件。\n# 如果提示缺少R包直接安装即可\nrm(list = ls())\ndata(\"bmtcrr\",package = \"casebase\")\nstr(bmtcrr)\n## 'data.frame':    177 obs. of  7 variables:\n##  $ Sex   : Factor w/ 2 levels \"F\",\"M\": 2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : Factor w/ 2 levels \"ALL\",\"AML\": 1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : Factor w/ 4 levels \"CR1\",\"CR2\",\"CR3\",..: 4 2 3 2 2 4 1 1 1 4 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Status: int  2 1 0 2 2 2 0 2 0 1 ...\n##  $ Source: Factor w/ 2 levels \"BM+PB\",\"PB\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ftime : num  0.67 9.5 131.77 24.03 1.47 ...\n这个数据一共7个变量，177行。\n# 竞争风险分析需要用的R包\nlibrary(cmprsk)",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Fine-Gray检验和竞争风险模型列线图</span>"
    ]
  },
  {
    "objectID": "1034-finegray.html#加载数据和r包",
    "href": "1034-finegray.html#加载数据和r包",
    "title": "27  Fine-Gray检验和竞争风险模型列线图",
    "section": "",
    "text": "Sex: 性别，F是女，M是男\nD: 疾病类型，ALL是急性淋巴细胞白血病，AML是急性髓系细胞白血病。\nPhase: 不同阶段，4个水平，CR1，CR2，CR3，Relapse。\nAge: 年龄。\nStatus: 结局变量，0=删失，1=复发，2=竞争风险事件。\nSource: 因子变量，2个水平：BM+PB(骨髓移植+血液移植)，PB(血液移植)。\nftime: 生存时间。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Fine-Gray检验和竞争风险模型列线图</span>"
    ]
  },
  {
    "objectID": "1034-finegray.html#fine-gray检验单因素分析",
    "href": "1034-finegray.html#fine-gray检验单因素分析",
    "title": "27  Fine-Gray检验和竞争风险模型列线图",
    "section": "27.2 Fine-Gray检验（单因素分析）",
    "text": "27.2 Fine-Gray检验（单因素分析）\n在普通的生存分析中，可以用log-rank检验做单因素分析，在竞争风险模型中，使用Fine-Gray检验进行单因素分析。\n\n\n\n\n\n\n\n\n\n比如现在我们想要比较不同疾病类型（D）有没有差异，可以进行Fine-Gray检验：\n\nbmtcrr$Status &lt;- factor(bmtcrr$Status)\nf &lt;- cuminc(bmtcrr$ftime, bmtcrr$Status, bmtcrr$D)\nf\n## Tests:\n##        stat         pv df\n## 1 2.8623325 0.09067592  1\n## 2 0.4481279 0.50322531  1\n## Estimates and Variances:\n## $est\n##              20        40        60        80       100       120\n## ALL 1 0.3713851 0.3875571 0.3875571 0.3875571 0.3875571 0.3875571\n## AML 1 0.2414530 0.2663827 0.2810390 0.2810390 0.2810390        NA\n## ALL 2 0.3698630 0.3860350 0.3860350 0.3860350 0.3860350 0.3860350\n## AML 2 0.4439103 0.4551473 0.4551473 0.4551473 0.4551473        NA\n## \n## $var\n##                20          40          60          80         100         120\n## ALL 1 0.003307032 0.003405375 0.003405375 0.003405375 0.003405375 0.003405375\n## AML 1 0.001801156 0.001995487 0.002130835 0.002130835 0.002130835          NA\n## ALL 2 0.003268852 0.003373130 0.003373130 0.003373130 0.003373130 0.003373130\n## AML 2 0.002430406 0.002460425 0.002460425 0.002460425 0.002460425          NA\n\n结果中1代表复发,2代表竞争风险事件。\n第一行统计量=2.8623325, P=0.09067592,表示在控制了竞争风险事件（即第二行计算的统计量和P值）后，两种疾病类型ALL和AML的累计复发风险无统计学差异P=0.09067592。\n第2行说明ALL和AML的累计竞争风险无统计学差异。\n$est表示估计的各时间点ALL和AML组的累计复发率与与累计竞争风险事件发生率（分别用1和2来区分，与第一行第二行一致）。\n$var表示估计的各时间点ALL和AML组的累计复发率与与累计竞争风险事件发生率的方差（分别用1和2来区分，与第一行第二行一致）。\n\n27.2.1 图形展示结果\n对于上述结果可以使用图形展示：\n\nplot(f,xlab = 'Month', ylab = 'CIF',lwd=2,lty=1,\n     col = c('red','blue','black','forestgreen'))\n\n\n\n\n\n\n\n\n图形解读：\n纵坐标表示累计发生率CIF，横坐标是时间。我们从ALL1对应的红色曲线和AML1对应的蓝色曲线可以得出，ALL组的复发风险较AML 组高，但无统计学意义，P=0.09067592。同理，ALL2对应的黑色曲线在AML2对应的草绿色曲线下方，我们可以得出，ALL组的竞争风险事件发生率较AML组低，同样无统计学意义，P=0.50322531。\n简单来讲，这个图可以用一句话来概括：在控制了竞争风险事件后，ALL和AML累计复发风险无统计学差异P=0.09067592。\n\n\n27.2.2 ggplot2\n这个图不好看，非常的不ggplot，所以我们要用ggplot2重新画它！所以首先要提取数据，因为数就是图，图就是数。但是万能的broom包竟然没有不能提取这个对象的数据，只能手动来，太不优雅了！\n\n# 提取数据\nALL1 &lt;- data.frame(ALL1_t = f[[1]][[1]], ALL1_C = f[[1]][[2]])\nAML1 &lt;- data.frame(AML1_t = f[[2]][[1]], AML1_C = f[[2]][[2]])\nALL2 &lt;- data.frame(ALL2_t = f[[3]][[1]], ALL2_C = f[[3]][[2]])\nAML2 &lt;- data.frame(AML2_t = f[[4]][[1]], AML2_C = f[[4]][[2]])\n\nlibrary(ggplot2)\n\nggplot()+\n  geom_line(data = ALL1, aes(ALL1_t,ALL1_C))+\n  geom_line(data = ALL2, aes(ALL2_t,ALL2_C))+\n  geom_line(data = AML1, aes(AML1_t,AML1_C))+\n  geom_line(data = AML2, aes(AML2_t,AML2_C))+\n  labs(x=\"month\",y=\"cif\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n但是这种不好上色，所以我们美化一下，变成长数据再画图即可。\n\ntmp &lt;- data.frame(month = c(ALL1$ALL1_t,AML1$AML1_t,ALL2$ALL2_t,AML2$AML2_t),\n                  cif = c(ALL1$ALL1_C,AML1$AML1_C,ALL2$ALL2_C,AML2$AML2_C),\n                  type = rep(c(\"ALL1\",\"AML1\",\"ALL2\",\"AML2\"), c(58,58,58,88))\n                  )\n\nggplot(tmp, aes(month, cif))+\n  geom_line(aes(color=type, group=type),size=1.2)+\n  theme_bw()+\n  theme(legend.position = \"top\")\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Fine-Gray检验和竞争风险模型列线图</span>"
    ]
  },
  {
    "objectID": "1034-finegray.html#竞争风险模型多因素分析",
    "href": "1034-finegray.html#竞争风险模型多因素分析",
    "title": "27  Fine-Gray检验和竞争风险模型列线图",
    "section": "27.3 竞争风险模型（多因素分析）",
    "text": "27.3 竞争风险模型（多因素分析）\n做完了单因素分析，再看看竞争风险模型的多因素分析。\n首先要把自变量单独放在一个数据框里，使用中发现一个问题，这里如果把分类变量变为因子型不会自动进行哑变量编码，所以需要手动进行哑变量编码！\n但是我这里偷懒了，并没有进行哑变量设置！实际中是需要的哦！！\n\ncovs &lt;- subset(bmtcrr, select = - c(ftime,Status))\ncovs[,c(1:3,5)] &lt;- lapply(covs[,c(1:3,5)],as.integer)\n\nstr(covs)\n## 'data.frame':    177 obs. of  5 variables:\n##  $ Sex   : int  2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : int  1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : int  4 2 3 2 2 4 1 1 1 4 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Source: int  1 1 1 1 1 1 1 1 1 1 ...\n\n指定failcode=1, cencode=0, 分别代表结局事件1与截尾0，其他默认为竞争风险事件2。\n\n# 构建竞争风险模型\nf2 &lt;- crr(bmtcrr$ftime, bmtcrr$Status, covs, failcode=1, cencode=0)\nsummary(f2)\n## Competing Risks Regression\n## \n## Call:\n## crr(ftime = bmtcrr$ftime, fstatus = bmtcrr$Status, cov1 = covs, \n##     failcode = 1, cencode = 0)\n## \n##           coef exp(coef) se(coef)      z p-value\n## Sex     0.0494     1.051   0.2867  0.172 0.86000\n## D      -0.4860     0.615   0.3040 -1.599 0.11000\n## Phase   0.4144     1.514   0.1194  3.470 0.00052\n## Age    -0.0174     0.983   0.0118 -1.465 0.14000\n## Source  0.9526     2.592   0.5469  1.742 0.08200\n## \n##        exp(coef) exp(-coef)  2.5% 97.5%\n## Sex        1.051      0.952 0.599  1.84\n## D          0.615      1.626 0.339  1.12\n## Phase      1.514      0.661 1.198  1.91\n## Age        0.983      1.018 0.960  1.01\n## Source     2.592      0.386 0.888  7.57\n## \n## Num. cases = 177\n## Pseudo Log-likelihood = -267 \n## Pseudo likelihood ratio test = 23.6  on 5 df,\n\n结果解读：在控制了竞争分险事件后，phase变量，即疾病所处阶段是患者复发的独立影响因素(p =0.00052)。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Fine-Gray检验和竞争风险模型列线图</span>"
    ]
  },
  {
    "objectID": "1035-psm.html",
    "href": "1035-psm.html",
    "title": "28  R语言倾向性评分：匹配",
    "section": "",
    "text": "28.1 准备数据\n下面的数据及演示的方法主要参考了这篇文章：10.21037/atm-20-3998。大家感兴趣的可以去阅读原文。\n我们虚构一个数据，用于演示研究吸烟对心血管疾病的影响，性别和年龄作为混杂因素。\nset.seed(2020)\n\nx.Gender &lt;- rep(0:1, c(400,600)) # 400女 600男\nx.Age &lt;- round(abs(rnorm(1000, mean=45, sd=15)))\n\n# 对于这个数据来说，实际PS（tps）是可以计算出来的，如果这里不理解，也问题不大！\nz &lt;- (x.Age - 45) / 15 - (x.Age-45) ^ 2 / 100 + 2 * x.Gender\ntps &lt;- exp(z) / (1+exp(z)) \nSmoke &lt;- as.numeric(runif(1000) &lt; tps)\nz.y &lt;- x.Gender + 0.3*x.Age + 5*Smoke - 20\ny &lt;- exp(z.y) / (1+exp(z.y))\nCVD &lt;- as.numeric(runif(1000) &lt; y)\nx.Age.mask &lt;- rbinom(1000, 1, 0.2) # 随机产生几个缺失值\nx.Age &lt;- ifelse(x.Age.mask==1, NA, x.Age)\n\n# 原始数据长这样：\ndata &lt;- data.frame(x.Age, x.Gender, Smoke, CVD)\nhead(data)\n##   x.Age x.Gender Smoke CVD\n## 1    51        0     1   0\n## 2    50        0     0   0\n## 3    29        0     0   0\n## 4    28        0     0   0\n## 5     3        0     0   0\n## 6    56        0     1   1\n首先可以看一下原始数据的基线资料表，用的是tableone这个包，之前也做过介绍，做基线资料表的R包还有非常多，比如：\n为什么用tableone呢？因为它能计算SMD（后面会介绍这个SMD的作用），而且其他教程都是用的它…\nlibrary(tableone)\n\ntable2 &lt;- CreateTableOne(vars = c('x.Age', 'x.Gender', 'CVD'),\n                         data = data,\n                         factorVars = c('x.Gender', 'CVD'),\n                         strata = 'Smoke',\n                         smd=TRUE)\ntable2 &lt;- print(table2,smd=TRUE,\n                showAllLevels = TRUE,\n                noSpaces = TRUE,\n                printToggle = FALSE)\ntable2\n##                    Stratified by Smoke\n##                     level 0               1              p        test SMD    \n##   n                 \"\"    \"549\"           \"451\"          \"\"       \"\"   \"\"     \n##   x.Age (mean (SD)) \"\"    \"42.76 (19.69)\" \"47.04 (8.14)\" \"&lt;0.001\" \"\"   \"0.284\"\n##   x.Gender (%)      \"0\"   \"299 (54.5)\"    \"101 (22.4)\"   \"&lt;0.001\" \"\"   \"0.698\"\n##                     \"1\"   \"250 (45.5)\"    \"350 (77.6)\"   \"\"       \"\"   \"\"     \n##   CVD (%)           \"0\"   \"452 (82.3)\"    \"230 (51.0)\"   \"&lt;0.001\" \"\"   \"0.705\"\n##                     \"1\"   \"97 (17.7)\"     \"221 (49.0)\"   \"\"       \"\"   \"\"\n#write.csv(table2, file = \"Table2_before_matching.csv\")\n结果中可以看出x.Age和x.Gender两个变量在两组间是有差异的，其中SMD(standardized mean differences)可以用来衡量协变量在不同组间的差异；除此之外，这两个变量的P值在不同性别间也是小于0.001的，说明不同性别间这两个变量是有明显差别的。\n如果此时直接探讨是否吸烟对CVD的影响，很有可能会得到错误的答案，经典的辛普森悖论就是由于混杂因素的存在才导致出现神奇的结果（比如有种药对男人有效，对女人也有效，但是对全人类就没效了！）。\n所以要想办法解决x.Age和x.Gender两个变量在两组间的差异，达到基线可比的目的。今天要介绍的方法就是倾向性评分匹配。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R语言倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#准备数据",
    "href": "1035-psm.html#准备数据",
    "title": "28  R语言倾向性评分：匹配",
    "section": "",
    "text": "CVD：结果变量，1是有心血管疾病，0是没有\nx.Age：年龄\nx.Gender：0是女，1是男\nSmoke：1吸烟，0不吸烟，是我们的处理因素\n\n\n\n\n使用compareGroups包1行代码生成基线资料表\n使用R语言快速绘制三线表\ntableone？table1？傻傻分不清楚\n超强的gtSummary ≈ gt + comparegroups ??",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R语言倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#matchit包进行psm",
    "href": "1035-psm.html#matchit包进行psm",
    "title": "28  R语言倾向性评分：匹配",
    "section": "28.2 matchIt包进行PSM",
    "text": "28.2 matchIt包进行PSM\nmatchIt包支持非常多计算PS的方法，比如自带的logistic回归、广义可加模型、分类和回归树、神经网络，除了自带的方法，也支持其他方法计算的PS。这些方法通过distance参数指定：\n\ndistance:指定PS的计算方法，默认是logit，即logistic回归，GAMlogit（广义可加模型），rpart（决策树）,nnet(神经网络)，除此之外，也可以是使用其他包或方法计算的PS值！\ndistance.options:当你选择好了方法之后，不同的方法会有不同的额外选项。\n\n下面演示使用logistic回归的方法计算PS，这里我们的处理因素是二分类变量(是否吸烟)，可以通过逻辑回归计算这些协变量（也就是混杂因素）的P值，这个P值就是倾向性评分。倾向性评分就是P值！（有网友指出这样说不对，应该是参数pi表示事件的概率）\n\nlibrary(MatchIt)\n\n# 这里为了方便演示直接删掉了缺失值\ndata.complete &lt;- na.omit(data)\n\n# 因变量是处理因素，自变量是需要平衡的协变量\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\n                 data = data.complete,\n                 distance = \"logit\" # 选择logistic回归\n                 )\n\nm.out\n## A matchit object\n##  - method: 1:1 nearest neighbor matching without replacement\n##  - distance: Propensity score\n##              - estimated with logistic regression\n##  - number of obs.: 831 (original), 738 (matched)\n##  - target estimand: ATT\n##  - covariates: x.Age, x.Gender\n\n上面是一个简单的结果，告诉你匹配方法是1:1无放回最近邻匹配，计算方法是logistic回归，匹配了738例等信息。\n可通过以下方法获得算法估计的PS值：\n\neps &lt;- m.out$distance\nlength(eps)\n## [1] 831\nhead(eps)\n##         1         2         3         4         5         6 \n## 0.2583040 0.2545807 0.1847661 0.1818430 0.1200378 0.2774451\n\n一开始我们已经计算出了实际PS值（tps），所以我们可以画一个tps和估计ps的散点图，以tps为横坐标，以eps为纵坐标：\n\nlibrary(ggplot2)\n\n# 去掉缺失值\ntps.comp &lt;- tps[complete.cases(data)]\nSmoke.comp &lt;- as.factor(Smoke[complete.cases(data)])\ndf &lt;- data.frame(True=tps.comp, Estimated=eps, Smoke=Smoke.comp)\n\nggplot(df, aes(x=True, y=Estimated, colour=Smoke)) +\ngeom_point() +\ngeom_abline(intercept=0,slope=1, colour=\"#990000\", linetype=\"dashed\") +\nexpand_limits(x=c(0,1),y=c(0,1))\n\n\n\n\n\n\n\n\n可以看到拟合结果非常烂！因为一开始计算tps时用了平方（二次项），但是使用logistic估计ps时并没有用平方。\n我们把公式也变成平方即可，此时再画一个拟合图就完美一致了！如下所示：\n\n# 对x.Age平方\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age +x.Gender,\n                 data=data.complete) \n\neps &lt;- m.out$distance\n\ntps.comp &lt;- tps[complete.cases(data)]\nSmoke.comp &lt;- as.factor(Smoke[complete.cases(data)])\ndf &lt;- data.frame(True=tps.comp, Estimated=eps, Smoke=Smoke.comp)\nggplot(df, aes(x=True, y=Estimated, colour=Smoke)) +\ngeom_point() +\ngeom_abline(intercept=0,slope=1, colour=\"#990000\", linetype=\"dashed\") +\nexpand_limits(x=c(0,1),y=c(0,1))\n\n\n\n\n\n\n\n\n此时的PS是通过logistic回归计算的，既然PS就是P值，当然你完全可以用glm自己计算，通过以下方法：\n\ntmp &lt;- glm(Smoke~I(x.Age^2)+x.Age +x.Gender, data=data.complete,\n           family = binomial())\n\ntmp.df &lt;- data.frame(estimated = tmp$fitted.values,\n                     true = tps.comp,\n                     Smoke=Smoke.comp)\n\nggplot(tmp.df, aes(true, estimated))+\n  geom_point(aes(color=Smoke))+\n  geom_abline(intercept=0,slope=1, colour=\"#990000\", linetype=\"dashed\") +\n  expand_limits(x=c(0,1),y=c(0,1))\n\n\n\n\n\n\n\n\n看这个结果和matchit得到的结果完全一样！\n改变matchit()的参数即可使用不同的算法估计PS，比如下面是分类和回归树及神经网络方法：\n\n# cart\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\ndata=data.complete,\ndistance='rpart')\n\n# nnet\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\ndata=data.complete,\ndistance='nnet',\ndistance.options=list(size=16))\n\n默认的只有4种方法，但是完全可以自己通过其他方法计算PS，然后提供给distance参数即可！非常强大！\n\n28.2.1 使用随机森林计算PS\n默认没提供随机森林的算法，我们可以通过其他R包计算，反正PS就是P值，只要拿到P值就可以了！\n\n# 使用随机森林构建模型\nlibrary(randomForest)\ndata.complete$Smoke &lt;- factor(data.complete$Smoke)\n\nrf.out &lt;- randomForest(Smoke~x.Age+x.Gender, data=data.complete)\nrf.out\n## \n## Call:\n##  randomForest(formula = Smoke ~ x.Age + x.Gender, data = data.complete) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 1\n## \n##         OOB estimate of  error rate: 22.5%\n## Confusion matrix:\n##     0   1 class.error\n## 0 360 102   0.2207792\n## 1  85 284   0.2303523\n\n从随机森林结果中提取预测类别为1（有CVD）的概率：\n\neps &lt;- rf.out$votes[,2] # Estimated PS\n\n接下来只要把这个eps提供给distance参数即可：\n\nmatchit(formula=Smoke~x.Age+x.Gender,\n        data=data.complete,\n        distance=eps, # 自己估计的eps\n        method='nearest',\n        replace=TRUE,\n        discard='both',\n        ratio=2)\n## A matchit object\n##  - method: 2:1 nearest neighbor matching with replacement\n##  - distance: User-defined [common support]\n##  - common support: units from both groups dropped\n##  - number of obs.: 831 (original), 548 (matched)\n##  - target estimand: ATT\n##  - covariates: x.Age, x.Gender\n\n其他方法也是同理，只需要提供P值即可，但并不是越复杂的方法效果越好哦！不信的话可以把几种方法得到的eps都画一个散点拟合图看看效果，这个数据是逻辑回归最好哈！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R语言倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#主要匹配方法选择",
    "href": "1035-psm.html#主要匹配方法选择",
    "title": "28  R语言倾向性评分：匹配",
    "section": "28.3 主要匹配方法选择",
    "text": "28.3 主要匹配方法选择\n在确定了使用哪种算法计算PS后，匹配方法也是需要注意的一个问题，需要注意以下几个方面，首先是匹配方法的选择（method），然后是采样手段（有无放回），相似度的度量（卡钳值或其他），匹配比例（1:1或1：多）。\n\nmethod:\n\n默认的匹配方法是最近邻匹配nearest，其他方法还有\n“exact” (exact matching),\n“full” (optimal full matching),\n“optimal” (optimal pair matching),\n“cardinality”(cardinality and template matching),\n“subclass” (subclassification),\n“genetic” (genetic matching),\n“cem” (coarsened exact matching)\n\n\n每个匹配方法都提供了详细的解释，大家感兴趣的自己查看即可。\n\ncaliper:卡钳值，也就是配对标准，两组的概率值（PS）差距在这个标准内才会配对。这里的卡钳值是PS标准差的倍数，默认是不设置卡钳值。还有一个std.caliper参数，默认是TRUE，如果设置FALSE，你设置的卡钳值就直接是PS的倍数。\nreplace:能否重复匹配，默认是FALSE，意思是假如干预组的1号匹配到了对照组的A，那A就不能再和其他的干预组进行匹配了。\nratio:设置匹配比例，干预组:对照组到底是1比几，默认为1:1。ratio=2即是干预组：对照组是1:2。所以一般要求数据的对照组数量多于干预组才行。如果对照组比干预组多出很多，完全可以设置1:n进行匹配，这样还能损失更少的样本信息，但是一般也不会超过1:4。\nreestimate:如果是TRUE，丢掉没匹配上的样本，PS会使用剩下的样本重新计算PS，如果是FALSE或者不写就不会重新计算PS。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n比如下面是一个有放回的，1:2的，最近邻匹配：\n\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 replace=TRUE,\n                 ratio=2)\n\n可以通过m.out$match.matrix获取配好的对子：\n\nhead(m.out$match.matrix)\n##    [,1]  [,2] \n## 1  \"204\" \"23\" \n## 6  \"163\" \"283\"\n## 10 \"30\"  \"56\" \n## 12 \"28\"  \"41\" \n## 20 \"38\"  \"79\" \n## 26 \"70\"  \"84\"\n\n第一列是干预组的序号，第二列是和干预组配好对的，对照组的序号。\nm.out$discarded查看某个样本是否被丢弃：\n\ntable(m.out$discarded)\n## \n## FALSE \n##   831",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R语言倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#匹配后数据的平衡性检验",
    "href": "1035-psm.html#匹配后数据的平衡性检验",
    "title": "28  R语言倾向性评分：匹配",
    "section": "28.4 匹配后数据的平衡性检验",
    "text": "28.4 匹配后数据的平衡性检验\n检查匹配后的数据，主要是看协变量在不同组间是否已经均衡了（是不是没有差异了）。\n关于这个倾向性评分匹配后数据的平衡性检验，文献中比较推荐使用SMD和VR(variance ratio)，SMD&lt;0.25说明均衡了，VR&gt;2.0或者VR&lt;0.5说明很不均衡（越接近1越均衡）！\n但其实也可以用假设检验，比如t检验、卡方检验等，也是没有统一的标准！\n做一个1:1无放回的最近邻匹配：\n\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 replace=FALSE,\n                 ratio = 1)\n\n通过summary()查看匹配前后，不同组间协变量的各种统计量。通常建议选择standardize = TRUE查看标准后的各协变量的平衡性指标：\n\nsummary(m.out,standardize = TRUE)\n## \n## Call:\n## matchit(formula = Smoke ~ x.Age + x.Gender, data = data.complete, \n##     method = \"nearest\", distance = \"logit\", replace = FALSE, \n##     ratio = 1)\n## \n## Summary of Balance for All Data:\n##          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance        0.5170        0.3858          0.9050     0.6205    0.2069\n## x.Age          47.0352       42.7619          0.5247     0.1711    0.1306\n## x.Gender        0.7832        0.4502          0.8081          .    0.3330\n##          eCDF Max\n## distance   0.4833\n## x.Age      0.3629\n## x.Gender   0.3330\n## \n## Summary of Balance for Matched Data:\n##          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance        0.5170        0.4380          0.5448     0.7605    0.1298\n## x.Age          47.0352       47.3035         -0.0329     0.1908    0.1072\n## x.Gender        0.7832        0.5610          0.5393          .    0.2222\n##          eCDF Max Std. Pair Dist.\n## distance   0.4255          0.7108\n## x.Age      0.2439          2.2436\n## x.Gender   0.2222          0.5393\n## \n## Sample Sizes:\n##           Control Treated\n## All           462     369\n## Matched       369     369\n## Unmatched      93       0\n## Discarded       0       0\n\n结果主要是3个部分：\n\nSummary of Balance for All Data：原始数据中干预组和对照组的平均PS值和平均协变量，SMD,VR，每个协变量和PS的CDF（cumulative distribution functions）的均值和最大值\nSummary of Balance for Matched Data：匹配后数据的指标\nSample Sizes：样本数量\n\n通过观察比较匹配前后的数据指标可知，x.Age均衡了（0.0329&lt;0.1），但是x.Gender并没有均衡(0.5393&gt;0.1)！\n这个默认的函数在计算SMD的时候会把分类变量按照连续性变量进行计算，所以计算结果是有一些问题的。在一开始计算匹配前数据的SMD时我们用的是tableone这个包，匹配后数据的SMD理论上也是可以用这个包的：\n\n# 首先提取匹配后的数据\nmdata &lt;- match.data(m.out)\n\nlibrary(tableone)\n\ntable5 &lt;- CreateTableOne(vars = c('x.Age', 'x.Gender', 'CVD'),\n                         data = mdata,\n                         factorVars = c('x.Gender', 'CVD'),\n                         strata = 'Smoke',\n                         smd=TRUE)\ntable5 &lt;- print(table5, smd=TRUE, showAllLevels = TRUE, \n                noSpaces = TRUE, printToggle = FALSE)\ntable5\n##                    Stratified by Smoke\n##                     level 0               1              p        test SMD    \n##   n                 \"\"    \"369\"           \"369\"          \"\"       \"\"   \"\"     \n##   x.Age (mean (SD)) \"\"    \"47.30 (18.65)\" \"47.04 (8.14)\" \"0.800\"  \"\"   \"0.019\"\n##   x.Gender (%)      \"0\"   \"162 (43.9)\"    \"80 (21.7)\"    \"&lt;0.001\" \"\"   \"0.487\"\n##                     \"1\"   \"207 (56.1)\"    \"289 (78.3)\"   \"\"       \"\"   \"\"     \n##   CVD (%)           \"0\"   \"287 (77.8)\"    \"190 (51.5)\"   \"&lt;0.001\" \"\"   \"0.572\"\n##                     \"1\"   \"82 (22.2)\"     \"179 (48.5)\"   \"\"       \"\"   \"\"\n\n这个tableone计算的x.Gender的SMD是0.487，也是表明这个变量并没有被均衡。\n但是tableone这个包计算的SMD也是有一些问题的，具体原因大家自己读文献吧：Zhang Z, Kim HJ, Lonjon G, et al. Balance diagnostics after propensity score matching. Ann Transl Med 2019;7:16.\n所以推荐大家使用cobalt包进行平衡性指标的计算。\n\n28.4.1 cobalt包\n使用cobalt包进行平衡性指标的计算，这个包很专业，专门处理这类匹配问题的，大家可以去它的官网学习更多的细节！\n\nlibrary(cobalt)\n\n# m.threshold表示SMD的阈值，小于这个阈值的协变量是平衡的\nbal.tab(m.out, m.threshold = 0.1, un = TRUE)\n## Balance Measures\n##              Type Diff.Un Diff.Adj        M.Threshold\n## distance Distance  0.9050   0.5448                   \n## x.Age     Contin.  0.5247  -0.0329     Balanced, &lt;0.1\n## x.Gender   Binary  0.3330   0.2222 Not Balanced, &gt;0.1\n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1         1\n## Not Balanced, &gt;0.1     1\n## \n## Variable with the greatest mean difference\n##  Variable Diff.Adj        M.Threshold\n##  x.Gender   0.2222 Not Balanced, &gt;0.1\n## \n## Sample sizes\n##           Control Treated\n## All           462     369\n## Matched       369     369\n## Unmatched      93       0\n\nx.Age的SMD和默认是一样的，但是x.Gender是0.2222，比默认的小多了！\n这个结果比默认的结果更可靠，具体原因大家自己去读上面那篇文献。\n计算VR，结果中并没有计算x.Gender的VR，而且根据VR来看，x.Age也没有均衡。\n\nbal.tab(m.out, v.threshold = 2)\n## Balance Measures\n##              Type Diff.Adj V.Ratio.Adj      V.Threshold\n## distance Distance   0.5448      0.7605     Balanced, &lt;2\n## x.Age     Contin.  -0.0329      0.1908 Not Balanced, &gt;2\n## x.Gender   Binary   0.2222           .                 \n## \n## Balance tally for variance ratios\n##                  count\n## Balanced, &lt;2         1\n## Not Balanced, &gt;2     1\n## \n## Variable with the greatest variance ratio\n##  Variable V.Ratio.Adj      V.Threshold\n##     x.Age      0.1908 Not Balanced, &gt;2\n## \n## Sample sizes\n##           Control Treated\n## All           462     369\n## Matched       369     369\n## Unmatched      93       0\n\n\n\n28.4.2 统计检验衡量均衡性\n除了SMD和VR之外，传统的统计检验也可以用于检查匹配后的数据有没有均衡！\n首先取出匹配好的数据：\n\nmdata &lt;- match.data(m.out)\nhead(mdata)\n##    x.Age x.Gender Smoke CVD  distance weights subclass\n## 1     51        0     1   0 0.2583040       1        1\n## 2     50        0     0   0 0.2545807       1      205\n## 6     56        0     1   1 0.2774451       1      229\n## 9     71        0     0   1 0.3397803       1      228\n## 10    47        0     1   1 0.2436248       1      345\n## 12    59        0     1   1 0.2893402       1        2\n\n其中distance是估计的PS，weights是权重，因为我们用的是1:1无放回匹配，所以全都是1。\n下面用t检验看看匹配后干预组和对照组的x.Age有没有差异：\n\nt.test(x.Age ~ Smoke, data = mdata)\n## \n##  Welch Two Sample t-test\n## \n## data:  x.Age by Smoke\n## t = 0.25327, df = 503.47, p-value = 0.8002\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -1.812969  2.349555\n## sample estimates:\n## mean in group 0 mean in group 1 \n##        47.30352        47.03523\n\n结果也是显示x.Age已经没有差异了！\n然后用卡方检验看看x.Gender是否还有差异：\n\nchisq.test(mdata$x.Gender, mdata$Smoke,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  mdata$x.Gender and mdata$Smoke\n## X-squared = 41.342, df = 1, p-value = 1.278e-10\n\n结果显示x.Gender还是有差异的，这个结果也和SMD的判断结果相同。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R语言倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#结果可视化",
    "href": "1035-psm.html#结果可视化",
    "title": "28  R语言倾向性评分：匹配",
    "section": "28.5 结果可视化",
    "text": "28.5 结果可视化\n默认提供3种图形，但是美观性太差，就不放图了，大家感兴趣的可以自己试试看。\n\nplot(m.out) # 默认QQ图\nplot(m.out, type = 'jitter') # 散点图\nplot(m.out, type = 'hist') # 直方图\n\n默认的不好看，还是用cobalt包进行结果的可视化。\n\ncowplot::plot_grid(\n  bal.plot(m.out, var.name = 'x.Age', which = 'both', grid=TRUE),\n  bal.plot(m.out, var.name = 'x.Gender', which = 'both', grid=TRUE),\n  bal.plot(m.out, var.name = 'x.Age', which = 'both', grid=TRUE, type=\"ecdf\"),\n  # 还有很多参数可调整\n  love.plot(bal.tab(m.out, m.threshold=0.1),\n            stat = \"mean.diffs\",\n            grid=TRUE,\n            stars=\"raw\",\n            abs = F)\n  )\n\n\n\n\n\n\n\n\n上面两幅图展示的是协变量在匹配前（unadjusted sample）和匹配后（adjusted sample）的数据中的分布情况，连续型变量默认是画密度图，分类变量默认是画柱状图。\n左下图是累计密度图。右下的love plot图可视化匹配前后协变量的SMD，两条竖线是0.1阈值线，匹配后x.Age在两条竖线之间，说明平衡，x.Gender不在两条竖线之间，说明还是没平衡。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R语言倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#不平衡怎么办",
    "href": "1035-psm.html#不平衡怎么办",
    "title": "28  R语言倾向性评分：匹配",
    "section": "28.6 不平衡怎么办？",
    "text": "28.6 不平衡怎么办？\n比如这里的x.Gender这个变量就是不平衡的。\n有非常多的方法可以尝试，这里提供5种方法，但是非常有可能你各种方法都试过了还是不平衡！\n首先可以换一种计算PS的方法，可以换算法，换公式（增加二次项、交互项等）。\n\n# 增加二次项，结果依然不平衡\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 replace=FALSE,\n                 ratio=1)\n\n第二可以换匹配方法及对应的参数。\n\n# 还是不平衡\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='genetic',\n                 pop.size=100)\nbal.tab(m.out, m.threshold=0.1)\n\n第三，可以使用精确匹配，性别不平衡，那就在匹配时要求按照性别精确匹配，可以使用参数exact=c('x.Gender')。\n但是这样做的代价是大部分样本都浪费了！只有一小部分才能匹配上！\n\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 exact = c('x.Gender','x.Age'), # 精准！\n                 replace=FALSE,\n                 ratio=1)\n## Warning: Fewer control units than treated units in some `exact` strata; not all\n## treated units will get a match.\nbal.tab(m.out, m.threshold=0.1)\n## Balance Measures\n##                Type Diff.Adj    M.Threshold\n## distance   Distance        0 Balanced, &lt;0.1\n## I(x.Age^2)  Contin.        0 Balanced, &lt;0.1\n## x.Age       Contin.        0 Balanced, &lt;0.1\n## x.Gender     Binary        0 Balanced, &lt;0.1\n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1         4\n## Not Balanced, &gt;0.1     0\n## \n## Variable with the greatest mean difference\n##    Variable Diff.Adj    M.Threshold\n##  I(x.Age^2)        0 Balanced, &lt;0.1\n## \n## Sample sizes\n##           Control Treated\n## All           462     369\n## Matched       147     147\n## Unmatched     315     222\n\n第四，增加样本量（一切误差问题都可以通过增加样本量解决）。\n第五，匹配后结合其他方法，比如回归、分层等。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R语言倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#其他问题",
    "href": "1035-psm.html#其他问题",
    "title": "28  R语言倾向性评分：匹配",
    "section": "28.7 其他问题",
    "text": "28.7 其他问题\n这篇推文关于倾向性评分匹配说的还算详细，尤其是matchIt包的使用，但大部分都是基于开头说的那篇文献。\n除此之外，关于倾向性评分，还有一些很重要的问题并没有涉及到。比如：\n\n样本权重不同，匹配后数据如何检查平衡性？\n倾向性评分只能平衡记录到的协变量，对于潜在的、未被记录的误差不能平衡，怎么办？\n处理因素多分组或者是连续型变量时如何处理？\n倾向性评分的加权、回归、分层如何做？\n\n这些问题待以后有时间慢慢解决！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R语言倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#参考资料",
    "href": "1035-psm.html#参考资料",
    "title": "28  R语言倾向性评分：匹配",
    "section": "28.8 参考资料",
    "text": "28.8 参考资料\n\nhttps://zhuanlan.zhihu.com/p/386501046\nhttps://mp.weixin.qq.com/s/ITWBruRe5LhuPq8TXjxPZQ\nhttps://zhuanlan.zhihu.com/p/559469895\nPropensity score matching with R: conventional methods and new features",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R语言倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html",
    "href": "1036-pssc.html",
    "title": "29  R语言倾向性评分：回归和分层",
    "section": "",
    "text": "29.1 演示数据\n下面这个例子探讨不同学校对学生成绩的影响，这个数据一共有11078行，23列，我们只用其中一部分数据演示倾向性评分回归和分层。\n我们用到以下几个变量：\n首先加载数据，已上传到QQ群，需要的加群下载即可。\nlibrary(tidyverse)\n\necls &lt;- read.csv(\"datasets/ecls.csv\") %&gt;% \n  dplyr::select(c5r2mtsc_std,catholic,race_white,w3momed_hsb,p5hmage,\n                w3momscr,w3dadscr) %&gt;%\n  na.omit()\n\ndim(ecls)\n## [1] 5548    7\nglimpse(ecls)\n## Rows: 5,548\n## Columns: 7\n## $ c5r2mtsc_std &lt;dbl&gt; 0.98175332, 0.59437751, 0.49061062, 1.45127793, 2.5956991…\n## $ catholic     &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ race_white   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, …\n## $ w3momed_hsb  &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, …\n## $ p5hmage      &lt;int&gt; 47, 41, 43, 38, 47, 41, 31, 38, 26, 38, 27, 40, 33, 36, 4…\n## $ w3momscr     &lt;dbl&gt; 53.50, 34.95, 63.43, 53.50, 61.56, 38.18, 34.95, 63.43, 3…\n## $ w3dadscr     &lt;dbl&gt; 77.50, 53.50, 53.50, 53.50, 77.50, 53.50, 29.60, 33.42, 2…",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R语言倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#演示数据",
    "href": "1036-pssc.html#演示数据",
    "title": "29  R语言倾向性评分：回归和分层",
    "section": "",
    "text": "catholic：是我们的处理因素，1是天主教（catholic）学校，0是公立（public）学校，\nc5r2mtsc_std：结果变量（因变量），标准化之后的学生成绩，\nrace_white：是否是白人，1是0否，\nw3momed_hsb：妈妈的教育水平，1高中及以下，0大学及以上，\np5hmage：妈妈的年龄，要控制的混杂因素，\nw3momscr：妈妈的成绩，\nw3dadscr：爸爸的成绩。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R语言倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#原始数据的概况",
    "href": "1036-pssc.html#原始数据的概况",
    "title": "29  R语言倾向性评分：回归和分层",
    "section": "29.2 原始数据的概况",
    "text": "29.2 原始数据的概况\n首先看一下原始数据的情况。\n\necls %&gt;%\n  group_by(catholic) %&gt;%\n  summarise(n_students = n(),\n            mean_math = mean(c5r2mtsc_std),\n            std_error = sd(c5r2mtsc_std) / sqrt(n_students))\n## # A tibble: 2 × 4\n##   catholic n_students mean_math std_error\n##      &lt;int&gt;      &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1        0       4597     0.156    0.0144\n## 2        1        951     0.221    0.0277\n\n可以看到去公立学校的4597人，去天主教学校的才951人，并且去天主教的学校的学生成绩明显高于去公立学校的学生。\n此时如果不控制混杂因素直接进行t检验，结果是有统计学意义的，但是由于基线资料不可比，一开始两组学生的各种情况就不一样，所以结果很难说明成绩不同到底是不同学校导致的还是混杂因素导致的。\n\nwith(ecls, t.test(c5r2mtsc_std ~ catholic))\n## \n##  Welch Two Sample t-test\n## \n## data:  c5r2mtsc_std by catholic\n## t = -2.0757, df = 1508.1, p-value = 0.03809\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -0.126029105 -0.003564746\n## sample estimates:\n## mean in group 0 mean in group 1 \n##       0.1562757       0.2210727\n\n我们可以看看不同组别间混杂因素的差异，首先是3个连续型变量在两组间的平均值，可以看到都是不一样的：\n\necls %&gt;%\n  group_by(catholic) %&gt;%\n  select(p5hmage, w3momscr, w3dadscr) %&gt;%\n  summarise_all(list(~mean(., na.rm = T)))\n## # A tibble: 2 × 4\n##   catholic p5hmage w3momscr w3dadscr\n##      &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n## 1        0    37.8     43.8     42.6\n## 2        1    39.8     47.5     45.8\n\n可以看到不同组别间混杂因素明显是不同的，还可以分别对3个连续型变量做t检验，结果也显示这些混杂因素在一开始就是存在差异的。\n\necls %&gt;% \n  pivot_longer(cols = c(p5hmage,w3momscr,w3dadscr),\n               names_to = \"covs\",\n               values_to = \"values\"\n               ) %&gt;% \n  group_split(covs) %&gt;% \n  map(~t.test(values ~ catholic, data = .x)) %&gt;% \n  map_dbl(\"p.value\")\n## [1] 1.062659e-28 3.722314e-16 2.208513e-18\n\n对于两个分类变量，我们可以看看分别在两组间的数量构成比有没有差异。\n\ntab &lt;- xtabs(~race_white+catholic,data = ecls)\ntab\n##           catholic\n## race_white    0    1\n##          0 1610  222\n##          1 2987  729\nchisq.test(tab,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  tab\n## X-squared = 48.596, df = 1, p-value = 3.145e-12\n\n\ntab &lt;- xtabs(~w3momed_hsb+catholic,data = ecls)\ntab\n##            catholic\n## w3momed_hsb    0    1\n##           0 2777  751\n##           1 1820  200\nchisq.test(tab,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  tab\n## X-squared = 117.24, df = 1, p-value &lt; 2.2e-16\n\n可以看到两个分类变量在两组间的差异是非常明显的！\n所以我们现在要做的事就是控制混杂因素，让这些混杂因素变成可比的状态，不要影响我们的处理因素。\n开头也说过，控制混杂因素的方法其实是很多的，比如分层、协方差分析、多因素分析等，每种情况都要具体分析，选择一种最合适的。\n下面我们介绍倾向性评分回归和分层。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R语言倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#计算倾向性评分",
    "href": "1036-pssc.html#计算倾向性评分",
    "title": "29  R语言倾向性评分：回归和分层",
    "section": "29.3 计算倾向性评分",
    "text": "29.3 计算倾向性评分\n倾向性评分就是倾向干预的概率，所以可以通过逻辑回归计算P值，这个P值就是倾向性评分，所以也不一定要用到专用的R包！\n首先以处理因素（这里是catholic）为因变量，混杂因素为自变量构建逻辑回归模型：\n\nm_ps &lt;- glm(catholic ~ race_white+w3momed_hsb+p5hmage+w3momscr+w3dadscr,\n            family = binomial(), data = ecls)\n\n提取P值，也就是倾向性评分：\n\nprs_df &lt;- data.frame(pr_score = predict(m_ps, type = \"response\"),\n                     catholic = m_ps$model$catholic)\nhead(prs_df)\n##    pr_score catholic\n## 1 0.3755223        0\n## 2 0.2340976        0\n## 4 0.2990706        0\n## 5 0.2394663        1\n## 6 0.3920115        0\n## 8 0.2391453        0\n\n可以看一下不同处理因素间的P值（倾向性评分）分布：\n\nlabs &lt;- paste(\"Actual school type attended:\", c(\"Catholic\", \"Public\"))\nprs_df %&gt;%\n  mutate(catholic = ifelse(catholic == 1, labs[1], labs[2])) %&gt;%\n  ggplot(aes(x = pr_score)) +\n  geom_histogram(color = \"white\") +\n  facet_wrap(~catholic) +\n  xlab(\"Probability of going to Catholic school\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n计算倾向性评分只是第一步，有了这个倾向性评分后，就可以进行下面的分析了，比如回归、匹配、加权、分层等。\n可以看出我们这个PS是偏态的，其实是可以对PS做一些变换的，比如log，然后使用变换后的PS继续进行后面的分析。这里就不做变换了。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R语言倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#倾向性评分回归",
    "href": "1036-pssc.html#倾向性评分回归",
    "title": "29  R语言倾向性评分：回归和分层",
    "section": "29.4 倾向性评分回归",
    "text": "29.4 倾向性评分回归\n此时如果直接把这个评分和catholic作为自变量进行回归分析，就是倾向性评分回归了（也叫协变量调整/倾向性评分矫正等）！应该是倾向性评分4种方法里面最简单的一种了。\n\n# 计算倾向性评分\npr_score &lt;- predict(m_ps, type = \"response\")\n\n# 把倾向性评分加入到原数据中\necls_ps &lt;- ecls %&gt;% \n  mutate(ps = pr_score)\n\n# 把处理因素和倾向性评分作为自变量进行回归\npsl &lt;- lm(c5r2mtsc_std ~ catholic + ps, data = ecls_ps)\nsummary(psl)\n## \n## Call:\n## lm(formula = c5r2mtsc_std ~ catholic + ps, data = ecls_ps)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4.0525 -0.5741  0.0462  0.6106  3.1468 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.58249    0.02929 -19.885  &lt; 2e-16 ***\n## catholic    -0.10772    0.03241  -3.324 0.000893 ***\n## ps           4.48236    0.15873  28.239  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8934 on 5545 degrees of freedom\n## Multiple R-squared:  0.1263, Adjusted R-squared:  0.126 \n## F-statistic: 400.8 on 2 and 5545 DF,  p-value: &lt; 2.2e-16\n\n结果表明处理因素(分组变量)还是有意义的！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R语言倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#倾向性评分分层",
    "href": "1036-pssc.html#倾向性评分分层",
    "title": "29  R语言倾向性评分：回归和分层",
    "section": "29.5 倾向性评分分层",
    "text": "29.5 倾向性评分分层\n顾名思义，根据PS值进行分层，然后在每层内进行分析。每一层的协变量分布可认为是同质或均衡的。先对每一层干预与结局之间的关联进行估算，然后对所有层的关联作加权平均，最后得出干预与结局之间的总的关联效应。\n一般来说最好保证干预组和对照组两组的PS范围在差不多的范围内，如果相差很大，那分层效果肯定不好。比如干预组PS范围是0.50.9，对照组PS范围是0.010.4，这样两组PS完全没有交集，按照PS进行分层没啥意义。\n首先看一下PS的范围：\n\necls_ps %&gt;% group_by(catholic) %&gt;% \n  summarise(range = range(ps))\n## Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\n## dplyr 1.1.0.\n## ℹ Please use `reframe()` instead.\n## ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n##   always returns an ungrouped data frame and adjust accordingly.\n## # A tibble: 4 × 2\n## # Groups:   catholic [2]\n##   catholic  range\n##      &lt;int&gt;  &lt;dbl&gt;\n## 1        0 0.0370\n## 2        0 0.477 \n## 3        1 0.0492\n## 4        1 0.404\n\n两组分别是0.0370.477和0.0490.404，范围基本一致，所以我们就直接按照总体PS的最大值和最小值进行分层，如果两组PS差很多，可以按照两组PS的交集进行分层。\n文献一般建议分5-10层，可以根据PS进行平分，也可以按照百分位数进行分层，具体方法很多，大家自己看文献即可。\n我们这里简单点，结合上面PS的分布图，分4层，切点就用0.1,0.2,0.3。\n\necls_pslevel &lt;- ecls_ps %&gt;% \n  mutate(ps_level = case_when(ps&lt;=0.1 ~ \"level_1\",\n                              ps&gt;0.1 & ps&lt;=0.2 ~ \"level_2\",\n                              ps&gt;0.2 & ps&lt;=0.3 ~ \"level_3\",\n                              TRUE ~ \"level_4\"\n                              ),\n         #ps_level = factor(ps_level),\n         p5hmage = as.double(p5hmage),\n         across(where(is.integer), as.factor)\n         )\n\nglimpse(ecls_pslevel)\n## Rows: 5,548\n## Columns: 9\n## $ c5r2mtsc_std &lt;dbl&gt; 0.98175332, 0.59437751, 0.49061062, 1.45127793, 2.5956991…\n## $ catholic     &lt;fct&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ race_white   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, …\n## $ w3momed_hsb  &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, …\n## $ p5hmage      &lt;dbl&gt; 47, 41, 43, 38, 47, 41, 31, 38, 26, 38, 27, 40, 33, 36, 4…\n## $ w3momscr     &lt;dbl&gt; 53.50, 34.95, 63.43, 53.50, 61.56, 38.18, 34.95, 63.43, 3…\n## $ w3dadscr     &lt;dbl&gt; 77.50, 53.50, 53.50, 53.50, 77.50, 53.50, 29.60, 33.42, 2…\n## $ ps           &lt;dbl&gt; 0.37552233, 0.23409764, 0.29907061, 0.23946627, 0.3920115…\n## $ ps_level     &lt;chr&gt; \"level_4\", \"level_3\", \"level_3\", \"level_3\", \"level_4\", \"l…",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R语言倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#分层后的数据",
    "href": "1036-pssc.html#分层后的数据",
    "title": "29  R语言倾向性评分：回归和分层",
    "section": "29.6 分层后的数据",
    "text": "29.6 分层后的数据\n下面我们对每一层内的3个连续型协变量和我们的因变量进行t检验，其实这里可以直接用rstatix包解决，非常好用，但其实rstatix包就是基于purrr的，所以直接用purrr也可以。\n\necls_pslevel %&gt;% \n  pivot_longer(cols = c(1,5:7),names_to = \"variates\",values_to = \"values\") %&gt;% \n  group_nest(ps_level,variates) %&gt;% \n  dplyr::mutate(tt = map(data, ~ t.test(values ~ catholic,data = .x)),\n                res = map_dfr(tt, broom::tidy)\n                ) %&gt;% \n  unnest(res)\n## # A tibble: 16 × 14\n##    ps_level variates         data tt      estimate estimate1 estimate2 statistic\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;list&lt;tibb&gt; &lt;list&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n##  1 level_1  c5r2mtsc… [1,202 × 5] &lt;htest&gt; -0.00108    -0.347   -0.346   -0.00973\n##  2 level_1  p5hmage   [1,202 × 5] &lt;htest&gt; -1.00       32.9     33.9     -1.66   \n##  3 level_1  w3dadscr  [1,202 × 5] &lt;htest&gt; -0.639      36.8     37.4     -0.886  \n##  4 level_1  w3momscr  [1,202 × 5] &lt;htest&gt; -1.07       37.0     38.1     -1.40   \n##  5 level_2  c5r2mtsc… [2,388 × 5] &lt;htest&gt;  0.0685      0.142    0.0737   1.54   \n##  6 level_2  p5hmage   [2,388 × 5] &lt;htest&gt; -0.724      37.4     38.1     -2.92   \n##  7 level_2  w3dadscr  [2,388 × 5] &lt;htest&gt; -0.818      40.7     41.5     -1.73   \n##  8 level_2  w3momscr  [2,388 × 5] &lt;htest&gt; -1.13       41.3     42.5     -2.21   \n##  9 level_3  c5r2mtsc… [1,618 × 5] &lt;htest&gt;  0.171       0.533    0.361    3.46   \n## 10 level_3  p5hmage   [1,618 × 5] &lt;htest&gt;  0.00290    41.1     41.1      0.0141 \n## 11 level_3  w3dadscr  [1,618 × 5] &lt;htest&gt; -1.36       47.5     48.8     -2.17   \n## 12 level_3  w3momscr  [1,618 × 5] &lt;htest&gt; -0.371      50.9     51.3     -0.573  \n## 13 level_4  c5r2mtsc…   [340 × 5] &lt;htest&gt;  0.0580      0.728    0.670    0.548  \n## 14 level_4  p5hmage     [340 × 5] &lt;htest&gt;  0.820      46.2     45.4      1.84   \n## 15 level_4  w3dadscr    [340 × 5] &lt;htest&gt;  0.868      59.6     58.7      0.582  \n## 16 level_4  w3momscr    [340 × 5] &lt;htest&gt; -0.739      60.0     60.7     -0.637  \n## # ℹ 6 more variables: p.value &lt;dbl&gt;, parameter &lt;dbl&gt;, conf.low &lt;dbl&gt;,\n## #   conf.high &lt;dbl&gt;, method &lt;chr&gt;, alternative &lt;chr&gt;\n\n直接看p.value这一列，可以看到大部分都是大于0.05的，因变量c5r2mtsc_std只有在第3层是有差异的！\nlevel_2中的p5hmage和w3momscr变量的P值是小于0.05的，level_3中的w3dadscr变量P值也是小于0.05的。\n这说明我们的分层并没有很好的解决这几个混杂因素的影响，而且分层后每一层内（除了第3层）的因变量都没有差异了。。。理想的结果应该是分层后每一层内混杂因素在两组间都是没有差异的，而因变量都是有差异的！这样才能说明我们的分层很好地控制了混杂因素！\n但我们的这个结果很明显很差劲！大家可以考虑不同的分层方法再重新尝试几次，或者这个数据并不适合使用这种方法，可以用其他方法试试看，比如匹配、回归等。\n下面再看看分类变量，首先是race_white，在每一层内使用卡方检验，我们直接提取P值：\n\necls_pslevel %&gt;% \n  group_split(ps_level) %&gt;% \n  map(~chisq.test(.$race_white,.$catholic,correct=F)) %&gt;% \n  map_dbl(\"p.value\")\n## Warning in chisq.test(.$race_white, .$catholic, correct = F): Chi-squared\n## approximation may be incorrect\n## [1] 0.4755703 0.8423902 0.5696924 0.2667193\n\n结果还不错，每一层内都没有差异了。\n然后是w3momed_hsb这个变量，但是由于我们的分层有问题，导致level_4这一层中w3momed_hsb全都是0！\n\n# level_4有问题\necls_pslevel %&gt;% \n  group_by(ps_level,w3momed_hsb,catholic) %&gt;% \n  summarise(count=n())\n## # A tibble: 14 × 4\n## # Groups:   ps_level, w3momed_hsb [7]\n##    ps_level w3momed_hsb catholic count\n##    &lt;chr&gt;    &lt;fct&gt;       &lt;fct&gt;    &lt;int&gt;\n##  1 level_1  0           0           61\n##  2 level_1  0           1            5\n##  3 level_1  1           0         1082\n##  4 level_1  1           1           54\n##  5 level_2  0           0         1262\n##  6 level_2  0           1          261\n##  7 level_2  1           0          724\n##  8 level_2  1           1          141\n##  9 level_3  0           0         1192\n## 10 level_3  0           1          407\n## 11 level_3  1           0           14\n## 12 level_3  1           1            5\n## 13 level_4  0           0          262\n## 14 level_4  0           1           78\n\n所以我们就对前3层做一个统计检验吧。\n\necls_pslevel %&gt;% \n  filter(!ps_level == \"level_4\") %&gt;% \n  group_split(ps_level) %&gt;% \n  map(~chisq.test(.$w3momed_hsb,.$catholic,correct=F)) %&gt;% \n  map_dbl(\"p.value\")\n## Warning in chisq.test(.$w3momed_hsb, .$catholic, correct = F): Chi-squared\n## approximation may be incorrect\n## Warning in chisq.test(.$w3momed_hsb, .$catholic, correct = F): Chi-squared\n## approximation may be incorrect\n## [1] 0.3022080 0.5994507 0.9316443\n\n可以看到每一层内也是没有明显差别的。\n说明我们的分层对2个分类变量的平衡效果还是可以的，但是对连续型变量的效果真是一言难尽！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R语言倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#总结",
    "href": "1036-pssc.html#总结",
    "title": "29  R语言倾向性评分：回归和分层",
    "section": "29.7 总结",
    "text": "29.7 总结\n倾向性评分回归和分层的大致过程就是这样的，但其实很多细节我都忽略了，比如到底分几层？依据是什么？用PS还是log(PS)？\n而且特地找了一个不是很成功的例子（可能不是很恰当），结果并不是很完美，还有很多可以调整测试的空间，大家可以适当修改其中的方法细节，最后得到一个笔记好的结果。\n实际使用时大家要根据自己的实际情况选择最合适的方法，多读文献，从文献中找灵感。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R语言倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#参考资料",
    "href": "1036-pssc.html#参考资料",
    "title": "29  R语言倾向性评分：回归和分层",
    "section": "29.8 参考资料",
    "text": "29.8 参考资料\n\nhttps://sejdemyr.github.io/r-tutorials/statistics/tutorial8.html",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R语言倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1037-psw.html",
    "href": "1037-psw.html",
    "title": "30  R语言倾向性评分：加权",
    "section": "",
    "text": "30.1 演示数据\n# 如果提示缺少R包直接安装即可\ndata(lindner, package = \"twang\")\n\nlindner[,c(3,4,6,7,8,10)] &lt;- lapply(lindner[,c(3,4,6,7,8,10)],factor)\n\nstr(lindner)\n## 'data.frame':    996 obs. of  11 variables:\n##  $ lifepres       : num  0 11.6 11.6 11.6 11.6 11.6 11.6 11.6 11.6 11.6 ...\n##  $ cardbill       : int  14301 3563 4694 7366 8247 8319 8410 8517 8763 8823 ...\n##  $ abcix          : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ stent          : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ height         : int  163 168 188 175 168 178 185 173 152 180 ...\n##  $ female         : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 1 1 2 2 1 ...\n##  $ diabetic       : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 1 1 1 1 1 1 ...\n##  $ acutemi        : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ejecfrac       : int  56 56 50 50 55 50 58 30 60 60 ...\n##  $ ves1proc       : Factor w/ 6 levels \"0\",\"1\",\"2\",\"3\",..: 2 2 2 2 2 2 2 2 2 2 ...\n##  $ sixMonthSurvive: logi  FALSE TRUE TRUE TRUE TRUE TRUE ...\n其中abcix是处理因素变量，sixMonthSurvive是二分类的结局变量，cardbill是连续型的结局变量，其余变量是协变量。\n首先可以通过tableone包看一下加权前的数据情况：\nlibrary(tableone)\n\ncovs &lt;- colnames(lindner)[c(1,4:10)]\n\ntab &lt;- CreateTableOne(vars = covs,\n                      strata = \"abcix\",\n                      data = lindner\n                      )\nprint(tab,showAllLevels = T,smd = T)\n##                       Stratified by abcix\n##                        level 0              1              p      test SMD   \n##   n                             298            698                           \n##   lifepres (mean (SD))        11.02 (2.54)   11.42 (1.45)   0.002       0.194\n##   stent (%)            0        124 (41.6)     206 (29.5)  &lt;0.001       0.255\n##                        1        174 (58.4)     492 (70.5)                    \n##   height (mean (SD))         171.45 (10.59) 171.44 (10.69)  0.996      &lt;0.001\n##   female (%)           0        183 (61.4)     467 (66.9)   0.111       0.115\n##                        1        115 (38.6)     231 (33.1)                    \n##   diabetic (%)         0        218 (73.2)     555 (79.5)   0.034       0.150\n##                        1         80 (26.8)     143 (20.5)                    \n##   acutemi (%)          0        280 (94.0)     573 (82.1)  &lt;0.001       0.372\n##                        1         18 ( 6.0)     125 (17.9)                    \n##   ejecfrac (mean (SD))        52.29 (10.30)  50.40 (10.42)  0.009       0.182\n##   ves1proc (%)         0          1 ( 0.3)       3 ( 0.4)  &lt;0.001       0.446\n##                        1        243 (81.5)     437 (62.6)                    \n##                        2         47 (15.8)     205 (29.4)                    \n##                        3          6 ( 2.0)      39 ( 5.6)                    \n##                        4          1 ( 0.3)      13 ( 1.9)                    \n##                        5          0 ( 0.0)       1 ( 0.1)\n如果只是看一下协变量是否在不同组间均衡，可以通过之前介绍过的cobalt实现：\nlibrary(cobalt)\n\n# 选择只有协变量的数据框\ncovariates &lt;- subset(lindner, select = c(1,4:10))\n\nbal.tab(covariates,treat = lindner$abcix, s.d.denom = \"pooled\",\n        m.threshold = 0.1, un = TRUE,\n        v.threshold = 2\n        )\n## Balance Measures\n##               Type Diff.Un     M.Threshold.Un V.Ratio.Un   V.Threshold.Un\n## lifepres   Contin.  0.1941 Not Balanced, &gt;0.1     0.3239 Not Balanced, &gt;2\n## stent       Binary  0.1210 Not Balanced, &gt;0.1          .                 \n## height     Contin. -0.0003     Balanced, &lt;0.1     1.0201     Balanced, &lt;2\n## female      Binary -0.0550     Balanced, &lt;0.1          .                 \n## diabetic    Binary -0.0636     Balanced, &lt;0.1          .                 \n## acutemi     Binary  0.1187 Not Balanced, &gt;0.1          .                 \n## ejecfrac   Contin. -0.1821 Not Balanced, &gt;0.1     1.0238     Balanced, &lt;2\n## ves1proc_0  Binary  0.0009     Balanced, &lt;0.1          .                 \n## ves1proc_1  Binary -0.1894 Not Balanced, &gt;0.1          .                 \n## ves1proc_2  Binary  0.1360 Not Balanced, &gt;0.1          .                 \n## ves1proc_3  Binary  0.0357     Balanced, &lt;0.1          .                 \n## ves1proc_4  Binary  0.0153     Balanced, &lt;0.1          .                 \n## ves1proc_5  Binary  0.0014     Balanced, &lt;0.1          .                 \n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1         7\n## Not Balanced, &gt;0.1     6\n## \n## Variable with the greatest mean difference\n##  Variable Diff.Un     M.Threshold.Un\n##  lifepres  0.1941 Not Balanced, &gt;0.1\n## \n## Balance tally for variance ratios\n##                  count\n## Balanced, &lt;2         2\n## Not Balanced, &gt;2     1\n## \n## Variable with the greatest variance ratio\n##  Variable V.Ratio.Un   V.Threshold.Un\n##  lifepres     0.3239 Not Balanced, &gt;2\n## \n## Sample sizes\n##     Control Treated\n## All     298     698\nDiff.Adj就是SMD",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>R语言倾向性评分：加权</span>"
    ]
  },
  {
    "objectID": "1037-psw.html#iptw",
    "href": "1037-psw.html#iptw",
    "title": "30  R语言倾向性评分：加权",
    "section": "30.2 IPTW",
    "text": "30.2 IPTW\n倾向性评分只是一个概率（倾向干预组的概率），计算概率的算法是在是太多了，选择自己喜欢的就好，我这里就用最简单的逻辑回归，之前的推文中也演示过随机森林等其他估计PS的方法。\n\npsfit &lt;- glm(abcix ~ stent + height + female + diabetic + acutemi + \n               ejecfrac + ves1proc,\n             data = lindner, family = binomial())\nps &lt;- psfit$fitted.values\n\n逆概率加权以全部研究对象（ATE）为目标人群，通过加权是每一组研究对象的协变量分布于全部研究对象相似。\n该种加权方法下，研究对象的权重为该对象所在组的概率的倒数。\n\n干预组：1/ps\n对照组：1/(1-ps)\n\n下面根据计算出的PS计算每一个样本的权重：\n\niptw &lt;- ifelse(lindner$abcix == 1, 1/ps, 1/(1-ps))\n\nlindner$iptw &lt;- iptw\n\n加权后可以再次看看数据是否已经均衡：\n\nbal.tab(covariates,treat = lindner$abcix, s.d.denom = \"pooled\",\n        weights = lindner$iptw,\n        m.threshold = 0.1, un = TRUE,\n        v.threshold = 2\n        )\n## Balance Measures\n##               Type Diff.Un V.Ratio.Un Diff.Adj        M.Threshold V.Ratio.Adj\n## lifepres   Contin.  0.1941     0.3239   0.3310 Not Balanced, &gt;0.1      0.2185\n## stent       Binary  0.1210          .   0.0036     Balanced, &lt;0.1           .\n## height     Contin. -0.0003     1.0201  -0.0175     Balanced, &lt;0.1      0.8647\n## female      Binary -0.0550          .   0.0101     Balanced, &lt;0.1           .\n## diabetic    Binary -0.0636          .  -0.0175     Balanced, &lt;0.1           .\n## acutemi     Binary  0.1187          .  -0.0028     Balanced, &lt;0.1           .\n## ejecfrac   Contin. -0.1821     1.0238  -0.0119     Balanced, &lt;0.1      0.9784\n## ves1proc_0  Binary  0.0009          .   0.0009     Balanced, &lt;0.1           .\n## ves1proc_1  Binary -0.1894          .   0.0211     Balanced, &lt;0.1           .\n## ves1proc_2  Binary  0.1360          .  -0.0073     Balanced, &lt;0.1           .\n## ves1proc_3  Binary  0.0357          .  -0.0155     Balanced, &lt;0.1           .\n## ves1proc_4  Binary  0.0153          .  -0.0002     Balanced, &lt;0.1           .\n## ves1proc_5  Binary  0.0014          .   0.0010     Balanced, &lt;0.1           .\n##                 V.Threshold\n## lifepres   Not Balanced, &gt;2\n## stent                      \n## height         Balanced, &lt;2\n## female                     \n## diabetic                   \n## acutemi                    \n## ejecfrac       Balanced, &lt;2\n## ves1proc_0                 \n## ves1proc_1                 \n## ves1proc_2                 \n## ves1proc_3                 \n## ves1proc_4                 \n## ves1proc_5                 \n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1        12\n## Not Balanced, &gt;0.1     1\n## \n## Variable with the greatest mean difference\n##  Variable Diff.Adj        M.Threshold\n##  lifepres    0.331 Not Balanced, &gt;0.1\n## \n## Balance tally for variance ratios\n##                  count\n## Balanced, &lt;2         2\n## Not Balanced, &gt;2     1\n## \n## Variable with the greatest variance ratio\n##  Variable V.Ratio.Adj      V.Threshold\n##  lifepres      0.2185 Not Balanced, &gt;2\n## \n## Effective sample sizes\n##            Control Treated\n## Unadjusted  298.    698.  \n## Adjusted    202.27  671.09\n\n可以看到除了lifepres之外，其他全都均衡了，效果还是挺不错的。加权后，干预组和对照组的样本量已经变了哦！\n如果想要画出加权后数据的基线资料表，可以借助survey包。\n\nlibrary(survey)\n\n# 获取加权后的数据\ndf &lt;- svydesign(ids = ~1, data = lindner, weights = ~ iptw)\n\n# 使用tableone中的函数创建加权后的三线表\ntab_IPTW=svyCreateTableOne(vars=covs, strata=\"abcix\",data=df ,test=T) \nprint(tab_IPTW,showAllLevels=TRUE,smd=TRUE)\n##                       Stratified by abcix\n##                        level 0               1              p      test SMD   \n##   n                          1004.38         994.47                           \n##   lifepres (mean (SD))         10.74 (3.05)   11.42 (1.43)   0.024       0.288\n##   stent (%)            0       333.7 (33.2)   326.8 (32.9)   0.921       0.008\n##                        1       670.7 (66.8)   667.7 (67.1)                    \n##   height (mean (SD))          171.60 (11.39) 171.41 (10.60)  0.849       0.017\n##   female (%)           0       668.5 (66.6)   651.8 (65.5)   0.782       0.021\n##                        1       335.9 (33.4)   342.7 (34.5)                    \n##   diabetic (%)         0       762.7 (75.9)   772.6 (77.7)   0.610       0.042\n##                        1       241.7 (24.1)   221.8 (22.3)                    \n##   acutemi (%)          0       857.6 (85.4)   851.8 (85.7)   0.942       0.008\n##                        1       146.8 (14.6)   142.6 (14.3)                    \n##   ejecfrac (mean (SD))         51.07 (10.23)  50.95 (10.12)  0.879       0.012\n##   ves1proc (%)         0         3.0 ( 0.3)     3.9 ( 0.4)   0.937       0.088\n##                        1       664.1 (66.1)   678.6 (68.2)                    \n##                        2       261.6 (26.0)   251.7 (25.3)                    \n##                        3        61.3 ( 6.1)    45.3 ( 4.6)                    \n##                        4        14.4 ( 1.4)    14.0 ( 1.4)                    \n##                        5         0.0 ( 0.0)     1.0 ( 0.1)\n\n加权之后，就可以做各种分析了，比如回归分析等，分析时把权重因素也考虑进去即可。\n这里演示逻辑回归，根据因变量的类型，可选择不同的回归方法。\n\nf &lt;- glm(sixMonthSurvive~abcix+stent+height+female+diabetic+acutemi+\n           ejecfrac+ves1proc,\n             data = lindner, family = binomial(),\n         weights = iptw # 把权重加进去\n         )\n## Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nsummary(f)\n## \n## Call:\n## glm(formula = sixMonthSurvive ~ abcix + stent + height + female + \n##     diabetic + acutemi + ejecfrac + ves1proc, family = binomial(), \n##     data = lindner, weights = iptw)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  8.598e+00  1.585e+03   0.005  0.99567    \n## abcix1       1.785e+00  3.215e-01   5.551 2.84e-08 ***\n## stent1      -6.401e-01  3.024e-01  -2.117  0.03426 *  \n## height       3.491e-02  1.154e-02   3.025  0.00248 ** \n## female1      8.123e-03  3.143e-01   0.026  0.97938    \n## diabetic1   -7.314e-01  2.813e-01  -2.599  0.00934 ** \n## acutemi1    -1.540e+00  3.011e-01  -5.116 3.11e-07 ***\n## ejecfrac     5.923e-02  1.057e-02   5.604 2.10e-08 ***\n## ves1proc1   -1.378e+01  1.585e+03  -0.009  0.99306    \n## ves1proc2   -1.184e+01  1.585e+03  -0.007  0.99404    \n## ves1proc3   -1.569e+01  1.585e+03  -0.010  0.99210    \n## ves1proc4   -7.180e-02  1.787e+03   0.000  0.99997    \n## ves1proc5   -1.817e+00  4.262e+03   0.000  0.99966    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 732.98  on 995  degrees of freedom\n## Residual deviance: 469.49  on 983  degrees of freedom\n## AIC: 485.36\n## \n## Number of Fisher Scoring iterations: 16\n\n\n但是这种方法存在问题，我在stackoverflow中的帖子中看到有人指出，R自带的lm和glm中的weights参数并不是样本的权重，这点可以查看帮助文档确定，所以如果想要使用加权后的数据进行线性回归和逻辑回归，需要使用其他的R包，比如survey包。\n\n除了上面介绍的手动计算权重的方法，也可以通过多个R包实现，比如PSW/PSweight/twang等，大家感兴趣的可以自己查看相关说明。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>R语言倾向性评分：加权</span>"
    ]
  },
  {
    "objectID": "1037-psw.html#重叠加权",
    "href": "1037-psw.html#重叠加权",
    "title": "30  R语言倾向性评分：加权",
    "section": "30.3 重叠加权",
    "text": "30.3 重叠加权\n重叠加权的目标人群是两组协变量相似的人，即PS值分布重叠的人，其估计的效应为重叠人群平均处理效应（ATO）。\n\n干预组：1-ps\n对照组：ps\n\n重叠加权的优缺点可以看这篇文章：最强的倾向性评分方法—重叠加权\n使用PSweight包演示重叠加权，这个包不仅可以用于二分类，还可以用于多分类。\n还是使用lindner这个数据集。\n\n## 数据准备\nrm(list = ls())\ndata(lindner, package = \"twang\") \n\n# 构建估计PS的formula\nformula.ps &lt;- abcix ~ stent + height + female + diabetic + acutemi + ejecfrac + ves1proc\n\n进行重叠加权：\n\nlibrary(PSweight)\n\nPSweight &lt;- PSweight(ps.formula = formula.ps, data = lindner, \n                     weight = \"overlap\", # 重叠加权\n                     yname = \"cardbill\", # 因变量\n                     family = \"gaussian\", \n                     ps.method = \"glm\", \n                     out.method = \"glm\"\n                     )\n\n#返回结果，效应估计及其标准误、置信区间、P值\nsummary(PSweight)\n## \n## Closed-form inference: \n## \n## Original group value:  0, 1 \n## \n## Contrast: \n##             0 1\n## Contrast 1 -1 1\n## \n##             Estimate Std.Error       lwr    upr  Pr(&gt;|z|)    \n## Contrast 1 1134.9079    1.3253 1132.3103 1137.5 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n计算数据均衡性：\n\nSumStat&lt;-SumStat(ps.formula = formula.ps, data = lindner, weight = \"overlap\")\nSumStat[[\"ess\"]] #有效样本量\n##   unweighted  overlap\n## 0        298 287.4367\n## 1        698 569.5826\nplot(SumStat) #均衡性检验图形\n\n\n\n\n\n\n\nsummary(SumStat) #均衡性检验\n## unweighted result\n##           Mean 0  Mean 1   SMD\n## stent      0.584   0.705 0.254\n## height   171.446 171.443 0.000\n## female     0.386   0.331 0.115\n## diabetic   0.268   0.205 0.150\n## acutemi    0.060   0.179 0.371\n## ejecfrac  52.289  50.403 0.182\n## ves1proc   1.205   1.463 0.427\n## \n## overlap result\n##           Mean 0  Mean 1 SMD\n## stent      0.633   0.633   0\n## height   171.464 171.464   0\n## female     0.359   0.359   0\n## diabetic   0.242   0.242   0\n## acutemi    0.078   0.078   0\n## ejecfrac  51.830  51.830   0\n## ves1proc   1.257   1.257   0\n\n加权后，可以进行后续的各种分析，这里就不演示了。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>R语言倾向性评分：加权</span>"
    ]
  },
  {
    "objectID": "1037-psw.html#参考资料",
    "href": "1037-psw.html#参考资料",
    "title": "30  R语言倾向性评分：加权",
    "section": "30.4 参考资料",
    "text": "30.4 参考资料\n涂博祥, 秦婴逸, 吴骋, 等. 倾向性评分加权方法介绍及R软件实现[J]. 中国循证医学杂志, 2022, 22(3): 365–372.",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>R语言倾向性评分：加权</span>"
    ]
  },
  {
    "objectID": "1038-p4trend.html",
    "href": "1038-p4trend.html",
    "title": "31  p-for-trend/ p-for-interaction/ per-1-sd R语言实现",
    "section": "",
    "text": "31.1 P for trend\nP for trend是线性趋势检验的P值，用于反映自变量和因变量是否存在线性趋势关系。线性趋势检验，之前介绍过Cochran Armitage检验，不过是针对分类变量的。\n今天要介绍的P for trend主要是针对连续型变量的。\n关于p for trend具体含义和数值型变量分箱的方法，大家可以参考医咖会的文章：p for trend是个啥\n把连续性变量转换为分类变量(在R里转变为因子)，设置哑变量，进行回归分析，即可得到OR值及95%的可信区间；把转换好的分类变量当做数值型，进行回归分析，即可得到P for trend\n使用之前逻辑回归的例子演示，来自孙振球版医学统计学第4版，电子版和配套数据均放在QQ群文件中，需要的加群下载即可。\ndf16_2 &lt;- foreign::read.spss(\"datasets/例16-02.sav\", \n                             to.data.frame = T,\n                             use.value.labels = F,\n                             reencode  = \"utf-8\")\n\nstr(df16_2)\n## 'data.frame':    54 obs. of  11 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : num  3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : num  1 0 1 0 0 0 0 0 0 0 ...\n##  $ x3   : num  0 1 0 0 0 1 1 1 0 0 ...\n##  $ x4   : num  1 1 1 1 1 1 0 1 0 1 ...\n##  $ x5   : num  0 0 0 0 0 0 0 1 0 0 ...\n##  $ x6   : num  0 0 0 0 1 0 0 0 0 0 ...\n##  $ x7   : num  1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : num  1 0 0 0 1 1 0 0 1 0 ...\n##  $ y    : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n数据一共11列，第1列是编号，第2-9列是自变量，第10列是因变量。\n具体说明： - x1：年龄，小于45岁是1,45-55是2,55-65是3,65以上是4； - x2：高血压病史，1代表有，0代表无； - x3：高血压家族史，1代表有，0代表无； - x4：吸烟，1代表吸烟，0代表不吸烟； - x5：高血脂病史，1代表有，0代表无； - x6：动物脂肪摄入，0表示低，1表示高 - x7：BMI，小于24是1,24-26是2，大于26是3； - x8：A型性格，1代表是，0代表否； - y：是否是冠心病，1代表是，0代表否\n这里的x1~y虽然是数值型，但并不是真的代表数字大小，只是为了方便标识，\n年龄x1应该是数值型的，但是为了方便解释逻辑回归的意义，我们对它进行了分箱处理，也就是把它转换为了分类变量。数值型变量进行分箱，是回归分析中计算p for trend的第一步\n此时x1是数值型，我们直接进行逻辑回归，得到的P值就是 p for trend\nf &lt;- glm(y ~ x1 + x2, \n         data = df16_2, \n         family = binomial())\n\nbroom::tidy(f)\n## # A tibble: 3 × 5\n##   term        estimate std.error statistic p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 (Intercept)   -2.22      1.03      -2.15  0.0313\n## 2 x1             0.712     0.423      1.68  0.0928\n## 3 x2             1.08      0.625      1.73  0.0840\n0.09279918就是x1的p for trend，而且还是校正了x2这个变量之后的p for trend，是不是很简单？\n此时如果我们把x1变成因子型，那在进行回归分析时会自动进行哑变量编码，就可以得到几个组的OR值和95%的可信区间，关于R语言中分类变量进行回归分析时常用的一些编码方法，强烈你看一下这篇推文：R语言分类变量进行回归分析的编码方案。\n# 变为因子型\ndf16_2$x1.f &lt;- factor(df16_2$x1)\n\n# 把因子放入自变量\nf &lt;- glm(y ~ x1.f + x2, \n         data = df16_2, \n         family = binomial())\nbroom::tidy(f,conf.int=T,exponentiate=T)\n## # A tibble: 5 × 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    0.200     1.10     -1.47   0.142    0.0104      1.24\n## 2 x1.f2          2.32      1.19      0.704  0.481    0.289      49.3 \n## 3 x1.f3          4.48      1.26      1.19   0.233    0.485     102.  \n## 4 x1.f4          9.42      1.63      1.38   0.169    0.508     438.  \n## 5 x2             2.94      0.639     1.69   0.0918   0.854      10.7\n这样就得到了x1.f中4/3/2分别和1进行比较的OR值和95%的可信区间。当然你写函数提取也行：\n# OR值\nexp(coef(f))\n## (Intercept)       x1.f2       x1.f3       x1.f4          x2 \n##    0.200000    2.319343    4.476753    9.415697    2.936212\n\n# OR值的95%的可信区间\nexp(confint(f))\n##                  2.5 %     97.5 %\n## (Intercept) 0.01043892   1.240092\n## x1.f2       0.28932733  49.284803\n## x1.f3       0.48497992 102.335996\n## x1.f4       0.50766137 437.541812\n## x2          0.85353009  10.723068\n这样就得到了每个组的OR值和95%的可信区间，可以看到没有第1组的，因为第一组是参考，所有组都是和第一组进行比较。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>p-for-trend/ p-for-interaction/ per-1-sd R语言实现</span>"
    ]
  },
  {
    "objectID": "1038-p4trend.html#p-for-interaction",
    "href": "1038-p4trend.html#p-for-interaction",
    "title": "31  p-for-trend/ p-for-interaction/ per-1-sd R语言实现",
    "section": "31.2 p for interaction",
    "text": "31.2 p for interaction\np for interaction是交互作用的P值，关于其含义可以参考松哥统计的这篇文章：p for interaction是什么\n\n目前计算P for interaction两种方法： 1. 对于数值与等级或二分类，可以直接模型中增加相乘项【如x1×X2】，然后看交互项有无意义。 2. 而对于多项分类【如血型】，产生哑变量后，相乘则会产生多个交互项，此时不能整体判断交互作用是否有意义。我们可以先构建一个无交互作用项的模型，再构建一个有交互作用项的模型。然后采用似然比检验（likelihood ratio test）进行比较有个模型差异，则可以判定交互项整体是否有意义。\n\n\n31.2.1 方法1\n假如探索年龄(x1)和BMI(x7)之间对因变量y有没有交互作用，我们首先新建一列相乘列，然后进行回归分析。\n\n# 新建1列\ndf16_2$x17 &lt;- df16_2$x1 * df16_2$x7\nstr(df16_2)\n## 'data.frame':    54 obs. of  13 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : num  3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : num  1 0 1 0 0 0 0 0 0 0 ...\n##  $ x3   : num  0 1 0 0 0 1 1 1 0 0 ...\n##  $ x4   : num  1 1 1 1 1 1 0 1 0 1 ...\n##  $ x5   : num  0 0 0 0 0 0 0 1 0 0 ...\n##  $ x6   : num  0 0 0 0 1 0 0 0 0 0 ...\n##  $ x7   : num  1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : num  1 0 0 0 1 1 0 0 1 0 ...\n##  $ y    : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  $ x1.f : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 3 2 2 2 3 3 2 3 2 1 ...\n##  $ x17  : num  3 2 2 2 3 6 2 3 2 1 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n\n\n# 进行逻辑回归\nf &lt;- glm(y ~ x1 + x7 + x17, \n         family = binomial(),\n         data = df16_2\n         )\nsummary(f)\n## \n## Call:\n## glm(formula = y ~ x1 + x7 + x17, family = binomial(), data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)\n## (Intercept)  -0.3953     2.6477  -0.149    0.881\n## x1           -0.4867     1.1142  -0.437    0.662\n## x7           -1.1509     1.7095  -0.673    0.501\n## x17           0.9249     0.7489   1.235    0.217\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 62.508  on 50  degrees of freedom\n## AIC: 70.508\n## \n## Number of Fisher Scoring iterations: 5\n\n结果中显示x17的P值(p for interaction)是：0.217，交互作用项是没有统计学意义的。\n\n\n31.2.2 方法2\n\n# 先构建一个没有交互项的逻辑回归模型\nf1 &lt;- glm(y ~ x1 + x7, \n         family = binomial(),\n         data = df16_2)\n\n# 再构建一个有交互作用的逻辑回归模型\nf2 &lt;- glm(y ~ x1 + x7 + x17, \n         family = binomial(),\n         data = df16_2)\n\n# 似然比检验\nlmtest::lrtest(f1,f2)\n## Likelihood ratio test\n## \n## Model 1: y ~ x1 + x7\n## Model 2: y ~ x1 + x7 + x17\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n## 1   3 -32.216                     \n## 2   4 -31.254  1 1.9238     0.1654\n\n结果显示P(p for interaction)=0.1654，也就是交互作用项没有统计学意义。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>p-for-trend/ p-for-interaction/ per-1-sd R语言实现</span>"
    ]
  },
  {
    "objectID": "1038-p4trend.html#per-1-sd",
    "href": "1038-p4trend.html#per-1-sd",
    "title": "31  p-for-trend/ p-for-interaction/ per-1-sd R语言实现",
    "section": "31.3 per 1 sd",
    "text": "31.3 per 1 sd\n关于什么是per 1 sd，可以参考松哥统计的这篇文章：per 1 sd\n\nPer 1 sd的实现，其实就是把原始数据进行标准化，另存为一个新的变量X，新变量X因为是被标准化后的数据，因此其均数和标准差为0和1。然后让x进入模型进行分析。请问大家此时x每增加1个单位，效应量增加的风险为HR。因为标准差为1，此时x增加1个单位，就是Per 1 sd。1=Per 1 sd。就是自变量每增加1个标准差。\n\n为了方便演示，我们新建一列数据weight，然后进行标准化，再进行逻辑回归。\n\n# 新建一列weight\ndf16_2$weight &lt;- rnorm(54, 70,11)\n\n# 进行标准化\ndf16_2$weight.scaled &lt;- scale(df16_2$weight)\n\n# 进行逻辑回归\nf &lt;- glm(y ~ weight.scaled, data = df16_2)\nbroom::tidy(f,conf.int=T,exponentiate=T)\n## # A tibble: 2 × 7\n##   term          estimate std.error statistic       p.value conf.low conf.high\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)      1.62     0.0692     6.96  0.00000000579    1.41       1.85\n## 2 weight.scaled    0.975    0.0699    -0.359 0.721            0.850      1.12\n\n结果给出了P值，OR值以及95%的可信区间。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>p-for-trend/ p-for-interaction/ per-1-sd R语言实现</span>"
    ]
  },
  {
    "objectID": "1039-nonlinear.html",
    "href": "1039-nonlinear.html",
    "title": "32  R语言多项式拟合",
    "section": "",
    "text": "32.1 多项式拟合\n我们用car包里面的USPop数据集进行演示。这个数据集一共两列，一列是年份，另一列是美国每一年的人口数量，数据一共22行。\n# 加载数据\nlibrary(car)\ndata(\"USPop\")\npsych::headTail(USPop)\n##     year population\n## 1   1790       3.93\n## 2   1800       5.31\n## 3   1810       7.24\n## 4   1820       9.64\n## ...  ...        ...\n## 19  1970      203.3\n## 20  1980     226.54\n## 21  1990     248.71\n## 22  2000     281.42\n我们首先画图看看两列数据的情况：\nplot(population ~ year, data = USPop)\n这个数据很明显是曲线的形状，并不是一条直线，所以此时我们直接用线性回归（直线）拟合这样的数据是不合适的。不信我们可以画图看看。\n# 拟合线性回归\nf &lt;- lm(population ~ year, data = USPop)\n\n# 画出原来的数据\nplot(population ~ year, data = USPop)\n\n# 添加拟合线\nlines(USPop$year, fitted(f), col = \"blue\")\n图中这条蓝色的线就是线性拟合的线，很明显，对数据的拟合很差。\n那我们应该用什么方法拟合这个关系呢？\n根据之前的两篇推文，拟合非线性关系有非常多的方法，至少有3种：\n我们这里先介绍多项式回归。\n多项式回归非常简单，就是个高中学过的高次方程的曲线。\n现在我们先拟合一个二次项的多项式回归：\n# 2次项，注意用法\nf1 &lt;- lm(population ~ year + I(year^2), data = USPop)\n\n# 画出拟合线\nplot(population ~ year, data = USPop)\nlines(USPop$year, fitted(f1))\n结果拟合很好，二次项就已经拟合效果非常好了，如果你还想看一下更高次项拟合，可以继续试试，比如3次项：\n# 3次项，注意用法\nf2 &lt;- lm(population ~ year + I(year^2) + I(year^3), data = USPop)\n\n# 画出拟合线\nplot(population ~ year, data = USPop)\nlines(USPop$year, fitted(f2))\n结果可见增加了一个3次项，结果并没有好很多。所以我们可以就选2次项即可。\n当然也有一些统计方法可以检验，加了2次项、3次项之后是不是有统计学意义，可以用似然比检验，比如anova：\n# 线性回归和2次项比较\nanova(f, f1)\n## Analysis of Variance Table\n## \n## Model 1: population ~ year\n## Model 2: population ~ year + I(year^2)\n##   Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n## 1     20 12819.0                                  \n## 2     19   170.7  1     12648 1408.1 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2次项和3次项比较\nanova(f1, f2)\n## Analysis of Variance Table\n## \n## Model 1: population ~ year + I(year^2)\n## Model 2: population ~ year + I(year^2) + I(year^3)\n##   Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n## 1     19 170.66                              \n## 2     18 143.64  1    27.027 3.3868 0.08227 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n结果很明显，加入2次项之后，P值是小于0.05的，说明是有统计学意义的，但是2次项和3次项比较，就没有统计学意义了，说明我们只要用到2次项即可。\n在写论文的时候应该如何描述这些方法呢？请多看文献，这不在本文的讨论范围。\n为了加深理解，下面再给大家举一个例子。\n首先是构造一个数据，构造数据的过程不需要看。\nx &lt;- 1:100         \nk &lt;- c(25, 50, 75) \nu &lt;- function(x)ifelse(x &gt; 0, x, 0)\nx2 &lt;- u(x - k[1])\nx3 &lt;- u(x - k[2])\nx4 &lt;- u(x - k[3])\nset.seed(1)\ny &lt;- 0.8 + 1*x + -1.2*x2 + 1.4*x3 + -1.6*x4 + rnorm(100,sd = 2.2)\nplot(x, y)\n这样的一个数据，很明显也不是线性的，所以此时线性回归肯定不合适。我们尝试用多项式回归来拟合这个数据。\n这个数据，我已经帮大家试好了，需要拟合6次项才会比较完美。\n# 拟合6次项\nf.6 &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))\n\n# 画出拟合线\nplot(x,y)\nlines(x, fitted(f.6))\n可以看到拟合线比较贴合数据。但是在拟合线的开头和末尾可以发现有点上翘的趋势，这也是多项式拟合的缺点，如果此时在两头多点数据，可能拟合效果就不是很好了。解决方法也很简单，就是我们下次要介绍的样条回归。\n多项式回归的公式写法像上面这样略显复杂，如果是更高次的项，岂不是更复杂？当然是有简便写法的。可以使用poly()函数。\n# 多项式拟合的简便写法，拟合6次项，和上面结果完全一样\nf.6 &lt;- lm(y ~ poly(x, 6))\n\n# 画出拟合线\nplot(x,y)\nlines(x, fitted(f.6))\n可以看到使用poly()函数极大的简化了公式写法，而且很好理解，后面的数字就代表了次方。看到这里，不知道你有没有想起重复测量数据的多重比较中用过的正交多项式呢？没有印象的赶紧去复习下：重复测量数据的多重比较\n这样的拟合线，当然也是可以用ggplot2画的。\nlibrary(ggplot2)\n\nggplot()+\n  geom_point(aes(x,y),size=2)+\n  geom_line(aes(x, fitted(f.6)), color=\"red\",size=2)+\n  theme_bw()\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n或者下面这样，好理解，还可以添加可信区间：\ndf.tmp &lt;- data.frame(x = x, y= y)\n\nggplot(df.tmp, aes(x,y))+\n  geom_point(size=2)+\n  geom_smooth(method = \"lm\",\n              formula = y ~ poly(x,6),\n              color=\"red\",\n              size=2,\n              se = T, # 可信区间\n              )+\n  theme_bw()\n最后一个问题：多项式能用于逻辑回归吗？Cox回归呢？\n当然可以了，只是把自变量变成多次项而已，和lm用法一模一样，函数使用glm()/coxph()等即可！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>R语言多项式拟合</span>"
    ]
  },
  {
    "objectID": "1039-nonlinear.html#多项式拟合",
    "href": "1039-nonlinear.html#多项式拟合",
    "title": "32  R语言多项式拟合",
    "section": "",
    "text": "多项式回归\n分段回归\n样条回归",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>R语言多项式拟合</span>"
    ]
  },
  {
    "objectID": "1040-rcs.html",
    "href": "1040-rcs.html",
    "title": "33  R语言样条回归",
    "section": "",
    "text": "33.0.1 线性的立方样条\n演示所用数据还是用上一篇的数据：\nrm(list = ls())\nx &lt;- 1:100         \nk &lt;- c(25, 50, 75) \nu &lt;- function(x)ifelse(x &gt; 0, x, 0)\nx2 &lt;- u(x - k[1])\nx3 &lt;- u(x - k[2])\nx4 &lt;- u(x - k[3])\nset.seed(1)\ny &lt;- 0.8 + 1*x + -1.2*x2 + 1.4*x3 + -1.6*x4 + rnorm(100,sd = 2.2)\nplot(x, y)\n假设我们有这样一个数据，很明显这不是一个直线的关系，这时候再用直线回归就不适合了。但是为了演示，这里还是给大家用直线回归方法拟合一下看看效果。\nf &lt;- lm(y ~ x)\nplot(x, y)\nlines(x, fitted(f),col=\"red\")\n很明显，直线回归是不可能有很好的拟合效果的。这时候我们应该用什么方法拟合这个关系呢？\n根据之前的3篇推文，拟合非线性关系有非常多的方法，至少有3种：\n多项式回归在上一篇推文中已经介绍过了，效果不错，但是有一个小小的缺点，在数据两端有上翘趋势，具体可参考之前的推文。\n今天给大家演示限制性立方样条回归。做限制性立方样条回归的R包很多，这里以rms为例，以后有机会再介绍其他R包，比如splines。\nrms做限制性立方样条回归很简单，对需要使用的变量使用rcs()函数即可。\n# 加载R包\nlibrary(rms)\nlibrary(ggplot2)\n# 拟合限制性立方样条，这里对变量x使用，跟多项式回归差不多\nf &lt;- lm(y ~ rcs(x,5))\n\n# 画出原数据\nplot(x,y)\nlines(x, fitted(f),col=\"red\") # 画出拟合线\n可以看到，拟合结果非常完美，甚至比我们的6次多项式拟合还要好一点！\n下面解释下上述代码中的意思。rcs是我们的立方样条函数，其中的数字5表示我们要用5个节点（不理解这里的节点啥意思的请去看开头的两篇推文）。\n默认节点是4，一般建议选3-6个左右，可以分别试试效果，选择拟合较好的那一个，参考文献是这篇：F. Harrell. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis.\n我们可以自己指定，比如根据分位数、中位数、平均数等，都可以作为我们的节点。\n下面用ggplot2画图。\ndf.tmp &lt;- data.frame(x=x,y=y)\n\nggplot(df.tmp, aes(x,y))+\n  geom_point(size=2)+\n  geom_smooth(method = \"lm\",\n              formula = y ~ rcs(x,5),\n              se = T,\n              color=\"red\"\n              )+\n  theme_bw()\n此方法同样也是适用于logistic回归和cox回归的，建议使用rms包中的lrm函数和cph进行拟合。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>R语言样条回归</span>"
    ]
  },
  {
    "objectID": "1040-rcs.html#推荐阅读",
    "href": "1040-rcs.html#推荐阅读",
    "title": "33  R语言样条回归",
    "section": "33.1 推荐阅读",
    "text": "33.1 推荐阅读\n聂博士的10+篇高质量RCS合集：RCS系列合集",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>R语言样条回归</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html",
    "href": "1041-subgroupanalysis.html",
    "title": "34  R语言亚组分析及森林图绘制",
    "section": "",
    "text": "34.1 准备数据\n使用survival包中的colon数据集用于演示，这是一份关于结肠癌患者的生存数据，共有1858行，16列，共分为3个组，1个观察组+2个治疗组，观察他们发生终点事件的差异。\n各变量的解释如下：\nrm(list = ls())\nlibrary(survival)\n\nstr(colon)\n## 'data.frame':    1858 obs. of  16 variables:\n##  $ id      : num  1 1 2 2 3 3 4 4 5 5 ...\n##  $ study   : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ rx      : Factor w/ 3 levels \"Obs\",\"Lev\",\"Lev+5FU\": 3 3 3 3 1 1 3 3 1 1 ...\n##  $ sex     : num  1 1 1 1 0 0 0 0 1 1 ...\n##  $ age     : num  43 43 63 63 71 71 66 66 69 69 ...\n##  $ obstruct: num  0 0 0 0 0 0 1 1 0 0 ...\n##  $ perfor  : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ adhere  : num  0 0 0 0 1 1 0 0 0 0 ...\n##  $ nodes   : num  5 5 1 1 7 7 6 6 22 22 ...\n##  $ status  : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ differ  : num  2 2 2 2 2 2 2 2 2 2 ...\n##  $ extent  : num  3 3 3 3 2 2 3 3 3 3 ...\n##  $ surg    : num  0 0 0 0 0 0 1 1 1 1 ...\n##  $ node4   : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ time    : num  1521 968 3087 3087 963 ...\n##  $ etype   : num  2 1 2 1 2 1 2 1 2 1 ...\n可以使用cox回归探索危险因素。分类变量需要变为因子型，这样在进行回归时会自动进行哑变量设置。\n为了演示，我们只选择Obs组和Lev+5FU组的患者，所有的分类变量都变为factor，把年龄也变为分类变量并变成factor。\nsuppressMessages(library(tidyverse))\n\ndf &lt;- colon %&gt;% \n  mutate(rx=as.numeric(rx)) %&gt;% \n  filter(etype == 1, !rx == 2) %&gt;%  #rx %in% c(\"Obs\",\"Lev+5FU\"), \n  select(time, status,rx, sex, age,obstruct,perfor,adhere,differ,extent,surg,node4) %&gt;% \n  mutate(sex=factor(sex, levels=c(0,1),labels=c(\"female\",\"male\")),\n         age=ifelse(age &gt;65,\"&gt;65\",\"&lt;=65\"),\n         age=factor(age, levels=c(\"&gt;65\",\"&lt;=65\")),\n         obstruct=factor(obstruct, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         perfor=factor(perfor, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         adhere=factor(adhere, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         differ=factor(differ, levels=c(1,2,3),labels=c(\"well\",\"moderate\",\"poor\")),\n         extent=factor(extent, levels=c(1,2,3,4),\n                       labels=c(\"submucosa\",\"muscle\",\"serosa\",\"contiguous\")),\n         surg=factor(surg, levels=c(0,1),labels=c(\"short\",\"long\")),\n         node4=factor(node4, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         rx=ifelse(rx==3,0,1),\n         rx=factor(rx,levels=c(0,1))\n         )\n\nstr(df)\n## 'data.frame':    619 obs. of  12 variables:\n##  $ time    : num  968 3087 542 245 523 ...\n##  $ status  : num  1 0 1 1 1 1 0 0 0 1 ...\n##  $ rx      : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 2 1 1 2 ...\n##  $ sex     : Factor w/ 2 levels \"female\",\"male\": 2 2 1 1 2 1 2 1 2 2 ...\n##  $ age     : Factor w/ 2 levels \"&gt;65\",\"&lt;=65\": 2 2 1 1 1 2 2 1 2 2 ...\n##  $ obstruct: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n##  $ perfor  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ adhere  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 1 1 1 1 1 ...\n##  $ differ  : Factor w/ 3 levels \"well\",\"moderate\",..: 2 2 2 2 2 2 2 2 3 2 ...\n##  $ extent  : Factor w/ 4 levels \"submucosa\",\"muscle\",..: 3 3 2 3 3 3 3 3 3 3 ...\n##  $ surg    : Factor w/ 2 levels \"short\",\"long\": 1 1 1 2 2 1 1 2 2 1 ...\n##  $ node4   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 1 1 1 1 ...",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>R语言亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html#准备数据",
    "href": "1041-subgroupanalysis.html#准备数据",
    "title": "34  R语言亚组分析及森林图绘制",
    "section": "",
    "text": "id：患者id\nstudy：没啥用，所有患者都是1\nrx：治疗方法，共3种，Obs(观察组), Lev(左旋咪唑), Lev+5FU(左旋咪唑+5-FU)\nsex：性别，1是男性\nage：年龄\nobstruct：肠梗阻，1是有\nperfor：肠穿孔，1是有\nadhere：和附近器官粘连，1是有\nnodes：转移的淋巴结数量\nstatus：生存状态，0代表删失，1代表发生终点事件\ndiffer：肿瘤分化程度，1-well,2-moderate,3-poor\nextent：局部扩散情况，1-submucosa，2-muscle，3-serosa，4-contiguous structures\nsurg：手术后多久了，1-long,2-short\nnode4：是否有超过4个阳性淋巴结，1代表是\ntime：生存时间\netype：终点事件类型，1-复发，2-死亡",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>R语言亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html#不分亚组的分析",
    "href": "1041-subgroupanalysis.html#不分亚组的分析",
    "title": "34  R语言亚组分析及森林图绘制",
    "section": "34.2 不分亚组的分析",
    "text": "34.2 不分亚组的分析\n直接使用所有数据，拟合多因素Cox回归模型：\n\nfit &lt;- coxph(Surv(time, status) ~ rx, data = df)\nbroom::tidy(fit,exponentiate = T,conf.int = T)\n## # A tibble: 1 × 7\n##   term  estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 rx1       1.67     0.119      4.32 0.0000156     1.32      2.11\n\n这个结果我们在之前说过无数遍了，各项意义就不做具体解释了。\n通过这个结果可以看出，Lev+5FU组是明显好于Obs组的，那么问题来了。\n有一个著名的东西叫辛普森悖论，这个方法对所有人有效，但是把这个方法单独对男人或女人使用，就没效了！\n这就是由于性别这个混杂因素导致的，控制混杂因素的方法，我们在医学统计系列推文中说过至少3种，今天就给大家演示最好理解的亚组分析。\n思路其实很简单，单独在男性患者中拟合模型看看结果是不是和所有患者的结果一样；然后单独在女性患者中也拟合模型。\n对于其他的分类变量，都是一样的操作。\n所以我说这个方法最简单，没有什么高深的数学理论，只是操作起来比较复杂，因为需要在每个分类变量的每个亚组中分别拟合模型。\n刚开始我是想通过嵌套for循环实现的，但是有点费脑子，所以我给大家演示下tidyverse的做法，后期会考虑再写个R包，实现这个功能。\n其实我已经找到了一个R包Publish可以实现回归分析的亚组分析，但是它的方法是错误的。。。\n通常最笨的方法也是最靠谱的方法，如果你实在不会，也可以手动实现这个过程，就以sex为例，先在male中拟合模型：\n\nfit0 &lt;- coxph(Surv(time, status) ~ rx, data = df[df$sex == \"male\",])\nbroom::tidy(fit0,exponentiate = T,conf.int = T)\n## # A tibble: 1 × 7\n##   term  estimate std.error statistic    p.value conf.low conf.high\n##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 rx1       2.29     0.181      4.57 0.00000495     1.60      3.26\n\n然后在female中拟合模型：\n\nfit0 &lt;- coxph(Surv(time, status) ~ rx, data = df[df$sex == \"female\",])\nbroom::tidy(fit0,exponentiate = T,conf.int = T)\n## # A tibble: 1 × 7\n##   term  estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 rx1       1.32     0.161      1.71  0.0878    0.960      1.80\n\n就这样不断的重复即可，然后把数据手动摘录一下。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>R语言亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html#亚组分析",
    "href": "1041-subgroupanalysis.html#亚组分析",
    "title": "34  R语言亚组分析及森林图绘制",
    "section": "34.3 亚组分析",
    "text": "34.3 亚组分析\n对于先分组，再做某事这种分析思路，tidyverse天生就比较擅长。\n以下是tidyverse实现方法，借助purrr。\n首先把数据变为长数据，经典的长宽转换：\n\ndfl &lt;- df %&gt;% \n  pivot_longer(cols = 4:ncol(.),names_to = \"var\",values_to = \"value\") %&gt;% \n  arrange(var)\n\nhead(dfl)\n## # A tibble: 6 × 5\n##    time status rx    var    value\n##   &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;  &lt;fct&gt;\n## 1   968      1 0     adhere No   \n## 2  3087      0 0     adhere No   \n## 3   542      1 1     adhere Yes  \n## 4   245      1 0     adhere No   \n## 5   523      1 1     adhere No   \n## 6   904      1 0     adhere No\n\n根据rx（治疗方式）和var（需要分亚组的变量）分组，分别在每个组内拟合cox回归，并提取结果，一气呵成，这个操作我们在之前的倾向性评分分层中也演示过：倾向性评分回归和分层\n\nress &lt;- dfl %&gt;% \n  #group_by(var,value) %&gt;% \n  group_nest(var,value) %&gt;% \n  drop_na(value) %&gt;% \n  mutate(#sample_size=map(data, ~ nrow(.x)),\n         model=map(data, ~ coxph(Surv(time, status) ~ rx,data = .x)),\n         res = map(model, broom::tidy,conf.int = T, exponentiate = T)\n         ) %&gt;% \n  dplyr::select(var,value,res)\n## Warning: There was 1 warning in `mutate()`.\n## ℹ In argument: `model = map(data, ~coxph(Surv(time, status) ~ rx, data = .x))`.\n## Caused by warning in `coxph.fit()`:\n## ! Loglik converged before variable  1 ; coefficient may be infinite.\n\nglimpse(ress)\n## Rows: 21\n## Columns: 3\n## $ var   &lt;chr&gt; \"adhere\", \"adhere\", \"age\", \"age\", \"differ\", \"differ\", \"differ\", …\n## $ value &lt;fct&gt; No, Yes, &gt;65, &lt;=65, well, moderate, poor, submucosa, muscle, ser…\n## $ res   &lt;list&gt; [&lt;tbl_df[1 x 7]&gt;], [&lt;tbl_df[1 x 7]&gt;], [&lt;tbl_df[1 x 7]&gt;], [&lt;tbl_…\n\nres是列表列，其中每个元素就是我们的结果。\n顺便把每个亚组中每种治疗方式的人数也一起计算出来：\n\nss &lt;- dfl %&gt;% \n  group_by(var,value,rx) %&gt;% \n  drop_na(value) %&gt;% \n  summarise(sample_size=n()) %&gt;% \n  dplyr::select(var,value,rx,sample_size)\n\n然后把两个结果合并到一起：\n\nresss &lt;- ress %&gt;% \n  left_join(ss,b=c(\"var\",\"value\")) %&gt;% \n  unnest(res,rx,sample_size) %&gt;% \n  pivot_wider(names_from = \"rx\",values_from = \"sample_size\",names_prefix = \"rx_\") %&gt;% \n  select(-c(term,std.error,statistic)) %&gt;% \n  mutate(across(where(is.numeric), round,digits=2)) %&gt;% \n  mutate(`HR(95%CI)`=paste0(estimate,\"(\",conf.low,\"-\",conf.high,\")\"))\n## Warning: `unnest()` has a new interface. See `?unnest` for details.\n## ℹ Try `df %&gt;% unnest(c(res, rx, sample_size))`, with `mutate()` if needed.\n## Warning: There was 1 warning in `mutate()`.\n## ℹ In argument: `across(where(is.numeric), round, digits = 2)`.\n## Caused by warning:\n## ! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\n## Supply arguments directly to `.fns` through an anonymous function instead.\n## \n##   # Previously\n##   across(a:b, mean, na.rm = TRUE)\n## \n##   # Now\n##   across(a:b, \\(x) mean(x, na.rm = TRUE))\n\nstr(resss)\n## tibble [21 × 9] (S3: tbl_df/tbl/data.frame)\n##  $ var      : chr [1:21] \"adhere\" \"adhere\" \"age\" \"age\" ...\n##  $ value    : Factor w/ 15 levels \"female\",\"male\",..: 5 6 3 4 7 8 9 10 11 12 ...\n##  $ estimate : num [1:21] 1.69 1.5 1.97 1.52 2.68 1.67 1.32 0 2.41 1.68 ...\n##  $ p.value  : num [1:21] 0 0.17 0 0 0.02 0 0.28 1 0.07 0 ...\n##  $ conf.low : num [1:21] 1.31 0.84 1.33 1.14 1.19 1.26 0.8 0 0.93 1.31 ...\n##  $ conf.high: num [1:21] 2.18 2.67 2.93 2.03 6.02 ...\n##  $ rx_0     : num [1:21] 265 39 114 190 29 215 54 10 32 251 ...\n##  $ rx_1     : num [1:21] 268 47 110 205 27 229 52 8 38 249 ...\n##  $ HR(95%CI): chr [1:21] \"1.69(1.31-2.18)\" \"1.5(0.84-2.67)\" \"1.97(1.33-2.93)\" \"1.52(1.14-2.03)\" ...\n\nhead(resss)\n## # A tibble: 6 × 9\n##   var    value    estimate p.value conf.low conf.high  rx_0  rx_1 `HR(95%CI)`   \n##   &lt;chr&gt;  &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         \n## 1 adhere No           1.69    0        1.31      2.18   265   268 1.69(1.31-2.1…\n## 2 adhere Yes          1.5     0.17     0.84      2.67    39    47 1.5(0.84-2.67)\n## 3 age    &gt;65          1.97    0        1.33      2.93   114   110 1.97(1.33-2.9…\n## 4 age    &lt;=65         1.52    0        1.14      2.03   190   205 1.52(1.14-2.0…\n## 5 differ well         2.68    0.02     1.19      6.02    29    27 2.68(1.19-6.0…\n## 6 differ moderate     1.67    0        1.26      2.21   215   229 1.67(1.26-2.2…\n\n这样亚组分析就做好了，HR，可信区间，P值，每个组的人数都有了，还记得前面做的整体的结果吗，我们把它合并进来，方便后面画森林图用。\n\nfit &lt;- coxph(Surv(time, status) ~ rx, data = df)\nres_all &lt;- broom::tidy(fit,exponentiate = T,conf.int = T)\n\n#看下不同治疗组的人数\ndf %&gt;% count(rx)\n##   rx   n\n## 1  0 304\n## 2  1 315\n\nres_all &lt;- res_all %&gt;% \n  mutate(var=\"All people\",\n         value=\" \",\n         rx_0=304,\n         rx_1=305,\n         across(where(is.numeric), round,digits=2)\n         ) %&gt;% \n  mutate(`HR(95%CI)`=paste0(estimate,\"(\",conf.low,\"-\",conf.high,\")\")\n         ) %&gt;% \n  select(var,value,estimate,p.value,conf.low,conf.high,rx_0,rx_1,`HR(95%CI)`)\n\nres_all\n## # A tibble: 1 × 9\n##   var        value estimate p.value conf.low conf.high  rx_0  rx_1 `HR(95%CI)`  \n##   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n## 1 All people \" \"       1.67       0     1.32      2.11   304   305 1.67(1.32-2.…\n\n合并到一起：\n\nresss &lt;- bind_rows(res_all,resss)\nhead(resss)\n## # A tibble: 6 × 9\n##   var        value  estimate p.value conf.low conf.high  rx_0  rx_1 `HR(95%CI)` \n##   &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n## 1 All people \" \"        1.67    0        1.32      2.11   304   305 1.67(1.32-2…\n## 2 adhere     \"No\"       1.69    0        1.31      2.18   265   268 1.69(1.31-2…\n## 3 adhere     \"Yes\"      1.5     0.17     0.84      2.67    39    47 1.5(0.84-2.…\n## 4 age        \"&gt;65\"      1.97    0        1.33      2.93   114   110 1.97(1.33-2…\n## 5 age        \"&lt;=65\"     1.52    0        1.14      2.03   190   205 1.52(1.14-2…\n## 6 differ     \"well\"     2.68    0.02     1.19      6.02    29    27 2.68(1.19-6…\n\n到这里所有数据就都准备好了！下面只要整理下格式，画图即可。\n但是forestploter包画森林图的格式还是蛮复杂的，所以我们直接另存为csv，用excel修改好，再读进来。\n\nwrite.csv(resss, file = \"resss.csv\",quote = F,row.names = T)",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>R语言亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html#画森林图",
    "href": "1041-subgroupanalysis.html#画森林图",
    "title": "34  R语言亚组分析及森林图绘制",
    "section": "34.4 画森林图",
    "text": "34.4 画森林图\n把数据整理成这样：\n\n\n\n\n\n\n\n\n\n还有一些细节你可以自己修改下，比如各个亚组的顺序，首字母大写，各个变体的大小写，分组变量的名字，把P值为0的改成&lt;0.0001，等。我就不改了\n然后读取进来：\n\nplot_df &lt;- read.csv(file = \"datasets/resss.csv\",check.names = F)\nplot_df\n##         Subgroup estimate p.value conf.low conf.high rx_0 rx_1        HR(95%CI)\n## 1     All people     1.67    0.00     1.32      2.11  304  305  1.67(1.32-2.11)\n## 2         adhere       NA      NA       NA        NA   NA   NA                 \n## 3             No     1.69    0.00     1.31      2.18  265  268  1.69(1.31-2.18)\n## 4            Yes     1.50    0.17     0.84      2.67   39   47   1.5(0.84-2.67)\n## 5            age       NA      NA       NA        NA   NA   NA                 \n## 6            &gt;65     1.97    0.00     1.33      2.93  114  110  1.97(1.33-2.93)\n## 7           &lt;=65     1.52    0.00     1.14      2.03  190  205  1.52(1.14-2.03)\n## 8         differ       NA      NA       NA        NA   NA   NA                 \n## 9           well     2.68    0.02     1.19      6.02   29   27  2.68(1.19-6.02)\n## 10      moderate     1.67    0.00     1.26      2.21  215  229  1.67(1.26-2.21)\n## 11          poor     1.32    0.28     0.80      2.19   54   52   1.32(0.8-2.19)\n## 12        extent       NA      NA       NA        NA   NA   NA                 \n## 13     submucosa     0.00    1.00     0.00       Inf   10    8         0(0-Inf)\n## 14        muscle     2.41    0.07     0.93      6.22   32   38  2.41(0.93-6.22)\n## 15        serosa     1.68    0.00     1.31      2.16  251  249  1.68(1.31-2.16)\n## 16    contiguous     1.44    0.46     0.55      3.75   11   20  1.44(0.55-3.75)\n## 17         node4       NA      NA       NA        NA   NA   NA                 \n## 18            No     1.85    0.00     1.37      2.49  225  228  1.85(1.37-2.49)\n## 19           Yes     1.41    0.07     0.97      2.05   79   87  1.41(0.97-2.05)\n## 20      obstruct       NA      NA       NA        NA   NA   NA                 \n## 21            No     1.65    0.00     1.27      2.13  250  252  1.65(1.27-2.13)\n## 22           Yes     1.73    0.05     1.01      2.95   54   63  1.73(1.01-2.95)\n## 23        perfor       NA      NA       NA        NA   NA   NA                 \n## 24            No     1.64    0.00     1.30      2.08  296  306   1.64(1.3-2.08)\n## 25           Yes     2.87    0.13     0.74     11.21    8    9 2.87(0.74-11.21)\n## 26           sex       NA      NA       NA        NA   NA   NA                 \n## 27        female     1.32    0.09     0.96      1.80  163  149   1.32(0.96-1.8)\n## 28          male     2.29    0.00     1.60      3.26  141  166   2.29(1.6-3.26)\n## 29          surg       NA      NA       NA        NA   NA   NA                 \n## 30         short     1.82    0.00     1.37      2.40  228  224   1.82(1.37-2.4)\n## 31          long     1.31    0.21     0.86      1.99   76   91  1.31(0.86-1.99)\n\n把数据中的说明部分的NA变成空格，这样画森林图时就不会显示了，然后增加1列空值用于展示可信区间：\n\nplot_df[,c(3,6,7)][is.na(plot_df[,c(3,6,7)])] &lt;- \" \"\nplot_df$` ` &lt;- paste(rep(\" \", nrow(plot_df)), collapse = \" \")\nplot_df\n##         Subgroup estimate p.value conf.low conf.high rx_0 rx_1        HR(95%CI)\n## 1     All people     1.67       0     1.32      2.11  304  305  1.67(1.32-2.11)\n## 2         adhere       NA               NA        NA                           \n## 3             No     1.69       0     1.31      2.18  265  268  1.69(1.31-2.18)\n## 4            Yes     1.50    0.17     0.84      2.67   39   47   1.5(0.84-2.67)\n## 5            age       NA               NA        NA                           \n## 6            &gt;65     1.97       0     1.33      2.93  114  110  1.97(1.33-2.93)\n## 7           &lt;=65     1.52       0     1.14      2.03  190  205  1.52(1.14-2.03)\n## 8         differ       NA               NA        NA                           \n## 9           well     2.68    0.02     1.19      6.02   29   27  2.68(1.19-6.02)\n## 10      moderate     1.67       0     1.26      2.21  215  229  1.67(1.26-2.21)\n## 11          poor     1.32    0.28     0.80      2.19   54   52   1.32(0.8-2.19)\n## 12        extent       NA               NA        NA                           \n## 13     submucosa     0.00       1     0.00       Inf   10    8         0(0-Inf)\n## 14        muscle     2.41    0.07     0.93      6.22   32   38  2.41(0.93-6.22)\n## 15        serosa     1.68       0     1.31      2.16  251  249  1.68(1.31-2.16)\n## 16    contiguous     1.44    0.46     0.55      3.75   11   20  1.44(0.55-3.75)\n## 17         node4       NA               NA        NA                           \n## 18            No     1.85       0     1.37      2.49  225  228  1.85(1.37-2.49)\n## 19           Yes     1.41    0.07     0.97      2.05   79   87  1.41(0.97-2.05)\n## 20      obstruct       NA               NA        NA                           \n## 21            No     1.65       0     1.27      2.13  250  252  1.65(1.27-2.13)\n## 22           Yes     1.73    0.05     1.01      2.95   54   63  1.73(1.01-2.95)\n## 23        perfor       NA               NA        NA                           \n## 24            No     1.64       0     1.30      2.08  296  306   1.64(1.3-2.08)\n## 25           Yes     2.87    0.13     0.74     11.21    8    9 2.87(0.74-11.21)\n## 26           sex       NA               NA        NA                           \n## 27        female     1.32    0.09     0.96      1.80  163  149   1.32(0.96-1.8)\n## 28          male     2.29       0     1.60      3.26  141  166   2.29(1.6-3.26)\n## 29          surg       NA               NA        NA                           \n## 30         short     1.82       0     1.37      2.40  228  224   1.82(1.37-2.4)\n## 31          long     1.31    0.21     0.86      1.99   76   91  1.31(0.86-1.99)\n##                                                                 \n## 1                                                               \n## 2                                                               \n## 3                                                               \n## 4                                                               \n## 5                                                               \n## 6                                                               \n## 7                                                               \n## 8                                                               \n## 9                                                               \n## 10                                                              \n## 11                                                              \n## 12                                                              \n## 13                                                              \n## 14                                                              \n## 15                                                              \n## 16                                                              \n## 17                                                              \n## 18                                                              \n## 19                                                              \n## 20                                                              \n## 21                                                              \n## 22                                                              \n## 23                                                              \n## 24                                                              \n## 25                                                              \n## 26                                                              \n## 27                                                              \n## 28                                                              \n## 29                                                              \n## 30                                                              \n## 31\n\n然后画图即可，默认的出图就已经很美观了，但是大家要注意，这里每个组的人数和开头那张图的每个组的人数稍有不同哦~\n\nlibrary(forestploter)\nlibrary(grid)\n\np &lt;- forest(\n  data = plot_df[,c(1,6,7,9,8,3)],\n  lower = plot_df$conf.low,\n  upper = plot_df$conf.high,\n  est = plot_df$estimate,\n  ci_column = 4,\n  sizes = (plot_df$estimate+0.001)*0.3, \n  ref_line = 1, \n  xlim = c(0.1,4)\n  )\nprint(p)\n\n\n\n\n\n\n\n\n如果你还需要美化，我们在之前也详细介绍过这个包的使用细节了：\n\n画一个好看的森林图\n用更简单的方式画森林图\n森林图展示回归系数\nggforestplot绘制森林图\nR语言画误差线的5种方法\nggplot2绘制森林图(有亚组和没亚组)\n\n下面是我们美化后的森林图，其实变化不是非常大，只要数据好，默认的图形也很好看：\n\n\n\n\n\n\n\n\n\n和开头那张NEJM的风格一模一样！\n\npdf(\"aaa.pdf\",width = 8,height = 10)\ntm &lt;- forest_theme(base_size = 12, # 基础大小\n                   # 可信区间点的形状，线型、颜色、宽度\n                   #ci_lty = 1,\n                   ci_lwd = 1.5,\n                   #ci_Theight = 0.2, # 可信区间两端加短竖线\n                   # 参考线宽度、形状、颜色\n                   refline_lwd = 1.5,\n                   refline_lty = \"dashed\",\n                   refline_col = \"grey20\",\n                   # 汇总菱形的填充色和边框色\n                   #summary_fill = \"#4575b4\",\n                   #summary_col = \"#4575b4\",\n                   # 脚注大小、字体、颜色\n                   footnote_cex = 0.8,\n                   footnote_fontface = \"italic\",\n                   footnote_col = \"grey30\",\n                   # 自定义背景色、前景色。fontface:1常规，2粗体，3斜体，4粗斜体 \n                   core = list(bg_params = list(fill = c(\"#FFFFFF\",\"#f5f7f6\"), col=NA))\n                   )\n\np &lt;- forest(\n  data = plot_df[,c(1,6,7,9,8,3)],\n  lower = plot_df$conf.low,\n  upper = plot_df$conf.high,\n  est = plot_df$estimate,\n  ci_column = 4,\n  sizes = (plot_df$estimate+0.001)*0.3, # 不能是负值或NA，而且不能太大\n  ref_line = 1, # 把竖线放到1的位置\n  xlim = c(0.1,4), # x轴范围,如果有的可信区间超过这个范围会显示为箭头\n  arrow_lab = c(\"Obs better\",\"Lev+5-FU better\"), # x轴下面的文字\n  theme = tm\n  )\nprint(p)\ndev.off()",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>R语言亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html#其他资源",
    "href": "1041-subgroupanalysis.html#其他资源",
    "title": "34  R语言亚组分析及森林图绘制",
    "section": "34.5 其他资源",
    "text": "34.5 其他资源\n亚组分析和森林图的内容还有非常多的细节问题，为了不影响该合集的主要内容，我把它们放在下面的链接中，大家感兴趣的话可点击下面的链接查看，或者在公众号后台回复亚组分析获取合集链接：\n\n使用R语言画森林图和误差线\nggplot2绘制森林图(有亚组和没亚组)\nR语言亚组分析及森林图绘制\nR语言亚组分析1行代码实现！\n亚组分析和多因素回归的森林图比较\n多列可信区间、多组别的森林图绘制\n逻辑回归亚组分析森林图绘制\n协变量调整的亚组分析和森林图绘制\n1行代码实现：多因素回归的亚组分析，并绘制森林图\nR语言亚组分析及森林图绘制手册",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>R语言亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1042-subgroup1code.html",
    "href": "1042-subgroup1code.html",
    "title": "35  亚组分析1行代码实现",
    "section": "",
    "text": "35.1 安装\ninstall.packages(\"jstable\")\n\n## From github: latest version\nremotes::install_github('jinseob2kim/jstable')\nlibrary(jstable)",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>亚组分析1行代码实现</span>"
    ]
  },
  {
    "objectID": "1042-subgroup1code.html#准备数据",
    "href": "1042-subgroup1code.html#准备数据",
    "title": "35  亚组分析1行代码实现",
    "section": "35.2 准备数据",
    "text": "35.2 准备数据\n还是使用之前演示的数据。\n使用survival包中的colon数据集用于演示，这是一份关于结肠癌患者的生存数据，共有1858行，16列，共分为3个组，1个观察组+2个治疗组，观察他们发生终点事件的差异。\n各变量的解释如下：\n\nid：患者id\nstudy：没啥用，所有患者都是1\nrx：治疗方法，共3种，Obs(观察组), Lev(左旋咪唑), Lev+5FU(左旋咪唑+5-FU)\nsex：性别，1是男性\nage：年龄\nobstruct：肠梗阻，1是有\nperfor：肠穿孔，1是有\nadhere：和附近器官粘连，1是有\nnodes：转移的淋巴结数量\nstatus：生存状态，0代表删失，1代表发生终点事件\ndiffer：肿瘤分化程度，1-well,2-moderate,3-poor\nextent：局部扩散情况，1-submucosa，2-muscle，3-serosa，4-contiguous structures\nsurg：手术后多久了，1-long,2-short\nnode4：是否有超过4个阳性淋巴结，1代表是\ntime：生存时间\netype：终点事件类型，1-复发，2-死亡\n\n\nrm(list = ls())\nlibrary(survival)\n\nstr(colon)\n## 'data.frame':    1858 obs. of  16 variables:\n##  $ id      : num  1 1 2 2 3 3 4 4 5 5 ...\n##  $ study   : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ rx      : Factor w/ 3 levels \"Obs\",\"Lev\",\"Lev+5FU\": 3 3 3 3 1 1 3 3 1 1 ...\n##  $ sex     : num  1 1 1 1 0 0 0 0 1 1 ...\n##  $ age     : num  43 43 63 63 71 71 66 66 69 69 ...\n##  $ obstruct: num  0 0 0 0 0 0 1 1 0 0 ...\n##  $ perfor  : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ adhere  : num  0 0 0 0 1 1 0 0 0 0 ...\n##  $ nodes   : num  5 5 1 1 7 7 6 6 22 22 ...\n##  $ status  : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ differ  : num  2 2 2 2 2 2 2 2 2 2 ...\n##  $ extent  : num  3 3 3 3 2 2 3 3 3 3 ...\n##  $ surg    : num  0 0 0 0 0 0 1 1 1 1 ...\n##  $ node4   : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ time    : num  1521 968 3087 3087 963 ...\n##  $ etype   : num  2 1 2 1 2 1 2 1 2 1 ...\n\n可以使用cox回归探索危险因素。分类变量需要变为因子型，这样在进行回归时会自动进行哑变量设置。\n为了演示，我们只选择Obs组和Lev+5FU组的患者，所有的分类变量都变为factor，把年龄也变为分类变量并变成factor。\n\nsuppressMessages(library(tidyverse))\n\ndf &lt;- colon %&gt;% \n  mutate(rx=as.numeric(rx)) %&gt;% \n  filter(etype == 1, !rx == 2) %&gt;%  #rx %in% c(\"Obs\",\"Lev+5FU\"), \n  select(time, status,rx, sex, age,obstruct,perfor,adhere,differ,extent,surg,node4) %&gt;% \n  mutate(sex=factor(sex, levels=c(0,1),labels=c(\"female\",\"male\")),\n         age=ifelse(age &gt;65,\"&gt;65\",\"&lt;=65\"),\n         age=factor(age, levels=c(\"&gt;65\",\"&lt;=65\")),\n         obstruct=factor(obstruct, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         perfor=factor(perfor, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         adhere=factor(adhere, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         differ=factor(differ, levels=c(1,2,3),labels=c(\"well\",\"moderate\",\"poor\")),\n         extent=factor(extent, levels=c(1,2,3,4),\n                       labels=c(\"submucosa\",\"muscle\",\"serosa\",\"contiguous\")),\n         surg=factor(surg, levels=c(0,1),labels=c(\"short\",\"long\")),\n         node4=factor(node4, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         rx=ifelse(rx==3,0,1),\n         rx=factor(rx,levels=c(0,1))\n         )\n\nstr(df)\n## 'data.frame':    619 obs. of  12 variables:\n##  $ time    : num  968 3087 542 245 523 ...\n##  $ status  : num  1 0 1 1 1 1 0 0 0 1 ...\n##  $ rx      : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 2 1 1 2 ...\n##  $ sex     : Factor w/ 2 levels \"female\",\"male\": 2 2 1 1 2 1 2 1 2 2 ...\n##  $ age     : Factor w/ 2 levels \"&gt;65\",\"&lt;=65\": 2 2 1 1 1 2 2 1 2 2 ...\n##  $ obstruct: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n##  $ perfor  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ adhere  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 1 1 1 1 1 ...\n##  $ differ  : Factor w/ 3 levels \"well\",\"moderate\",..: 2 2 2 2 2 2 2 2 3 2 ...\n##  $ extent  : Factor w/ 4 levels \"submucosa\",\"muscle\",..: 3 3 2 3 3 3 3 3 3 3 ...\n##  $ surg    : Factor w/ 2 levels \"short\",\"long\": 1 1 1 2 2 1 1 2 2 1 ...\n##  $ node4   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 1 1 1 1 ...",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>亚组分析1行代码实现</span>"
    ]
  },
  {
    "objectID": "1042-subgroup1code.html#亚组分析",
    "href": "1042-subgroup1code.html#亚组分析",
    "title": "35  亚组分析1行代码实现",
    "section": "35.3 亚组分析",
    "text": "35.3 亚组分析\n使用jstable，只要1行代码即可！！！\n\nlibrary(jstable)\n\nres &lt;- TableSubgroupMultiCox(\n  \n  # 指定公式\n  formula = Surv(time, status) ~ rx, \n  \n  # 指定哪些变量有亚组\n  var_subgroups = c(\"sex\",\"age\",\"obstruct\",\"perfor\",\"adhere\",\n                    \"differ\",\"extent\",\"surg\",\"node4\"), \n  data = df #指定你的数据\n  )\n## Warning in coxph.fit(X, Y, istrat, offset, init, control, weights = weights, :\n## Loglik converged before variable 1 ; coefficient may be infinite.\n## Warning in coxph.fit(X, Y, istrat, offset, init, control, weights = weights, :\n## Loglik converged before variable 1,5,6,7 ; coefficient may be infinite.\nres\n##         Variable Count Percent Point Estimate Lower Upper rx=0 rx=1 P value\n## rx       Overall   619     100           1.67  1.32  2.11 34.4 48.9  &lt;0.001\n## 1            sex  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 2         female   312    50.4           1.32  0.96   1.8 41.1 47.8   0.088\n## 3           male   307    49.6           2.29   1.6  3.26 26.6 50.1  &lt;0.001\n## 4            age  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 5            &gt;65   224    36.2           1.97  1.33  2.93 29.8 50.6   0.001\n## 6           &lt;=65   395    63.8           1.52  1.14  2.03   37 48.1   0.004\n## 7       obstruct  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 8             No   502    81.1           1.65  1.27  2.13 34.4 46.8  &lt;0.001\n## 9            Yes   117    18.9           1.73  1.01  2.95 34.2 57.6   0.046\n## 10        perfor  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 11            No   602    97.3           1.64   1.3  2.08 34.3 48.1  &lt;0.001\n## 12           Yes    17     2.7           2.87  0.74 11.21 37.5 77.8   0.129\n## 13        adhere  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 14            No   533    86.1           1.69  1.31  2.18 32.9 47.4  &lt;0.001\n## 15           Yes    86    13.9            1.5  0.84  2.67 44.4 57.9   0.173\n## 16        differ  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 17          well    56     9.2           2.68  1.19  6.02   31 55.6   0.017\n## 18      moderate   444    73.3           1.67  1.26  2.21   32 44.8  &lt;0.001\n## 19          poor   106    17.5           1.32   0.8  2.19 47.5 64.5   0.277\n## 20        extent  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 21     submucosa    18     2.9              0     0   Inf   30    0   0.999\n## 22        muscle    70    11.3           2.41  0.93  6.22  9.4 28.9   0.069\n## 23        serosa   500    80.8           1.68  1.31  2.16 36.7 52.1  &lt;0.001\n## 24    contiguous    31       5           1.44  0.55  3.75 58.4 67.2   0.459\n## 25          surg  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 26         short   452      73           1.82  1.37   2.4 31.5 48.9  &lt;0.001\n## 27          long   167      27           1.31  0.86  1.99 43.2 49.1   0.208\n## 28         node4  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 29            No   453    73.2           1.85  1.37  2.49 27.7 41.5  &lt;0.001\n## 30           Yes   166    26.8           1.41  0.97  2.05 53.2 68.7   0.074\n##    P for interaction\n## rx              &lt;NA&gt;\n## 1              0.029\n## 2               &lt;NA&gt;\n## 3               &lt;NA&gt;\n## 4              0.316\n## 5               &lt;NA&gt;\n## 6               &lt;NA&gt;\n## 7              0.752\n## 8               &lt;NA&gt;\n## 9               &lt;NA&gt;\n## 10             0.442\n## 11              &lt;NA&gt;\n## 12              &lt;NA&gt;\n## 13             0.756\n## 14              &lt;NA&gt;\n## 15              &lt;NA&gt;\n## 16             0.402\n## 17              &lt;NA&gt;\n## 18              &lt;NA&gt;\n## 19              &lt;NA&gt;\n## 20               0.1\n## 21              &lt;NA&gt;\n## 22              &lt;NA&gt;\n## 23              &lt;NA&gt;\n## 24              &lt;NA&gt;\n## 25             0.183\n## 26              &lt;NA&gt;\n## 27              &lt;NA&gt;\n## 28             0.338\n## 29              &lt;NA&gt;\n## 30              &lt;NA&gt;\n\n直接就得出了结果！除了亚组分析的各种结果，还给出了交互作用的P值！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>亚组分析1行代码实现</span>"
    ]
  },
  {
    "objectID": "1042-subgroup1code.html#画森林图",
    "href": "1042-subgroup1code.html#画森林图",
    "title": "35  亚组分析1行代码实现",
    "section": "35.4 画森林图",
    "text": "35.4 画森林图\n这个结果不需要另存为csv也能直接使用（除非你是细节控，需要修改各种大小写等信息），当然如果你需要HR(95%CI)这种信息，还是需要自己添加一下的。\n我们添加个空列用于显示可信区间，并把不想显示的NA去掉即可，还需要把P值，可信区间这些列变为数值型。\n\nplot_df &lt;- res\nplot_df[,c(2,3,9)][is.na(plot_df[,c(2,3,9)])] &lt;- \" \"\nplot_df$` ` &lt;- paste(rep(\" \", nrow(plot_df)), collapse = \" \")\nplot_df[,4:6] &lt;- apply(plot_df[,4:6],2,as.numeric)\nplot_df\n##         Variable Count Percent Point Estimate Lower Upper rx=0 rx=1 P value\n## rx       Overall   619     100           1.67  1.32  2.11 34.4 48.9  &lt;0.001\n## 1            sex                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 2         female   312    50.4           1.32  0.96  1.80 41.1 47.8   0.088\n## 3           male   307    49.6           2.29  1.60  3.26 26.6 50.1  &lt;0.001\n## 4            age                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 5            &gt;65   224    36.2           1.97  1.33  2.93 29.8 50.6   0.001\n## 6           &lt;=65   395    63.8           1.52  1.14  2.03   37 48.1   0.004\n## 7       obstruct                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 8             No   502    81.1           1.65  1.27  2.13 34.4 46.8  &lt;0.001\n## 9            Yes   117    18.9           1.73  1.01  2.95 34.2 57.6   0.046\n## 10        perfor                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 11            No   602    97.3           1.64  1.30  2.08 34.3 48.1  &lt;0.001\n## 12           Yes    17     2.7           2.87  0.74 11.21 37.5 77.8   0.129\n## 13        adhere                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 14            No   533    86.1           1.69  1.31  2.18 32.9 47.4  &lt;0.001\n## 15           Yes    86    13.9           1.50  0.84  2.67 44.4 57.9   0.173\n## 16        differ                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 17          well    56     9.2           2.68  1.19  6.02   31 55.6   0.017\n## 18      moderate   444    73.3           1.67  1.26  2.21   32 44.8  &lt;0.001\n## 19          poor   106    17.5           1.32  0.80  2.19 47.5 64.5   0.277\n## 20        extent                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 21     submucosa    18     2.9           0.00  0.00   Inf   30    0   0.999\n## 22        muscle    70    11.3           2.41  0.93  6.22  9.4 28.9   0.069\n## 23        serosa   500    80.8           1.68  1.31  2.16 36.7 52.1  &lt;0.001\n## 24    contiguous    31       5           1.44  0.55  3.75 58.4 67.2   0.459\n## 25          surg                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 26         short   452      73           1.82  1.37  2.40 31.5 48.9  &lt;0.001\n## 27          long   167      27           1.31  0.86  1.99 43.2 49.1   0.208\n## 28         node4                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 29            No   453    73.2           1.85  1.37  2.49 27.7 41.5  &lt;0.001\n## 30           Yes   166    26.8           1.41  0.97  2.05 53.2 68.7   0.074\n##    P for interaction\n## rx              &lt;NA&gt;\n## 1              0.029\n## 2               &lt;NA&gt;\n## 3               &lt;NA&gt;\n## 4              0.316\n## 5               &lt;NA&gt;\n## 6               &lt;NA&gt;\n## 7              0.752\n## 8               &lt;NA&gt;\n## 9               &lt;NA&gt;\n## 10             0.442\n## 11              &lt;NA&gt;\n## 12              &lt;NA&gt;\n## 13             0.756\n## 14              &lt;NA&gt;\n## 15              &lt;NA&gt;\n## 16             0.402\n## 17              &lt;NA&gt;\n## 18              &lt;NA&gt;\n## 19              &lt;NA&gt;\n## 20               0.1\n## 21              &lt;NA&gt;\n## 22              &lt;NA&gt;\n## 23              &lt;NA&gt;\n## 24              &lt;NA&gt;\n## 25             0.183\n## 26              &lt;NA&gt;\n## 27              &lt;NA&gt;\n## 28             0.338\n## 29              &lt;NA&gt;\n## 30              &lt;NA&gt;\n##                                                                 \n## rx                                                              \n## 1                                                               \n## 2                                                               \n## 3                                                               \n## 4                                                               \n## 5                                                               \n## 6                                                               \n## 7                                                               \n## 8                                                               \n## 9                                                               \n## 10                                                              \n## 11                                                              \n## 12                                                              \n## 13                                                              \n## 14                                                              \n## 15                                                              \n## 16                                                              \n## 17                                                              \n## 18                                                              \n## 19                                                              \n## 20                                                              \n## 21                                                              \n## 22                                                              \n## 23                                                              \n## 24                                                              \n## 25                                                              \n## 26                                                              \n## 27                                                              \n## 28                                                              \n## 29                                                              \n## 30\n\n画图就非常简单！\n\nlibrary(forestploter)\nlibrary(grid)\n\np &lt;- forest(\n  data = plot_df[,c(1,2,3,11,9)],\n  lower = plot_df$Lower,\n  upper = plot_df$Upper,\n  est = plot_df$`Point Estimate`,\n  ci_column = 4,\n  #sizes = (plot_df$estimate+0.001)*0.3, \n  ref_line = 1, \n  xlim = c(0.1,4)\n  )\nprint(p)\n\n\n\n\n\n\n\n\n\n\n这样就搞定了，真的是非常简单了，省去了大量的步骤。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>亚组分析1行代码实现</span>"
    ]
  },
  {
    "objectID": "1042-subgroup1code.html#其他资源",
    "href": "1042-subgroup1code.html#其他资源",
    "title": "35  亚组分析1行代码实现",
    "section": "35.5 其他资源",
    "text": "35.5 其他资源\n亚组分析和森林图的内容还有非常多的细节问题，为了不影响该合集的主要内容，我把它们放在下面的链接中，大家感兴趣的话可点击下面的链接查看，或者在公众号后台回复亚组分析获取合集链接：\n\n使用R语言画森林图和误差线\nggplot2绘制森林图(有亚组和没亚组)\nR语言亚组分析及森林图绘制\nR语言亚组分析1行代码实现！\n亚组分析和多因素回归的森林图比较\n多列可信区间、多组别的森林图绘制\n逻辑回归亚组分析森林图绘制\n协变量调整的亚组分析和森林图绘制\n1行代码实现：多因素回归的亚组分析，并绘制森林图\nR语言亚组分析及森林图绘制手册",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>亚组分析1行代码实现</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html",
    "href": "亚组分析和多因素回归的森林图.html",
    "title": "36  亚组分析和多因素回归的森林图比较",
    "section": "",
    "text": "36.1 准备数据\n使用survival包中的colon数据集用于演示，这是一份关于结肠癌患者的生存数据，共有1858行，16列，共分为3个组，1个观察组+2个治疗组，观察他们发生终点事件的差异。\n各变量的解释如下：\nrm(list = ls())\nlibrary(survival)\n\nstr(colon)\n## 'data.frame':    1858 obs. of  16 variables:\n##  $ id      : num  1 1 2 2 3 3 4 4 5 5 ...\n##  $ study   : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ rx      : Factor w/ 3 levels \"Obs\",\"Lev\",\"Lev+5FU\": 3 3 3 3 1 1 3 3 1 1 ...\n##  $ sex     : num  1 1 1 1 0 0 0 0 1 1 ...\n##  $ age     : num  43 43 63 63 71 71 66 66 69 69 ...\n##  $ obstruct: num  0 0 0 0 0 0 1 1 0 0 ...\n##  $ perfor  : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ adhere  : num  0 0 0 0 1 1 0 0 0 0 ...\n##  $ nodes   : num  5 5 1 1 7 7 6 6 22 22 ...\n##  $ status  : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ differ  : num  2 2 2 2 2 2 2 2 2 2 ...\n##  $ extent  : num  3 3 3 3 2 2 3 3 3 3 ...\n##  $ surg    : num  0 0 0 0 0 0 1 1 1 1 ...\n##  $ node4   : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ time    : num  1521 968 3087 3087 963 ...\n##  $ etype   : num  2 1 2 1 2 1 2 1 2 1 ...\n为了演示，我们只选择Obs组和Lev+5FU组的患者，所有的分类变量都变为factor，把年龄也变为分类变量并变成factor。\nlibrary(tidyverse)\n\ndf &lt;- colon %&gt;% \n  mutate(rx=as.numeric(rx)) %&gt;% \n  filter(etype == 1, !rx == 2) %&gt;%  #rx %in% c(\"Obs\",\"Lev+5FU\"), \n  select(time, status,rx, sex, age,obstruct,perfor,adhere,differ,extent,surg,node4) %&gt;% \n  mutate(sex=factor(sex, levels=c(0,1),labels=c(\"female\",\"male\")),\n         age=ifelse(age &gt;65,\"&gt;65\",\"&lt;=65\"),\n         age=factor(age, levels=c(\"&gt;65\",\"&lt;=65\")),\n         obstruct=factor(obstruct, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         perfor=factor(perfor, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         adhere=factor(adhere, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         differ=factor(differ, levels=c(1,2,3),labels=c(\"well\",\"moderate\",\"poor\")),\n         extent=factor(extent, levels=c(1,2,3,4),\n                       labels=c(\"submucosa\",\"muscle\",\"serosa\",\"contiguous\")),\n         surg=factor(surg, levels=c(0,1),labels=c(\"short\",\"long\")),\n         node4=factor(node4, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         rx=ifelse(rx==3,0,1),\n         rx=factor(rx,levels=c(0,1))\n         )\n\nstr(df)\n## 'data.frame':    619 obs. of  12 variables:\n##  $ time    : num  968 3087 542 245 523 ...\n##  $ status  : num  1 0 1 1 1 1 0 0 0 1 ...\n##  $ rx      : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 2 1 1 2 ...\n##  $ sex     : Factor w/ 2 levels \"female\",\"male\": 2 2 1 1 2 1 2 1 2 2 ...\n##  $ age     : Factor w/ 2 levels \"&gt;65\",\"&lt;=65\": 2 2 1 1 1 2 2 1 2 2 ...\n##  $ obstruct: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n##  $ perfor  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ adhere  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 1 1 1 1 1 ...\n##  $ differ  : Factor w/ 3 levels \"well\",\"moderate\",..: 2 2 2 2 2 2 2 2 3 2 ...\n##  $ extent  : Factor w/ 4 levels \"submucosa\",\"muscle\",..: 3 3 2 3 3 3 3 3 3 3 ...\n##  $ surg    : Factor w/ 2 levels \"short\",\"long\": 1 1 1 2 2 1 1 2 2 1 ...\n##  $ node4   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 1 1 1 1 ...\n多因素cox回归现在的很多文章中都是用来筛选变量的，但其实它是一种识别危险因素的方法，通常是根据P值和可信区间判断某个变量对终点事件是不是有影响。\n如果某个变量是分类变量，那么它在进入回归分析后会自动被执行哑变量编码，以其中第一个水平作为参考，其他水平都和参考组进行比较。具体的编码细节我在很久之前就详细介绍过了：分类数据回归分析时的编码方案",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#准备数据",
    "href": "亚组分析和多因素回归的森林图.html#准备数据",
    "title": "36  亚组分析和多因素回归的森林图比较",
    "section": "",
    "text": "id：患者id\nstudy：没啥用，所有患者都是1\nrx：治疗方法，共3种，Obs(观察组), Lev(左旋咪唑), Lev+5FU(左旋咪唑+5-FU)\nsex：性别，1是男性\nage：年龄\nobstruct：肠梗阻，1是有\nperfor：肠穿孔，1是有\nadhere：和附近器官粘连，1是有\nnodes：转移的淋巴结数量\nstatus：生存状态，0代表删失，1代表发生终点事件\ndiffer：肿瘤分化程度，1-well,2-moderate,3-poor\nextent：局部扩散情况，1-submucosa，2-muscle，3-serosa，4-contiguous_structures\nsurg：手术后多久了，1-long,2-short\nnode4：是否有超过4个阳性淋巴结，1代表是\ntime：生存时间\netype：终点事件类型，1-复发，2-死亡",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#多因素回归",
    "href": "亚组分析和多因素回归的森林图.html#多因素回归",
    "title": "36  亚组分析和多因素回归的森林图比较",
    "section": "36.2 多因素回归",
    "text": "36.2 多因素回归\n\nlibrary(survival)\n\nfit_multi &lt;- coxph(Surv(time, status) ~ ., data = df)\nsummary(fit_multi)\n## Call:\n## coxph(formula = Surv(time, status) ~ ., data = df)\n## \n##   n= 606, number of events= 292 \n##    (13 observations deleted due to missingness)\n## \n##                       coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \n## rx1               0.521198  1.684043  0.120261  4.334 1.47e-05 ***\n## sexmale          -0.125724  0.881858  0.118615 -1.060   0.2892    \n## age&lt;=65           0.022860  1.023123  0.124973  0.183   0.8549    \n## obstructYes       0.001102  1.001103  0.150254  0.007   0.9941    \n## perforYes         0.219640  1.245629  0.335564  0.655   0.5128    \n## adhereYes         0.121203  1.128854  0.172725  0.702   0.4829    \n## differmoderate   -0.214304  0.807103  0.211843 -1.012   0.3117    \n## differpoor        0.196139  1.216696  0.240222  0.816   0.4142    \n## extentmuscle      0.413055  1.511429  0.620625  0.666   0.5057    \n## extentserosa      1.043101  2.838005  0.584977  1.783   0.0746 .  \n## extentcontiguous  1.336959  3.807447  0.637908  2.096   0.0361 *  \n## surglong          0.198218  1.219229  0.127288  1.557   0.1194    \n## node4Yes          0.811284  2.250796  0.123699  6.559 5.43e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##                  exp(coef) exp(-coef) lower .95 upper .95\n## rx1                 1.6840     0.5938    1.3304     2.132\n## sexmale             0.8819     1.1340    0.6989     1.113\n## age&lt;=65             1.0231     0.9774    0.8008     1.307\n## obstructYes         1.0011     0.9989    0.7457     1.344\n## perforYes           1.2456     0.8028    0.6453     2.404\n## adhereYes           1.1289     0.8859    0.8047     1.584\n## differmoderate      0.8071     1.2390    0.5329     1.223\n## differpoor          1.2167     0.8219    0.7598     1.948\n## extentmuscle        1.5114     0.6616    0.4478     5.101\n## extentserosa        2.8380     0.3524    0.9017     8.932\n## extentcontiguous    3.8074     0.2626    1.0906    13.293\n## surglong            1.2192     0.8202    0.9500     1.565\n## node4Yes            2.2508     0.4443    1.7662     2.868\n## \n## Concordance= 0.672  (se = 0.016 )\n## Likelihood ratio test= 93.76  on 13 df,   p=3e-14\n## Wald test            = 93.41  on 13 df,   p=3e-14\n## Score (logrank) test = 98.95  on 13 df,   p=3e-15\n\n可以看到这个多因素回归的结果，对于每一个分类变量，都会进行哑变量编码（参考上面的推文），所有结果中会有rx1，sexmale这样的结果，rx这个变量是有2个类别的，分别是类别0和类别1，结果只有rx1，因为列别0是参考，对于sex也是，其中的female时参考，所以只有sexmale的结果。\n此时的森林图是这样的，也是表达的一模一样的意思，你可以看到结果中都有一个reference，这个就是参考了，参考类别是没有P值的，也没有可信区间，HR都是1。\n\nlibrary(survminer)\n\nggforest(fit_multi,fontsize = 1)\n## Warning in .get_data(model, data = data): The `data` argument is not provided.\n## Data will be extracted from model fit.\n\n\n\n\n\n\n\n\n为了和亚组分析的森林图比较一下，我们重新提取一下数据，使用forestploter包再画一遍。\n\nmultidf &lt;- broom::tidy(fit_multi,exponentiate = T,conf.int = T) %&gt;% \n  mutate(across(where(is.numeric), round,digits=2),\n         `HR(95%CI)`=paste0(estimate,\"(\",conf.low,\"-\",conf.high,\")\")\n         ) %&gt;% \n  select(term,estimate,p.value,conf.low,conf.high,`HR(95%CI)`)\n## Warning: There was 1 warning in `mutate()`.\n## ℹ In argument: `across(where(is.numeric), round, digits = 2)`.\n## Caused by warning:\n## ! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\n## Supply arguments directly to `.fns` through an anonymous function instead.\n## \n##   # Previously\n##   across(a:b, mean, na.rm = TRUE)\n## \n##   # Now\n##   across(a:b, \\(x) mean(x, na.rm = TRUE))\n\nmultidf\n## # A tibble: 13 × 6\n##    term             estimate p.value conf.low conf.high `HR(95%CI)`     \n##    &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;           \n##  1 rx1                  1.68    0        1.33      2.13 1.68(1.33-2.13) \n##  2 sexmale              0.88    0.29     0.7       1.11 0.88(0.7-1.11)  \n##  3 age&lt;=65              1.02    0.85     0.8       1.31 1.02(0.8-1.31)  \n##  4 obstructYes          1       0.99     0.75      1.34 1(0.75-1.34)    \n##  5 perforYes            1.25    0.51     0.65      2.4  1.25(0.65-2.4)  \n##  6 adhereYes            1.13    0.48     0.8       1.58 1.13(0.8-1.58)  \n##  7 differmoderate       0.81    0.31     0.53      1.22 0.81(0.53-1.22) \n##  8 differpoor           1.22    0.41     0.76      1.95 1.22(0.76-1.95) \n##  9 extentmuscle         1.51    0.51     0.45      5.1  1.51(0.45-5.1)  \n## 10 extentserosa         2.84    0.07     0.9       8.93 2.84(0.9-8.93)  \n## 11 extentcontiguous     3.81    0.04     1.09     13.3  3.81(1.09-13.29)\n## 12 surglong             1.22    0.12     0.95      1.56 1.22(0.95-1.56) \n## 13 node4Yes             2.25    0        1.77      2.87 2.25(1.77-2.87)\n\n#write.csv(multidf,file = \"multidf.csv\",quote = F)\n\n保存后重新整理下格式再读取进来：\n\nplot_df &lt;- read.csv(file=\"./datasets/multidf.csv\",check.names = F)\nplot_df\n##         subgroup estimate p.value conf.low conf.high        HR(95%CI)\n## 1             rx       NA      NA       NA        NA                 \n## 2            rx0     1.00      NA     1.00      1.00                 \n## 3            rx1     1.68    0.00     1.33      2.13  1.68(1.33-2.13)\n## 4            sex       NA      NA       NA        NA                 \n## 5         female     1.00      NA     1.00      1.00                 \n## 6           male     0.88    0.29     0.70      1.11   0.88(0.7-1.11)\n## 7            age       NA      NA       NA        NA                 \n## 8            &gt;65     1.00      NA     1.00      1.00                 \n## 9           &lt;=65     1.02    0.85     0.80      1.31   1.02(0.8-1.31)\n## 10      obstruct       NA      NA       NA        NA                 \n## 11            No     1.00      NA     1.00      1.00                 \n## 12           Yes     1.00    0.99     0.75      1.34     1(0.75-1.34)\n## 13        perfor       NA      NA       NA        NA                 \n## 14            No     1.00      NA     1.00      1.00                 \n## 15           Yes     1.25    0.51     0.65      2.40   1.25(0.65-2.4)\n## 16        adhere       NA      NA       NA        NA                 \n## 17            No     1.00      NA     1.00      1.00                 \n## 18           Yes     1.13    0.48     0.80      1.58   1.13(0.8-1.58)\n## 19        differ       NA      NA       NA        NA                 \n## 20          well     1.00      NA     1.00      1.00                 \n## 21      moderate     0.81    0.31     0.53      1.22  0.81(0.53-1.22)\n## 22          poor     1.22    0.41     0.76      1.95  1.22(0.76-1.95)\n## 23        extent       NA      NA       NA        NA                 \n## 24     submucosa     1.00      NA     1.00      1.00                 \n## 25        muscle     1.51    0.51     0.45      5.10   1.51(0.45-5.1)\n## 26        serosa     2.84    0.07     0.90      8.93   2.84(0.9-8.93)\n## 27    contiguous     3.81    0.04     1.09     13.29 3.81(1.09-13.29)\n## 28          surg       NA      NA       NA        NA                 \n## 29         short     1.00      NA     1.00      1.00                 \n## 30          long     1.22    0.12     0.95      1.56  1.22(0.95-1.56)\n## 31         node4       NA      NA       NA        NA                 \n## 32            No     1.00      NA     1.00      1.00                 \n## 33           Yes     2.25    0.00     1.77      2.87  2.25(1.77-2.87)\n\n把数据中的P值部分的NA变成空格，这样画森林图时就不会显示了，然后增加1列空值用于展示可信区间：\n\nplot_df[,c(3)][is.na(plot_df[,c(3)])] &lt;- \" \"\nplot_df$` ` &lt;- paste(rep(\" \", nrow(plot_df)), collapse = \" \")\nplot_df\n##         subgroup estimate p.value conf.low conf.high        HR(95%CI)\n## 1             rx       NA               NA        NA                 \n## 2            rx0     1.00             1.00      1.00                 \n## 3            rx1     1.68       0     1.33      2.13  1.68(1.33-2.13)\n## 4            sex       NA               NA        NA                 \n## 5         female     1.00             1.00      1.00                 \n## 6           male     0.88    0.29     0.70      1.11   0.88(0.7-1.11)\n## 7            age       NA               NA        NA                 \n## 8            &gt;65     1.00             1.00      1.00                 \n## 9           &lt;=65     1.02    0.85     0.80      1.31   1.02(0.8-1.31)\n## 10      obstruct       NA               NA        NA                 \n## 11            No     1.00             1.00      1.00                 \n## 12           Yes     1.00    0.99     0.75      1.34     1(0.75-1.34)\n## 13        perfor       NA               NA        NA                 \n## 14            No     1.00             1.00      1.00                 \n## 15           Yes     1.25    0.51     0.65      2.40   1.25(0.65-2.4)\n## 16        adhere       NA               NA        NA                 \n## 17            No     1.00             1.00      1.00                 \n## 18           Yes     1.13    0.48     0.80      1.58   1.13(0.8-1.58)\n## 19        differ       NA               NA        NA                 \n## 20          well     1.00             1.00      1.00                 \n## 21      moderate     0.81    0.31     0.53      1.22  0.81(0.53-1.22)\n## 22          poor     1.22    0.41     0.76      1.95  1.22(0.76-1.95)\n## 23        extent       NA               NA        NA                 \n## 24     submucosa     1.00             1.00      1.00                 \n## 25        muscle     1.51    0.51     0.45      5.10   1.51(0.45-5.1)\n## 26        serosa     2.84    0.07     0.90      8.93   2.84(0.9-8.93)\n## 27    contiguous     3.81    0.04     1.09     13.29 3.81(1.09-13.29)\n## 28          surg       NA               NA        NA                 \n## 29         short     1.00             1.00      1.00                 \n## 30          long     1.22    0.12     0.95      1.56  1.22(0.95-1.56)\n## 31         node4       NA               NA        NA                 \n## 32            No     1.00             1.00      1.00                 \n## 33           Yes     2.25       0     1.77      2.87  2.25(1.77-2.87)\n##                                                                     \n## 1                                                                   \n## 2                                                                   \n## 3                                                                   \n## 4                                                                   \n## 5                                                                   \n## 6                                                                   \n## 7                                                                   \n## 8                                                                   \n## 9                                                                   \n## 10                                                                  \n## 11                                                                  \n## 12                                                                  \n## 13                                                                  \n## 14                                                                  \n## 15                                                                  \n## 16                                                                  \n## 17                                                                  \n## 18                                                                  \n## 19                                                                  \n## 20                                                                  \n## 21                                                                  \n## 22                                                                  \n## 23                                                                  \n## 24                                                                  \n## 25                                                                  \n## 26                                                                  \n## 27                                                                  \n## 28                                                                  \n## 29                                                                  \n## 30                                                                  \n## 31                                                                  \n## 32                                                                  \n## 33\n\n然后画图即可，默认的出图就已经很美观了:\n\nlibrary(forestploter)\nlibrary(grid)\n\np &lt;- forest(\n  data = plot_df[,c(1,6,7,3)],\n  lower = plot_df$conf.low,\n  upper = plot_df$conf.high,\n  est = plot_df$estimate,\n  ci_column = 3,\n  sizes = 1, \n  ref_line = 1, \n  xlim = c(0.1,4)\n  )\nprint(p)",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#亚组分析",
    "href": "亚组分析和多因素回归的森林图.html#亚组分析",
    "title": "36  亚组分析和多因素回归的森林图比较",
    "section": "36.3 亚组分析",
    "text": "36.3 亚组分析\n亚组分析的思路非常简单，就是在每一个亚组中进行分析，详细过程我们就不介绍了，大家可以参考之前的推文（不理解亚组分析怎么做的一定要看）：\n\nR语言亚组分析及森林图绘制\nR语言亚组分析1行代码实现！\n\n对于我们这个演示数据，它画出来的亚组分析的森林图是这样的（绘制代码参考上面两篇推文）：",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#比较",
    "href": "亚组分析和多因素回归的森林图.html#比较",
    "title": "36  亚组分析和多因素回归的森林图比较",
    "section": "36.4 比较",
    "text": "36.4 比较\n不知道看到这里你明白了没有，亚组分析是在所有数据的子集中做分析，在每一个亚组中都进行一次分析，每次分析都能得到一个HR值和可信区间，把所有结果放在一起，就得到森林图了。\n而多因素回归其实只是把分类变量进行哑变量编码而已，其中一个是参考，其余都和参考比，这样也能得到不同类别的HR值和可信区间。如果是数值型变量而不是分类变量不用进行哑变量编码了，自然也不会出现“亚组”的形式。\n虽有都有HR值、可信区间、P值等信息，但是表达的意思和实现方法确实去安全不同的！\n还有一个我没见过的形式：多因素分析+亚组分析的森林图，但是粉丝群里有群友问到过，意思是在每一个亚组内都做多因素分析，这样的森林图就要在每个亚组内展示多个HR和可信区间了。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#其他资源",
    "href": "亚组分析和多因素回归的森林图.html#其他资源",
    "title": "36  亚组分析和多因素回归的森林图比较",
    "section": "36.5 其他资源",
    "text": "36.5 其他资源\n亚组分析和森林图的内容还有非常多的细节问题，为了不影响该合集的主要内容，我把它们放在下面的链接中，大家感兴趣的话可点击下面的链接查看，或者在公众号后台回复亚组分析获取合集链接：\n\n使用R语言画森林图和误差线\nggplot2绘制森林图(有亚组和没亚组)\nR语言亚组分析及森林图绘制\nR语言亚组分析1行代码实现！\n亚组分析和多因素回归的森林图比较\n多列可信区间、多组别的森林图绘制\n逻辑回归亚组分析森林图绘制\n协变量调整的亚组分析和森林图绘制\n1行代码实现：多因素回归的亚组分析，并绘制森林图\nR语言亚组分析及森林图绘制手册",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html",
    "href": "9999-appendix.html",
    "title": "附录 A — 其他合集",
    "section": "",
    "text": "A.1 R语言零基础入门\n专门为编程零基础的医学生/医生等群体录制的R语言零基础入门视频教程，已放在B站，且配套文档、数据都是免费的，无任何套路。",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#r语言零基础入门",
    "href": "9999-appendix.html#r语言零基础入门",
    "title": "附录 A — 其他合集",
    "section": "",
    "text": "R语言零基础入门\n配套文档链接：R语言零基础入门配套文档",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#pass软件链接",
    "href": "9999-appendix.html#pass软件链接",
    "title": "附录 A — 其他合集",
    "section": "A.2 PASS软件链接",
    "text": "A.2 PASS软件链接\n关注公众号：医学和生信笔记，后台回复PASS即可获得软件链接。",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#临床预测模型",
    "href": "9999-appendix.html#临床预测模型",
    "title": "附录 A — 其他合集",
    "section": "A.3 临床预测模型",
    "text": "A.3 临床预测模型\n临床预测模型合集：临床预测模型\n在线版电子书：R语言实战临床预测模型",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#机器学习",
    "href": "9999-appendix.html#机器学习",
    "title": "附录 A — 其他合集",
    "section": "A.4 机器学习",
    "text": "A.4 机器学习\n医学和生信笔记后台回复caret即可获取caret包的合集教程；回复tidymodels即可获取tidymodels的合集教程；回复mlr3即可获取mlr3合集教程，回复机器学习即可获取机器学习推文合集。\nR语言机器学习合集：R语言机器学习\n在线版电子书：R语言实战机器学习\n本号很少涉及理论知识，主要还是R语言实战，所以理论部分大家需要自己多多学习。",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#生信数据挖掘",
    "href": "9999-appendix.html#生信数据挖掘",
    "title": "附录 A — 其他合集",
    "section": "A.5 生信数据挖掘",
    "text": "A.5 生信数据挖掘\n生信数据挖掘合集：生信数据挖掘\n医学和生信笔记公众号所有关于生信数据挖掘的推文都可以免费下载使用，请看：“灌水”生信类文章会用到哪些生信下游分析？（附下载地址）\ngithub地址：R语言生信数据挖掘",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#扫码关注",
    "href": "9999-appendix.html#扫码关注",
    "title": "附录 A — 其他合集",
    "section": "A.6 扫码关注",
    "text": "A.6 扫码关注\n欢迎扫码关注：医学和生信笔记",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R语言实战医学统计",
    "section": "",
    "text": "前言",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#本书缘起",
    "href": "index.html#本书缘起",
    "title": "R语言实战医学统计",
    "section": "本书缘起",
    "text": "本书缘起\nR语言是一门编程语言，但同时也是一个统计软件，R语言是由统计学家开发的，所以天生就适合做统计。\n很多刚接触R语言的朋友不知道如何入手，只知道目前R语言在临床医学领域很火爆，做统计分析、画图、做生信分析、孟德尔随机化、数据库挖掘等都离不开R语言。\n万事开头难，我非常理解新手面对R语言的痛苦，因为我也是从0开始的，作为从未接触过编程的医学生/医生来说，初学R语言简直就是读天书！我最开始接触R语言是因为偶然间听师兄师姐说R语言可以做统计学，当时的我对SPSS的使用不熟练，觉得SPSS的使用步骤太多，难以记住，于是入了R语言的坑…没想到从此一发不可收拾，打开了新世界的大门。\n这个系列也是我最开始学习R语言时的笔记，在我的公众号：医学和生信笔记，都可以找到，现在对原内容进行重新整理，并把数据一起打包，方便有需要的同学学习。\n\n\n\n\n\n\n提醒\n\n\n\n本书不适合R语言零基础的人。 如果你是刚入门的小白，我首先推荐你了解下R语言的基础知识，比如R语言和R包安装、Rstudio的界面、R语言中数据类型（向量、矩阵、数据框、列表等）、R语言中的数据导入导出、R语言的基础数据操作等。\n\n\n我结合自己学习R语言时的经验，也专门为编程零基础的医学生/医生等群体录制了R语言零基础入门的视频教程，已放在B站，且配套文档、数据都是免费的，无任何套路。各种在初学R时遇到的“坑”，我都替你踩过了，并且也在视频中指出来了。强烈建议没接触过R语言的朋友先去了解下基础知识，切勿直接上手实操！\n然后你就可以跟着本系列一起学习R语言在医学统计学中的使用。这个系列非常适合初学者，因为有很多内容是按照课本来的，尤其是基础统计分析部分，完全使用R语言复现课本中的例题，得到结果后可以与课本对照！我使用的课本是孙振球主编的《医学统计学》第4版以及第5版，封面如下：\n\n\n\n\n\n\n\n\n\n第4版\n\n\n\n\n\n\n\n第5版\n\n\n\n\n\n由于R和SPSS在进行统计分析时的一些数学计算方面并不是完全一致，所以导致有些结果和课本中的结果有些出入，但是并不影响结果的正确性。\n本系列还有配套的视频教程，也在B站，免费观看，点击直达：R语言实战医学统计\n\n\n\n\n\n\n注意\n\n\n\n本书实际上是我公众号部分历史推文的整理和汇总（部分内容有改动），书中涉及的所有数据都可以在相关历史推文中免费获取！推文合集链接：医学统计学\n我也准备了一个PDF版合集(附赠Word版)，内容和网页版一致，只是打包了所有的数据，付费获取（10元，1次付费，长期更新，无需重复付费），介意勿扰！PDF版合集获取链接：R语言实战医学统计\n\n\n限于本人水平等问题，难免会有一些错误，欢迎大家以各种方式批评指正，比如公众号留言、粉丝QQ群、github、个人微信等。\n本书会不定期更新，内容和格式都会不断完善。\n更新日志：\n\n20251128：增加新内容：广义估计方程；增加大量对专有名词和结果的解释说明；增加大量题干内容；修改github中网友提出的错误；优化章节结构和其他小细节修改；\n20250902：增加新内容：多变量数据的统计分析、对数线性模型、泊松回归和负二项回归、验证性因子分析、结构方程模型、多水平模型、主成分回归；其他细节小修改；\n20241018：本次更新是一次大更新！\n\n首先是细节修改，错别字改正，并修正一些错误内容，主要涉及以下章节：t检验、方差分析（所有方差分析内容皆有改动）、卡方检验、秩和检验、双变量回归与相关等；\n新增“三线表和统计绘图”一章，对应课本第十章：统计图和统计表；\n精简一些和医学统计关联性较小的内容，把这部分内容放在参考链接中；\n合并一些章节的内容，比如ROC曲线和三线表等；\n重新安排章节顺序，和课本中的顺序更加对应；\n增加配套的视频教程（b站：阿越就是我：R语言实战医学统计）！\n\n20231230：全部内容从Rmarkdown改为quarto；增加三线表内容\n20230905：\n\n格式升级，改为3列式；\nRCS增加推荐阅读；\nt检验增加正态性检验和方差齐性检验；\ntidy风格医学统计增加秩和检验和计数资料统计分析；\n增加亚组分析和森林图绘制；\n\n20230612：改正样条回归中的一个笔误（age的Nonlinear的P&lt;0.05……）\n20230407：首次上传",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#作者简介",
    "href": "index.html#作者简介",
    "title": "R语言实战医学统计",
    "section": "作者简介",
    "text": "作者简介\n\n阿越，外科医生，R语言爱好者，长期分享R语言和医学统计学、临床预测模型、生信数据挖掘、R语言机器学习等知识。\n公众号：医学和生信笔记\n哔哩哔哩：阿越就是我\n知乎：医学和生信笔记\nCSDN：医学和生信笔记\nGithub：ayueme",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "1001-ttest.html",
    "href": "1001-ttest.html",
    "title": "1  t检验",
    "section": "",
    "text": "1.1 单样本t检验\n使用课本例3-5的数据。\n首先是读取数据，可以自己录入，也可以使用课本光盘里的数据，我这里直接使用了光盘里的数据。\n# 使用foreign包读取SPSS数据\nlibrary(foreign)\ndf &lt;- read.spss('datasets/例03-05.sav',to.data.frame = T)\n\nhead(df)\n##   no  hb\n## 1  1 112\n## 2  2 137\n## 3  3 129\n## 4  4 126\n## 5  5  88\n## 6  6  90\n进行单样本t检验，与140进行比较：\nst &lt;- t.test(df$hb,mu=140,alternative = 'two.sided') # 双侧检验\n\nst\n## \n##  One Sample t-test\n## \n## data:  df$hb\n## t = -2.1367, df = 35, p-value = 0.03969\n## alternative hypothesis: true mean is not equal to 140\n## 95 percent confidence interval:\n##  122.1238 139.5428\n## sample estimates:\n## mean of x \n##  130.8333\n结果显示t=-2.1367，自由度df=35，p=0.03969,和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t检验</span>"
    ]
  },
  {
    "objectID": "1001-ttest.html#配对样本t检验",
    "href": "1001-ttest.html#配对样本t检验",
    "title": "1  t检验",
    "section": "1.2 配对样本t检验",
    "text": "1.2 配对样本t检验\n使用课本例3-6的数据，首先是读取数据。\n\nlibrary(foreign)\ndf &lt;- read.spss('datasets/例03-06.sav',to.data.frame = T)\n\nhead(df)\n##   no    x1    x2\n## 1  1 0.840 0.580\n## 2  2 0.591 0.509\n## 3  3 0.674 0.500\n## 4  4 0.632 0.316\n## 5  5 0.687 0.337\n## 6  6 0.978 0.517\n\n数据一共3列10行，第1列是样本编号，第2列和第3列是要比较的值。\n进行配对样本t检验：\n\npt &lt;- t.test(df$x1,df$x2,paired = T,var.equal = T)\npt\n## \n##  Paired t-test\n## \n## data:  df$x1 and df$x2\n## t = 7.926, df = 9, p-value = 2.384e-05\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  0.1946542 0.3501458\n## sample estimates:\n## mean difference \n##          0.2724\n\n结果显示t=7.926，自由度df=9，p&lt;0.001，结果和课本一致。\n如果数据格式是两列数据，R语言也支持formula的形式。\n首先我们要把数据变成formula形式需要的格式，也就是长数据：\n\nsuppressMessages(library(tidyverse))\n\ndf.l &lt;- df |&gt;\n  pivot_longer(2:3,names_to = \"group\",values_to = \"x\")\n\n# 数据一列是分组，另一列是数值，还有一列id没什么用\ndf.l\n## # A tibble: 20 × 3\n##       no group     x\n##    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n##  1     1 x1    0.84 \n##  2     1 x2    0.58 \n##  3     2 x1    0.591\n##  4     2 x2    0.509\n##  5     3 x1    0.674\n##  6     3 x2    0.5  \n##  7     4 x1    0.632\n##  8     4 x2    0.316\n##  9     5 x1    0.687\n## 10     5 x2    0.337\n## 11     6 x1    0.978\n## 12     6 x2    0.517\n## 13     7 x1    0.75 \n## 14     7 x2    0.454\n## 15     8 x1    0.73 \n## 16     8 x2    0.512\n## 17     9 x1    1.2  \n## 18     9 x2    0.997\n## 19    10 x1    0.87 \n## 20    10 x2    0.506\n\n这时可以用formula形式：\n\nt.test(x ~ group, data = df.l, paired = T, var.equal = T)\n\n    Two Sample t-test\n\ndata:  x by group\nt = 3.2894, df = 18, p-value = 0.004076\nalternative hypothesis: true difference in means between group x1 and group x2 is not equal to 0\n95 percent confidence interval:\n 0.09841834 0.44638166\nsample estimates:\nmean in group x1 mean in group x2 \n          0.7952           0.5228 \n\n结果也是一模一样的。\n\n\n\n\n\n\n警告\n\n\n\nt.test在R4.4.0及以后不再支持在公式形式中使用paired参数，所以如果你的R版本在4.4.0及以后的，在公式形式中使用paired参数会得到以下报错：\nError in t.test.formula(x ~ group, data = df.l, paired = T, var.equal = T) : \n  cannot use 'paired' in formula method\n逆天更新！！\n\n\n\n在R语言中，绝大多数统计检验函数都是支持多种形式的输入样式的，需要自己注意。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t检验</span>"
    ]
  },
  {
    "objectID": "1001-ttest.html#两样本t检验",
    "href": "1001-ttest.html#两样本t检验",
    "title": "1  t检验",
    "section": "1.3 两样本t检验",
    "text": "1.3 两样本t检验\n使用课本例3-7的数据。\n首先是读取数据.\n\nlibrary(foreign)\ndf &lt;- read.spss('datasets/例03-07.sav',to.data.frame = T)\ndf$group &lt;- c(rep('阿卡波糖',20),rep('拜糖平',20))\nattributes(df)[3] &lt;- NULL\n\nhead(df)\n##   no    x    group\n## 1  1 -0.7 阿卡波糖\n## 2  2 -5.6 阿卡波糖\n## 3  3  2.0 阿卡波糖\n## 4  4  2.8 阿卡波糖\n## 5  5  0.7 阿卡波糖\n## 6  6  3.5 阿卡波糖\n\n一共有3列40行，第1列是编号，第2列是血糖值，第3列是组别（阿卡波糖组和拜糖平组，每组20个人）。\n进行两样本t检验：\n\ntt &lt;- t.test(x ~ group, data = df, var.equal = T)\ntt\n## \n##  Two Sample t-test\n## \n## data:  x by group\n## t = -0.64187, df = 38, p-value = 0.5248\n## alternative hypothesis: true difference in means between group 阿卡波糖 and group 拜糖平 is not equal to 0\n## 95 percent confidence interval:\n##  -2.326179  1.206179\n## sample estimates:\n## mean in group 阿卡波糖   mean in group 拜糖平 \n##                  2.065                  2.625\n\n结果显示t=-0.64187，自由度df=38，p=0.5248，结果和课本一致。\n假如两样本方差不等，可以使用近似t检验，或者使用Mann-Whitney-U检验。课本中给出了3种近似t检验的方法（例3-8），而t.test只能给出Welch校正的结果。\n课本中的例3-8只给出了均值和方差，没给原始数据，所以没法复现其结果。下面使用例3-7的数据演示下如何实现近似t检验：\n\ntt &lt;- t.test(x ~ group, data = df, var.equal = F)#这里指定方差不等即可\ntt\n## \n##  Welch Two Sample t-test\n## \n## data:  x by group\n## t = -0.64187, df = 36.086, p-value = 0.525\n## alternative hypothesis: true difference in means between group 阿卡波糖 and group 拜糖平 is not equal to 0\n## 95 percent confidence interval:\n##  -2.32926  1.20926\n## sample estimates:\n## mean in group 阿卡波糖   mean in group 拜糖平 \n##                  2.065                  2.625",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t检验</span>"
    ]
  },
  {
    "objectID": "1001-ttest.html#正态性检验和两样本方差比较的f检验",
    "href": "1001-ttest.html#正态性检验和两样本方差比较的f检验",
    "title": "1  t检验",
    "section": "1.4 正态性检验和两样本方差比较的F检验",
    "text": "1.4 正态性检验和两样本方差比较的F检验\n\n1.4.1 正态性检验\n例3-9\n\n# 使用foreign包读取SPSS数据\nlibrary(foreign)\ndf &lt;- read.spss('datasets/例03-01.sav',to.data.frame = T)\n\nhead(df)\n##   no   mean   sd\n## 1  1 167.41 2.74\n## 2  2 165.56 6.57\n## 3  3 168.20 5.36\n## 4  4 166.67 4.81\n## 5  5 164.89 5.41\n## 6  6 166.36 4.50\n\n进行Shapiro-Wilk正态性检验：\n\nshapiro.test(df$mean)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  df$mean\n## W = 0.99409, p-value = 0.9444\n\n这个结果和SPSS的结果是一样的。\n计算偏度和峰度：\n\nlibrary(moments) # psych\n\nskewness(df$mean)\n## [1] 0.1423707\nkurtosis(df$mean)\n## [1] 3.045566\n\n# 偏度和峰度检验\nagostino.test(df$mean)\n## \n##  D'Agostino skewness test\n## \n## data:  df$mean\n## skew = 0.14237, z = 0.61614, p-value = 0.5378\n## alternative hypothesis: data have a skewness\nanscombe.test(df$mean)\n## \n##  Anscombe-Glynn kurtosis test\n## \n## data:  df$mean\n## kurt = 3.04557, z = 0.41992, p-value = 0.6745\n## alternative hypothesis: kurtosis is not equal to 3\n\n\n\n1.4.2 方差齐性检验\n课本中的例3-11没给数据（使用了例3-8的数据，但是只给了均值和方差，没给原始数据），所以我们用例3-7的数据演示F检验。\n\nlibrary(foreign)\ndf &lt;- read.spss('datasets/例03-07.sav',to.data.frame = T)\ndf$group &lt;- c(rep('阿卡波糖',20),rep('拜糖平',20))\nattributes(df)[3] &lt;- NULL\n\nhead(df)\n##   no    x    group\n## 1  1 -0.7 阿卡波糖\n## 2  2 -5.6 阿卡波糖\n## 3  3  2.0 阿卡波糖\n## 4  4  2.8 阿卡波糖\n## 5  5  0.7 阿卡波糖\n## 6  6  3.5 阿卡波糖\n\n进行F检验：\n\nvar.test(x ~ group, data = df)\n## \n##  F test to compare two variances\n## \n## data:  x by group\n## F = 1.5984, num df = 19, denom df = 19, p-value = 0.3153\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 95 percent confidence interval:\n##  0.6326505 4.0381795\n## sample estimates:\n## ratio of variances \n##           1.598361\n\np大于0.05，不拒绝原假设，可以认为方差齐。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t检验</span>"
    ]
  },
  {
    "objectID": "1002-anova.html",
    "href": "1002-anova.html",
    "title": "2  多样本均数比较的方差分析",
    "section": "",
    "text": "2.1 完全随机设计资料的方差分析\n使用课本例4-2的数据。\n首先是构造数据，本次数据自己从书上摘录。\ntrt&lt;-c(rep(\"group1\",30),rep(\"group2\",30),rep(\"group3\",30),rep(\"group4\",30))\n\nweight&lt;-c(3.53,4.59,4.34,2.66,3.59,3.13,3.30,4.04,3.53,3.56,3.85,4.07,1.37,\n          3.93,2.33,2.98,4.00,3.55,2.64,2.56,3.50,3.25,2.96,4.30,3.52,3.93,\n          4.19,2.96,4.16,2.59,2.42,3.36,4.32,2.34,2.68,2.95,2.36,2.56,2.52,\n          2.27,2.98,3.72,2.65,2.22,2.90,1.98,2.63,2.86,2.93,2.17,2.72,1.56,\n          3.11,1.81,1.77,2.80,3.57,2.97,4.02,2.31,2.86,2.28,2.39,2.28,2.48,\n          2.28,3.48,2.42,2.41,2.66,3.29,2.70,2.66,3.68,2.65,2.66,2.32,2.61,\n          3.64,2.58,3.65,3.21,2.23,2.32,2.68,3.04,2.81,3.02,1.97,1.68,0.89,\n          1.06,1.08,1.27,1.63,1.89,1.31,2.51,1.88,1.41,3.19,1.92,0.94,2.11,\n          2.81,1.98,1.74,2.16,3.37,2.97,1.69,1.19,2.17,2.28,1.72,2.47,1.02,\n          2.52,2.10,3.71)\n\ndata1&lt;-data.frame(trt,weight)\n\nhead(data1)\n##      trt weight\n## 1 group1   3.53\n## 2 group1   4.59\n## 3 group1   4.34\n## 4 group1   2.66\n## 5 group1   3.59\n## 6 group1   3.13\n数据一共两列，第一列是分组（一共四组），第二列是低密度脂蛋白测量值。\n先简单看下数据分布：\nboxplot(weight ~ trt, data = data1)\n进行完全随机设计资料的方差分析，或者叫单因素方差分析（one-factor ANOVA）：\nfit &lt;- aov(weight ~ trt, data = data1)\nsummary(fit)\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## trt           3  32.16  10.719   24.88 1.67e-12 ***\n## Residuals   116  49.97   0.431                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n结果显示组间自由度为3，组内自由度为116，组间离均差平方和为32.16，组内离均差平方和为49.97，组间均方为10.719，组内均方为0.431，F值=24.88，p=1.67e-12，和课本一致。\n再简单介绍一下可视化的平均数和可信区间的方法：\nlibrary(gplots)\nplotmeans(weight~trt,xlab = \"treatment\",ylab = \"weight\",\n          main=\"mean plot\\nwith95% CI\")",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "1002-anova.html#随机区组设计资料的方差分析",
    "href": "1002-anova.html#随机区组设计资料的方差分析",
    "title": "2  多样本均数比较的方差分析",
    "section": "2.2 随机区组设计资料的方差分析",
    "text": "2.2 随机区组设计资料的方差分析\n使用例4-4的数据。\n首先是构造数据，本次数据自己从书上摘录。。\n\nweight &lt;- c(0.82,0.65,0.51,0.73,0.54,0.23,0.43,0.34,0.28,0.41,0.21,\n            0.31,0.68,0.43,0.24)\nblock &lt;- c(rep(c(\"1\",\"2\",\"3\",\"4\",\"5\"),each=3))\ngroup &lt;- c(rep(c(\"A\",\"B\",\"C\"),5))\n\ndata4_4 &lt;- data.frame(weight,block,group)\n\nhead(data4_4)\n##   weight block group\n## 1   0.82     1     A\n## 2   0.65     1     B\n## 3   0.51     1     C\n## 4   0.73     2     A\n## 5   0.54     2     B\n## 6   0.23     2     C\n\n数据一共3列，第一列是小白鼠肉瘤重量，第二列是区组因素（5个区组），第三列是分组（一共3组）\n进行随机区组设计资料的方差分析：\n\nfit &lt;- aov(weight ~ block + group,data = data4_4)#随机区组设计方差分析，注意顺序\nsummary(fit)\n##             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \n## block        4 0.2284 0.05709   5.978 0.01579 * \n## group        2 0.2280 0.11400  11.937 0.00397 **\n## Residuals    8 0.0764 0.00955                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果显示区组间自由度为4，分组间自由度为2，组内自由度为8，区组间离均差平方和为0.2284，分组间离均差平方和为0.2280，组内离均差平方和为0.0764，区组间均方为0.05709，分组间均方为0.1140，组内均方为0.00955，区组间F值=5.798，p=0.01579，分组间F值=11.937，p=0.00397，和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "1002-anova.html#拉丁方设计方差分析",
    "href": "1002-anova.html#拉丁方设计方差分析",
    "title": "2  多样本均数比较的方差分析",
    "section": "2.3 拉丁方设计方差分析",
    "text": "2.3 拉丁方设计方差分析\n使用课本例4-5的数据。\n首先是构造数据，本次数据自己从书上摘录。\n\npsize &lt;- c(87,75,81,75,84,66,73,81,87,85,64,79,73,73,74,78,73,77,77,68,69,74,76,73,\n           64,64,72,76,70,81,75,77,82,61,82,61)\ndrug &lt;- c(\"C\",\"B\",\"E\",\"D\",\"A\",\"F\",\"B\",\"A\",\"D\",\"C\",\"F\",\"E\",\"F\",\"E\",\"B\",\"A\",\"D\",\"C\",\n          \"A\",\"F\",\"C\",\"B\",\"E\",\"D\",\"D\",\"C\",\"F\",\"E\",\"B\",\"A\",\"E\",\"D\",\"A\",\"F\",\"C\",\"B\")\ncol_block &lt;- c(rep(1:6,6))\nrow_block &lt;- c(rep(1:6,each=6))\nmydata &lt;- data.frame(psize,drug,col_block,row_block)\nmydata$col_block &lt;- factor(mydata$col_block)\nmydata$row_block &lt;- factor(mydata$row_block)\nstr(mydata)\n## 'data.frame':    36 obs. of  4 variables:\n##  $ psize    : num  87 75 81 75 84 66 73 81 87 85 ...\n##  $ drug     : chr  \"C\" \"B\" \"E\" \"D\" ...\n##  $ col_block: Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 1 2 3 4 ...\n##  $ row_block: Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 2 2 2 2 ...\n\n数据一共4列，第一列是皮肤疱疹大小，第二列是不同药物（处理因素，共6种），第三列是列区组因素，第四列是行区组因素。\n进行拉丁方设计的方差分析：\n\nfit &lt;- aov(psize ~ drug + row_block + col_block, data = mydata)\nsummary(fit)\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## drug         5  667.1  133.43   3.906 0.0124 *\n## row_block    5  250.5   50.09   1.466 0.2447  \n## col_block    5   85.5   17.09   0.500 0.7723  \n## Residuals   20  683.2   34.16                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果显示行区组间自由度为5，列区组间自由度为5，分组（处理因素）间自由度为5，组内自由度为20； 行区组间离均差平方和为250.5，列区组间离均差平方和为85.5，分组间离均差平方和为667.1，组内离均差平方和为0.0683.2； 行区组间均方为50.09，列区组间均方为17.09，分组间均方为133.43，组内均方为34.16， 行区组间F值=1.466，p=0.2447，列区组间F值=0.5，p=0.7723，分组间F值=3.906，p=0.0124，和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "1002-anova.html#两阶段交叉设计资料方差分析",
    "href": "1002-anova.html#两阶段交叉设计资料方差分析",
    "title": "2  多样本均数比较的方差分析",
    "section": "2.4 两阶段交叉设计资料方差分析",
    "text": "2.4 两阶段交叉设计资料方差分析\n使用课本例4-6的数据。\n首先是构造数据，本次数据自己从书上摘录。\n\ncontain &lt;- c(760,770,860,855,568,602,780,800,960,958,940,952,635,650,440,450,\n             528,530,800,803)\nphase &lt;- rep(c(\"phase_1\",\"phase_2\"),10)\ntype &lt;- c(\"A\",\"B\",\"B\",\"A\",\"A\",\"B\",\"A\",\"B\",\"B\",\"A\",\"B\",\"A\",\"A\",\"B\",\"B\",\"A\",\n          \"A\",\"B\",\"B\",\"A\")\ntestid &lt;- rep(1:10,each=2)\nmydata &lt;- data.frame(testid,phase,type,contain)\n\nstr(mydata)\n## 'data.frame':    20 obs. of  4 variables:\n##  $ testid : int  1 1 2 2 3 3 4 4 5 5 ...\n##  $ phase  : chr  \"phase_1\" \"phase_2\" \"phase_1\" \"phase_2\" ...\n##  $ type   : chr  \"A\" \"B\" \"B\" \"A\" ...\n##  $ contain: num  760 770 860 855 568 602 780 800 960 958 ...\n\nmydata$testid &lt;- factor(mydata$testid)\n\n数据一共4列，第一列是受试者id，第二列是不同阶段，第三列是测定方法，第四列是测量值。\n简单看下2个阶段情况：\n\ntable(mydata$phase,mydata$type)\n##          \n##           A B\n##   phase_1 5 5\n##   phase_2 5 5\n\n进行两阶段交叉设计资料方差分析：\n\nfit &lt;- aov(contain~phase+type+testid,mydata)\nsummary(fit)\n##             Df Sum Sq Mean Sq  F value   Pr(&gt;F)    \n## phase        1    490     490    9.925   0.0136 *  \n## type         1    198     198    4.019   0.0799 .  \n## testid       9 551111   61235 1240.195 1.32e-11 ***\n## Residuals    8    395      49                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果和课本一致！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "1002-anova.html#多个样本均数间的多重比较",
    "href": "1002-anova.html#多个样本均数间的多重比较",
    "title": "2  多样本均数比较的方差分析",
    "section": "2.5 多个样本均数间的多重比较",
    "text": "2.5 多个样本均数间的多重比较\n课本例4-7，使用了课本例4-2的数据。\n首先是构造数据，本次数据自己从书上摘录。\n\ntrt&lt;-c(rep(\"group1\",30),rep(\"group2\",30),rep(\"group3\",30),rep(\"group4\",30))\n\nweight&lt;-c(3.53,4.59,4.34,2.66,3.59,3.13,3.30,4.04,3.53,3.56,3.85,4.07,1.37,\n          3.93,2.33,2.98,4.00,3.55,2.64,2.56,3.50,3.25,2.96,4.30,3.52,3.93,\n          4.19,2.96,4.16,2.59,2.42,3.36,4.32,2.34,2.68,2.95,2.36,2.56,2.52,\n          2.27,2.98,3.72,2.65,2.22,2.90,1.98,2.63,2.86,2.93,2.17,2.72,1.56,\n          3.11,1.81,1.77,2.80,3.57,2.97,4.02,2.31,2.86,2.28,2.39,2.28,2.48,\n          2.28,3.48,2.42,2.41,2.66,3.29,2.70,2.66,3.68,2.65,2.66,2.32,2.61,\n          3.64,2.58,3.65,3.21,2.23,2.32,2.68,3.04,2.81,3.02,1.97,1.68,0.89,\n          1.06,1.08,1.27,1.63,1.89,1.31,2.51,1.88,1.41,3.19,1.92,0.94,2.11,\n          2.81,1.98,1.74,2.16,3.37,2.97,1.69,1.19,2.17,2.28,1.72,2.47,1.02,\n          2.52,2.10,3.71)\n\ndata1&lt;-data.frame(trt,weight)\ndata1$trt &lt;- factor(data1$trt)\n\nstr(data1)\n## 'data.frame':    120 obs. of  2 variables:\n##  $ trt   : Factor w/ 4 levels \"group1\",\"group2\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ weight: num  3.53 4.59 4.34 2.66 3.59 3.13 3.3 4.04 3.53 3.56 ...\n\n数据一共两列，第一列是分组（一共四组），第二列是低密度脂蛋白测量值。\n进行完全随机设计资料的方差分析：\n\nfit &lt;- aov(weight ~ trt, data = data1)\nsummary(fit)\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## trt           3  32.16  10.719   24.88 1.67e-12 ***\n## Residuals   116  49.97   0.431                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n2.5.1 LSD-t检验\n使用超级全能的PMCMRplus包实现，需要自己安装。\n\nlibrary(PMCMRplus)\n\nres &lt;- lsdTest(fit)\n# lsdTest(weight ~ trt, data = data1) 也可以\n\nsummary(res)\n##                      t value   Pr(&gt;|t|)    \n## group2 - group1 == 0  -4.219 4.8872e-05 ***\n## group3 - group1 == 0  -4.322 3.2889e-05 ***\n## group4 - group1 == 0  -8.639 3.5772e-14 ***\n## group3 - group2 == 0  -0.102    0.91871    \n## group4 - group2 == 0  -4.420 2.2345e-05 ***\n## group4 - group3 == 0  -4.318 3.3397e-05 ***\n\n结果比SPSS的结果更加直接，给出了统计量和P值，可以非常直观的看出哪两个组之间有差别。group2和group1的t值是-4.219，和课本的-4.18略有差别，问题不大。\n从结果中可知：group2和 group3是没差别的，和另外两组有差别。\n还可以可视化结果（字母相同的是没差别的）：\n\nplot(res)\n\n\n\n\n\n\n\n\n\n\n2.5.2 TukeyHSD\n这里介绍一种 TukeyHSD方法：\n\nTukeyHSD(fit) ### 每个组之间进行比较,多重比较\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = weight ~ trt, data = data1)\n## \n## $trt\n##                      diff        lwr        upr     p adj\n## group2-group1 -0.71500000 -1.1567253 -0.2732747 0.0002825\n## group3-group1 -0.73233333 -1.1740587 -0.2906080 0.0001909\n## group4-group1 -1.46400000 -1.9057253 -1.0222747 0.0000000\n## group3-group2 -0.01733333 -0.4590587  0.4243920 0.9996147\n## group4-group2 -0.74900000 -1.1907253 -0.3072747 0.0001302\n## group4-group3 -0.73166667 -1.1733920 -0.2899413 0.0001938\n\n这个结果更直观，可以直接看到每个组之间的比较，后面给出了P值。\n可视化结果：\n\npar(las=2)#旋转纵坐标标签\npar(mar=c(5,8,4,2))#设置四个画图边距\nplot(TukeyHSD(fit))\n\n\n\n\n\n\n\n\n图形中置信区间包含0的疗法说明差异不显著。\n\n\n2.5.3 Dunnett-t检验\n使用超级全能的PMCMRplus包实现\n\nlibrary(PMCMRplus)\n\nres &lt;- dunnettTest(fit)\n# 或者 dunnettTest(weight ~ trt, data = data1)\n\nsummary(res)\n##                      t value   Pr(&gt;|t|)    \n## group2 - group1 == 0  -4.219 0.00012148 ***\n## group3 - group1 == 0  -4.322 0.00010083 ***\n## group4 - group1 == 0  -8.639 1.4655e-14 ***\n\n结果也是非常明显，所有组和安慰剂组相比都有意义（3个t值和课本也是略有差别，问题不大）。\n可视化结果：\n\nplot(res)\n\n\n\n\n\n\n\n\nDunnett-t检验用于g-1个实验组和一个对照组的均数差别的多重比较，所以从上图看：group1和其他3组都是有差别的。\n\n\n2.5.4 SNK-q检验\n课本例4-9，使用了例4-4的数据。\n还是使用超级全能的PMCMRplus包实现。\n\nlibrary(PMCMRplus)\n\n# 需要把字符型变成因子型\n#data4_4$block &lt;- factor(data4_4$block)#没用到区组\ndata4_4$group &lt;- factor(data4_4$group)\n\nfit &lt;- aov(weight ~ group,data = data4_4)\nres &lt;- snkTest(fit)\n\nsummary(res)\n##            q value Pr(&gt;|q|)  \n## B - A == 0  -2.526 0.099390 .\n## C - A == 0  -4.209 0.028913 *\n## C - B == 0  -1.684 0.256834\n#summaryGroup(res)\n\n这个结果也很直观，可以直接看到每个组之间的比较，给出了q值和P值(但是结果和课本不一样，试了多种方法，q值全都不一样)。\n可视化结果：\n\nplot(res)",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "1002-anova.html#多样本方差比较的bartlett检验和levene检验",
    "href": "1002-anova.html#多样本方差比较的bartlett检验和levene检验",
    "title": "2  多样本均数比较的方差分析",
    "section": "2.6 多样本方差比较的Bartlett检验和Levene检验",
    "text": "2.6 多样本方差比较的Bartlett检验和Levene检验\n\n2.6.1 多样本方差比较的Bartlett检验\n课本例4-10，使用课本例4-2的数据。\n\ntrt&lt;-c(rep(\"group1\",30),rep(\"group2\",30),rep(\"group3\",30),rep(\"group4\",30))\n\nweight&lt;-c(3.53,4.59,4.34,2.66,3.59,3.13,3.30,4.04,3.53,3.56,3.85,4.07,1.37,\n          3.93,2.33,2.98,4.00,3.55,2.64,2.56,3.50,3.25,2.96,4.30,3.52,3.93,\n          4.19,2.96,4.16,2.59,2.42,3.36,4.32,2.34,2.68,2.95,2.36,2.56,2.52,\n          2.27,2.98,3.72,2.65,2.22,2.90,1.98,2.63,2.86,2.93,2.17,2.72,1.56,\n          3.11,1.81,1.77,2.80,3.57,2.97,4.02,2.31,2.86,2.28,2.39,2.28,2.48,\n          2.28,3.48,2.42,2.41,2.66,3.29,2.70,2.66,3.68,2.65,2.66,2.32,2.61,\n          3.64,2.58,3.65,3.21,2.23,2.32,2.68,3.04,2.81,3.02,1.97,1.68,0.89,\n          1.06,1.08,1.27,1.63,1.89,1.31,2.51,1.88,1.41,3.19,1.92,0.94,2.11,\n          2.81,1.98,1.74,2.16,3.37,2.97,1.69,1.19,2.17,2.28,1.72,2.47,1.02,\n          2.52,2.10,3.71)\n\ndata1&lt;-data.frame(trt,weight)\ndata1$trt &lt;- factor(data1$trt)\n\nstr(data1)\n## 'data.frame':    120 obs. of  2 variables:\n##  $ trt   : Factor w/ 4 levels \"group1\",\"group2\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ weight: num  3.53 4.59 4.34 2.66 3.59 3.13 3.3 4.04 3.53 3.56 ...\n\n进行Bartlett检验：\n\nbartlett.test(weight ~ trt, data = data1)\n## \n##  Bartlett test of homogeneity of variances\n## \n## data:  weight by trt\n## Bartlett's K-squared = 5.2192, df = 3, p-value = 0.1564\n\n由结果可知，K-squared（卡方）=5.2192，P值为0.1564，不拒绝H0，还不能认为4个实验组的低密度脂蛋白值不满足方差齐性！\n\n\n2.6.2 多样本方差比较的Levene检验\n使用car包实现。\n\nlibrary(car)\n\nleveneTest(weight ~ trt, data = data1)\n## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value Pr(&gt;F)\n## group   3   1.493 0.2201\n##       116\n\n由结果可知，不能认为不满足方差齐性！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>多样本均数比较的方差分析</span>"
    ]
  },
  {
    "objectID": "几种离散型变量的分布及其应用.html",
    "href": "几种离散型变量的分布及其应用.html",
    "title": "3  几种离散型变量的分布及其应用",
    "section": "",
    "text": "3.1 二项分布\n又称：伯努利分布。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>几种离散型变量的分布及其应用</span>"
    ]
  },
  {
    "objectID": "几种离散型变量的分布及其应用.html#二项分布",
    "href": "几种离散型变量的分布及其应用.html#二项分布",
    "title": "3  几种离散型变量的分布及其应用",
    "section": "",
    "text": "3.1.1 总体率的区间估计\n例6-2。直接使用二项分布计算。13名输卵管结扎的育龄妇女术后，有6人受孕，求95%可信区间。\n\nbinom.test(x = 6, n = 13, conf.level = 0.95)\n## \n##  Exact binomial test\n## \n## data:  6 and 13\n## number of successes = 6, number of trials = 13, p-value = 1\n## alternative hypothesis: true probability of success is not equal to 0.5\n## 95 percent confidence interval:\n##  0.1922324 0.7486545\n## sample estimates:\n## probability of success \n##              0.4615385\n\n由结果可知，可信区间为(0.1922324,0.7486545)。\n例6-3。100人有55人有效，求可信区间。近似正态法，则95%可信区间为的计算方法类似于：均值±1.96*标准误\n按课本公式6-8计算Sp:\n\n# 计算Sp\nSp &lt;- sqrt((0.55 * (1-0.55)) / 100)\nSp\n## [1] 0.04974937\n\n计算95%可信区间：\n\n0.55+1.96*Sp\n## [1] 0.6475088\n0.55-1.96*Sp\n## [1] 0.4524912\n\n由结果可知，可信区间为(0.4524912,0.6475088)。\n或者直接使用prop.test()函数（结果略有不同，非常接近）：\n\nprop.test(x=55,n=100,correct = F)\n## \n##  1-sample proportions test without continuity correction\n## \n## data:  55 out of 100, null probability 0.5\n## X-squared = 1, df = 1, p-value = 0.3173\n## alternative hypothesis: true p is not equal to 0.5\n## 95 percent confidence interval:\n##  0.4524460 0.6438546\n## sample estimates:\n##    p \n## 0.55\n\n\n\n3.1.2 样本率和总体率的比较\n例6-4。使用直接法，或者精确二项检验。已知受孕率为0.55，10名育龄妇女有9人受孕，问A方法的受孕率是否高于B方法？\n\nbinom.test(x = 9, n = 10, p = 0.55, alternative = \"greater\")\n## \n##  Exact binomial test\n## \n## data:  9 and 10\n## number of successes = 9, number of trials = 10, p-value = 0.02326\n## alternative hypothesis: true probability of success is greater than 0.55\n## 95 percent confidence interval:\n##  0.6058367 1.0000000\n## sample estimates:\n## probability of success \n##                    0.9\n\n结论：拒绝H0，接受H1，可认为A方法的受孕率高于B方法。\n例6-5。使用直接法，或者精确二项检验。10人患病，9人有效，问甲、乙两种药物疗效是否不同？\n\nbinom.test(x=9, n=10, p=0.6, alternative = \"two.sided\")\n## \n##  Exact binomial test\n## \n## data:  9 and 10\n## number of successes = 9, number of trials = 10, p-value = 0.05865\n## alternative hypothesis: true probability of success is not equal to 0.6\n## 95 percent confidence interval:\n##  0.5549839 0.9974714\n## sample estimates:\n## probability of success \n##                    0.9\n\n结论：不拒绝H0，尚不能认为两种药物的疗效不同。\n例6-6。180名患者治愈117人，问新方法是否比常规方法好？这个是正态近似法，可以根据公式6-13计算u值，然后查表。\n或者直接用prop.test()：\n\nprop.test(x=117,n=180,p=0.45, alternative = \"greater\",correct = F)\n## \n##  1-sample proportions test without continuity correction\n## \n## data:  117 out of 180, null probability 0.45\n## X-squared = 29.091, df = 1, p-value = 3.453e-08\n## alternative hypothesis: true p is greater than 0.45\n## 95 percent confidence interval:\n##  0.5896943 1.0000000\n## sample estimates:\n##    p \n## 0.65\n\n结论：拒绝H0，接受H1，即新疗法比常规疗法效果好。\n\n\n3.1.3 两样本率的比较\n例6-7。研究颈椎病的发病有没有差异。\n准备数据，矩阵或者table格式：\n\nt67 &lt;- matrix(c(36,84,22,88), ncol = 2,byrow = T,\n              dimnames = list(c(\"male\",\"female\"),c(\"success\",\"failure\")))\nt67\n##        success failure\n## male        36      84\n## female      22      88\n\n进行两样本率的比较：\n\nprop.test(t67,correct = F)\n## \n##  2-sample test for equality of proportions without continuity correction\n## \n## data:  t67\n## X-squared = 3.0433, df = 1, p-value = 0.08107\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  -0.01095102  0.21095102\n## sample estimates:\n## prop 1 prop 2 \n##    0.3    0.2\n\n结论：不拒绝H0，尚不能认为该职业人群颈椎病的发病有性别差异。\n例6-8。分析家族集聚性。\n\nx &lt;- c(26,10,28,18)\np &lt;- c(0.13265,0.38235,0.36735,0.11765)\n\n进行卡方检验：\n\nchisq.test(x=x,p=p)\n## \n##  Chi-squared test for given probabilities\n## \n## data:  x\n## X-squared = 42.949, df = 3, p-value = 2.523e-09\n\n结论：拒绝H0，接受H1，认为此种疾病存在家族集聚性。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>几种离散型变量的分布及其应用</span>"
    ]
  },
  {
    "objectID": "几种离散型变量的分布及其应用.html#泊松分布",
    "href": "几种离散型变量的分布及其应用.html#泊松分布",
    "title": "3  几种离散型变量的分布及其应用",
    "section": "3.2 泊松分布",
    "text": "3.2 泊松分布\n描述小概率事件的发生规律。\n\n3.2.1 总体均数的区间估计\n例6-10。某工厂在环境监测中，对一实施技术改造的生产车间做空气中粉尘浓度的监测，1立升空气中测得粉尘粒子数为21，假定粉尘分布均匀，试估计该车间平均每立方空气中所含粉尘颗粒数的95%和99%可信区间。\n直接法，也就是精确泊松检验。\n\n# 95%可信区间\npoisson.test(x=21)\n## \n##  Exact Poisson test\n## \n## data:  21 time base: 1\n## number of events = 21, time base = 1, p-value &lt; 2.2e-16\n## alternative hypothesis: true event rate is not equal to 1\n## 95 percent confidence interval:\n##  12.99933 32.10073\n## sample estimates:\n## event rate \n##         21\n\n# 99%可信区间\npoisson.test(x=21, conf.level = 0.99)\n## \n##  Exact Poisson test\n## \n## data:  21 time base: 1\n## number of events = 21, time base = 1, p-value &lt; 2.2e-16\n## alternative hypothesis: true event rate is not equal to 1\n## 99 percent confidence interval:\n##  11.06923 35.94628\n## sample estimates:\n## event rate \n##         21\n\n例6-11。正态近似法。直接根据公式（6-18）计算。\n\n# 95%可信区间\n68-1.96*sqrt(68)\n## [1] 51.83743\n68+1.96*sqrt(68)\n## [1] 84.16257\n\n# 99%可信区间\n68-2.58*sqrt(68)\n## [1] 46.72477\n68-2.58*sqrt(68)\n## [1] 46.72477\n\n\n\n3.2.2 样本均数和总体均数的比较\n例6-12。直接法。探讨母亲吸烟是否会增加小孩先心病的发病风险。\n\npoisson.test(x = 4, T=120, r=0.008,alternative = \"greater\")\n## \n##  Exact Poisson test\n## \n## data:  4 time base: 120\n## number of events = 4, time base = 120, p-value = 0.01663\n## alternative hypothesis: true event rate is greater than 0.008\n## 95 percent confidence interval:\n##  0.01138599        Inf\n## sample estimates:\n## event rate \n## 0.03333333\n# 或者\n1-ppois(q=4-1,lambda = 120*0.008)\n## [1] 0.01663305\n\n结论：拒绝H0，接受H1，认为母亲吸烟会增加小孩先心病的发病风险。\n例6-13。正态近似法。\n\nprop.test(x = 123, n=25000, p=0.003,alternative = \"greater\",correct = F)\n## \n##  1-sample proportions test without continuity correction\n## \n## data:  123 out of 25000, null probability 0.003\n## X-squared = 30.812, df = 1, p-value = 1.421e-08\n## alternative hypothesis: true p is greater than 0.003\n## 95 percent confidence interval:\n##  0.004243748 1.000000000\n## sample estimates:\n##       p \n## 0.00492\n\n结论：拒绝H0，接受H1，认为有亲缘血统婚配关系的后代其精神发育不全的发生率高于一般人群。\n\n\n3.2.3 两个样本均数的比较（有问题）\n例6-14。两种纯净水分别抽检1ml，分别培养出大肠杆菌4个和7个，问有无差别？\n\npoisson.test(c(4,7),c(1,1))\n## \n##  Comparison of Poisson rates\n## \n## data:  c(4, 7) time base: c(1, 1)\n## count1 = 4, expected count1 = 5.5, p-value = 0.5488\n## alternative hypothesis: true rate ratio is not equal to 1\n## 95 percent confidence interval:\n##  0.1226664 2.2477580\n## sample estimates:\n## rate ratio \n##  0.5714286\n\n结论：不拒绝H0，尚不能认为有差别。\n例6-15。分析一种罕见的非传染性疾病发病的地域差异。\n\npoisson.test(c(32,12),c(4,3))\n## \n##  Comparison of Poisson rates\n## \n## data:  c(32, 12) time base: c(4, 3)\n## count1 = 32, expected count1 = 25.143, p-value = 0.04653\n## alternative hypothesis: true rate ratio is not equal to 1\n## 95 percent confidence interval:\n##  1.002761 4.264145\n## sample estimates:\n## rate ratio \n##          2\n\n结论：拒绝H0，接受H1，可认为该疾病的发病存在地域性差异。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>几种离散型变量的分布及其应用</span>"
    ]
  },
  {
    "objectID": "几种离散型变量的分布及其应用.html#负二项分布略",
    "href": "几种离散型变量的分布及其应用.html#负二项分布略",
    "title": "3  几种离散型变量的分布及其应用",
    "section": "3.3 负二项分布（略）",
    "text": "3.3 负二项分布（略）",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>几种离散型变量的分布及其应用</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html",
    "href": "1006-chisq.html",
    "title": "4  卡方检验",
    "section": "",
    "text": "4.1 不同类型卡方检验的选择\n课本中关于四格表资料的卡方检验的方法选择以及RxC表资料的检验方法选择做了非常好的总结，在这里一并和大家分享一下：\n四格表资料的方法选择：\nR×C表资料的分类及其检验方法的选择：\nR×C表资料可以分为双向无序、单向有序、双向有序属性相同和双向有序属性不同4类。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#不同类型卡方检验的选择",
    "href": "1006-chisq.html#不同类型卡方检验的选择",
    "title": "4  卡方检验",
    "section": "",
    "text": "当 n(样本量)≥40 且所有的T(期望频数)≥5时，用χ2检验的基本公式或四格表资料之χ2检验的专用公式；当P ≈ α时，改用四格表资料的 Fisher 确切概率法；\n当 n≥40 但有 1≤T&lt;5 时，用四格表资料χ2检验的校正公式，或改用四格表资料的 Fisher 确切概率法。\n当 n&lt;40，或 T&lt;1时，用四格表资料的 Fisher 确切概率法。\n\n\n\n\n双向无序R×C表资料 R×C表资料中两个分类变量皆为无序分类变量对于该类资料，若研究目的为多个样本率（或构成比）的比较，可用行×列表资料的χ2检验：若研究目的为分析两个分类变量之间有无关联性以及关系的密切程度时，可用行×列表资料的χ2检验以及Pearson列联系数进行分析。\n单向有序R×C表资料 有两种形式。一种是R×C表资料中的分组变量（如年龄）是有序的，而指标变量（如传染病的类型）是无序的。其研究目的通常是分析不同年龄组各种传染病的构成情况，此种单向有序R×C表资料可用行×列表资料的χ2检验进行分析。另一种情况是R×C表资料中的分组变量(如疗法)为无序的，而指标变量（如疗效按等级分组）是有序的。其研究目的为比较不同疗法的疗效，此种单向有序R×C表资料宜用秩转换的非参数检验进行分析。\n双向有序属性相同的R×C表资料 R×C表资料中的两个分类变量皆为有序且属性相同。实际上是配对四格表资料的扩展，即水平数≥3的配伍资料，如用两种检测方法同时对同一批样品的测定结果。其研究目的通常是分析两种检测方法的一致性，此时宜用一致性检验或称Kappa检验；也可用特殊模型分析方法（可用SAS软件）。\n双向有序属性不同的R×C表资料 R×C表资料中两个分类变量皆为有序的，但属性不同。对于该类资料，若研究目的为分析不同年龄组患者疗效之间有无差别时，可把它视为单向有序R×C表资料，选用秩转换的非参数检验；若研究目的为分析两个有序分类变量间是否存在相关关系，宜用等级相关分析：若研究目的为分析两个有序分类变量间是否存在线性变化趋势，宜用前述的双向有序分组资料的线性趋势检验(test for linear trend)。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#四格表资料的卡方检验",
    "href": "1006-chisq.html#四格表资料的卡方检验",
    "title": "4  卡方检验",
    "section": "4.2 四格表资料的卡方检验",
    "text": "4.2 四格表资料的卡方检验\n使用课本例7-1的数据。\n首先是构造数据，本次数据自己从书上摘录。\n\nID&lt;-seq(1,200)\ntreat&lt;-c(rep(\"treated\",104),rep(\"placebo\",96))\ntreat&lt;- factor(treat)\nimpro&lt;-c(rep(\"marked\",99),rep(\"none\",5),rep(\"marked\",75),rep(\"none\",21))\nimpro&lt;-factor(impro)\ndata1&lt;-data.frame(ID,treat,impro)\n\nstr(data1)\n## 'data.frame':    200 obs. of  3 variables:\n##  $ ID   : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ treat: Factor w/ 2 levels \"placebo\",\"treated\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ impro: Factor w/ 2 levels \"marked\",\"none\": 1 1 1 1 1 1 1 1 1 1 ...\n\n数据一共3列，第一列是id，第二列是治疗方法，第三列是等级（有效和无效）。\n简单看下各组情况：\n\ntable(data1$treat,data1$impro)\n##          \n##           marked none\n##   placebo     75   21\n##   treated     99    5\n\n做卡方检验有2种方法，分别演示：\n\n4.2.1 方法1\n直接使用gmodels包里面的CrossTable()函数，非常强大，直接给出所有结果，和SPSS差不多。\n\nlibrary(gmodels)\n\nCrossTable(data1$treat, data1$impro, digits = 4, \n           expected = T, chisq = T, fisher = T, mcnemar = T, \n           format = \"SPSS\")\n## \n##    Cell Contents\n## |-------------------------|\n## |                   Count |\n## |         Expected Values |\n## | Chi-square contribution |\n## |             Row Percent |\n## |          Column Percent |\n## |           Total Percent |\n## |-------------------------|\n## \n## Total Observations in Table:  200 \n## \n##              | data1$impro \n##  data1$treat |   marked  |     none  | Row Total | \n## -------------|-----------|-----------|-----------|\n##      placebo |       75  |       21  |       96  | \n##              |  83.5200  |  12.4800  |           | \n##              |   0.8691  |   5.8165  |           | \n##              |  78.1250% |  21.8750% |  48.0000% | \n##              |  43.1034% |  80.7692% |           | \n##              |  37.5000% |  10.5000% |           | \n## -------------|-----------|-----------|-----------|\n##      treated |       99  |        5  |      104  | \n##              |  90.4800  |  13.5200  |           | \n##              |   0.8023  |   5.3691  |           | \n##              |  95.1923% |   4.8077% |  52.0000% | \n##              |  56.8966% |  19.2308% |           | \n##              |  49.5000% |   2.5000% |           | \n## -------------|-----------|-----------|-----------|\n## Column Total |      174  |       26  |      200  | \n##              |  87.0000% |  13.0000% |           | \n## -------------|-----------|-----------|-----------|\n## \n##  \n## Statistics for All Table Factors\n## \n## \n## Pearson's Chi-squared test \n## ------------------------------------------------------------\n## Chi^2 =  12.85707     d.f. =  1     p =  0.0003362066 \n## \n## Pearson's Chi-squared test with Yates' continuity correction \n## ------------------------------------------------------------\n## Chi^2 =  11.3923     d.f. =  1     p =  0.0007374901 \n## \n##  \n## McNemar's Chi-squared test \n## ------------------------------------------------------------\n## Chi^2 =  50.7     d.f. =  1     p =  1.076196e-12 \n## \n## McNemar's Chi-squared test with continuity correction \n## ------------------------------------------------------------\n## Chi^2 =  49.40833     d.f. =  1     p =  2.078608e-12 \n## \n##  \n## Fisher's Exact Test for Count Data\n## ------------------------------------------------------------\n## Sample estimate odds ratio:  0.1818332 \n## \n## Alternative hypothesis: true odds ratio is not equal to 1\n## p =  0.0005286933 \n## 95% confidence interval:  0.05117986 0.5256375 \n## \n## Alternative hypothesis: true odds ratio is less than 1\n## p =  0.0002823226 \n## 95% confidence interval:  0 0.4569031 \n## \n## Alternative hypothesis: true odds ratio is greater than 1\n## p =  0.9999541 \n## 95% confidence interval:  0.06281418 Inf \n## \n## \n##  \n##        Minimum expected frequency: 12.48\n\n可以看到这个函数直接给出所有结果，根据需要自己选择合适的即可。\n本例符合pearson卡方，卡方值为12.85707，p&lt;0.01，和课本一致。\n\n\n4.2.2 方法2\n先把数据变成2x2列联表，然后用 chisq.test函数做\n\nmytable &lt;- table(data1$treat,data1$impro)\n\nmytable\n##          \n##           marked none\n##   placebo     75   21\n##   treated     99    5\n\n\nchisq.test(mytable,correct = F) # 和SPSS一样\n## \n##  Pearson's Chi-squared test\n## \n## data:  mytable\n## X-squared = 12.857, df = 1, p-value = 0.0003362\n\n这个结果和课本也是一致的，和SPSS算出来的也是一样的。\n\n\n\n\n\n\n注释\n\n\n\n四格表资料卡方检验的专用公式/四格表资料卡方检验的校正公式/配对四格表资料的卡方检验/四格表资料的Fisher精确概率法，都可以用方法1直接解决。\n\n\n下面使用R语言自带的chisq.test()函数进行演示。\n使用课本例7-2的数据，这是一个连续校正卡方检验。\n\nper &lt;- matrix(c(46,6,18,8),\n              nrow = 2, byrow = T,\n              dimnames = list(group = c(\"胞磷胆碱\",\"神经节苷脂\"),\n                              effect = c(\"有效\",\"无效\")\n                              )\n              )\n\nper\n##             effect\n## group        有效 无效\n##   胞磷胆碱     46    6\n##   神经节苷脂   18    8\n\n进行连续校正的卡方检验：\n\nchisq.test(per, correct = T)\n## Warning in chisq.test(per, correct = T): Chi-squared approximation may be\n## incorrect\n## \n##  Pearson's Chi-squared test with Yates' continuity correction\n## \n## data:  per\n## X-squared = 3.1448, df = 1, p-value = 0.07617\n\n结果和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#配对四格表资料的卡方检验",
    "href": "1006-chisq.html#配对四格表资料的卡方检验",
    "title": "4  卡方检验",
    "section": "4.3 配对四格表资料的卡方检验",
    "text": "4.3 配对四格表资料的卡方检验\n使用课本例7-3的数据。\n\nana &lt;- matrix(c(11,12,2,33), nrow = 2, byrow = T,\n              dimnames = list(免疫荧光 = c(\"阳性\",\"阴性\"),\n                              乳胶凝集 = c(\"阳性\",\"阴性\")\n                              )\n              )\n\nana\n##         乳胶凝集\n## 免疫荧光 阳性 阴性\n##     阳性   11   12\n##     阴性    2   33\n\n配对四个表资料需要用McNemar检验：\n\nmcnemar.test(ana, correct = T)\n## \n##  McNemar's Chi-squared test with continuity correction\n## \n## data:  ana\n## McNemar's chi-squared = 5.7857, df = 1, p-value = 0.01616\n\n结果和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#四格表资料的fisher确切概率法",
    "href": "1006-chisq.html#四格表资料的fisher确切概率法",
    "title": "4  卡方检验",
    "section": "4.4 四格表资料的Fisher确切概率法",
    "text": "4.4 四格表资料的Fisher确切概率法\n使用课本例7-4的数据。\n\nhbv &lt;- matrix(c(4,18,5,6), nrow = 2, byrow = T,\n              dimnames = list(组别 = c(\"预防注射组\",\"非预防组\"),\n                              效果 = c(\"阳性\",\"阴性\")\n                              )\n              )\nhbv\n##             效果\n## 组别       阳性 阴性\n##   预防注射组    4   18\n##   非预防组      5    6\n\n进行 Fisher 检验：\n\nfisher.test(hbv)\n## \n##  Fisher's Exact Test for Count Data\n## \n## data:  hbv\n## p-value = 0.121\n## alternative hypothesis: true odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  0.03974151 1.76726409\n## sample estimates:\n## odds ratio \n##  0.2791061\n\nP值为0.121，和课本一样。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#行-x-列表资料的卡方检验",
    "href": "1006-chisq.html#行-x-列表资料的卡方检验",
    "title": "4  卡方检验",
    "section": "4.5 行 x 列表资料的卡方检验",
    "text": "4.5 行 x 列表资料的卡方检验\n行 x 列表资料的卡方检验有很多种情况，不是所有的列联表资料都可以直接用卡方检验，大家要注意甄别！方法选择可以参考本篇开头部分。\n\n4.5.1 多个样本率的比较\n使用课本例7-6的数据。\n首先是构造数据，本次数据直接读取，也可以自己手动摘录。\n\ndf &lt;- read.csv(\"datasets/例07-06.csv\", header = T)\n\nstr(df)\n## 'data.frame':    6 obs. of  3 variables:\n##  $ 疗法: int  1 1 2 2 3 3\n##  $ 疗效: int  1 2 1 2 1 2\n##  $ f   : int  199 7 164 18 118 26\n\nhead(df)\n##   疗法 疗效   f\n## 1    1    1 199\n## 2    1    2   7\n## 3    2    1 164\n## 4    2    2  18\n## 5    3    1 118\n## 6    3    2  26\n\n数据一共3列，第1列是疗法，第2列是有效无效，第3列是频数.\n进行 行 x 列表资料的卡方检验，首先要对数据格式转换一下，变成 table或者 矩阵：\n\nM &lt;- matrix(df$f,nrow = 3,byrow = T,\n            dimnames = list(trt = c(\"物理\", \"药物\", \"外用\"),\n                            effect = c(\"有效\",\"无效\")))\n\nM\n##       effect\n## trt    有效 无效\n##   物理  199    7\n##   药物  164   18\n##   外用  118   26\n\n这里教大家一个可视化列联表资料非常好用的马赛克图：\n\nmosaicplot(M)\n\n\n\n\n\n\n\n\n进行 行 x 列表资料的卡方检验：\n\nkf &lt;- chisq.test(M, correct = F)\n\nkf\n## \n##  Pearson's Chi-squared test\n## \n## data:  M\n## X-squared = 21.038, df = 2, p-value = 2.702e-05\n\n结果和课本一致。\n多个样本率的比较也可以使用以下函数进行检验：\n\n# 只适用于两列的，类似于 有效/无效 这种！\nprop.test(M, correct = TRUE)\n## \n##  3-sample test for equality of proportions without continuity correction\n## \n## data:  M\n## X-squared = 21.038, df = 2, p-value = 2.702e-05\n## alternative hypothesis: two.sided\n## sample estimates:\n##    prop 1    prop 2    prop 3 \n## 0.9660194 0.9010989 0.8194444\n\n可以看到两种结果是一样的，和课本一致的！\n\n\n4.5.2 样本构成比的比较\n使用课本例7-7的数据。\n\nace &lt;- matrix(c(42,48,21,30,72,36),nrow = 2,byrow = T,\n              dimnames = list(dn = c(\"dn组\",\"非dn组\"),\n                              idi = c(\"dd\",\"id\",\"ii\")\n                              )\n              )\nace\n##         idi\n## dn       dd id ii\n##   dn组   42 48 21\n##   非dn组 30 72 36\n\n进行卡方检验：\n\nchisq.test(ace, correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  ace\n## X-squared = 7.9127, df = 2, p-value = 0.01913\n\n卡方值为7.91，和课本一致。\n\n\n4.5.3 双向无序分类资料的关联性检验\n使用课本例例7-8的数据。\n\nblood &lt;- matrix(c(431,490,902,388,410,800,495,587,950,137,179,32),\n                nrow = 4,byrow = T,\n                dimnames = list(abo = c(\"o\",\"a\",\"b\",\"ab\"),\n                                mn = c(\"m\",\"n\",\"mn\")\n                                )\n                )\nblood\n##     mn\n## abo    m   n  mn\n##   o  431 490 902\n##   a  388 410 800\n##   b  495 587 950\n##   ab 137 179  32\n\n进行 关联性检验：\n\nchisq.test(blood,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  blood\n## X-squared = 213.16, df = 6, p-value &lt; 2.2e-16\n\n计算列联系数：\n\nlibrary(vcd)\n\nassocstats(blood)\n##                     X^2 df P(&gt; X^2)\n## Likelihood Ratio 248.14  6        0\n## Pearson          213.16  6        0\n## \n## Phi-Coefficient   : NA \n## Contingency Coeff.: 0.188 \n## Cramer's V        : 0.136\n\nPearson列联系数是0.188，和课本一样。\n\n\n4.5.4 双向有序分组资料的线性趋势检验\n使用课本例7-9的数据。\n\nather &lt;- matrix(c(70,22,4,2,27,24,9,3,16,23,13,7,9,20,15,14),\n                nrow = 4,byrow = T,\n                dimnames = list(age = c(\"20~\",\"30~\",\"40~\",\"≥50\"),\n                                level = c(\"-\",\"+\",\"++\",\"+++\")\n                                )\n                )\nather\n##      level\n## age    -  + ++ +++\n##   20~ 70 22  4   2\n##   30~ 27 24  9   3\n##   40~ 16 23 13   7\n##   ≥50  9 20 15  14\n\n进行卡方检验：\n\nchisq.test(ather)\n## \n##  Pearson's Chi-squared test\n## \n## data:  ather\n## X-squared = 71.432, df = 9, p-value = 7.97e-12\n\n这里的71.432是总变异的值，如果要计算线性回归分量的卡方值，可以使用DescTools::MHChisqTest()函数实现：\n\nlibrary(DescTools)\nMHChisqTest(ather)\n## \n##  Mantel-Haenszel Chi-Square\n## \n## data:  ather\n## X-squared = 63.389, df = 1, p-value = 1.696e-15\n\n结果显示线性回归分量的卡方值是63.389（与课本基本一致，由于计算过程差异，略有不同），P值小于0.01。\n非线性回归分量的卡方值=总的卡方值-线性回归分量的卡方值=8.043\n有了卡方值和自由度，就可以使用pchisq计算P值：\n\n# 计算非线性回归分量的P值\npchisq(q=8.043, df=8, lower.tail = F)\n## [1] 0.4292811\n\n这个值和课本是一致的。\n课本是看两者之间有没有线性趋势，我们可以直接用lm()函数做，把age作为自变量，把level作为因变量即可，由于没有原始数据，这里就不演示了。\n对于这种双向有序的列联表资料，也可以用下一节介绍的MH卡方统计量检验行变量和列变量是否存在线性相关(以下代码参考：Mantel-Haenszel Test for Linear Trend)：\n\nsource(\"Mantel_Haenszel_test.R\")\n\nather &lt;- matrix(c(70,22,4,2,27,24,9,3,16,23,13,7,9,20,15,14),\n                nrow = 4,byrow = T)\n\nage &lt;- c(1,2,3,4)\nlevel &lt;- c(1,2,3)\n\nMH.test(ather,age,level)\n## Warning in margin.table(table, 2) * cscore: longer object length is not a\n## multiple of shorter object length\n## Warning in margin.table(table, 2) * (cdif^2): longer object length is not a\n## multiple of shorter object length\n## Warning in t(table * rdif) * cdif: longer object length is not a multiple of\n## shorter object length\n## $pcor\n## [1] 0.2955288\n## \n## $M2\n## [1] 24.19242\n## \n## $pval\n## [1] 8.717448e-07\n## \n## $rscore\n## [1] 1 2 3 4\n## \n## $cscore\n## [1] 1 2 3\n#MH.test.mid(ather)\n\n结果中P值小于0.05，可以认为行变量和列变量存在线性关系。\n或者可以进行Kappa一致性检验，Kappa的值的大小代表的一致性的程度，此值介于0到1之间，越大一致性程度越大。\n\n# 2选1\nDescTools::CohenKappa(ather, weight=\"Unweighted\")\n## [1] 0.2177295\nvcd::Kappa(ather)\n##             value     ASE     z  Pr(&gt;|z|)\n## Unweighted 0.2177 0.03799 5.732 9.953e-09\n## Weighted   0.3368 0.03949 8.529 1.477e-17",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#多个样本率间的多重比较",
    "href": "1006-chisq.html#多个样本率间的多重比较",
    "title": "4  卡方检验",
    "section": "4.6 多个样本率间的多重比较",
    "text": "4.6 多个样本率间的多重比较\n主要有卡方分割法、Scheffe可信区间法、SNK法等，这里主要演示卡方分割法。\n其实非常简单，就是把多个组手动拆分为多个两个组，分别进行卡方检验，和P值比较，只不过这里的P值不再是0.05，而是和组数（比较次数）有关。\n使用例7-10的数据。\n\ndf &lt;- read.csv(\"datasets/例07-06.csv\", header = T)\n\nM &lt;- matrix(df$f,nrow = 3,byrow = T,\n            dimnames = list(trt = c(\"物理\", \"药物\", \"外用\"),\n                            effect = c(\"有效\",\"无效\")))\n\nM\n##       effect\n## trt    有效 无效\n##   物理  199    7\n##   药物  164   18\n##   外用  118   26\n\n手动拆分，两两比较，直接取子集即可：\n\n# 物理治疗组和药物治疗组的卡方检验\nchisq.test(M[1:2,], correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  M[1:2, ]\n## X-squared = 6.756, df = 1, p-value = 0.009343\n\n\n# 物理治疗组和外用膏药组的卡方检验\nchisq.test(M[c(1,3),], correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  M[c(1, 3), ]\n## X-squared = 21.323, df = 1, p-value = 3.881e-06\n\n\n# 药物治疗组和外用膏药组的卡方检验\nchisq.test(M[2:3,], correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  M[2:3, ]\n## X-squared = 4.591, df = 1, p-value = 0.03214\n\n可以看到和课本是一样的。\n这时的 P’ = P / (K * (K - 1) / 2 + 1)，K是组数，一般情况下P=0.05，所以P’ = 0.05/(3*(3-1)/2+1) = 0.0125，上面3个卡方分析的P值和0.0125比较即可！\n也可以使用rcompanion::pairwiseNominalIndependence()实现多重比较，直接给出P值和调整后的P值：\n\nlibrary(rcompanion)\nff &lt;- pairwiseNominalIndependence(M)\nff\n##    Comparison p.Fisher p.adj.Fisher  p.Gtest p.adj.Gtest  p.Chisq p.adj.Chisq\n## 1 物理 : 药物 1.20e-02     1.80e-02 8.60e-03    1.29e-02 9.34e-03    1.40e-02\n## 2 物理 : 外用 7.53e-06     2.26e-05 3.48e-06    1.04e-05 3.88e-06    1.16e-05\n## 3 药物 : 外用 3.49e-02     3.49e-02 3.27e-02    3.27e-02 3.21e-02    3.21e-02\n\n结果中的p.Chisq和分开计算的P值是一样的，和0.0125进行比较即可。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#cochran-mantel-haenszel卡方统计量检验",
    "href": "1006-chisq.html#cochran-mantel-haenszel卡方统计量检验",
    "title": "4  卡方检验",
    "section": "4.7 Cochran-Mantel-Haenszel卡方统计量检验",
    "text": "4.7 Cochran-Mantel-Haenszel卡方统计量检验\n有一个叫Mantel-Haenszel卡方统计量的方法是用来检验两个有序分类变量是否存在线性相关的，Cochran-Mantel-Haenszel卡方统计量是在其基础上提出的，用于高维列联表的分析，即控制了某一个或几个混杂因素（分层变量）之后，检验二维列联表的行变量和列变量是否存在统计学关联。Cochran-Mantel-Haenszel检验属于分层卡方检验方法。\n根据行变量X和列变量Y的类型不同，Cochran-Mantel-Haenszel卡方统计量包括以下几种：\n\n相关统计量：适用于X和Y均为有序分类变量的资料。对于一维列联表，CMH统计量即为MH卡方统计量。\n方差分析统计量：也称行平均得分统计量（行均分检验），适用于列变量Y为有序分类变量的资料。\n一般关联统计量：适用于X和Y均为无序分类变量的资料，目的是检验X和Y是否存在关联性。\n\n使用课本例7-12的数据。\n这个数据有3个变量，首先是年龄，根据年龄分成两层，然后是是否心肌梗死和是否口服避孕药，我们可以直接把这个数据录入成3维array的形式：\n\nmyo &lt;- array(c(17,47,\n               121,944,\n               12,158,\n               14,663),\n             dim = c(2,2,2),\n             dimnames = list(心肌梗死 = c(\"病例\",\"对照\"),\n                             口服避孕药 = c(\"是\",\"否\"),\n                             年龄分层 = c(\"&lt;40岁\",\"≥40岁\")\n                             )\n             )\nmyo\n## , , 年龄分层 = &lt;40岁\n## \n##         口服避孕药\n## 心肌梗死 是  否\n##     病例 17 121\n##     对照 47 944\n## \n## , , 年龄分层 = ≥40岁\n## \n##         口服避孕药\n## 心肌梗死  是  否\n##     病例  12  14\n##     对照 158 663\n\n这样就能直接进行Cochran-Mantel-Haenszel检验了，这个检验的函数是R语言自带的，不需要另外的包：\n\nmantelhaen.test(myo,correct = F)\n## \n##  Mantel-Haenszel chi-squared test without continuity correction\n## \n## data:  myo\n## Mantel-Haenszel X-squared = 24.184, df = 1, p-value = 8.755e-07\n## alternative hypothesis: true common odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  1.930775 4.933840\n## sample estimates:\n## common odds ratio \n##          3.086444\n\n这样就得到结果了，这个结果和课本也是一样的。\n课本还介绍了Breslow-Day对各层的效应值进行齐性检验，这个检验可以通过DescTools包实现：\n\nlibrary(DescTools)\n\nBreslowDayTest(myo)\n## \n##  Breslow-Day test on Homogeneity of Odds Ratios\n## \n## data:  myo\n## X-squared = 0.23409, df = 1, p-value = 0.6285\n\n结果也是和课本一模一样。\n如果你的是原始数据的形式，也是很简单的，我们构造一个和上面数据一样的原始数据：\n\nmyoo &lt;- data.frame(年龄分层 = c(rep(\"&lt;40岁\",1129),rep(\"≥40岁\",847)),\n                   心肌梗死 = c(rep(\"病例\",64),rep(\"对照\",1065),\n                                rep(\"病例\",170),rep(\"对照\",677)\n                                ),\n                   口服避孕药 = c(rep(\"是\",17),rep(\"否\",47),\n                                  rep(\"是\",121),rep(\"否\",944),\n                                  rep(\"是\",12),rep(\"否\",158),\n                                  rep(\"是\",14),rep(\"否\",663)\n                                  )\n                   )\n\n# 分类变量变为因子型\nmyoo$年龄分层 &lt;- factor(myoo$年龄分层,levels = c(\"&lt;40岁\",\"≥40岁\"))\nmyoo$心肌梗死 &lt;- factor(myoo$心肌梗死, levels = c(\"病例\",\"对照\"))\nmyoo$口服避孕药 &lt;- factor(myoo$口服避孕药, levels = c(\"是\",\"否\"))\n\nhead(myoo)\n##   年龄分层 心肌梗死 口服避孕药\n## 1    &lt;40岁     病例         是\n## 2    &lt;40岁     病例         是\n## 3    &lt;40岁     病例         是\n## 4    &lt;40岁     病例         是\n## 5    &lt;40岁     病例         是\n## 6    &lt;40岁     病例         是\n\n用xtabs查看数据，结果和我们的array的形式是一样的：\n\nmyoo.tab &lt;- xtabs(~口服避孕药+心肌梗死+年龄分层,data = myoo)\nmyoo.tab\n## , , 年龄分层 = &lt;40岁\n## \n##           心肌梗死\n## 口服避孕药 病例 对照\n##         是   17  121\n##         否   47  944\n## \n## , , 年龄分层 = ≥40岁\n## \n##           心肌梗死\n## 口服避孕药 病例 对照\n##         是   12   14\n##         否  158  663\n\n这样就可以直接进行Cochran-Mantel-Haenszel检验了：\n\nmantelhaen.test(myoo.tab, correct = F)\n## \n##  Mantel-Haenszel chi-squared test without continuity correction\n## \n## data:  myoo.tab\n## Mantel-Haenszel X-squared = 24.184, df = 1, p-value = 8.755e-07\n## alternative hypothesis: true common odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  1.930775 4.933840\n## sample estimates:\n## common odds ratio \n##          3.086444\n\n结果也是一样的。\n还可用woolf法检验不同分层之间的效应值有没有统计学显著性，通过使用?mantelhaen.test查看帮助文档，作者直接给了一个现成的计算方法：\n\nwoolf &lt;- function(x) {\n  x &lt;- x + 1 / 2\n  k &lt;- dim(x)[3]\n  or &lt;- apply(x, 3, function(x) (x[1,1]*x[2,2])/(x[1,2]*x[2,1]))\n  w &lt;-  apply(x, 3, function(x) 1 / sum(1 / x))\n  1 - pchisq(sum(w * (log(or) - weighted.mean(log(or), w)) ^ 2), k - 1)\n}\n\nwoolf(myoo.tab)\n## [1] 0.6400154\n\n直接给出了P值。\n下面是CMH检验的一个补充。\n默认的CMH检验只能进行3个变量的检验，vcdExtra中的CMHtest()可以进行两个变量的CMH检验。\n现在我们想要了解某种药物剂量和疗效之间的关系，药物剂量有50mg，100mg，200mg，300mg，500mg，5个水平，疗效分为有效/无效两个水平。\n\ndf &lt;- matrix(c(13, 136, 17, 125, 16, 104, 32, 149, 9, 45), \n             nrow = 5, byrow = T,\n             dimnames = list(\"Dose\" = c(\"50\", \"100\", \"200\", \"300\", \"500\"),\n                             \"effect\" = c(\"Yes\", \"No\"))\n             )\n\ndf\n##      effect\n## Dose  Yes  No\n##   50   13 136\n##   100  17 125\n##   200  16 104\n##   300  32 149\n##   500   9  45\n\n\nvcdExtra::CMHtest(df, types = \"cor\")\n## Cochran-Mantel-Haenszel Statistics for Dose by effect \n## \n##           AltHypothesis  Chisq Df     Prob\n## cor Nonzero correlation 5.8217  1 0.015829",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1006-chisq.html#频数分布拟合优度卡方检验",
    "href": "1006-chisq.html#频数分布拟合优度卡方检验",
    "title": "4  卡方检验",
    "section": "4.8 频数分布拟合优度卡方检验",
    "text": "4.8 频数分布拟合优度卡方检验\n使用课本例7-13的数据。\nR语言做卡方拟合优度检验非常简单，关键是概率的计算，这里我们直接用课本中的概率。\n\nx &lt;- c(26,51,75,63,38,17,9)\np &lt;- c(0.0854,0.2102,0.2585,0.2120,0.1304,0.0641,0.0394)\n\nchisq.test(x=x, p =p)\n## \n##  Chi-squared test for given probabilities\n## \n## data:  x\n## X-squared = 2.0377, df = 6, p-value = 0.9162\n\n结果和课本非常接近。\n这里贴一个网络教程的概率计算方法：\n\nx&lt;-0:6\ny&lt;-c(26,51,75,63,38,17,9)\nmean&lt;-mean(rep(x,y))\nq&lt;-ppois(x,mean)\nn&lt;-length(y)\np&lt;-c()\np[1]&lt;-q[1]\np[n]&lt;-1-q[n-1]\nfor(i in 2:(n-1))\n  p[i]&lt;-q[i]-q[i-1]\nchisq.test(y, p=p,correct=F)\n## \n##  Chi-squared test for given probabilities\n## \n## data:  y\n## X-squared = 2.0569, df = 6, p-value = 0.9144\n\n结果和课本非常接近。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>卡方检验</span>"
    ]
  },
  {
    "objectID": "1009-cochranarmitage.html",
    "href": "1009-cochranarmitage.html",
    "title": "5  Cochran-Armitage检验",
    "section": "",
    "text": "Cochran Armitage检验是一种线性趋势检验，常用于自变量是有序分类变量，而因变量是二分类变量的资料，可以用来检验自变量和因变量存不存在线性趋势。\n注意和Cochran-Mantel-Haenszel检验区分，CMH检验是研究两个分类变量之间关联性的一种检验方法。但有时数据除了我们研究的变量外，还混杂或隐含了其它的变量，如果将这些变量纳入分析中，则有可能得出完全不同的结论，著名的Simpson悖论就是这个问题的典型案例。\n换句话说，在2 x 2表格数据的基础上，引入了第三个分类变量，称之为混杂变量。混杂变量的引入使得CMH检验可以用于分析分层样本，作为生物统计学领域的一种常用技术，该检验常用于疾病对照研究。\n现在我们想要了解某种药物剂量和疗效之间的关系，药物剂量有50mg，100mg，200mg，300mg，500mg，5个水平，疗效分为有效/无效两个水平。这种情况可以使用Cochran Armitage检验。\n\ndf &lt;- matrix(c(13, 136, 17, 125, 16, 104, 32, 149, 9, 45), \n             nrow = 5, byrow = T,\n             dimnames = list(\"Dose\" = c(\"50\", \"100\", \"200\", \"300\", \"500\"),\n                             \"effect\" = c(\"Yes\", \"No\"))\n             )\n\ndf\n##      effect\n## Dose  Yes  No\n##   50   13 136\n##   100  17 125\n##   200  16 104\n##   300  32 149\n##   500   9  45\n\n首先可以计算一下不同药物剂量下的有效率是多少：\n\ndf[,1]/rowSums(df)\n##         50        100        200        300        500 \n## 0.08724832 0.11971831 0.13333333 0.17679558 0.16666667\n\n可以看到随着药物剂量增加，有效率整体也是增加的，下面使用CAM检验验证一下。\n使用DescTools包中的CochranArmitageTest()函数进行检验：\n\nDescTools::CochranArmitageTest(df)\n## \n##  Cochran-Armitage test for trend\n## \n## data:  df\n## Z = 2.2116, dim = 5, p-value = 0.02699\n## alternative hypothesis: two.sided\n\n结果显示P值为p-value = 0.02699，小于0.05，可以认为疗效会随着药物剂量增加而增加。\n现在的df是一个频数统计表类型的数据，我们可以把它变成每行一个患者的数据，然后进行logistic回归看看结果。\n\ndf1 &lt;- rstatix::counts_to_cases(df)\npsych::headTail(df1)\n##     Dose effect\n## 1     50    Yes\n## 2     50    Yes\n## 3     50    Yes\n## 4     50    Yes\n## ... &lt;NA&gt;   &lt;NA&gt;\n## 643  500     No\n## 644  500     No\n## 645  500     No\n## 646  500     No\n\n把Dose变成数值型：\n\ndf1$Dose &lt;-  as.numeric(factor(df1$Dose))\n\nsummary(glm(effect~Dose, data = df1,family = binomial()))\n## \n## Call:\n## glm(formula = effect ~ Dose, family = binomial(), data = df1)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  2.48493    0.29598   8.396   &lt;2e-16 ***\n## Dose        -0.21544    0.08985  -2.398   0.0165 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 510.57  on 645  degrees of freedom\n## Residual deviance: 504.71  on 644  degrees of freedom\n## AIC: 508.71\n## \n## Number of Fisher Scoring iterations: 4\n\nlogistic回归的结果也显示，剂量的P值是小于0.05的。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cochran-Armitage检验</span>"
    ]
  },
  {
    "objectID": "1007-wilcoxon.html",
    "href": "1007-wilcoxon.html",
    "title": "6  秩转换的非参数检验",
    "section": "",
    "text": "6.1 配对样本比较的Wilcoxon符号秩检验\n英文名字：wilcoxon signed-rank test，即符号秩检验。\n使用课本例8-1的数据，自己手动摘录：\ntest1&lt;-c(60,142,195,80,242,220,190,25,198,38,236,95)\ntest2&lt;-c(76,152,243,82,240,220,205,38,243,44,190,100)\n两列数据，和配对t检验的数据结果完全一样。\n简单看一下数据情况：\nboxplot(test1,test2)\n进行秩和检验：\nwilcox.test(test1,test2,paired = T,alternative = \"two.sided\",exact = F)\n## \n##  Wilcoxon signed rank test with continuity correction\n## \n## data:  test1 and test2\n## V = 11.5, p-value = 0.06175\n## alternative hypothesis: true location shift is not equal to 0\n结果和课本一致！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>秩转换的非参数检验</span>"
    ]
  },
  {
    "objectID": "1007-wilcoxon.html#配对样本比较的wilcoxon符号秩检验",
    "href": "1007-wilcoxon.html#配对样本比较的wilcoxon符号秩检验",
    "title": "6  秩转换的非参数检验",
    "section": "",
    "text": "注意\n\n\n\nwilcox.test在R4.4.0及以后不再支持在公式形式中使用paired参数，所以如果你的R版本在4.4.0及以后的，在公式形式中使用paired参数会得到以下报错：cannot use 'paired' in formula method。逆天更新！！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>秩转换的非参数检验</span>"
    ]
  },
  {
    "objectID": "1007-wilcoxon.html#两独立样本比较的wilcoxon秩和检验",
    "href": "1007-wilcoxon.html#两独立样本比较的wilcoxon秩和检验",
    "title": "6  秩转换的非参数检验",
    "section": "6.2 两独立样本比较的Wilcoxon秩和检验",
    "text": "6.2 两独立样本比较的Wilcoxon秩和检验\n英文名字：wilcoxon rank sum test，即秩和检验。\n\n根据课本上的说法，wilcoxon rank sum test 和 mann-whitney U test 并不完全是一个东西。\n\n和两样本t检验的数据格式完全一样！\n使用课本例8-3的数据，自己手动摘录。\n\nRD1&lt;-c(2.78,3.23,4.20,4.87,5.12,6.21,7.18,8.05,8.56,9.60)\nRD2&lt;-c(3.23,3.50,4.04,4.15,4.28,4.34,4.47,4.64,4.75,4.82,4.95,5.10)\n\n进行两独立样本比较的Wilcoxon秩和检验：\n\nwilcox.test(RD1,RD2,paired = F, correct = F, exact = F)\n## \n##  Wilcoxon rank sum test\n## \n## data:  RD1 and RD2\n## W = 86.5, p-value = 0.08049\n## alternative hypothesis: true location shift is not equal to 0\n\n结果取单侧检验，还是和课本一致！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>秩转换的非参数检验</span>"
    ]
  },
  {
    "objectID": "1007-wilcoxon.html#完全随机设计多个样本比较的-kruskal-wallis-h-检验",
    "href": "1007-wilcoxon.html#完全随机设计多个样本比较的-kruskal-wallis-h-检验",
    "title": "6  秩转换的非参数检验",
    "section": "6.3 完全随机设计多个样本比较的 Kruskal-Wallis H 检验",
    "text": "6.3 完全随机设计多个样本比较的 Kruskal-Wallis H 检验\n\n6.3.1 多样本比较的kruskal-wallis H检验\n使用课本例8-5的数据，手动摘录：\n\nrm(list = ls())\ndeath_rate &lt;- c(32.5,35.5,40.5,46,49,16,20.5,22.5,29,36,6.5,\n                9.0,12.5,18,24)\ndrug &lt;- rep(c(\"Drug_A\",\"drug_B\",\"drug_C\"),each=5)\nmydata &lt;- data.frame(death_rate,drug)\n\nstr(mydata)\n## 'data.frame':    15 obs. of  2 variables:\n##  $ death_rate: num  32.5 35.5 40.5 46 49 16 20.5 22.5 29 36 ...\n##  $ drug      : chr  \"Drug_A\" \"Drug_A\" \"Drug_A\" \"Drug_A\" ...\n\n数据一共2列，第1列是死亡率，第2列是药物（3种）。\n简单看下数据：\n\nboxplot(death_rate ~ drug, data = mydata)\n\n\n\n\n\n\n\n\n进行 Kruskal-Wallis H 检验：\n\nkruskal.test(death_rate ~ drug, data = mydata)\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  death_rate by drug\n## Kruskal-Wallis chi-squared = 9.74, df = 2, p-value = 0.007673\n\n算出来结果和课本一致！\n使用课本例8-6的数据，手动摘录：\n\ndata8_6 &lt;- data.frame(days=c(2,2,2,3,4,4,4,5,7,7,\n                             5,5,6,6,6,7,8,10,12,\n                             3,5,6,6,6,7,7,9,10,11,11),\n                      type=c(rep(\"9D\",10),rep(\"11C\",9),rep(\"DSC\",11)))\nhead(data8_6)\n##   days type\n## 1    2   9D\n## 2    2   9D\n## 3    2   9D\n## 4    3   9D\n## 5    4   9D\n## 6    4   9D\n\n进行 Kruskal-Wallis H 检验：\n\nkruskal.test(days ~ type, data = data8_6)\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  days by type\n## Kruskal-Wallis chi-squared = 9.9405, df = 2, p-value = 0.006941\n\n结果和课本一致（课本中Hc=9.97）。\n\n\n6.3.2 kruskal-Wallis H检验后的多重比较\n例8-8使用的是例8-6的数据。\n课本上是使用的 Nemenyi检验，我们使用非参数检验的全能R包：PMCMRplus实现。\n\nlibrary(PMCMRplus)\n\n# 首先要把分类变量因子化\ndata8_6$type &lt;- factor(data8_6$type)\n\n下面就可以使用 Nemenyi检验了。\n\n# 也可以把kwh检验的结果作为输入\nres &lt;- kwAllPairsNemenyiTest(days ~ type, data = data8_6)\n## Warning in kwAllPairsNemenyiTest.default(c(2, 2, 2, 3, 4, 4, 4, 5, 7, 7, : Ties\n## are present, p-values are not corrected.\nsummary(res)\n##                q value Pr(&gt;|q|)  \n## 9D - 11C == 0    3.628 0.027794 *\n## DSC - 11C == 0   0.177 0.991411  \n## DSC - 9D == 0    3.998 0.013057 *\n\n得到的数值和课本有差别，但是结论是一样的。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>秩转换的非参数检验</span>"
    ]
  },
  {
    "objectID": "1007-wilcoxon.html#随记区组设计多个样本比较的friedman-m检验",
    "href": "1007-wilcoxon.html#随记区组设计多个样本比较的friedman-m检验",
    "title": "6  秩转换的非参数检验",
    "section": "6.4 随记区组设计多个样本比较的Friedman M检验",
    "text": "6.4 随记区组设计多个样本比较的Friedman M检验\n\n6.4.1 多个相关样本比较的Friedman M检验\n使用课本例8-9的数据：\n\ndf &lt;- foreign::read.spss(\"datasets/例08-09.sav\", to.data.frame = T)\n\nstr(df)\n## 'data.frame':    8 obs. of  4 variables:\n##  $ a: num  8.4 11.6 9.4 9.8 8.3 8.6 8.9 7.8\n##  $ b: num  9.6 12.7 9.1 8.7 8 9.8 9 8.2\n##  $ c: num  9.8 11.8 10.4 9.9 8.6 9.6 10.6 8.5\n##  $ d: num  11.7 12 9.8 12 8.6 10.6 11.4 10.8\n##  - attr(*, \"codepage\")= int 65001\n\n数据一共4列，分别是4中不同频率下的反应率。\n简单看下数据：\n\nboxplot(df$a,df$b,df$c,df$d)\n\n\n\n\n\n\n\n\n进行 Friedman M 检验前先把数据格式转换一下：\n\nM &lt;- as.matrix(df) # 变成矩阵\n\n进行 Friedman M 检验：\n\nfriedman.test(M)\n## \n##  Friedman rank sum test\n## \n## data:  M\n## Friedman chi-squared = 15.152, df = 3, p-value = 0.001691\n\n结果和课本一致！\n\n\n6.4.2 多个相关样本两两比较的q检验\nP126页，多个相关样本两两比较的q检验。课本上说的这个q检验，应该是quade test。\n接下来就是使用R语言实现quade-test。但是自带的quade.test()函数不能进行两两比较，还是要借助第三方包。\n\n# 准备数据，也是用的课本例8-9的数据\ndf &lt;- matrix(\n  c(8.4, 11.6, 9.4, 9.8, 8.3, 8.6, 8.9, 7.8,\n    9.6, 12.7, 9.1, 8.7, 8, 9.8, 9, 8.2,\n    9.8, 11.8, 10.4, 9.9, 8.6, 9.6, 10.6, 8.5,\n    11.7, 12, 9.8, 12, 8.6, 10.6, 11.4, 10.8\n    ),\n  byrow = F, nrow = 8,\n  dimnames = list(1:8,LETTERS[1:4])\n  )\n\nprint(df)\n##      A    B    C    D\n## 1  8.4  9.6  9.8 11.7\n## 2 11.6 12.7 11.8 12.0\n## 3  9.4  9.1 10.4  9.8\n## 4  9.8  8.7  9.9 12.0\n## 5  8.3  8.0  8.6  8.6\n## 6  8.6  9.8  9.6 10.6\n## 7  8.9  9.0 10.6 11.4\n## 8  7.8  8.2  8.5 10.8\n\n先进行 Friedman M检验看看：\n\nfriedman.test(df)\n## \n##  Friedman rank sum test\n## \n## data:  df\n## Friedman chi-squared = 15.152, df = 3, p-value = 0.001691\n\n接下来进行quade检验：\n\nlibrary(PMCMRplus)\n\nquadeAllPairsTest(df, dist = \"Normal\")\n##   A       B       C     \n## B 0.2200  -       -     \n## C 0.0017  0.0644  -     \n## D 1.7e-07 7.7e-05 0.0860\n\n当然也可以有更加详细的结果：\n\nres &lt;- quadeAllPairsTest(df,dist = \"Normal\")\ntoTidy(res)\n##   group1 group2 statistic      p.value alternative\n## 1      B      A  1.226488 2.200150e-01   two.sided\n## 2      C      A  3.526154 1.686568e-03   two.sided\n## 3      C      B  2.299666 6.440153e-02   two.sided\n## 4      D      A  5.549859 1.715396e-07   two.sided\n## 5      D      B  4.323371 7.683144e-05   two.sided\n## 6      D      C  2.023706 8.600089e-02   two.sided\n##                                           method distribution p.adjust.method\n## 1 Quade's testwith standard-normal approximation            z            holm\n## 2 Quade's testwith standard-normal approximation            z            holm\n## 3 Quade's testwith standard-normal approximation            z            holm\n## 4 Quade's testwith standard-normal approximation            z            holm\n## 5 Quade's testwith standard-normal approximation            z            holm\n## 6 Quade's testwith standard-normal approximation            z            holm\n\n这个结果和课本上也不是完全一样，不过不影响结果。还有很多其他的方法可以选择，除了这个quade检验，还可以用Nemenyi等检验方法。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>秩转换的非参数检验</span>"
    ]
  },
  {
    "objectID": "1015-twocorrelation.html",
    "href": "1015-twocorrelation.html",
    "title": "7  双变量回归与相关",
    "section": "",
    "text": "7.1 直线回归\n例9-1。\ndf9_1 &lt;- data.frame(x = c(13,11,9,6,8,10,12,7),\n                    y = c(3.54,3.01,3.09,2.48,2.56,3.36,3.18,2.65))\n建立回归方程：\nfit &lt;- lm(y ~ x, data = df9_1)\nsummary(fit)\n## \n## Call:\n## lm(formula = y ~ x, data = df9_1)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.21500 -0.15937 -0.00125  0.09583  0.30667 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)  1.66167    0.29700   5.595  0.00139 **\n## x            0.13917    0.03039   4.579  0.00377 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.197 on 6 degrees of freedom\n## Multiple R-squared:  0.7775, Adjusted R-squared:  0.7404 \n## F-statistic: 20.97 on 1 and 6 DF,  p-value: 0.003774\n截距是1.66167，x的系数是0.13917。同时该结果也给出了回归方程的假设检验结果。\n例9-2：F-statistic: 20.97，p-value: 0.003774；回归系数：t=4.579,p=0.00377。\n例9-3，计算回归系数的95%的可信区间，直接使用broom计算即可：\nbroom::tidy(fit,conf.int = T)\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    1.66     0.297       5.59 0.00139   0.935      2.39 \n## 2 x              0.139    0.0304      4.58 0.00377   0.0648     0.214\n例9-4，计算总体均数的可信区间和个体Y值的预测区间，1行代码即可实现:\nnew_x &lt;- data.frame(x=12)\n\n# 总体均数的可信区间\npredict(fit, newdata = new_x,interval = \"confidence\",level = 0.95)\n##        fit      lwr      upr\n## 1 3.331667 3.079481 3.583852\n\n# 个体Y值的预测区间\npredict(fit, newdata = new_x,interval = \"prediction\",level = 0.95)\n##        fit      lwr      upr\n## 1 3.331667 2.787731 3.875602\n以上结果均和课本一致。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>双变量回归与相关</span>"
    ]
  },
  {
    "objectID": "1015-twocorrelation.html#直线相关",
    "href": "1015-twocorrelation.html#直线相关",
    "title": "7  双变量回归与相关",
    "section": "7.2 直线相关",
    "text": "7.2 直线相关\n使用课本例9-5的数据。\n\ndf &lt;- data.frame(\n  weight = c(43,74,51,58,50,65,54,57,67,69,80,48,38,85,54),\n  kv = c(217.22,316.18,231.11,220.96,254.70,293.84,263.28,271.73,263.46,\n         276.53,341.15,261.00,213.20,315.12,252.08)\n)\n\nstr(df)\n## 'data.frame':    15 obs. of  2 variables:\n##  $ weight: num  43 74 51 58 50 65 54 57 67 69 ...\n##  $ kv    : num  217 316 231 221 255 ...\n\n两变量是否有关联？其方向和密切程度如何？\n直接用cor可计算相关系数r：\n\ncor(df$weight, df$kv)\n## [1] 0.8754315\n\n或者直接用cor.test，既可以计算相关系数，又可以计算相关系数的P值：\n\ncor.test(~ weight + kv, data = df)\n## \n##  Pearson's product-moment correlation\n## \n## data:  weight and kv\n## t = 6.5304, df = 13, p-value = 1.911e-05\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.6584522 0.9580540\n## sample estimates:\n##       cor \n## 0.8754315\n\n从结果可以看出，两者是正相关，相关系数r=0.8754，且P值小于0.05，并给出了相关系数的可信区间：（0.6584522 0.9580540），具有统计学意义！\n可视化结果：\n\nlibrary(ggplot2)\n\nggplot(df, aes(weight, kv)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\",se=F) +\n  geom_vline(xintercept = mean(df$weight),linetype=2)+\n  geom_hline(yintercept = mean(df$kv),linetype=2)+\n  labs(x=\"体重(kg)X\",y=\"双肾体积(ml)Y\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n相关性分析的过程比较简单，在选择方法时要注意是使用pearson相关还是秩相关。\nR2的计算：\n\nsummary(lm(weight ~ kv, data = df))\n## \n## Call:\n## lm(formula = weight ~ kv, data = df)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -9.947 -4.469 -1.338  4.285 12.500 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -23.1845    12.7869  -1.813    0.093 .  \n## kv            0.3109     0.0476   6.530 1.91e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.777 on 13 degrees of freedom\n## Multiple R-squared:  0.7664, Adjusted R-squared:  0.7484 \n## F-statistic: 42.65 on 1 and 13 DF,  p-value: 1.911e-05\n\nMultiple R-squared: 0.7664, Adjusted R-squared: 0.7484",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>双变量回归与相关</span>"
    ]
  },
  {
    "objectID": "1015-twocorrelation.html#秩相关",
    "href": "1015-twocorrelation.html#秩相关",
    "title": "7  双变量回归与相关",
    "section": "7.3 秩相关",
    "text": "7.3 秩相关\n例9-8\n\ndf9_8 &lt;- foreign::read.spss(\"datasets/例09-08.sav\", to.data.frame = T)\n\nstr(df9_8)\n## 'data.frame':    18 obs. of  3 variables:\n##  $ number: num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x     : num  0.03 0.14 0.2 0.43 0.44 0.45 0.47 0.65 0.95 0.96 ...\n##  $ y     : num  0.05 0.34 0.93 0.69 0.38 0.79 1.19 4.74 2.31 5.95 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:3] \"\\xb1\\xe0\\xba\\xc5\" \"\\xcb\\xc0\\xd2򹹳\\xc9\" \"WYPLL\\xb9\\xb9\\xb3\\xc9\"\n##   ..- attr(*, \"names\")= chr [1:3] \"number\" \"x\" \"y\"\n\n计算相关系数：\n\ncor(df9_8$x,df9_8$y, method = \"spearman\")\n## [1] 0.9050568\n\n计算P值：\n\ncor.test(df9_8$x,df9_8$y, method = \"spearman\")\n## \n##  Spearman's rank correlation rho\n## \n## data:  df9_8$x and df9_8$y\n## S = 92, p-value &lt; 2.2e-16\n## alternative hypothesis: true rho is not equal to 0\n## sample estimates:\n##       rho \n## 0.9050568\n\nP&lt;0.001。\n如何校正？直接使用continuity即可（看帮助文档）：\n\ncor.test(df9_8$x,df9_8$y, method = \"spearman\",continuity=T)\n## \n##  Spearman's rank correlation rho\n## \n## data:  df9_8$x and df9_8$y\n## S = 92, p-value &lt; 2.2e-16\n## alternative hypothesis: true rho is not equal to 0\n## sample estimates:\n##       rho \n## 0.9050568",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>双变量回归与相关</span>"
    ]
  },
  {
    "objectID": "1015-twocorrelation.html#两条回归直线的比较",
    "href": "1015-twocorrelation.html#两条回归直线的比较",
    "title": "7  双变量回归与相关",
    "section": "7.4 两条回归直线的比较",
    "text": "7.4 两条回归直线的比较\n正常儿童数据见例9-1，大骨节病儿童数据见例9-9，问：回归直线是否不平行？\n\n# 例9-1数据\ndf9_1 &lt;- data.frame(x = c(13,11,9,6,8,10,12,7),\n                    y = c(3.54,3.01,3.09,2.48,2.56,3.36,3.18,2.65))\n\n# 例9-9数据\ndf9_9 &lt;- foreign::read.spss(\"datasets/例09-09.sav\", to.data.frame = T)\n\n建立回归方程：\n\n# 例9-1的回归方程\nfit9_1 &lt;- lm(y ~ x, data = df9_1)\n\n# 例9-2的回归方程\nfit9_9 &lt;- lm(y ~ x, data = df9_9)\n\na1 &lt;- anova(fit9_1)\na1\n## Analysis of Variance Table\n## \n## Response: y\n##           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \n## x          1 0.81343 0.81343  20.968 0.003774 **\n## Residuals  6 0.23276 0.03879                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\na2 &lt;- anova(fit9_9)\na2\n## Analysis of Variance Table\n## \n## Response: y\n##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \n## x          1 2.61351 2.61351  58.362 6.076e-05 ***\n## Residuals  8 0.35825 0.04478                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n如果此时你直接使用anova进行F检验，会得到以下报错：\n\nanova(fit9_1,fit9_9)\n\nError in anova.lmlist(object, ...) : \n  models were not all fitted to the same size of dataset\n\n这是因为这个函数只能处理样本量完全一样的两个模型的比较，此时我们可以把两个数据集合并到一起，添加一个交互项，查看交互项的显著性，以此来判断两条回归直线是否平行(如果有现成的函数可以比较的，请大神告诉我)。\n\ndf9_1$group &lt;- \"group9_1\"\ndf9_9$group &lt;- \"group9_9\"\n\ndf9 &lt;- rbind(df9_1,df9_9[,-1])\ndf9$group &lt;- factor(df9$group)\nstr(df9)\n## 'data.frame':    18 obs. of  3 variables:\n##  $ x    : num  13 11 9 6 8 10 12 7 10 9 ...\n##  $ y    : num  3.54 3.01 3.09 2.48 2.56 3.36 3.18 2.65 3.01 2.83 ...\n##  $ group: Factor w/ 2 levels \"group9_1\",\"group9_9\": 1 1 1 1 1 1 1 1 2 2 ...\n\n建立回归方程，并比较：\n\n# 建立不包含交互项的模型\nmodel_no_interaction &lt;- lm(y ~ x + group, data = df9)\n\n# 建立包含交互项的模型\nmodel_interaction &lt;- lm(y ~ x * group, data = df9)\n\n# 使用anova函数比较两个模型\nanova(model_no_interaction, model_interaction)\n## Analysis of Variance Table\n## \n## Model 1: y ~ x + group\n## Model 2: y ~ x * group\n##   Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)\n## 1     15 0.62211                           \n## 2     14 0.59101  1  0.031103 0.7368 0.4052\n\n得到的P值大于0.05，还不能认为两条总体回归直线不平行。\n当认为两条总体回归直线平行时，如果能进一步认为其总体截距是相等的，在两组数据的自变量取值范围接近时，便可认为两条总体回归直线基本重合，这时可合并两组样本资料，计算一个统一的回归方程。\n下面我们检测其截距是否相等，可通过直接查看有交互项模型的结果：\n\n# 查看模型摘要，检查group的显著性\nsummary(model_no_interaction)\n## \n## Call:\n## lm(formula = y ~ x + group, data = df9)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.29885 -0.15905  0.01675  0.14186  0.34023 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    1.44893    0.18427   7.863 1.06e-06 ***\n## x              0.16156    0.01785   9.049 1.83e-07 ***\n## groupgroup9_9 -0.23256    0.10181  -2.284   0.0373 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2037 on 15 degrees of freedom\n## Multiple R-squared:  0.8457, Adjusted R-squared:  0.8252 \n## F-statistic: 41.12 on 2 and 15 DF,  p-value: 8.162e-07\n\n结果中的groupgroup9_9的P值小于0.05，说明其截距是有差异的，不相等的。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>双变量回归与相关</span>"
    ]
  },
  {
    "objectID": "1015-twocorrelation.html#曲线拟合",
    "href": "1015-twocorrelation.html#曲线拟合",
    "title": "7  双变量回归与相关",
    "section": "7.5 曲线拟合",
    "text": "7.5 曲线拟合\n有些情况两个变量的关系并不是直线形式的，有可能是曲线形式的，此时可以通过曲线拟合来刻画两变量之间的关系。主要方法就是对变量做转换，比如log、平方、多项式、样条等。\n例9-11。\n\ndf9_11 &lt;- foreign::read.spss(\"datasets/例09-11.sav\", to.data.frame = T)\nstr(df9_11)\n## 'data.frame':    5 obs. of  3 variables:\n##  $ number: num  1 2 3 4 5\n##  $ x     : num  0.005 0.05 0.5 5 25\n##  $ y     : num  34.1 58 94.5 128.5 170\n##  - attr(*, \"variable.labels\")= Named chr [1:3] \"编号\" \" CRF浓度\" \"ACTH的合成量\"\n##   ..- attr(*, \"names\")= chr [1:3] \"number\" \"x\" \"y\"\n##  - attr(*, \"codepage\")= int 936\n\n先画图查看趋势：\n\nlibrary(ggplot2)\n\nggplot(df9_11, aes(x,y))+\n  geom_point(size=4)\n\n\n\n\n\n\n\n\n可以发现这个趋势非常像高中学过的对数函数的图像，所以我们选择对自变量X做对数转换，再画图看一看：\n\nggplot(df9_11, aes(log10(x),y))+\n  geom_point(size=4)\n\n\n\n\n\n\n\n\n果然就基本上呈直线趋势了，所以我们选择对数转换后的X建立直线回归方程：\n\nf9_11 &lt;- lm(y ~ log10(x), data = df9_11)\nsummary(f9_11)\n## \n## Call:\n## lm(formula = y ~ log10(x), data = df9_11)\n## \n## Residuals:\n##      1      2      3      4      5 \n##  7.152 -5.083 -4.698 -6.804  9.433 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  110.060      4.095   26.88 0.000113 ***\n## log10(x)      36.115      2.968   12.17 0.001195 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 8.838 on 3 degrees of freedom\n## Multiple R-squared:  0.9801, Adjusted R-squared:  0.9735 \n## F-statistic: 148.1 on 1 and 3 DF,  p-value: 0.001195\n\n例9-12。\n\ndf9_12 &lt;- foreign::read.spss(\"datasets/例09-12.sav\", to.data.frame = T)\nstr(df9_12)\n## 'data.frame':    15 obs. of  2 variables:\n##  $ x: num  2 5 7 10 14 19 26 31 34 38 ...\n##  $ y: num  54 50 45 37 35 25 20 16 18 13 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:2] \"סԺ\\xcc\\xec\\xca\\xfd\" \"Ԥ\\xba\\xf3ָ\\xca\\xfd\"\n##   ..- attr(*, \"names\")= chr [1:2] \"x\" \"y\"\n\n先画图看趋势：\n\nggplot(df9_12, aes(x,y))+\n  geom_point(size=4)\n\n\n\n\n\n\n\n\n这个图有点像指数函数的图像，我们可以尝试对因变量Y做对数转换，再画图看看：\n\nggplot(df9_12, aes(x,log(y)))+\n  geom_point(size=4)\n\n\n\n\n\n\n\n\n果然就基本上呈直线趋势了，所以我们选择对数转换后的Y建立直线回归方程：\n\nf9_12 &lt;- lm(log(y) ~ x, data = df9_12)\nsummary(f9_12)\n## \n## Call:\n## lm(formula = log(y) ~ x, data = df9_12)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.37241 -0.07073  0.02777  0.05982  0.33539 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  4.037159   0.084103   48.00 5.08e-16 ***\n## x           -0.037974   0.002284  -16.62 3.86e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1794 on 13 degrees of freedom\n## Multiple R-squared:  0.9551, Adjusted R-squared:  0.9516 \n## F-statistic: 276.4 on 1 and 13 DF,  p-value: 3.858e-10",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>双变量回归与相关</span>"
    ]
  },
  {
    "objectID": "三线表绘制.html",
    "href": "三线表绘制.html",
    "title": "8  三线表绘制",
    "section": "",
    "text": "8.1 数据简介\ncompareGroups可用一句代码生成基线资料表、单因素分析表、多因素分析表等，可直接把结果导出为csv、Excel、Word、Markdown、LaTeX、PDF，而且十分美观，大大提高工作效率。\n之前也做过介绍，但是最近发现R包更新后自带数据换了，之前的predimed数据没有了，变成了regicor，所以重新再整理一遍。之前的介绍：使用compareGroups包1行代码生成基线资料表\n官方文档的名字就叫：分组描述（Descriptives-by-groups），充分说明该包的强项就是分组描述，特别适合基线资料表、各类SCI中的table 1的绘制。支持生存数据，可以计算HR、OR、P-for-trend、多重检验的P值等。\n使用方法和之前介绍的基本一样，主要就是3个函数：\n之前R包内置的predimed已经没有了，现在默认演示数据变成了regicor。\n查看数据:\nlibrary(compareGroups)\ndata(\"regicor\")\ndim(regicor)\n## [1] 2294   25\n# str(regicor) # 太长不显示，大家可以自己运行看看结果\n各个变量的信息如下：\n使用该R包的一些注意点：\n如果是生存数据（time-to-event），必须使用Surv()包装数据。比如：\nlibrary(survival)\n\nregicor$tmain &lt;- with(regicor, Surv(tocv, cv == \"Yes\"))\nattr(regicor$tmain, \"label\") &lt;- \"Time to CV event or censoring\"\n这里新建的tmain是生存时间。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>三线表绘制</span>"
    ]
  },
  {
    "objectID": "三线表绘制.html#数据简介",
    "href": "三线表绘制.html#数据简介",
    "title": "8  三线表绘制",
    "section": "",
    "text": "compareGroups：计算\ncreateTable：构建表格\nexport2xxx导出表格\n\n\n\n为了说明这个软件包是如何工作的，我们从REGICOR研究中取了一部分数据。REGICOR是一个 对来自西班牙东北部的参与者进行的横断面研究，包括：人口统计学信息（年龄、性别、身高、体重、腰围等）、血脂特征（总胆固醇和胆固醇、甘油三酯等）、问卷调查信息（体格，活动，生活质量，…）等。此外，心血管事件和 死亡信息来自医院和官方登记处。\n\n\n\n\n\n\n\nName\nLabel\nCodes\n\n\n\n\nid\nIndividual id\n\n\n\nyear\nRecruitment year\n1995; 2000; 2005\n\n\nage\nAge\n\n\n\nsex\nSex\nMale; Female\n\n\nsmoker\nSmoking status\nNever smoker; Current or former &lt; 1y; Former $\\geq$ 1y\n\n\nsbp\nSystolic blood pressure\n\n\n\ndbp\nDiastolic blood pressure\n\n\n\nhisthtn\nHistory of hypertension\nYes; No\n\n\ntxhtn\nHypertension treatment\nNo; Yes\n\n\nchol\nTotal cholesterol\n\n\n\nhdl\nHDL cholesterol\n\n\n\ntriglyc\nTriglycerides\n\n\n\nldl\nLDL cholesterol\n\n\n\nhistchol\nHistory of hyperchol.\nYes; No\n\n\ntxchol\nCholesterol treatment\nNo; Yes\n\n\nheight\nHeight (cm)\n\n\n\nweight\nWeight (Kg)\n\n\n\nbmi\nBody mass index\n\n\n\nphyact\nPhysical activity (Kcal/week)\n\n\n\npcs\nPhysical component\n\n\n\nmcs\nMental component\n\n\n\ncv\nCardiovascular event\nNo; Yes\n\n\ntocv\nDays to cardiovascular event or end of follow-up\n\n\n\ndeath\nOverall death\nNo; Yes\n\n\ntodeath\nDays to overall death or end of follow-up\n\n\n\n\n\n\n\n\n该包的重点是描述数据，不是对数据进行质量控制或其他目的。\n强烈建议数据框中只包含需要描述的数据，对于不需要的数据建议不要包含在数据框中。\n分类变量需要进行因子化。\n可以给各个变量增加label属性以展示其详细信息，该包默认会展示各个变量的label。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>三线表绘制</span>"
    ]
  },
  {
    "objectID": "三线表绘制.html#详细介绍",
    "href": "三线表绘制.html#详细介绍",
    "title": "8  三线表绘制",
    "section": "8.2 详细介绍",
    "text": "8.2 详细介绍\n以year为分组变量，统计各个变量的差异：\n\n# 不要id这个变量\ncompareGroups(year ~ . - id, data=regicor)\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##    var                                              N    p.value \n## 1  Age                                              2294 0.078*  \n## 2  Sex                                              2294 0.506   \n## 3  Smoking status                                   2233 &lt;0.001**\n## 4  Systolic blood pressure                          2280 &lt;0.001**\n## 5  Diastolic blood pressure                         2280 &lt;0.001**\n## 6  History of hypertension                          2286 &lt;0.001**\n## 7  Hypertension treatment                           2251 0.002** \n## 8  Total cholesterol                                2193 &lt;0.001**\n## 9  HDL cholesterol                                  2225 0.208   \n## 10 Triglycerides                                    2231 0.582   \n## 11 LDL cholesterol                                  2126 &lt;0.001**\n## 12 History of hyperchol.                            2273 &lt;0.001**\n## 13 Cholesterol treatment                            2239 &lt;0.001**\n## 14 Height (cm)                                      2259 0.003** \n## 15 Weight (Kg)                                      2259 0.150   \n## 16 Body mass index                                  2259 &lt;0.001**\n## 17 Physical activity (Kcal/week)                    2206 &lt;0.001**\n## 18 Physical component                               2054 0.032** \n## 19 Mental component                                 2054 &lt;0.001**\n## 20 Cardiovascular event                             2163 0.161   \n## 21 Days to cardiovascular event or end of follow-up 2163 0.099*  \n## 22 Overall death                                    2148 &lt;0.001**\n## 23 Days to overall death or end of follow-up        2148 0.252   \n## 24 Time to CV event or censoring                    2163 0.157   \n##    method            selection\n## 1  continuous normal ALL      \n## 2  categorical       ALL      \n## 3  categorical       ALL      \n## 4  continuous normal ALL      \n## 5  continuous normal ALL      \n## 6  categorical       ALL      \n## 7  categorical       ALL      \n## 8  continuous normal ALL      \n## 9  continuous normal ALL      \n## 10 continuous normal ALL      \n## 11 continuous normal ALL      \n## 12 categorical       ALL      \n## 13 categorical       ALL      \n## 14 continuous normal ALL      \n## 15 continuous normal ALL      \n## 16 continuous normal ALL      \n## 17 continuous normal ALL      \n## 18 continuous normal ALL      \n## 19 continuous normal ALL      \n## 20 categorical       ALL      \n## 21 continuous normal ALL      \n## 22 categorical       ALL      \n## 23 continuous normal ALL      \n## 24 Surv [Tmax=1718]  ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n比如第一个变量age，其实使用的方差分析，给出了age在不同year中的pvalue，method显示了该包把age这个变量看做连续型变量且符合正态分布。\n我们可以自己使用方差分析看下结果：\n\nsummary(aov(age ~ year, data = regicor))\n##               Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## year           2    623   311.6   2.556 0.0778 .\n## Residuals   2291 279320   121.9                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n可以看到结果是一样的哈~\nsex这个变量是用的卡方检验：\n\nchisq.test(regicor$sex, regicor$year)\n## \n##  Pearson's Chi-squared test\n## \n## data:  regicor$sex and regicor$year\n## X-squared = 1.364, df = 2, p-value = 0.5056\n\n结果也是一样的。\n支持之使用其中一部分数据，比如只看一下age、smoker、bmi这3个变量在不同的year中的差异，并且只选择性别为女性，对于bmi这个变量，只选择年龄大于50岁的：\n\ncompareGroups(year ~ age + smoker + bmi, data=regicor, \n              selec = list(bmi=age&gt;50), \n              subset = sex==\"Female\")\n## \n## \n## -------- Summary of results by groups of 'year'---------\n## \n## \n##   var             N    p.value  method           \n## 1 Age             1193 0.351    continuous normal\n## 2 Smoking status  1162 &lt;0.001** categorical      \n## 3 Body mass index  709 0.308    continuous normal\n##   selection                     \n## 1 sex == \"Female\"               \n## 2 sex == \"Female\"               \n## 3 (sex == \"Female\") & (age &gt; 50)\n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n连续型变量支持选择不同的统计方法（选择有限），通过method指定即可：\n\ncompareGroups(year ~ age + smoker + triglyc, data=regicor, \n              method = c(triglyc=NA), \n              alpha= 0.01)\n## Warning in cor.test.default(x, as.integer(y), method = \"spearman\"): Cannot\n## compute exact p-value with ties\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##   var            N    p.value  method                selection\n## 1 Age            2294 0.078*   continuous normal     ALL      \n## 2 Smoking status 2233 &lt;0.001** categorical           ALL      \n## 3 Triglycerides  2231 0.762    continuous non-normal ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n\nmethod：统计检验方法\n\n1：数值型变量，正态分布，\n2：数值型变量，非正态分布\n3：分类变量\nNA：使用shapiro.test()决定是否正态分布\n\nalpha：正态性检验的阈值\nmin.dis：所有非因子型向量都会被认为是连续型的，除非某个变量的取值少于5个，可通过此参数更改这个标准。\n\n\nregicor$age7gr &lt;- as.integer(cut(regicor$age, \n                                 breaks = c(-Inf, 40, 45, 50, 55, 65, 70, Inf), \n                                 right = TRUE))\n\ncompareGroups(year ~ age7gr, data = regicor, \n              method = c(age7gr = NA), min.dis = 8)\n## Warning in compare.i(X[, i], y = y, selec.i = selec[i], method.i = method[i], :\n## variable 'age7gr' converted to factor since few different values contained\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##   var    N    p.value method      selection\n## 1 age7gr 2294 0.012** categorical ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n\nmax.ylev：分组变量的最大组数\n\n\ncompareGroups(age7gr ~ sex + bmi + smoker, data = regicor, max.ylev = 7)\n## \n## \n## -------- Summary of results by groups of 'age7gr'---------\n## \n## \n##   var             N    p.value  method            selection\n## 1 Sex             2294 0.950    categorical       ALL      \n## 2 Body mass index 2259 &lt;0.001** continuous normal ALL      \n## 3 Smoking status  2233 &lt;0.001** categorical       ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n显示列的名字，而不是label：\n\ncompareGroups(year ~ age + smoker + bmi, data = regicor, include.label = FALSE)\n## \n## \n## -------- Summary of results by groups of 'year'---------\n## \n## \n##   var    N    p.value  method            selection\n## 1 age    2294 0.078*   continuous normal ALL      \n## 2 smoker 2233 &lt;0.001** categorical       ALL      \n## 3 bmi    2259 &lt;0.001** continuous normal ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\nQ1/Q3：非正态分布的变量默认显示中位数（25%，75%），这个百分位数可以更改，设置为0和1则是最小值和最大值\n\nresu2 &lt;- compareGroups(year ~ age + triglyc, data = regicor, \n                       method = c(triglyc = 2), \n                       Q1 = 0.025, Q3 = 0.975)\n## Warning in cor.test.default(x, as.integer(y), method = \"spearman\"): Cannot\n## compute exact p-value with ties\ncreateTable(resu2)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## _______________________________________________________________________ \n##                    1995            2000            2005       p.overall \n##                    N=431           N=786          N=1077                \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age             54.1 (11.7)     54.3 (11.2)     55.3 (10.6)     0.078   \n## Triglycerides 94.0 [47.0;292] 98.0 [47.0;278] 98.0 [42.0;293]   0.762   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nsimplify：有时某个变量在某个分组中可能样本数为0，这时候要设置为TRUE来排除这样的亚组，因为卡方检验和Fisher精确概率法的计算不能为0。\n下面新建一个变量smk，其中unknown这个分组的样本数为0：\n\nregicor$smk &lt;- regicor$smoker\nlevels(regicor$smk) &lt;- c(\"Never smoker\", \"Current or former &lt; 1y\",\n                         \"Former &gt;= 1y\", \"Unknown\")\nattr(regicor$smk, \"label\") &lt;- \"Smoking 4 cat.\"\ncbind(table(regicor$smk))\n##                        [,1]\n## Never smoker           1201\n## Current or former &lt; 1y  593\n## Former &gt;= 1y            439\n## Unknown                   0\n\n如果不加simplify = FALSE会给出warning：\n\ncompareGroups(year ~ age + smk + bmi, data = regicor)\n## Warning in compare.i(X[, i], y = y, selec.i = selec[i], method.i = method[i], :\n## Some levels of 'smk' are removed since no observation in that/those levels\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##   var             N    p.value  method            selection\n## 1 Age             2294 0.078*   continuous normal ALL      \n## 2 Smoking 4 cat.  2233 &lt;0.001** categorical       ALL      \n## 3 Body mass index 2259 &lt;0.001** continuous normal ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n加simplify = FALSE之后就不再对这个变量进行统计检验了，也就没有P值显示了：\n\ncompareGroups(year ~ age + smk + bmi, data = regicor, simplify = FALSE)\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##   var             N    p.value  method            selection\n## 1 Age             2294 0.078*   continuous normal ALL      \n## 2 Smoking 4 cat.  2233 .        categorical       ALL      \n## 3 Body mass index 2259 &lt;0.001** continuous normal ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n支持更新对象：\n\nres &lt;- compareGroups(year ~ age + sex + smoker + bmi, data = regicor)\nres\n## \n## \n## -------- Summary of results by groups of 'Recruitment year'---------\n## \n## \n##   var             N    p.value  method            selection\n## 1 Age             2294 0.078*   continuous normal ALL      \n## 2 Sex             2294 0.506    categorical       ALL      \n## 3 Smoking status  2233 &lt;0.001** categorical       ALL      \n## 4 Body mass index 2259 &lt;0.001** continuous normal ALL      \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n\nres &lt;- update(res, . ~ . - sex + triglyc + cv + tocv, \n              subset = sex == \"Female\", \n              method = c(triglyc = 2, tocv = 2), \n              selec = list(triglyc = txchol == \"No\"))\n## Warning in cor.test.default(x, as.integer(y), method = \"spearman\"): Cannot\n## compute exact p-value with ties\n## Warning in cor.test.default(x, as.integer(y), method = \"spearman\"): Cannot\n## compute exact p-value with ties\nres\n## \n## \n## -------- Summary of results by groups of 'year'---------\n## \n## \n##   var                                              N    p.value \n## 1 Age                                              1193 0.351   \n## 2 Smoking status                                   1162 &lt;0.001**\n## 3 Body mass index                                  1169 0.084*  \n## 4 Triglycerides                                    1020 0.993   \n## 5 Cardiovascular event                             1121 0.139   \n## 6 Days to cardiovascular event or end of follow-up 1121 0.427   \n##   method                selection                           \n## 1 continuous normal     sex == \"Female\"                     \n## 2 categorical           sex == \"Female\"                     \n## 3 continuous normal     sex == \"Female\"                     \n## 4 continuous non-normal (sex == \"Female\") & (txchol == \"No\")\n## 5 categorical           sex == \"Female\"                     \n## 6 continuous non-normal sex == \"Female\"                     \n## -----\n## Signif. codes:  0 '**' 0.05 '*' 0.1 ' ' 1\n\n支持提取其中的某些信息，比如P值、均值等，方便后续的操作：\n\ndata(SNPs)\ntab &lt;- createTable(compareGroups(casco ~ snp10001 + snp10002 + snp10005 +\n                                   snp10008 + snp10009, SNPs))\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\n\n# 提取P值，方便进行校正\npvals &lt;- getResults(tab, \"p.overall\")\np.adjust(pvals, method = \"BH\")\n##  snp10001  snp10002  snp10005  snp10008  snp10009 \n## 0.7051300 0.7072158 0.7583432 0.7583432 0.7072158\n\n4.6.0以后的版本还增加了计算调整P值的方法，和p.adjust的方法一样，比如Bonferroni/False Discovery Rate等。\n\ncg &lt;- compareGroups(casco ~ snp10001+snp10002+snp10005+snp10008+snp10009, \n                    SNPs)\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\n## Warning in chisq.test(xx, correct = FALSE): Chi-squared approximation may be\n## incorrect\ncreateTable(padjustCompareGroups(cg, method = \"BH\"))\n## \n## --------Summary descriptives table by 'casco'---------\n## \n## _________________________________________ \n##               0          1      p.overall \n##              N=47      N=110              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## snp10001:                         0.705   \n##     CC    2 (4.26%)  10 (9.09%)           \n##     CT    21 (44.7%) 32 (29.1%)           \n##     TT    24 (51.1%) 68 (61.8%)           \n## snp10002:                         0.707   \n##     AA    0 (0.00%)  5 (4.55%)            \n##     AC    25 (53.2%) 53 (48.2%)           \n##     CC    22 (46.8%) 52 (47.3%)           \n## snp10005:                         0.758   \n##     AA    0 (0.00%)  3 (2.73%)            \n##     AG    22 (46.8%) 48 (43.6%)           \n##     GG    25 (53.2%) 59 (53.6%)           \n## snp10008:                         0.758   \n##     CC    30 (63.8%) 74 (67.3%)           \n##     CG    15 (31.9%) 29 (26.4%)           \n##     GG    2 (4.26%)  7 (6.36%)            \n## snp10009:                         0.707   \n##     AA    21 (45.7%) 51 (46.4%)           \n##     AG    25 (54.3%) 54 (49.1%)           \n##     GG    0 (0.00%)  5 (4.55%)            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n支持计算OR值和HR值，如果是二分类会计算OR值，如果是time-to-event数据会计算HR值：\n\n# show.ratio = TRUE展示p.ratio和OR值\nres1 &lt;- compareGroups(cv ~ age + sex + bmi + smoker, data = regicor, ref = 1)\ncreateTable(res1, show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Cardiovascular event'---------\n## \n## ______________________________________________________________________________________ \n##                                 No          Yes            OR        p.ratio p.overall \n##                               N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        54.6 (11.1)  57.5 (11.0) 1.02 [1.00;1.04]  0.017    0.018   \n## Sex:                                                                           0.801   \n##     Male                   996 (48.1%)  46 (50.0%)        Ref.        Ref.             \n##     Female                 1075 (51.9%) 46 (50.0%)  0.93 [0.61;1.41]  0.721            \n## Body mass index            27.6 (4.56)  28.1 (4.48) 1.02 [0.98;1.07]  0.313    0.307   \n## Smoking status:                                                               &lt;0.001   \n##     Never smoker           1099 (54.3%) 37 (40.2%)        Ref.        Ref.             \n##     Current or former &lt; 1y 506 (25.0%)  47 (51.1%)  2.75 [1.77;4.32] &lt;0.001            \n##     Former &gt;= 1y           419 (20.7%)   8 (8.70%)  0.58 [0.25;1.19]  0.142            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nref可以更改参考水平，比如smoker以第一个水平为参考，sex以第二个水平为参考：\n\nres2 &lt;- compareGroups(cv ~ age + sex + bmi + smoker, data = regicor, \n                      ref = c(smoker = 1, sex = 2))\ncreateTable(res2, show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Cardiovascular event'---------\n## \n## ______________________________________________________________________________________ \n##                                 No          Yes            OR        p.ratio p.overall \n##                               N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        54.6 (11.1)  57.5 (11.0) 1.02 [1.00;1.04]  0.017    0.018   \n## Sex:                                                                           0.801   \n##     Male                   996 (48.1%)  46 (50.0%)  1.08 [0.71;1.64]  0.721            \n##     Female                 1075 (51.9%) 46 (50.0%)        Ref.        Ref.             \n## Body mass index            27.6 (4.56)  28.1 (4.48) 1.02 [0.98;1.07]  0.313    0.307   \n## Smoking status:                                                               &lt;0.001   \n##     Never smoker           1099 (54.3%) 37 (40.2%)        Ref.        Ref.             \n##     Current or former &lt; 1y 506 (25.0%)  47 (51.1%)  2.75 [1.77;4.32] &lt;0.001            \n##     Former &gt;= 1y           419 (20.7%)   8 (8.70%)  0.58 [0.25;1.19]  0.142            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n这里的p.overall是通过t检验计算的：(如果不符合正态分布是通过Kruskall-Wallis检验计算)\n\nt.test(age ~ cv, data = regicor)\n## \n##  Welch Two Sample t-test\n## \n## data:  age by cv\n## t = -2.4124, df = 99.303, p-value = 0.01768\n## alternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n## 95 percent confidence interval:\n##  -5.1660205 -0.5031794\n## sample estimates:\n##  mean in group No mean in group Yes \n##          54.62192          57.45652\n\n这个表格中的p.ratio和OR值就是做个逻辑回归得到的：\n\naa &lt;- glm(cv ~ age, data = regicor,family = binomial())\nbroom::tidy(aa,exponentiate = T,conf.int = T)\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   0.0120   0.572       -7.74 1.01e-14  0.00377    0.0357\n## 2 age           1.02     0.00980      2.39 1.69e- 2  1.00       1.04\n\nage的p.value就是表格中的p.ratio，estimate是OR值，conf.low和conf.high是可信区间。\nref.no表示，如果某个变量中有No（或者NO，no，不区分大小写）这个类别，那就以这个类别为参考，这是一个可适用于所有变量的参数。\n\nres &lt;- compareGroups(cv ~ age + sex + bmi + histhtn + txhtn, data = regicor, \n                     ref.no = \"NO\")\ncreateTable(res, show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Cardiovascular event'---------\n## \n## ____________________________________________________________________________________ \n##                               No          Yes            OR        p.ratio p.overall \n##                             N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.6 (11.1)  57.5 (11.0) 1.02 [1.00;1.04]  0.017    0.018   \n## Sex:                                                                         0.801   \n##     Male                 996 (48.1%)  46 (50.0%)        Ref.        Ref.             \n##     Female               1075 (51.9%) 46 (50.0%)  0.93 [0.61;1.41]  0.721            \n## Body mass index          27.6 (4.56)  28.1 (4.48) 1.02 [0.98;1.07]  0.313    0.307   \n## History of hypertension:                                                     0.058   \n##     Yes                  647 (31.3%)  38 (41.3%)  1.54 [1.00;2.36]  0.049            \n##     No                   1418 (68.7%) 54 (58.7%)        Ref.        Ref.             \n## Hypertension treatment:                                                      0.270   \n##     No                   1657 (81.3%) 70 (76.1%)        Ref.        Ref.             \n##     Yes                  382 (18.7%)  22 (23.9%)  1.37 [0.82;2.21]  0.223            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nsex/histhtn/txhtn这几个变量中都有No这个分类，所以都以这个类别为参考。\n对于连续性变量来说，OR值和HR值的解释是自变量每增加一个单位，因变量变化OR倍或HR倍，参数fact.ratio可以控制一个单位具体是多少。比如设置bmi的一个单位是2，age的一个单位是10：\n\nres &lt;- compareGroups(cv ~ age + bmi, data = regicor, \n                     fact.ratio = c(age = 10, bmi = 2))\ncreateTable(res, show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Cardiovascular event'---------\n## \n## __________________________________________________________________________ \n##                     No          Yes            OR        p.ratio p.overall \n##                   N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age             54.6 (11.1) 57.5 (11.0) 1.26 [1.04;1.53]  0.017    0.018   \n## Body mass index 27.6 (4.56) 28.1 (4.48) 1.05 [0.96;1.14]  0.313    0.307   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n通常对于OR值和HR值来说，是相对于因变量的某个类别来说的，比如自变量每增加一个单位，患癌症的风险增加OR倍，可以通过ref.y改成不患癌症的风险增加xx倍。\n\nres &lt;- compareGroups(cv ~ age + sex + bmi + txhtn, data = regicor, ref.y = 2)\ncreateTable(res, show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Cardiovascular event'---------\n## \n## ___________________________________________________________________________________ \n##                              No          Yes            OR        p.ratio p.overall \n##                            N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                     54.6 (11.1)  57.5 (11.0) 0.98 [1.00;0.96]  0.017    0.018   \n## Sex:                                                                        0.801   \n##     Male                996 (48.1%)  46 (50.0%)        Ref.        Ref.             \n##     Female              1075 (51.9%) 46 (50.0%)  1.08 [0.71;1.64]  0.721            \n## Body mass index         27.6 (4.56)  28.1 (4.48) 0.98 [1.02;0.93]  0.313    0.307   \n## Hypertension treatment:                                                     0.270   \n##     No                  1657 (81.3%) 70 (76.1%)        Ref.        Ref.             \n##     Yes                 382 (18.7%)  22 (23.9%)  0.73 [0.45;1.22]  0.223            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\ncreateTable函数用于把compareGroups计算的结果转换为表格，打印在屏幕上或者导出为CSV、LaTeX、HTML、Word、Excel等格式。\n\nres &lt;- compareGroups(year ~ age + sex + smoker + bmi + sbp, data = regicor, \n                     selec = list(sbp = txhtn == \"No\"))\nrestab &lt;- createTable(res)\n\nwhich.table = \"descr\"给出描述性三线表：\n\nprint(restab, which.table = \"descr\")\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ________________________________________________________________________ \n##                               1995        2000        2005     p.overall \n##                               N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex:                                                             0.506   \n##     Male                   206 (47.8%) 390 (49.6%) 505 (46.9%)           \n##     Female                 225 (52.2%) 396 (50.4%) 572 (53.1%)           \n## Smoking status:                                                 &lt;0.001   \n##     Never smoker           234 (56.4%) 414 (54.6%) 553 (52.2%)           \n##     Current or former &lt; 1y 109 (26.3%) 267 (35.2%) 217 (20.5%)           \n##     Former &gt;= 1y           72 (17.3%)  77 (10.2%)  290 (27.4%)           \n## Body mass index            27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  &lt;0.001   \n## Systolic blood pressure    129 (17.4)  130 (20.1)  124 (16.9)   &lt;0.001   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nwhich.table = \"avail\"给出可用数据和方法表：\n\nprint(restab, which.table = \"avail\")\n## \n## \n## \n## ---Available data----\n## \n## ____________________________________________________________________________ \n##                         [ALL] 1995 2000 2005      method          select     \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                     2294  431  786  1077 continuous-normal      ALL      \n## Sex                     2294  431  786  1077    categorical         ALL      \n## Smoking status          2233  415  758  1060    categorical         ALL      \n## Body mass index         2259  423  771  1065 continuous-normal      ALL      \n## Systolic blood pressure 1810  357  649  804  continuous-normal txhtn == \"No\" \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n某些二分类的变量，比如男性/女性这种，可以通过hide不展示其中某个类别，比如不展示sex中的male：\n\nupdate(restab, hide = c(sex = \"Male\"))\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ________________________________________________________________________ \n##                               1995        2000        2005     p.overall \n##                               N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex: Female                225 (52.2%) 396 (50.4%) 572 (53.1%)   0.506   \n## Smoking status:                                                 &lt;0.001   \n##     Never smoker           234 (56.4%) 414 (54.6%) 553 (52.2%)           \n##     Current or former &lt; 1y 109 (26.3%) 267 (35.2%) 217 (20.5%)           \n##     Former &gt;= 1y           72 (17.3%)  77 (10.2%)  290 (27.4%)           \n## Body mass index            27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  &lt;0.001   \n## Systolic blood pressure    129 (17.4)  130 (20.1)  124 (16.9)   &lt;0.001   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nhide.no和ref.no的含义类似，如果某个变量含有no这个类别，可以全部隐藏：\n\nres &lt;- compareGroups(year ~ age + sex + histchol + histhtn, data = regicor)\ncreateTable(res, hide.no = \"no\", hide = c(sex = \"Male\"))\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## _____________________________________________________________________ \n##                            1995        2000        2005     p.overall \n##                            N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                     54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex: Female             225 (52.2%) 396 (50.4%) 572 (53.1%)   0.506   \n## History of hyperchol.   97 (22.5%)  256 (33.2%) 356 (33.2%)  &lt;0.001   \n## History of hypertension 111 (25.8%) 233 (29.6%) 379 (35.5%)  &lt;0.001   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\ndigits用于控制表格中小数点的位数：\n\ncreateTable(res, digits = c(age = 2, sex = 3))\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ____________________________________________________________________________ \n##                              1995          2000          2005      p.overall \n##                              N=431         N=786        N=1077               \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.10 (11.72) 54.34 (11.22) 55.28 (10.63)   0.078   \n## Sex:                                                                 0.506   \n##     Male                 206 (47.796%) 390 (49.618%) 505 (46.890%)           \n##     Female               225 (52.204%) 396 (50.382%) 572 (53.110%)           \n## History of hyperchol.:                                              &lt;0.001   \n##     Yes                   97 (22.5%)    256 (33.2%)   356 (33.2%)            \n##     No                    334 (77.5%)   515 (66.8%)   715 (66.8%)            \n## History of hypertension:                                            &lt;0.001   \n##     Yes                   111 (25.8%)   233 (29.6%)   379 (35.5%)            \n##     No                    320 (74.2%)   553 (70.4%)   690 (64.5%)            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n对于分类变量，默认情况下表格中会给出计数和比例，可通过type参数更改只显示计数或比例：\n\n# 1只显示比例，默认是2都显示，3只显示计数\ncreateTable(res, type = 1)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ______________________________________________________________________ \n##                             1995        2000        2005     p.overall \n##                             N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex:                                                           0.506   \n##     Male                    47.8%       49.6%       46.9%              \n##     Female                  52.2%       50.4%       53.1%              \n## History of hyperchol.:                                        &lt;0.001   \n##     Yes                     22.5%       33.2%       33.2%              \n##     No                      77.5%       66.8%       66.8%              \n## History of hypertension:                                      &lt;0.001   \n##     Yes                     25.8%       29.6%       35.5%              \n##     No                      74.2%       70.4%       64.5%              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nshow.n = TRUE展示每个变量的所有可用数量：\n\n# 注意最后一列\ncreateTable(res, show.n = TRUE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ___________________________________________________________________________ \n##                             1995        2000        2005     p.overall  N   \n##                             N=431       N=786      N=1077                   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   2294 \n## Sex:                                                           0.506   2294 \n##     Male                 206 (47.8%) 390 (49.6%) 505 (46.9%)                \n##     Female               225 (52.2%) 396 (50.4%) 572 (53.1%)                \n## History of hyperchol.:                                        &lt;0.001   2273 \n##     Yes                  97 (22.5%)  256 (33.2%) 356 (33.2%)                \n##     No                   334 (77.5%) 515 (66.8%) 715 (66.8%)                \n## History of hypertension:                                      &lt;0.001   2286 \n##     Yes                  111 (25.8%) 233 (29.6%) 379 (35.5%)                \n##     No                   320 (74.2%) 553 (70.4%) 690 (64.5%)                \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nshow.descr = FALSE表示不展示描述统计部分，只显示P值：\n\ncreateTable(res, show.descr = FALSE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## __________________________________ \n##                          p.overall \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        0.078   \n## Sex:                               \n##     Male                   0.506   \n##     Female                         \n## History of hyperchol.:             \n##     Yes                   &lt;0.001   \n##     No                             \n## History of hypertension:           \n##     Yes                   &lt;0.001   \n##     No                             \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nshow.all = TRUE表示显示所有数量：（注意第一列）\n\ncreateTable(res, show.all = TRUE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ___________________________________________________________________________________ \n##                             [ALL]        1995        2000        2005     p.overall \n##                             N=2294       N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.7 (11.0)  54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex:                                                                        0.506   \n##     Male                 1101 (48.0%) 206 (47.8%) 390 (49.6%) 505 (46.9%)           \n##     Female               1193 (52.0%) 225 (52.2%) 396 (50.4%) 572 (53.1%)           \n## History of hyperchol.:                                                     &lt;0.001   \n##     Yes                  709 (31.2%)  97 (22.5%)  256 (33.2%) 356 (33.2%)           \n##     No                   1564 (68.8%) 334 (77.5%) 515 (66.8%) 715 (66.8%)           \n## History of hypertension:                                                   &lt;0.001   \n##     Yes                  723 (31.6%)  111 (25.8%) 233 (29.6%) 379 (35.5%)           \n##     No                   1563 (68.4%) 320 (74.2%) 553 (70.4%) 690 (64.5%)           \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nshow.p.overall = FALSE表示不显示P值：\n\ncreateTable(res, show.p.overall = FALSE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ____________________________________________________________ \n##                             1995        2000        2005     \n##                             N=431       N=786      N=1077    \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.1 (11.7) 54.3 (11.2) 55.3 (10.6) \n## Sex:                                                         \n##     Male                 206 (47.8%) 390 (49.6%) 505 (46.9%) \n##     Female               225 (52.2%) 396 (50.4%) 572 (53.1%) \n## History of hyperchol.:                                       \n##     Yes                  97 (22.5%)  256 (33.2%) 356 (33.2%) \n##     No                   334 (77.5%) 515 (66.8%) 715 (66.8%) \n## History of hypertension:                                     \n##     Yes                  111 (25.8%) 233 (29.6%) 379 (35.5%) \n##     No                   320 (74.2%) 553 (70.4%) 690 (64.5%) \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n如果因变量有2个以上的类别，可通过show.p.trend = TRUE展示p-for-trend，符合正态分布通过pearson计算，不符合通过spearman计算：\n\n# year这个变量是有3个类别的\ncreateTable(res, show.p.trend = TRUE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ______________________________________________________________________________ \n##                             1995        2000        2005     p.overall p.trend \n##                             N=431       N=786      N=1077                      \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078    0.032  \n## Sex:                                                           0.506    0.544  \n##     Male                 206 (47.8%) 390 (49.6%) 505 (46.9%)                   \n##     Female               225 (52.2%) 396 (50.4%) 572 (53.1%)                   \n## History of hyperchol.:                                        &lt;0.001   &lt;0.001  \n##     Yes                  97 (22.5%)  256 (33.2%) 356 (33.2%)                   \n##     No                   334 (77.5%) 515 (66.8%) 715 (66.8%)                   \n## History of hypertension:                                      &lt;0.001   &lt;0.001  \n##     Yes                  111 (25.8%) 233 (29.6%) 379 (35.5%)                   \n##     No                   320 (74.2%) 553 (70.4%) 690 (64.5%)                   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\nshow.p.mul：分组变量多于两组可以进行两两比较，符合正态分布用Turkey，不符合用Benjamini & Hochberg\n\ncreateTable(res, show.p.mul = TRUE)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ___________________________________________________________________________________________________________________ \n##                             1995        2000        2005     p.overall p.1995 vs 2000 p.1995 vs 2005 p.2000 vs 2005 \n##                             N=431       N=786      N=1077                                                           \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078       0.930          0.143          0.161      \n## Sex:                                                           0.506       0.794          0.794          0.792      \n##     Male                 206 (47.8%) 390 (49.6%) 505 (46.9%)                                                        \n##     Female               225 (52.2%) 396 (50.4%) 572 (53.1%)                                                        \n## History of hyperchol.:                                        &lt;0.001       &lt;0.001         &lt;0.001         1.000      \n##     Yes                  97 (22.5%)  256 (33.2%) 356 (33.2%)                                                        \n##     No                   334 (77.5%) 515 (66.8%) 715 (66.8%)                                                        \n## History of hypertension:                                      &lt;0.001       0.169          0.001          0.015      \n##     Yes                  111 (25.8%) 233 (29.6%) 379 (35.5%)                                                        \n##     No                   320 (74.2%) 553 (70.4%) 690 (64.5%)                                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n如果因变量是2分类或者是生存数据，show.ratio = TRUE可以展示Odds Ratios或者Hazard Ratios：\n\n# 展示OR和p.ratio\ncreateTable(update(res, subset = year != 1995), show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'year'---------\n## \n## ___________________________________________________________________________________ \n##                             2000        2005            OR        p.ratio p.overall \n##                             N=786      N=1077                                       \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                      54.3 (11.2) 55.3 (10.6) 1.01 [1.00;1.02]  0.064    0.066   \n## Sex:                                                                        0.264   \n##     Male                 390 (49.6%) 505 (46.9%)       Ref.        Ref.             \n##     Female               396 (50.4%) 572 (53.1%) 1.12 [0.93;1.34]  0.245            \n## History of hyperchol.:                                                      1.000   \n##     Yes                  256 (33.2%) 356 (33.2%)       Ref.        Ref.             \n##     No                   515 (66.8%) 715 (66.8%) 1.00 [0.82;1.22]  0.988            \n## History of hypertension:                                                    0.010   \n##     Yes                  233 (29.6%) 379 (35.5%)       Ref.        Ref.             \n##     No                   553 (70.4%) 690 (64.5%) 0.77 [0.63;0.93]  0.008            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n生存数据展示HR值：\n\ncreateTable(compareGroups(tmain ~ year + age + sex, data = regicor),\n            show.ratio = TRUE)\n## \n## --------Summary descriptives table by 'Time to CV event or censoring'---------\n## \n## _____________________________________________________________________________ \n##                     No event      Event           HR        p.ratio p.overall \n##                      N=2071       N=92                                        \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Recruitment year:                                                     0.157   \n##     1995          388 (18.7%)  10 (10.9%)        Ref.        Ref.             \n##     2000          706 (34.1%)  35 (38.0%)  1.95 [0.96;3.93]  0.063            \n##     2005          977 (47.2%)  47 (51.1%)  1.82 [0.92;3.59]  0.087            \n## Age               54.6 (11.1)  57.5 (11.0) 1.02 [1.00;1.04]  0.021    0.021   \n## Sex:                                                                  0.696   \n##     Male          996 (48.1%)  46 (50.0%)        Ref.        Ref.             \n##     Female        1075 (51.9%) 46 (50.0%)  0.92 [0.61;1.39]  0.696            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n这里的p.overall是logrank检验的P值，比如用sex这个变量演示下计算方法：\n\n# P=0.7（四舍五入了），和上面一样的\nsurvdiff(Surv(tocv,cv==\"Yes\")~sex,data = regicor)\n## Call:\n## survdiff(formula = Surv(tocv, cv == \"Yes\") ~ sex, data = regicor)\n## \n## n=2163, 131 observations deleted due to missingness.\n## \n##               N Observed Expected (O-E)^2/E (O-E)^2/V\n## sex=Male   1042       46     44.1    0.0793     0.152\n## sex=Female 1121       46     47.9    0.0731     0.152\n## \n##  Chisq= 0.2  on 1 degrees of freedom, p= 0.7\n\n同理，p.ratio是做cox回归得到的（和上面的分类数据类似，t检验和逻辑回归）：\n\naa &lt;- coxph(Surv(tocv,cv==\"Yes\")~sex,data = regicor)\nbroom::tidy(aa)\n## # A tibble: 1 × 5\n##   term      estimate std.error statistic p.value\n##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 sexFemale  -0.0814     0.209    -0.390   0.696\n\ndigits.ratio控制小数点位数：\n\ncreateTable(compareGroups(tmain ~ year + age + sex, data = regicor),\n            show.ratio = TRUE,\n            digits.ratio = 3)\n## \n## --------Summary descriptives table by 'Time to CV event or censoring'---------\n## \n## ________________________________________________________________________________ \n##                     No event      Event            HR          p.ratio p.overall \n##                      N=2071       N=92                                           \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Recruitment year:                                                        0.157   \n##     1995          388 (18.7%)  10 (10.9%)         Ref.          Ref.             \n##     2000          706 (34.1%)  35 (38.0%)  1.946 [0.964;3.930]  0.063            \n##     2005          977 (47.2%)  47 (51.1%)  1.816 [0.918;3.593]  0.087            \n## Age               54.6 (11.1)  57.5 (11.0) 1.022 [1.003;1.041]  0.021    0.021   \n## Sex:                                                                     0.696   \n##     Male          996 (48.1%)  46 (50.0%)         Ref.          Ref.             \n##     Female        1075 (51.9%) 46 (50.0%)  0.922 [0.613;1.387]  0.696            \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n在print或者导出表格时，header.labels可以修改某些关键列的名称：\n\ntab &lt;- createTable(compareGroups(tmain ~ year + age + sex, data = regicor),\n                   show.all = TRUE)\nprint(tab, header.labels = c(p.overall = \"p-value\", all = \"All\"))\n## \n## --------Summary descriptives table by 'Time to CV event or censoring'---------\n## \n## _______________________________________________________________ \n##                       All        No event      Event    p-value \n##                      N=2163       N=2071       N=92             \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Recruitment year:                                        0.157  \n##     1995          398 (18.4%)  388 (18.7%)  10 (10.9%)          \n##     2000          741 (34.3%)  706 (34.1%)  35 (38.0%)          \n##     2005          1024 (47.3%) 977 (47.2%)  47 (51.1%)          \n## Age               54.7 (11.1)  54.6 (11.1)  57.5 (11.0)  0.021  \n## Sex:                                                     0.696  \n##     Male          1042 (48.2%) 996 (48.1%)  46 (50.0%)          \n##     Female        1121 (51.8%) 1075 (51.9%) 46 (50.0%)          \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n支持按照行合并表格：\n\nrestab1 &lt;- createTable(compareGroups(year ~ age + sex, data = regicor))\nrestab2 &lt;- createTable(compareGroups(year ~ bmi + smoker, data = regicor))\nrbind(`Non-modifiable risk factors`=restab1,`Modifiable risk factors`=restab2)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ____________________________________________________________________________ \n##                                   1995        2000        2005     p.overall \n##                                   N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Non-modifiable risk factors:\n##     Age                        54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n##     Sex:                                                             0.506   \n##         Male                   206 (47.8%) 390 (49.6%) 505 (46.9%)           \n##         Female                 225 (52.2%) 396 (50.4%) 572 (53.1%)           \n## Modifiable risk factors:\n##     Body mass index            27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  &lt;0.001   \n##     Smoking status:                                                 &lt;0.001   \n##         Never smoker           234 (56.4%) 414 (54.6%) 553 (52.2%)           \n##         Current or former &lt; 1y 109 (26.3%) 267 (35.2%) 217 (20.5%)           \n##         Former &gt;= 1y           72 (17.3%)  77 (10.2%)  290 (27.4%)           \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n按照列合并表格，也就是分层表格：\n\nres &lt;- compareGroups(year ~ age + smoker + bmi + histhtn, data = regicor)\nalltab &lt;- createTable(res, show.p.overall = FALSE)\nfemaletab &lt;- createTable(update(res, subset = sex == \"Female\"), \n                         show.p.overall = FALSE)\nmaletab &lt;- createTable(update(res, subset = sex == \"Male\"), \n                       show.p.overall = FALSE)\n\ncbind(ALL = alltab, FEMALE = femaletab, MALE = maletab)\n## \n## --------Summary descriptives table ---------\n## \n## ________________________________________________________________________________________________________________________________________\n##                                            ALL                                FEMALE                                MALE                \n##                            ___________________________________  ___________________________________  ___________________________________\n##                               1995        2000        2005         1995        2000        2005         1995        2000        2005     \n##                               N=431       N=786      N=1077        N=225       N=396       N=572        N=206       N=390       N=505    \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n## Age                        54.1 (11.7) 54.3 (11.2) 55.3 (10.6)  54.1 (11.7) 54.4 (11.2) 55.2 (10.6)  54.1 (11.8) 54.3 (11.2) 55.4 (10.7) \n## Smoking status:                                                                                                                          \n##     Never smoker           234 (56.4%) 414 (54.6%) 553 (52.2%)  182 (83.1%) 302 (79.3%) 416 (74.0%)  52 (26.5%)  112 (29.7%) 137 (27.5%) \n##     Current or former &lt; 1y 109 (26.3%) 267 (35.2%) 217 (20.5%)  32 (14.6%)  68 (17.8%)  83 (14.8%)   77 (39.3%)  199 (52.8%) 134 (26.9%) \n##     Former &gt;= 1y           72 (17.3%)  77 (10.2%)  290 (27.4%)   5 (2.28%)  11 (2.89%)  63 (11.2%)   67 (34.2%)  66 (17.5%)  227 (45.6%) \n## Body mass index            27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  27.2 (4.57) 28.0 (5.25) 27.3 (5.39)  26.9 (3.64) 28.2 (3.89) 27.9 (3.58) \n## History of hypertension:                                                                                                                 \n##     Yes                    111 (25.8%) 233 (29.6%) 379 (35.5%)  61 (27.1%)  123 (31.1%) 198 (34.8%)  50 (24.3%)  110 (28.2%) 181 (36.2%) \n##     No                     320 (74.2%) 553 (70.4%) 690 (64.5%)  164 (72.9%) 273 (68.9%) 371 (65.2%)  156 (75.7%) 280 (71.8%) 319 (63.8%) \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n4.0版本以后提供了strataTable用于快速创建分层表格：\n\nres &lt;- compareGroups(year ~ age + bmi + smoker + histchol + histhtn, regicor)\nrestab &lt;- createTable(res, hide.no = \"no\")\n\nstrataTable(restab, \"sex\")\n## \n## --------Summary descriptives table ---------\n## \n## _______________________________________________________________________________________________________________________\n##                                                Male                                          Female                    \n##                            _____________________________________________  _____________________________________________\n##                               1995        2000        2005     p.overall     1995        2000        2005     p.overall \n##                               N=206       N=390       N=505                  N=225       N=396       N=572              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n## Age                        54.1 (11.8) 54.3 (11.2) 55.4 (10.7)   0.212    54.1 (11.7) 54.4 (11.2) 55.2 (10.6)   0.351   \n## Body mass index            26.9 (3.64) 28.2 (3.89) 27.9 (3.58)  &lt;0.001    27.2 (4.57) 28.0 (5.25) 27.3 (5.39)   0.084   \n## Smoking status:                                                 &lt;0.001                                         &lt;0.001   \n##     Never smoker           52 (26.5%)  112 (29.7%) 137 (27.5%)            182 (83.1%) 302 (79.3%) 416 (74.0%)           \n##     Current or former &lt; 1y 77 (39.3%)  199 (52.8%) 134 (26.9%)            32 (14.6%)  68 (17.8%)  83 (14.8%)            \n##     Former &gt;= 1y           67 (34.2%)  66 (17.5%)  227 (45.6%)             5 (2.28%)  11 (2.89%)  63 (11.2%)            \n## History of hyperchol.      48 (23.3%)  138 (35.8%) 167 (33.2%)   0.007    49 (21.8%)  118 (30.6%) 189 (33.3%)   0.006   \n## History of hypertension    50 (24.3%)  110 (28.2%) 181 (36.2%)   0.002    61 (27.1%)  123 (31.1%) 198 (34.8%)   0.097   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n以上介绍的创建表格基本上是两步，首先compareGroups，然后createTable，为了方便，作者提供了一个descrTable，直接完成以上两步：\n\ndescrTable(year ~ age + bmi + smoker + histchol + histhtn, data = regicor)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ________________________________________________________________________ \n##                               1995        2000        2005     p.overall \n##                               N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                        54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Body mass index            27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  &lt;0.001   \n## Smoking status:                                                 &lt;0.001   \n##     Never smoker           234 (56.4%) 414 (54.6%) 553 (52.2%)           \n##     Current or former &lt; 1y 109 (26.3%) 267 (35.2%) 217 (20.5%)           \n##     Former &gt;= 1y           72 (17.3%)  77 (10.2%)  290 (27.4%)           \n## History of hyperchol.:                                          &lt;0.001   \n##     Yes                    97 (22.5%)  256 (33.2%) 356 (33.2%)           \n##     No                     334 (77.5%) 515 (66.8%) 715 (66.8%)           \n## History of hypertension:                                        &lt;0.001   \n##     Yes                    111 (25.8%) 233 (29.6%) 379 (35.5%)           \n##     No                     320 (74.2%) 553 (70.4%) 690 (64.5%)           \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n上面介绍的所有参数descrTable都是支持的。\n支持的格式非常丰富：\n\nexport2csv(restab, file='table1.csv'), 导出为CSV\nexport2html(restab, file='table1.html'), 导出为HTML\nexport2latex(restab, file='table1.tex'), 导出为LaTeX\nexport2pdf(restab, file='table1.pdf'), 导出为PDF\nexport2md(restab, file='table1.md'), 导出为Markdown\nexport2word(restab, file='table1.docx'), 导出为Word\nexport2xls(restab, file='table1.xlsx'), 导出为Excel\n\n导出时还支持各种格式调整，比如添加阴影等。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>三线表绘制</span>"
    ]
  },
  {
    "objectID": "统计绘图.html",
    "href": "统计绘图.html",
    "title": "9  统计绘图",
    "section": "",
    "text": "9.1 条形图\n例10-4。条形图。\ndata10_4 &lt;- data.frame(`灌注方法`=c(\"方法1\",\"方法2\",\"方法3\"),\n                       rate = c(17.9,20.8,33.3))\ndata10_4\n##   灌注方法 rate\n## 1    方法1 17.9\n## 2    方法2 20.8\n## 3    方法3 33.3\nlibrary(ggplot2)\nlibrary(ggprism)\n\nggplot(data10_4, aes(`灌注方法`,rate))+\n  geom_bar(stat = \"identity\",fill=\"white\",color=\"black\",width = 0.4)+\n  ylab(\"再发率（%）\")+\n  scale_y_continuous(expand = c(0,0))+\n  theme_classic()+\n  theme(axis.title = element_text(color = \"black\",size = 15))",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>统计绘图</span>"
    ]
  },
  {
    "objectID": "统计绘图.html#分组条形图",
    "href": "统计绘图.html#分组条形图",
    "title": "9  统计绘图",
    "section": "9.2 分组条形图",
    "text": "9.2 分组条形图\n例10-5。分组条形图。\n\nlibrary(haven)\ndata10_5 &lt;- haven::read_sav(\"datasets/例10-05.sav\")\ndata10_5 &lt;- as_factor(data10_5)\ndata10_5\n## # A tibble: 4 × 3\n##   year  agent  rate\n##   &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;\n## 1 2005  男       75\n## 2 2005  女       60\n## 3 2010  男       56\n## 4 2010  女       53\n\n\nlibrary(ggplot2)\n\nggplot(data10_5, aes(agent,rate))+\n  geom_bar(stat = \"identity\",aes(fill=year),position = \"dodge\")+\n  labs(x=\"性别\",y=\"患龋率（%）\",fill=\"年份\")+\n  scale_y_continuous(expand = c(0,0))+\n  theme_classic()+\n  theme(axis.title = element_text(color = \"black\",size = 15))",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>统计绘图</span>"
    ]
  },
  {
    "objectID": "统计绘图.html#饼图",
    "href": "统计绘图.html#饼图",
    "title": "9  统计绘图",
    "section": "9.3 饼图",
    "text": "9.3 饼图\n例10-6。饼图。\n\ndata10_6 &lt;- data.frame(`失败原因`=c(\"无菌性松动\",\"感染\",\"假体周围骨折\",\"假体不稳定\",\"其他\"),\n                       `数量`=c(226,52,22,17,10))\n\nlibrary(dplyr)\n\ndata10_6 &lt;- data10_6 %&gt;% \n  arrange(desc(`数量`)) %&gt;% \n  mutate(`失败原因`=factor(`失败原因`,levels=c(\"无菌性松动\",\"感染\",\n                                       \"假体周围骨折\",\"假体不稳定\",\"其他\"))) %&gt;% \n  mutate(prop = round(`数量` / sum(`数量`),2),\n         prop = scales::percent(prop))\n\ndata10_6\n##       失败原因 数量 prop\n## 1   无菌性松动  226  69%\n## 2         感染   52  16%\n## 3 假体周围骨折   22   7%\n## 4   假体不稳定   17   5%\n## 5         其他   10   3%\n\n由于ggplot2的大佬们普遍认为饼图是一种很差劲的图形，所以ggplot2对饼图的支持并不好。\n\n# 默认的大概是这种程度\nggplot(data10_6, aes(x=\"\",y=`数量`,fill=`失败原因`))+\n  geom_bar(stat = \"identity\",width = 1,color=\"white\")+\n  geom_text(aes(label = prop),position = position_stack(vjust = 0.5))+\n  coord_polar(\"y\", start=0)+\n  theme_void()",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>统计绘图</span>"
    ]
  },
  {
    "objectID": "统计绘图.html#百分比条形图",
    "href": "统计绘图.html#百分比条形图",
    "title": "9  统计绘图",
    "section": "9.4 百分比条形图",
    "text": "9.4 百分比条形图\n例10-7。百分比条形图。\n\nlibrary(haven)\ndata10_7 &lt;- haven::read_sav(\"datasets/例10-07.sav\",encoding = \"GBK\")\ndata10_7 &lt;- as_factor(data10_7)\ndata10_7\n## # A tibble: 12 × 3\n##    year   reason   percent\n##    &lt;fct&gt;  &lt;fct&gt;      &lt;dbl&gt;\n##  1 1996年 肺炎        23.4\n##  2 1996年 早产        14.2\n##  3 1996年 出生窒息    14.1\n##  4 1996年 腹泻         5.6\n##  5 1996年 意外窒息     4.1\n##  6 1996年 其它        38.6\n##  7 2000年 肺炎        19.5\n##  8 2000年 早产        17  \n##  9 2000年 出生窒息    15.9\n## 10 2000年 腹泻         4.9\n## 11 2000年 意外窒息     3.7\n## 12 2000年 其它        39\n\n\nlibrary(scales)\n\nggplot(data10_7, aes(year, percent, fill=reason))+ \n  geom_bar(stat = \"identity\",position = \"stack\",width = 0.5,color=\"black\")+\n  labs(fill=\"\",x=\"\",y=\"\")+\n  scale_y_continuous(labels = percent_format(scale = 1))+\n  guides(fill=guide_legend(reverse = T))+\n  theme_bw()+\n  theme(axis.text = element_text(size = 18, colour = \"black\"),\n        legend.text = element_text(size = 18, colour = \"black\"),\n        legend.position = \"bottom\")+\n  coord_flip()\n\n\n\n\n\n\n\n#ggsave(\"xxxx.png\",width=10,height=5,dpi=300)",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>统计绘图</span>"
    ]
  },
  {
    "objectID": "统计绘图.html#折线图",
    "href": "统计绘图.html#折线图",
    "title": "9  统计绘图",
    "section": "9.5 折线图",
    "text": "9.5 折线图\n例10-8。折线图。\n\nlibrary(haven)\ndata10_8 &lt;- haven::read_sav(\"datasets/例10-08.sav\",encoding = \"GBK\")\ndata10_8 &lt;- as_factor(data10_8)\ndata10_8\n## # A tibble: 10 × 3\n##    year  agent counts\n##    &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n##  1 2006  男      5500\n##  2 2006  女      2500\n##  3 2007  男      6000\n##  4 2007  女      3000\n##  5 2008  男      9000\n##  6 2008  女      3500\n##  7 2009  男     11000\n##  8 2009  女      5000\n##  9 2010  男     11000\n## 10 2010  女      5000\n\n\nggplot(data10_8, aes(year,counts))+\n  geom_line(aes(group = agent,linetype=agent))+\n  labs(x=\"年份\",y=\"布氏菌病发病人数\",linetype=\"性别\")+\n  theme_classic()+\n  theme(axis.text = element_text(size = 18, colour = \"black\"),\n        axis.title = element_text(color = \"black\",size = 18),\n        legend.text = element_text(size = 18, colour = \"black\"),\n        legend.title = element_text(size = 18, colour = \"black\"))",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>统计绘图</span>"
    ]
  },
  {
    "objectID": "统计绘图.html#点线图",
    "href": "统计绘图.html#点线图",
    "title": "9  统计绘图",
    "section": "9.6 点线图",
    "text": "9.6 点线图\n例10-9。点线图。\n\nlibrary(haven)\ndata10_9 &lt;- haven::read_sav(\"datasets/例10-09.sav\",encoding = \"GBK\")\ndata10_9 &lt;- as_factor(data10_9)\ndata10_9\n## # A tibble: 10 × 3\n##     year 病型   发病率\n##    &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;\n##  1  1997 艾滋病 0.0069\n##  2  1998 艾滋病 0.0177\n##  3  1999 艾滋病 0.0187\n##  4  2000 艾滋病 0.0312\n##  5  2001 艾滋病 0.0468\n##  6  1997 梅毒   3.76  \n##  7  1998 梅毒   4.58  \n##  8  1999 梅毒   5.72  \n##  9  2000 梅毒   6.09  \n## 10  2001 梅毒   6.27\n\n\np1 &lt;- ggplot(data10_9, aes(year,`发病率`))+\n  geom_line(aes(group = `病型`,linetype=`病型`))+\n  geom_point(aes(group = `病型`,shape=`病型`),size=4)+\n  labs(x=\"年份\",y=\"发病率（1/10万）\")+\n  theme_classic()+\n  theme(axis.text = element_text(size = 14, colour = \"black\"),\n        axis.title = element_text(color = \"black\",size = 14),\n        legend.text = element_text(size = 14, colour = \"black\"),\n        legend.title = element_text(size = 14, colour = \"black\"))\n  \np2 &lt;- ggplot(data10_9, aes(year,log10(`发病率`)))+ # 不知道课本取的log几\n  geom_line(aes(group = `病型`,linetype=`病型`))+\n  geom_point(aes(group = `病型`,shape=`病型`),size=4)+\n  labs(x=\"年份\",y=\"发病率（1/10万）\")+\n  theme_classic()+\n  theme(axis.text = element_text(size = 14, colour = \"black\"),\n        axis.title = element_text(color = \"black\",size = 14),\n        legend.text = element_text(size = 14, colour = \"black\"),\n        legend.title = element_text(size = 14, colour = \"black\"))\n\nlibrary(patchwork)\np1+p2+plot_layout(guides = \"collect\")",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>统计绘图</span>"
    ]
  },
  {
    "objectID": "统计绘图.html#直方图",
    "href": "统计绘图.html#直方图",
    "title": "9  统计绘图",
    "section": "9.7 直方图",
    "text": "9.7 直方图\n例10-10。直方图。\n\nlibrary(haven)\ndata10_10 &lt;- haven::read_sav(\"datasets/例10-10.sav\",encoding = \"GBK\")\ndata10_10 &lt;- as_factor(data10_10)\ndata10_10\n## # A tibble: 16 × 2\n##    age   count\n##    &lt;fct&gt; &lt;dbl&gt;\n##  1 0-        7\n##  2 1-       15\n##  3 2-        8\n##  4 3-       11\n##  5 4-       14\n##  6 5-       14\n##  7 6-        8\n##  8 7-        6\n##  9 8-        3\n## 10 9-        2\n## 11 10-       8\n## 12 15-       3\n## 13 20-       1\n## 14 25-       2\n## 15 30-       1\n## 16 35-40     1\n\n下面这个其实假的直方图（虽然和课本中的看起来差不多），因为没给原始数据，给的是计数好的数据，所以是用条形图伪装的直方图：\n\nggplot(data10_10, aes(age,count))+\n  geom_bar(stat = \"identity\",fill=\"white\",color=\"black\",\n           width = 1,position = position_dodge(width = 1))+\n  labs(x=\"年龄（岁）\",y=\"每岁病例数\")+\n  scale_y_continuous(expand = c(0,0))+\n  theme_classic()+\n  theme(axis.title = element_text(color = \"black\",size = 15))",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>统计绘图</span>"
    ]
  },
  {
    "objectID": "统计绘图.html#地图",
    "href": "统计绘图.html#地图",
    "title": "9  统计绘图",
    "section": "9.8 地图",
    "text": "9.8 地图\n例10-11。地图。没给数据，直接自己编一个。\n首先下载中国地图。中国地图下载地址：地图选择器\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(dplyr)\n\nchina_map &lt;- st_read(\"datasets/中华人民共和国.json\")\n## Reading layer `中华人民共和国' from data source \n##   `F:\\R_books\\medstat_quartobook\\datasets\\中华人民共和国.json' \n##   using driver `GeoJSON'\n## Simple feature collection with 35 features and 10 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 73.50235 ymin: 3.397162 xmax: 135.0957 ymax: 53.56327\n## Geodetic CRS:  WGS 84\n\n然后给每个省编点数据：\n\nset.seed(123)\nchina_map &lt;- china_map %&gt;% \n  mutate(name_short=substr(name,1,2),\n         number = sample(10:100,35,replace=F),\n         group = sample(paste0(\"group\",1:5),35,replace=T))\n\nchina_map$name_short[c(5,8)] &lt;- c(\"内蒙古\",\"黑龙江\")\n\n画图即可,ggplot2画地图非常厉害，下面这个只是非常基础的，可以进行非常多的修改。\n\nggplot(data = china_map) + \n  geom_sf(aes(fill=group)) + \n  geom_sf_text(aes(label = name_short),nudge_y = 0,size=2)+\n  geom_sf_text(aes(label = number),nudge_y = -1,size=2)+\n  theme_minimal()\n## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\n## give correct results for longitude/latitude data\n## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\n## give correct results for longitude/latitude data",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>统计绘图</span>"
    ]
  },
  {
    "objectID": "统计绘图.html#箱线图",
    "href": "统计绘图.html#箱线图",
    "title": "9  统计绘图",
    "section": "9.9 箱线图",
    "text": "9.9 箱线图\n例10-12。箱线图。没给数据，直接用R语言自带的iris数据演示一下。\n\nlibrary(ggplot2)\n\nggplot(iris, aes(Species,Sepal.Length))+\n  stat_boxplot(geom = \"errorbar\",width = 0.2)+\n  geom_boxplot()",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>统计绘图</span>"
    ]
  },
  {
    "objectID": "统计绘图.html#茎叶图",
    "href": "统计绘图.html#茎叶图",
    "title": "9  统计绘图",
    "section": "9.10 茎叶图",
    "text": "9.10 茎叶图\n例10-13。茎叶图。\n\nlibrary(haven)\ndata10_13 &lt;- haven::read_sav(\"datasets/例10-13.sav\",encoding = \"GBK\")\ndata10_13 &lt;- as_factor(data10_13)\ndata10_13\n## # A tibble: 138 × 1\n##      rbc\n##    &lt;dbl&gt;\n##  1  3.96\n##  2  3.77\n##  3  4.63\n##  4  4.56\n##  5  4.66\n##  6  4.61\n##  7  4.98\n##  8  5.28\n##  9  5.11\n## 10  4.92\n## # ℹ 128 more rows\n\nR自带函数就可以画（但是这个图很少用）：\n\nstem(data10_13$rbc,scale = 1)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##   30 | 7\n##   31 | \n##   32 | 17\n##   33 | 9\n##   34 | 2\n##   35 | 299\n##   36 | 0124467789\n##   37 | 12266679\n##   38 | 3399\n##   39 | 166667778\n##   40 | 11122223344\n##   41 | 2234666789\n##   42 | 000011133345566666667888999\n##   43 | 01223466666\n##   44 | 12279\n##   45 | 4566677\n##   46 | 111366899\n##   47 | 15666\n##   48 | 139\n##   49 | 258\n##   50 | 13\n##   51 | 12\n##   52 | 348\n##   53 | \n##   54 | 6",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>统计绘图</span>"
    ]
  },
  {
    "objectID": "统计绘图.html#误差条图",
    "href": "统计绘图.html#误差条图",
    "title": "9  统计绘图",
    "section": "9.11 误差条图",
    "text": "9.11 误差条图\n例10-14。误差条图。\n\nlibrary(haven)\ndata10_14 &lt;- haven::read_sav(\"datasets/例10-14.sav\",encoding = \"GBK\")\ndata10_14 &lt;- as_factor(data10_14)\ndata10_14\n## # A tibble: 120 × 2\n##    group     dmdz\n##    &lt;fct&gt;    &lt;dbl&gt;\n##  1 安慰剂组  3.53\n##  2 安慰剂组  4.59\n##  3 安慰剂组  4.34\n##  4 安慰剂组  2.66\n##  5 安慰剂组  3.59\n##  6 安慰剂组  3.13\n##  7 安慰剂组  2.64\n##  8 安慰剂组  2.56\n##  9 安慰剂组  3.5 \n## 10 安慰剂组  3.25\n## # ℹ 110 more rows\n\n先计算每个组的均值和可信区间：\n\n95%可信区间的计算：均值±1.96*标准误，见课本第一章第三节：总体均数的估计\n\n\nlibrary(dplyr)\n\ndata10_14_1 &lt;- data10_14 %&gt;% \n  group_by(group) %&gt;% \n  summarise(mm = mean(dmdz),\n            lower = mm - 1.96*(sd(dmdz)/sqrt(30)),\n            upper = mm + 1.96*(sd(dmdz)/sqrt(30)))\ndata10_14_1\n## # A tibble: 4 × 4\n##   group       mm lower upper\n##   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 安慰剂组  3.43  3.17  3.69\n## 2 新药2.4   2.72  2.49  2.94\n## 3 新药4.8   2.70  2.52  2.88\n## 4 新药7.2   1.97  1.70  2.23\n\n\nggplot(data10_14_1)+\n  geom_point(aes(group,mm),size=4,shape=0)+\n  geom_errorbar(aes(x=group,ymin=lower,ymax=upper),\n                width=0.1)+\n  theme_classic()+\n  labs(x=\"分组\",y=\"95%CI\")+\n  theme_classic()+\n  theme(axis.title = element_text(color = \"black\",size = 15))",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>统计绘图</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html",
    "href": "1011-samplesize.html",
    "title": "10  样本量计算",
    "section": "",
    "text": "10.1 t检验的样本量计算\n对于t检验，可以使用pwr.t.test(n= , d= , sig.level= , power= , type= , alternative= )计算样本量，其中： - n：样本量 - d：效应值，即标准化的均值之差，d = (μ1 - μ2) / σ，也就是（组1均值 - 组2均值）/ 标准差 - sig.level：显著性水平，默认值0.05 - power：功效 - type：检验类型：两样本t检验（two.sample），单样本t检验（one.sample），配对t检验（paired），默认两样本t检验 - alternative：双侧检验还是单侧检验，双侧（two.sided），单侧（less或者greater），默认双侧检验",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#t检验的样本量计算",
    "href": "1011-samplesize.html#t检验的样本量计算",
    "title": "10  样本量计算",
    "section": "",
    "text": "10.1.1 单样本t检验（样本均数和已知总体均数比较）\n使用课本例36-3的例子。\n用某药治疗矽肺患者，估计可增加尿矽排出量，其标准差为25mg/L，若要求以α=0.05，β=0.1的概率，能辨别出尿矽排出量平均增加10mg/L，问需要多少矽肺患者做实验？\n\nlibrary(pwr)\n\npwr.t.test(d = 10/25, \n           sig.level = 0.05,\n           power = 1-0.1,\n           type = \"one.sample\",\n           alternative = \"greater\"\n           )\n## \n##      One-sample t test power calculation \n## \n##               n = 54.90553\n##               d = 0.4\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = greater\n\nn = 54.90553，结果和课本一模一样！是不是非常简单？\n单样本t检验也可以使用R自带的函数进行计算：\n\npower.t.test(delta = 10,\n             sd = 25,\n             sig.level = 0.05,\n             power = 1-0.1,\n             type = \"one.sample\",\n             alternative = \"one.sided\"\n             )\n## \n##      One-sample t test power calculation \n## \n##               n = 54.90553\n##           delta = 10\n##              sd = 25\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = one.sided\n\n结果都是一样的~\n\n\n10.1.2 两样本t检验（两样本均数比较）\n使用课本例36-4的例子\n在做两种处理动物冠状静脉窦的血流量实验时，比较A处理动物和B处理动物的平均血流量增加，设两处理的标准差相等。若要求以α=0.05，β=0.1的概率，达到能辨别出两者增加的差别是其标准差的60%，需要多少实验动物？\n感觉和小学做应用题差不多… 两者增加的差别是其标准差的60%，也就是 (μ1 - μ2) / σ = 0.6。\n\nlibrary(pwr)\n\npwr.t.test(d = 0.6,\n           sig.level = 0.05,\n           power = 1 - 0.1,\n           type = \"two.sample\",\n           alternative = \"two.sided\"\n           )\n## \n##      Two-sample t test power calculation \n## \n##               n = 59.35155\n##               d = 0.6\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\n\nn = 59.35155，和课本结果一模一样，每组需要大约60例！\n两样本t检验也可以使用R自带的函数power.t.test()进行计算，但是例题中的这种情况刚好没有给出具体的两组间差值和标准差，所以就不能用了。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#多样本均数比较",
    "href": "1011-samplesize.html#多样本均数比较",
    "title": "10  样本量计算",
    "section": "10.2 多样本均数比较",
    "text": "10.2 多样本均数比较\n使用课本例36-5的例子。\n拟用4种方法治疗贫血患者，估计治疗后血红蛋白增加的均数分别为18,13,16,10，标准差分别为10,9,9,8，设α=0.05，β=0.1，若要得出有差别的结论，每组需要多少例？\n这是一个完全随机设计多样本比较的方差分析的例子，相信大家都能看出来！\n但是，在R里面计算种类型的样本量非常困难，原因在于效应量effect size很难计算出来，最终结果也和课本上面的公式计算出来的样本量不一样，所以我推荐用PASS软件，点点点即可！\n这种情况使用函数pwr.anova.test(k= , n= , f= , sig.level= , power= )计算，其中 f是效应量effect size，计算方法如下：\n\n\n\n\n\n\n\n\n\nk是组数，其余的和t检验的相同。\n首先我们要计算f值，但是根据这个公式，很明显是计算不出来的！\n如果使用R自带函数power.anova.test(groups = NULL, n = NULL, between.var = NULL, within.var = NULL,sig.level = 0.05, power = NULL)函数计算，因为无法计算within.var，所以也是行不通的。\n还是乖乖用PASS吧…\n如果有大佬知道怎么计算，欢迎留言告知~",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#样本率和已知总体率的比较",
    "href": "1011-samplesize.html#样本率和已知总体率的比较",
    "title": "10  样本量计算",
    "section": "10.3 样本率和已知总体率的比较",
    "text": "10.3 样本率和已知总体率的比较\n使用课本例36-6的例子。\n已知常规方法治疗某种病的有效率是80%，现试验一种新的额治疗方法，预计有效率是90%，设α=0.05，β=0.1，问需要多少病例才能发现两种方法的有效率有10%的差别？\n\n# 首先计算h值（effect size），pwr包自带了函数，根据两个率可计算，\n# h的计算使用的是这个公式：2*asin(sqrt(0.9))-2*asin(sqrt(0.8))\nES.h(0.9,0.8)\n## [1] 0.2837941\n\n# 然后进行样本量计算\npwr.p.test(h = ES.h(0.9,0.8),\n           sig.level = 0.05,\n           power = 1-0.1,\n           alternative = \"greater\"\n           )\n## \n##      proportion power calculation for binomial distribution (arcsine transformation) \n## \n##               h = 0.2837941\n##               n = 106.3315\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = greater\n\n得到的结果和课本差别有点大，课本是137.1，而我们的结果是106，主要是由于计算方法不同，建议对于此类设计的样本量计算，还是直接套课本公式或者使用PASS软件。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#两独立样本率的比较",
    "href": "1011-samplesize.html#两独立样本率的比较",
    "title": "10  样本量计算",
    "section": "10.4 两独立样本率的比较",
    "text": "10.4 两独立样本率的比较\n使用课本例36-7的例子。\n初步观察甲乙两药对某病的疗效，甲药有效率为60%。乙药有效率为85%，现拟进一步做治疗实验，设α=0.05,1-β=0.9，问每组需要多少病例？\n下面演示使用pwr包计算：\n\n# 首先计算h值，pwr包自带了函数，根据两个率可计算\nES.h(0.85,0.60)\n## [1] 0.5740396\n\n# 然后进行样本量计算\npwr.2p.test(h = ES.h(0.85,0.60),\n           sig.level = 0.05,\n           power = 1-0.1,\n           alternative = \"two.sided\"\n           )\n## \n##      Difference of proportion power calculation for binomial distribution (arcsine transformation) \n## \n##               h = 0.5740396\n##               n = 63.77382\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = two.sided\n## \n## NOTE: same sample sizes\n\nn = 63.77382，和课本是一模一样的结果！\n这种情况下用R自带的``也是很好用的：\n\npower.prop.test(p1 = 0.85,\n                p2 = 0.6,\n                sig.level = 0.05,\n                power = 1-0.1,\n                alternative = \"two.sided\"\n                )\n## \n##      Two-sample comparison of proportions power calculation \n## \n##               n = 64.93465\n##              p1 = 0.85\n##              p2 = 0.6\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\n\n算出来结果是64.93465，和课本差别不大~",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#多样本率的比较",
    "href": "1011-samplesize.html#多样本率的比较",
    "title": "10  样本量计算",
    "section": "10.5 多样本率的比较",
    "text": "10.5 多样本率的比较\n使用课本例36-8的例子。\n拟观察3种方法治疗消化性溃疡的效果，初步估计甲法有效率为40%，乙法50%，丙法65%，设α=0.05，β=0.1，试估计样本量？\n很明显属于行x列表资料的卡方检验！所以我们使用pwr.chisq.test()函数进行计算样本量。\n首先我们要计算effect size：\n\n#               甲法 乙法 丙法\nprob &lt;- rbind(c(0.4, 0.5, 0.65), # 有效率\n              c(0.6, 0.5, 0.35)) # 无效率\n\n# pwr包自带的这个函数专门用于此种情况的effect size计算\nES.w2(prob/3) # 有几组就除以几，这里需要理解列联表资料的一些指标计算\n## [1] 0.2055947\n\n这样我们就得到effect size了，然后就可以计算样本量了。\n\npwr.chisq.test(w = ES.w2(prob/3), # effect size\n               df = 2, #（3-1）*（2-1）= 2\n               sig.level = 0.05,\n               power = 1-0.1\n               )\n## \n##      Chi squared power calculation \n## \n##               w = 0.2055947\n##               N = 299.3655\n##              df = 2\n##       sig.level = 0.05\n##           power = 0.9\n## \n## NOTE: N is the number of observations\n\n最终得到的结果是一共需要299例，课本是297例，基本一样~",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1011-samplesize.html#直线相关分析",
    "href": "1011-samplesize.html#直线相关分析",
    "title": "10  样本量计算",
    "section": "10.6 直线相关分析",
    "text": "10.6 直线相关分析\n使用课本例36-9的例子。\n根据以往经验，血硒与发硒含量间直线相关系数为0.8，若想在α=0.05，β=0.1的水平上得到相关系数有统计学意义的结论，应调查多少人？\n\npwr.r.test(r=0.8,\n           sig.level = 0.05,\n           power = 1-0.1,\n           alternative = \"two.sided\")\n## \n##      approximate correlation power calculation (arctangh transformation) \n## \n##               n = 11.16238\n##               r = 0.8\n##       sig.level = 0.05\n##           power = 0.9\n##     alternative = two.sided\n\nn = 11.16238，结果和课本一样~\nOK，以上就是使用R语言计算样本量的例子。可以看到大部分都是可以很简单的计算出来，但是在方便快捷性上还是差PASS软件太远了，对于此类样本量计算的问题，可能PASS是更好的选择。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>样本量计算</span>"
    ]
  },
  {
    "objectID": "1012-randomgroup.html",
    "href": "1012-randomgroup.html",
    "title": "11  随机分组",
    "section": "",
    "text": "11.1 简单随机\n比如30个人，按照完全随机化的方法分为2组，一组试验组，一组对照组，每组15人。\nid &lt;- c(1:30)\ngroup &lt;- rep(c(\"试验组\",\"对照组\"),15)\nrand &lt;- sample(group, 30, replace = T)\n(res &lt;- cbind(id, group))\n##       id   group   \n##  [1,] \"1\"  \"试验组\"\n##  [2,] \"2\"  \"对照组\"\n##  [3,] \"3\"  \"试验组\"\n##  [4,] \"4\"  \"对照组\"\n##  [5,] \"5\"  \"试验组\"\n##  [6,] \"6\"  \"对照组\"\n##  [7,] \"7\"  \"试验组\"\n##  [8,] \"8\"  \"对照组\"\n##  [9,] \"9\"  \"试验组\"\n## [10,] \"10\" \"对照组\"\n## [11,] \"11\" \"试验组\"\n## [12,] \"12\" \"对照组\"\n## [13,] \"13\" \"试验组\"\n## [14,] \"14\" \"对照组\"\n## [15,] \"15\" \"试验组\"\n## [16,] \"16\" \"对照组\"\n## [17,] \"17\" \"试验组\"\n## [18,] \"18\" \"对照组\"\n## [19,] \"19\" \"试验组\"\n## [20,] \"20\" \"对照组\"\n## [21,] \"21\" \"试验组\"\n## [22,] \"22\" \"对照组\"\n## [23,] \"23\" \"试验组\"\n## [24,] \"24\" \"对照组\"\n## [25,] \"25\" \"试验组\"\n## [26,] \"26\" \"对照组\"\n## [27,] \"27\" \"试验组\"\n## [28,] \"28\" \"对照组\"\n## [29,] \"29\" \"试验组\"\n## [30,] \"30\" \"对照组\"\n我们也可以通过randomizr这个包实现，没安装的需要先安装。\ninstall.packages(\"randomizr\")\n“抛硬币”式的简单随机分组通过simple_ra()函数实现：\nlibrary(randomizr)\n\n# 100人分2组\nsim &lt;- simple_ra(100, num_arms = 2, conditions = c(\"试验组\",\"对照组\"))\n\nsim\n##   [1] 对照组 对照组 对照组 试验组 试验组 试验组 试验组 试验组 对照组 对照组\n##  [11] 对照组 对照组 对照组 对照组 试验组 试验组 对照组 对照组 试验组 试验组\n##  [21] 试验组 试验组 对照组 试验组 试验组 对照组 对照组 对照组 对照组 对照组\n##  [31] 试验组 试验组 试验组 对照组 试验组 试验组 对照组 试验组 试验组 对照组\n##  [41] 对照组 试验组 试验组 对照组 对照组 试验组 试验组 对照组 对照组 对照组\n##  [51] 对照组 对照组 试验组 试验组 对照组 对照组 对照组 对照组 对照组 试验组\n##  [61] 对照组 对照组 试验组 对照组 对照组 对照组 对照组 对照组 对照组 试验组\n##  [71] 试验组 试验组 试验组 试验组 试验组 对照组 对照组 对照组 试验组 试验组\n##  [81] 试验组 试验组 试验组 对照组 试验组 对照组 试验组 试验组 对照组 试验组\n##  [91] 试验组 试验组 对照组 对照组 试验组 试验组 试验组 对照组 对照组 试验组\n## Levels: 试验组 对照组\ntable(sim)\n## sim\n## 试验组 对照组 \n##     49     51\n但是这种分组最大的问题是组间人数不一样，通常在临床研究设计中都是1:1的，我们可以使用另一个函数解决这个问题。\ncom &lt;- complete_ra(100, num_arms = 2, conditions = c(\"试验组\",\"对照组\"))\n\ncom\n##   [1] 试验组 试验组 对照组 对照组 试验组 试验组 试验组 对照组 对照组 对照组\n##  [11] 对照组 试验组 试验组 试验组 对照组 试验组 试验组 试验组 对照组 对照组\n##  [21] 试验组 对照组 对照组 对照组 对照组 对照组 试验组 试验组 试验组 试验组\n##  [31] 对照组 对照组 试验组 试验组 试验组 试验组 试验组 试验组 试验组 对照组\n##  [41] 试验组 对照组 对照组 试验组 试验组 对照组 试验组 对照组 试验组 试验组\n##  [51] 对照组 对照组 试验组 试验组 对照组 对照组 对照组 对照组 对照组 对照组\n##  [61] 对照组 试验组 试验组 对照组 对照组 对照组 试验组 试验组 试验组 对照组\n##  [71] 试验组 试验组 试验组 对照组 试验组 对照组 对照组 对照组 试验组 对照组\n##  [81] 对照组 试验组 试验组 试验组 试验组 对照组 对照组 试验组 对照组 对照组\n##  [91] 对照组 对照组 对照组 试验组 对照组 试验组 对照组 试验组 试验组 对照组\n## Levels: 试验组 对照组\ntable(com)\n## com\n## 试验组 对照组 \n##     50     50\n完美解决组间人数不相等问题。\n网络上的大神也给出了自己编写的函数：https://shumchi.github.io/Randomization/\nsimple_random &lt;- function(size, grp = 2, T_2_C = \"1:1\"){\n    set.seed(20210412)\n    id_num &lt;- seq(1, size, 1)\n    random_seq &lt;- runif(n = size, min = 0, max = 1)\n    int_rank &lt;- rank(random_seq)\n    \n    ratio_T &lt;- as.numeric(substr(T_2_C, 1, 1))\n    ratio_C &lt;- as.numeric(substr(T_2_C, 3, 3))\n    \n    if (grp == 2) {\n        group &lt;- ifelse(int_rank &lt;= size/(ratio_T + ratio_C), \"T\", \"C\")\n    } else if (grp &gt; 3) {\n        group &lt;- cut(int_rank, breaks = grp, labels = paste(\"Group\", 1:grp))\n    }\n    \n    df &lt;- data.frame(\"ID\" = id_num, \"RandomNum\" = random_seq, \n                        \"Rank\" = int_rank, \"Group\" = group)\n    \n    #write.csv(df, \"simple randomization table.csv\", row.names = FALSE)\n    \n    return(df)\n}\n20个人随机分组：\nsimple_random(20)\n##    ID  RandomNum Rank Group\n## 1   1 0.83237492   19     C\n## 2   2 0.95522177   20     C\n## 3   3 0.59787880   10     T\n## 4   4 0.35076793    4     T\n## 5   5 0.43157421    6     T\n## 6   6 0.63326323   13     C\n## 7   7 0.78015581   17     C\n## 8   8 0.46990952    7     T\n## 9   9 0.38535395    5     T\n## 10 10 0.63361183   14     C\n## 11 11 0.73655082   15     C\n## 12 12 0.49675139    8     T\n## 13 13 0.61201021   11     C\n## 14 14 0.23511285    3     T\n## 15 15 0.52894214    9     T\n## 16 16 0.04290597    1     T\n## 17 17 0.74367251   16     C\n## 18 18 0.79647582   18     C\n## 19 19 0.62653890   12     C\n## 20 20 0.22537775    2     T\n除此之外，还有非常多的R包可以实现随机分组，包括但不限于简单随机分组/区组随机/分层随机等。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>随机分组</span>"
    ]
  },
  {
    "objectID": "1012-randomgroup.html#区组随机",
    "href": "1012-randomgroup.html#区组随机",
    "title": "11  随机分组",
    "section": "11.2 区组随机",
    "text": "11.2 区组随机\n可以用randomizr实现区组随机：\n\n# Load built-in dataset\ndata(HairEyeColor)\nHairEyeColor &lt;- data.frame(HairEyeColor)\n\n# Transform so each row is a subject\n# Columns describe subject's hair color, eye color, and gender\nhec &lt;- HairEyeColor[rep(1:nrow(HairEyeColor),\n                        times = HairEyeColor$Freq), 1:3]\n\nN &lt;- nrow(hec)\n\n# Fix the rownames\nrownames(hec) &lt;- NULL\ndim(hec)\n## [1] 592   3\nhead(hec)\n##    Hair   Eye  Sex\n## 1 Black Brown Male\n## 2 Black Brown Male\n## 3 Black Brown Male\n## 4 Black Brown Male\n## 5 Black Brown Male\n## 6 Black Brown Male\n\n根据毛发颜色进入不同的区组，然后再分为3组：\n\nZ &lt;- block_ra(blocks = hec$Hair, conditions = c(\"Control\", \"Placebo\", \"Treatment\"))\ntable(Z, hec$Hair)\n##            \n## Z           Black Brown Red Blond\n##   Control      36    95  24    42\n##   Placebo      36    96  23    42\n##   Treatment    36    95  24    43\n\n可以自由控制入组数量：\n\nsort(unique(hec$Hair))\n## [1] Black Brown Red   Blond\n## Levels: Black Brown Red Blond\nblock_m_each &lt;- rbind(c(78, 30),\n                      c(186, 100),\n                      c(51, 20),\n                      c(87,40))\n\nblock_m_each\n##      [,1] [,2]\n## [1,]   78   30\n## [2,]  186  100\n## [3,]   51   20\n## [4,]   87   40\nZ &lt;- block_ra(blocks = hec$Hair, block_m_each = block_m_each)\ntable(Z, hec$Hair)\n##    \n## Z   Black Brown Red Blond\n##   0    78   186  51    87\n##   1    30   100  20    40\n\n但是这个区组随机对于临床研究来说不是很适用，因为这里的情况需要提前准备好所有的受试者，然后在进行分组，而对于临床研究来说，受试者是一个一个来的，不是一下子全部到齐的，所以block_ra()可能更适合动物实验或者基础研究的分组。更多关于区组随机的知识，大家可以参考医咖会的这篇文章：一文详解区组随机化，包教包懂！\n临床研究的随机分组可以通过blockrand包实现，特别适合一次招募1人的临床研究！\n比如100人随机分为2组每组50人：\n\nlibrary(blockrand)\n\nset.seed(111)\nres &lt;- blockrand(n=100, num.levels = 2, levels = c(\"试验组\",\"对照组\"))\nhead(res)\n##   id block.id block.size treatment\n## 1  1        1          4    试验组\n## 2  2        1          4    对照组\n## 3  3        1          4    试验组\n## 4  4        1          4    对照组\n## 5  5        2          6    试验组\n## 6  6        2          6    试验组\n\ntable(res$treatment)\n## \n## 对照组 试验组 \n##     51     51\n\n还可以顺便帮我们生成PDF文件，方便装入信封。\n\nshowtext::showtext_auto(enable = T)\n\nplotblockrand(res,file = \"res.pdf\",\n              top = list(text=c(\"xxx临床研究\",\"受试者编号：%ID%\",\"入组：%TREAT%\"),\n                         col=c(\"black\",\"blue\",\"red\"),\n                         font=c(2,2,4)\n                         ),\n              middle=list(text=c(\"xxx临床研究\",\"受试者编号：%ID%\"),\n                          col=c(\"black\",\"blue\",\"red\"),\n                          font=c(2,2,4)\n                          ),\n              bottom=\"联系电话：123456789\",\n              cut.marks=TRUE # 裁剪标记\n              )",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>随机分组</span>"
    ]
  },
  {
    "objectID": "1012-randomgroup.html#分层随机",
    "href": "1012-randomgroup.html#分层随机",
    "title": "11  随机分组",
    "section": "11.3 分层随机",
    "text": "11.3 分层随机\n比如120个受试者分成4组，试验组1，试验组2，阳性对照组，阴性对照组，每组30人，并且根据性别进行分层（男性和女性），要求男性60例，女性60例。\n使用blockrand实现。\n\nlibrary(blockrand)\n\n# 男性60例随机分组\nset.seed(123)\nres.M &lt;- blockrand(n = 60,\n                 num.levels = 4,\n                 levels = c(\"试验组1\",\"试验组2\",\"阳性对照组\",\"阴性对照组\"),\n                 stratum = \"男性\",\n                 id.prefix = \"男\", # id前缀\n                 block.sizes = c(3), \n                 block.prefix = \"男\" # 前缀\n                 )\ntable(res.M$treatment)\n## \n##    试验组1    试验组2 阳性对照组 阴性对照组 \n##         15         15         15         15\n\n# 女性60例随机分组\nset.seed(456)\nres.F &lt;- blockrand(n = 60,\n                 num.levels = 4,\n                 levels = c(\"试验组1\",\"试验组2\",\"阳性对照组\",\"阴性对照组\"),\n                 stratum = \"女性\",\n                 id.prefix = \"女\", # id前缀\n                 block.sizes = c(3), \n                 block.prefix = \"女\" # 前缀\n                 )\ntable(res.F$treatment)\n## \n##    试验组1    试验组2 阳性对照组 阴性对照组 \n##         15         15         15         15\n\n# 结果合并即可\nres &lt;- cbind(res.M,res.F)\ndim(res)\n## [1] 60 10\nhead(res)\n##     id stratum block.id block.size  treatment   id stratum block.id block.size\n## 1 男01    男性      男1         12 阳性对照组 女01    女性      女1         12\n## 2 男02    男性      男1         12 阴性对照组 女02    女性      女1         12\n## 3 男03    男性      男1         12    试验组2 女03    女性      女1         12\n## 4 男04    男性      男1         12    试验组2 女04    女性      女1         12\n## 5 男05    男性      男1         12    试验组2 女05    女性      女1         12\n## 6 男06    男性      男1         12 阳性对照组 女06    女性      女1         12\n##    treatment\n## 1    试验组1\n## 2 阳性对照组\n## 3    试验组2\n## 4 阴性对照组\n## 5 阴性对照组\n## 6 阳性对照组\n\n写入PDF文件，方便制作信封，分组隐匿：\n\nshowtext::showtext_auto(enable = T)\n\nplotblockrand(res, file = \"res1.pdf\",\n              top=list(text=c(\"不得了临床试验\",\"受试者编号: %ID%\",\"组别: %TREAT%\"),\n                       col=c('black','blue','red'),font=c(2,2,4)),\n              middle=list(text=c(\"不得了临床试验\",\"性别: %STRAT%\",\"受试者编号: %ID%\"),\n                          col=c('black','blue','red'),font=c(1,2,3)),\n              bottom=\"联系电话：123456789\",\n              cut.marks=TRUE\n              )",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>随机分组</span>"
    ]
  },
  {
    "objectID": "ROC曲线.html",
    "href": "ROC曲线.html",
    "title": "12  ROC曲线",
    "section": "",
    "text": "12.1 诊断试验\n科学地评价诊断试验是临床医师选择诊断试验的基础。诊断试验在临床上的应用涉及面甚广，包括病因和病原学诊断，疾病病理和功能损害的诊断，疗效的判断，药物毒副作用的监测，疾病预后的判断以及应用于普查、筛检无症状的患者等。用于不同场合的诊断试验有不同的要求，不同的诊断试验本身又有一定的特性。为了合理选用诊断试验以避免盲目性，临床医师就需要对诊断试验进行科学研究并得出科学的评价。\n评价诊断试验的优劣必须以金标准（gold standard）作为参照，没有金标准的诊断试验评价是没有科学性的。所谓诊断试验的金标准，是指当前临床医学界所公认的诊断某病最为可靠的方法。亦即利用金标准能正确地区分某人属”有病”还是”无病”。临床诊断中常用的金标准包括病理学诊断（组织活检和尸检）、外科手术发现、特殊的影像学诊断（如用冠状动脉造影术诊断冠心病等）以及目前尚无特异诊断方法而采用的国际公认的综合诊断标准（如诊断风湿热的Johes标准等）。有时用长期临床随访所获得的肯定诊断，也可作为金标准。必须注意，如果采用的金标准选择不当，就会造成分类错误，从而影响诊断试验正确性的评价。\n用于诊断试验评价的研究对象应包括病例组和对照组。病例组应是按金标准确诊的患者；对照组则应是按金标准证实无该病的患者或正常人群。病例组的选择，应包括各种类型的病例，即典型和不典型，早、中、晚各期，病情轻、中、重，有、无并发症等，这样试验的结果才具有普遍意义；而对照组则可选用经金标准证实无该病的其他病例或正常人，特别应当包括确实无该病，但易与该病相混淆的其他病例，这样选择的对照才具有临床意义，尤其具有鉴别诊断的价值。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ROC曲线</span>"
    ]
  },
  {
    "objectID": "ROC曲线.html#诊断试验的评价",
    "href": "ROC曲线.html#诊断试验的评价",
    "title": "12  ROC曲线",
    "section": "12.2 诊断试验的评价",
    "text": "12.2 诊断试验的评价\n根据诊断试验的结果和金标准的结果，可得到4种情况，将其整理成下面的表格，就得到一个诊断试验的四格表，实际上是一个配对四格表的形式：\n\n用于评价诊断试验的常用指标有：灵敏度和特异度，误诊率和漏诊率。\n\n灵敏度\n\n灵敏度（sensitivity），又称为真阳性率，是实际患病且被试验诊断为患者的概率，即患者被诊断为阳性的概率。灵敏度=a/(a+c)\n\n特异度\n\n特异度（specificity），又被称为真阴性率，是实际未患病而被试验诊断为非患者的概率，即非患者被诊断为阴性的概率。特异度=d/(b+d)\n灵敏度是反映检出能力的指标，而特异度是鉴别非患者能力的指标，二者都是越大越好。\n\n误诊率\n\n误诊率又称为假阳性率，表示实际未患病但被试验诊断为患者的概率，即非患者被诊断为阳性，反映非患者被错误诊断的可能性。误诊率=b/(b+d)\n\n漏诊率\n\n漏诊率又被称为假阳性率，表示实际患病但被试验诊断为非患者的概率，即患者被诊断为阴性，反映患者被遗漏诊断的可能性。漏诊率=c/(a+c)\n显然，灵敏度=1-假阴性率，特异度=1-假阳性率。\n除了以上4个指标外，还有阳性预测值和阴性预测值，用于评价诊断试验预测的准确性。\n\n阳性预测值：positive-predict-value，是试验诊断为阳性者，确为患者的概率。阳性预测值=a/(a+b)。\n阴性预测值：negative-predict-value，是试验诊断为阴性者，确为非患者的概率。阴性预测值=d/(c+d)。\n\n当两个诊断试验进行比较时，单独使用灵敏度与特异度指标，可能出现一个诊断试验的灵敏度高，而另一个诊断试验的特异度高，无法判断哪一个更好。所以发展出了可以将灵敏度和特异度综合起来评价的诊断试验指标，比如：正确率、比数积、阳新似然比、阴性似然比等。\n\n正确率：又称为准确率（机器学习中被称为accuracy），又称为总符合率，表示观察结果和实际结果的符合程度，反应正确诊断患者与非患者的能力。正确率=(a+d)/N，N是总人数。\n约登指数：Youden-index，YI，是反应诊断试验真实性的综合指标，YI=灵敏度+特异度=1。YI的值在-1到1之间，越大说明诊断试验的真实性越好，当YI小于等于0时，该诊断试验无任何临床应用价值。\n比数积：odd-product，OP，表示患者中诊断阳性数、阴性数之比和非患者中诊断阴性数、阳性数之比的乘积。OP=se/(1-se) * sp/(1-sp) = ad/bc。OP越大诊断价值越高。\n阳性似然比：positive-likelihood-ratio，LR+，表示真阳性率与假阳性率之比。LR+=se/(1-sp)\n阴性似然比：negative-likelihood-ratio，LR-，表示假阴性率与真阴性率之比。LR-=(1-se)/sp",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ROC曲线</span>"
    ]
  },
  {
    "objectID": "ROC曲线.html#roc曲线",
    "href": "ROC曲线.html#roc曲线",
    "title": "12  ROC曲线",
    "section": "12.3 ROC曲线",
    "text": "12.3 ROC曲线\n诊断试验把受试者分成患病和非患病，肯定是有一个标准的，高于这个标准就被判断为患病，低于这个标准就被判断为非患病。\n既然有一个标准，那么这个标准就可以有不同的取值，当这个标准变化时，那么被试验判断为患病和非患病的人数自然也就会发生变化。那么灵敏度和特异度这些指标也会发生变化。所以对于不同的诊断取值（被称为截断值），对应着不同的灵敏度和特异度。\n我们以灵敏度(或者1-灵敏度)为纵坐标，特异度为横坐标，把不同截断值下的灵敏度和特异度画在一张图上，并把这些点连接成线，就是ROC曲线。ROC曲线下的面积被称为AUC（area under the curve，AUC）。\nAUC的取值范围在0到1之间，ROC曲线下的面积越大，也就是AUC越大，说明分类越准确，当AUC值为1时，说明是完美的分类，当AUC为0.5时，说明和乱猜差不多。\n下面用一个简单的例子进行说明。\n假如，我想根据ca125的值判定一个人到底有没有肿瘤，找了10个肿瘤患者，20个非肿瘤患者，都给他们测一下ca125，这样就得到了30个ca125的值。\n\nset.seed(20220840)\nca125_1 &lt;- c(rnorm(10,80,20),rnorm(20,50,10))\n\n# 30个人的ca125的值如下\nca125_1\n##  [1]  51.88470  82.45907 113.66834  63.49476  98.29077  63.27374  74.25079\n##  [8]  80.22945  83.01740  99.17105  42.52889  54.56804  48.88383  65.67865\n## [15]  44.73153  45.99028  55.82554  42.79242  60.84917  64.80764  51.11468\n## [22]  43.40118  47.03850  44.75943  68.34163  60.83829  53.32599  59.92225\n## [29]  46.46360  30.02914\n\n假定前10个人是肿瘤，后20个人是非肿瘤。\n\noutcome &lt;- c(rep(c(\"肿瘤\",\"非肿瘤\"),c(10,20)))\noutcome &lt;- factor(outcome,levels = c(\"肿瘤\",\"非肿瘤\"))\noutcome\n##  [1] 肿瘤   肿瘤   肿瘤   肿瘤   肿瘤   肿瘤   肿瘤   肿瘤   肿瘤   肿瘤  \n## [11] 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤\n## [21] 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤 非肿瘤\n## Levels: 肿瘤 非肿瘤\n\n放在一个表格中方便观看：\n\ndf &lt;- data.frame(outcome=outcome,\n                 ca125=ca125_1\n                 )\npsych::headTail(df)\n##     outcome  ca125\n## 1      肿瘤  51.88\n## 2      肿瘤  82.46\n## 3      肿瘤 113.67\n## 4      肿瘤  63.49\n## ...    &lt;NA&gt;    ...\n## 27   非肿瘤  53.33\n## 28   非肿瘤  59.92\n## 29   非肿瘤  46.46\n## 30   非肿瘤  30.03\n\n现在如果我们设置ca125&gt;60，判断为肿瘤，ca125≤50判断为非肿瘤，就能得到如下的结果：\n\ndf1 &lt;- transform(df, pred = ifelse(ca125&gt;60,\"猜他是肿瘤\",\"猜他不是肿瘤\"))\ndf1$pred &lt;- factor(df1$pred,levels = c(\"猜他是肿瘤\",\"猜他不是肿瘤\"))\npsych::headTail(df1)\n##     outcome  ca125         pred\n## 1      肿瘤  51.88 猜他不是肿瘤\n## 2      肿瘤  82.46   猜他是肿瘤\n## 3      肿瘤 113.67   猜他是肿瘤\n## 4      肿瘤  63.49   猜他是肿瘤\n## ...    &lt;NA&gt;    ...         &lt;NA&gt;\n## 27   非肿瘤  53.33 猜他不是肿瘤\n## 28   非肿瘤  59.92 猜他不是肿瘤\n## 29   非肿瘤  46.46 猜他不是肿瘤\n## 30   非肿瘤  30.03 猜他不是肿瘤\n\n对以上结果稍加整理，就能得出一个四格表：\n\nxtabs(~pred+outcome,data = df1)\n##               outcome\n## pred           肿瘤 非肿瘤\n##   猜他是肿瘤      9      5\n##   猜他不是肿瘤    1     15\n\n这个表格就是诊断实验的四格表。\n通过这个表格，我们就可以计算灵敏度和特异度等指标了:\n特异度=15/(15+5)=0.75\n灵敏度=9/(1+9)=0.9\n这个表格是以（ca125）60为截断值的，如果我们换一个截断值，那么灵敏度和特异度就会发生变化。\n下面我们编写一个函数，让它帮我们计算在不同的截断值下，灵敏度和特异度的值。\n\ncal_roc &lt;- function(df, cutoff){\n  df &lt;- transform(df, pred = ifelse(ca125&gt;cutoff,\"猜他是肿瘤\",\"猜他不是肿瘤\"))\n  df$pred &lt;- factor(df$pred,levels = c(\"猜他是肿瘤\",\"猜他不是肿瘤\"))\n  tb &lt;- table(df$pred,df$outcome)\n  sens &lt;- tb[1,1]/colSums(tb)[1]\n  spec &lt;- tb[2,2]/colSums(tb)[2]\n  list(sens=sens, spec=spec)\n}\n\n阈值设置为60，看看是不是和我们上面的结果一样：\n\ncal_roc(df,60)\n## $sens\n## 肿瘤 \n##  0.9 \n## \n## $spec\n## 非肿瘤 \n##   0.75\n\n可以看到是一样的。\n下面就是自己选择多个阈值进行计算，先看下ca125的范围，超出这个范围的阈值没有意义。\n\nrange(ca125_1)\n## [1]  30.02914 113.66834\n\n下面我们确定截断值的范围在30到113之间，每次都加1，然后使用我们的函数计算不同截断值下的灵敏度和特异度：\n\n# 确定取哪些截断值\ncutoff &lt;- seq(30,113, 1)\n\n# 计算不同的灵敏度和特异度\nrocs &lt;- purrr::map_dfr(cutoff, cal_roc, df=df)\nrocs$cutoff &lt;- cutoff\npsych::headTail(rocs)\n##   sens spec cutoff\n## 1    1    0     30\n## 2    1 0.05     31\n## 3    1 0.05     32\n## 4    1 0.05     33\n## 5  ...  ...    ...\n## 6  0.1    1    110\n## 7  0.1    1    111\n## 8  0.1    1    112\n## 9  0.1    1    113\n\n这样我们就得到了不同截断值下的灵敏度和特异度。\n有了这个结果后，我们就可以自己画出ROC曲线了，以1-特异度为横坐标，灵敏度为纵坐标：\n\nlibrary(ggplot2)\n\nggplot(rocs, aes(1-spec,sens))+\n  geom_point(size=2,color=\"red\")+\n  geom_path()+\n  coord_fixed()+\n  theme_bw()\n\n\n\n\n\n\n\n\n这就是ROC曲线了。\n因为不同的截断值对应着不同的灵敏度和特异度，那么肯定就存在一个最佳截断值的问题，一般情况下，使约登指数最大的那个截断值，就是最佳截断值。\n\n约登指数=灵敏度+特异度-1\n\n此时的灵敏度+特异度最大，同时也是ROC曲线下的面积（即AUC）最大。\n如何寻找这个最佳的截断值呢？当然是挨个试了！但是借助计算机，这个过程一瞬间就完成了。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ROC曲线</span>"
    ]
  },
  {
    "objectID": "ROC曲线.html#用r包绘制roc",
    "href": "ROC曲线.html#用r包绘制roc",
    "title": "12  ROC曲线",
    "section": "12.4 用R包绘制ROC",
    "text": "12.4 用R包绘制ROC\nR语言中有非常多的方法可以实现ROC曲线，但是基本上都是至少需要2列数据，一列是真实结果，另一列是预测值（或者叫截断值、阈值、指标等），有了这两列数据，就可以轻松使用各种方法画出ROC曲线并计算AUC。这里给大家介绍这个用的最多的pROC包。\npROC包中提供了一个aSAH数据集，这是一个动脉瘤性蛛网膜下腔出血的数据集，一共113行，7列。其中outcome列是结果变量，1代表Good，2代表Poor：\n\ngos6：格拉斯哥量表评分\noutcome：结果变量\ngender：性别\nage：年龄\nwfns：世界神经外科医师联合会公认的神经学量表评分\ns100b：生物标志物\nndka：生物标志物\n\n\nlibrary(pROC)\n\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n假如我们想使用s100b这个指标作为我们的截断值，来判断结局到底是good还是poor，根据前面的ROC曲线的定义，我们可以绘制一个ROC曲线，使用R包实现非常简单：\n\n# 计算\nres &lt;- roc(aSAH$outcome,aSAH$s100b,auc=T)\n\n# 画图\nplot(res,auc.polygon=T,auc.polygon.col=\"steelblue\",\n     print.auc=T,\n     print.auc.x=0.95,print.auc.y=0.9,print.auc.col=\"firebrick\",\n     print.auc.cex=2\n     )\n\n\n\n\n\n\n\n\n给出的信息非常丰富，不仅画出了图，还给出了ROC曲线下面积（即：AUC）。\n\n# 结果已经存储在res中\nres\n## \n## Call:\n## roc.default(response = aSAH$outcome, predictor = aSAH$s100b,     auc = T)\n## \n## Data: aSAH$s100b in 72 controls (aSAH$outcome Good) &lt; 41 cases (aSAH$outcome Poor).\n## Area under the curve: 0.7314\n\n课本中还介绍了一种平滑的ROC曲线，在pROC中实现也很简单，添加一个参数即可：\n\n# 计算\nres_smooth &lt;- roc(aSAH$outcome,aSAH$s100b,auc=T,smooth=T)\n\n# 画图\nplot(res_smooth,auc.polygon=T,auc.polygon.col=\"steelblue\",\n     print.auc=T,\n     print.auc.x=0.95,print.auc.y=0.9,print.auc.col=\"firebrick\",\n     print.auc.cex=2\n     )\n\n\n\n\n\n\n\n\n计算最佳截点也是非常简单的：\n\nplot(res,auc.polygon=T,auc.polygon.col=\"steelblue\",\n     legacy.axes = TRUE, # 让x轴从0开始\n     print.thres=\"best\") # AUC最大的点\n\n\n\n\n\n\n\n\n图中的信息显示：最佳截点是0.205，此时的特异度是0.806，灵敏度是0.634，也就是说，当s100b的值是0.205的时候，约登指数最大，同时也是曲线下面积（AUC）最大。\n\n细心的朋友可以可能已经注意到了，上面的ROC曲线的x轴，有的是0-1，有的是1-0，这两种情况都是对的，没必要纠结哈。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ROC曲线</span>"
    ]
  },
  {
    "objectID": "ROC曲线.html#预测模型和roc",
    "href": "ROC曲线.html#预测模型和roc",
    "title": "12  ROC曲线",
    "section": "12.5 预测模型和ROC",
    "text": "12.5 预测模型和ROC\n了解了ROC曲线的原理后，使用起来就比较简单了。\n以二分类数据为例，如果是在机器学习和临床预测模型中，模型也会对我们的数据作出一个判断，对于每一个样本，模型都会给出两种类型的判断结果，一种是直接给出类别，另一种是给出每种类别的概率，如下所示：\n\n\n\n真实类别\n预测类别\n预测为阳性的概率\n预测为阴性的概率\n\n\n\n\n阳性\n阳性\n0.6\n0.4\n\n\n阳性\n阳性\n0.7\n0.3\n\n\n阴性\n阳性\n0.8\n0.2\n\n\n阳性\n阴性\n0.4\n0.6\n\n\n阴性\n阴性\n0.2\n0.8\n\n\n阴性\n阴性\n0.25\n0.75\n\n\n阴性\n阴性\n0.32\n0.68\n\n\n阳性\n阳性\n0.7\n0.3\n\n\n\n这个概率就是我们的截断值，取不同的概率阈值，就会得到不同的分类结果，我们可以规定当概率大于0.5判断为阳性，也可以规定概率大于0.8判断为阳性。所以不同的概率阈值，也对应着不同的灵敏度和特异度，据此就可以绘制出ROC曲线了。\n在实际使用时，我们可以借助多种R包帮我们实现”根据不同的阈值计算计算灵敏度/特异度并绘图”的过程，并不需要自己计算，常用的R包有：ROCR、pROC、yardstick等。\n除此之外，ROC曲线既然是通过不同的截断值实现的，那么就必然存在最佳截点的问题；对于生存分析来说，也涉及到time-dependent-ROC等问题；为了增加检验可信度，还有bootstrap-ROC等；还有多分类的ROC曲线等，这些问题可参考以下推文：\n\n多时间点和多指标的ROC曲线\n临床预测模型之二分类资料ROC曲线的绘制\n临床预测模型之生存资料ROC曲线的绘制\nROC曲线(AUC)的显著性检验\n生存资料ROC曲线的最佳截点和平滑曲线\nROC曲线纯手工绘制\nR语言计算AUC(ROC曲线)的注意事项\nROC阴性结果还是阳性结果\n多指标联合诊断的ROC曲线\nROC曲线最佳截点\nbootstrap ROC/AUC\nR语言多分类ROC曲线绘制\n\n公众号后台回复ROC即可获取以上合集链接。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ROC曲线</span>"
    ]
  },
  {
    "objectID": "1014-batchttest.html",
    "href": "1014-batchttest.html",
    "title": "13  “tidy”流统计分析",
    "section": "",
    "text": "13.1 简介\n虽然说是同时进行t检验，但是这是一种通用的方法，当然也可以同时进行方差分析、正态性检验、方差齐性检验、秩和检验等等。\n前面的介绍多数是基于R语言自带的函数进行医学统计学分析，下面介绍一个比较现代化、用法也更加简单直接的方法，也就是基于rstatix这个包实现。\nrstatix提供一个简单直观的管道友好的框架，与整洁的设计理念一致，用于执行基本的统计检验，包括t检验，Wilcoxon检验，方差分析，Kruskal-Wallis和相关性分析等。每个分析的输出会自动转换成一个整洁的数据框架，以方便可视化。\n附加功能可用于重塑，重新排序，操作和可视化相关矩阵。功能还包括析因实验的分析，包括重复测量设计、析因设计、正交设计等。\n还可以计算几个效应大小指标，包括方差分析eta平方，t检验的Cohen’s-d和分类变量之间的关联的Cramer’s-v。 该软件包包含用于识别单变量和多变量异常值、评估正态性和方差齐性的辅助函数。\n在spss中进行这些检验时，不管有多少列变量，只要都选中，就可以一次全部进行检验，在R语言里当然也可以！\n主要是通过rstatix这个包完成，数据格式要求是长数据。",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>“tidy”流统计分析</span>"
    ]
  },
  {
    "objectID": "1014-batchttest.html#实战",
    "href": "1014-batchttest.html#实战",
    "title": "13  “tidy”流统计分析",
    "section": "13.2 实战",
    "text": "13.2 实战\n下面通过一个例子进行说明，可能有些不太适当，只是演示使用方法。\n60个病人随机分为实验组和对照组，每组30人，记录患者的4项评分，现在需要对这4项评分同时进行正态性和方差齐性检验和t检验（可能不符合t检验的条件，这里只是演示方法）。\n\ndf &lt;- read.csv(\"datasets/20210801.csv\",header = T)\n\nstr(df)\n## 'data.frame':    60 obs. of  5 variables:\n##  $ 组别    : chr  \"实验组\" \"实验组\" \"实验组\" \"实验组\" ...\n##  $ 排便困难: int  12 11 15 14 11 13 11 14 13 15 ...\n##  $ 生活质量: int  87 94 95 85 101 91 84 89 84 92 ...\n##  $ 粪便性状: int  2 3 2 2 3 3 3 3 3 3 ...\n##  $ 排便时间: int  2 2 2 2 2 2 2 3 2 2 ...\nhead(df)\n##     组别 排便困难 生活质量 粪便性状 排便时间\n## 1 实验组       12       87        2        2\n## 2 实验组       11       94        3        2\n## 3 实验组       15       95        2        2\n## 4 实验组       14       85        2        2\n## 5 实验组       11      101        3        2\n## 6 实验组       13       91        3        2\n\ndf这样的数据是宽数据，首先变成长数据：\n\nsuppressMessages(library(tidyverse))\n\ndf_l &lt;- df %&gt;% \n  pivot_longer(cols = 2:5, names_to = \"变量\", values_to = \"积分\") %&gt;% \n  dplyr::mutate_if(is.character, as.factor)\n\nstr(df_l)\n## tibble [240 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ 组别: Factor w/ 2 levels \"对照组\",\"实验组\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ 变量: Factor w/ 4 levels \"粪便性状\",\"排便困难\",..: 2 4 1 3 2 4 1 3 2 4 ...\n##  $ 积分: int [1:240] 12 87 2 2 11 94 3 2 15 95 ...\nhead(df_l)\n## # A tibble: 6 × 3\n##   组别   变量      积分\n##   &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;\n## 1 实验组 排便困难    12\n## 2 实验组 生活质量    87\n## 3 实验组 粪便性状     2\n## 4 实验组 排便时间     2\n## 5 实验组 排便困难    11\n## 6 实验组 生活质量    94\n\n同时进行正态性检验：\n\nlibrary(rstatix)\n\ndf_l %&gt;% group_by(变量,组别) %&gt;% shapiro_test(积分)\n## # A tibble: 8 × 5\n##   组别   变量     variable statistic        p\n##   &lt;fct&gt;  &lt;fct&gt;    &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n## 1 对照组 粪便性状 积分         0.452 1.73e- 9\n## 2 实验组 粪便性状 积分         0.404 5.98e-10\n## 3 对照组 排便困难 积分         0.871 1.78e- 3\n## 4 实验组 排便困难 积分         0.915 1.95e- 2\n## 5 对照组 排便时间 积分         0.577 3.91e- 8\n## 6 实验组 排便时间 积分         0.597 6.64e- 8\n## 7 对照组 生活质量 积分         0.974 6.47e- 1\n## 8 实验组 生活质量 积分         0.962 3.46e- 1\n\n方差齐性检验：\n\ndf_l %&gt;% group_by(变量) %&gt;% levene_test(积分 ~ 组别)\n## # A tibble: 4 × 5\n##   变量       df1   df2 statistic      p\n##   &lt;fct&gt;    &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n## 1 粪便性状     1    58    0.127  0.723 \n## 2 排便困难     1    58    0.226  0.636 \n## 3 排便时间     1    58    0.0746 0.786 \n## 4 生活质量     1    58    3.96   0.0514\n\nt检验：\n\ndf_l %&gt;% group_by(变量) %&gt;% t_test(积分 ~ 组别)\n## # A tibble: 4 × 9\n##   变量     .y.   group1 group2    n1    n2 statistic    df     p\n## * &lt;fct&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 粪便性状 积分  对照组 实验组    30    30   -0.356   57.5 0.723\n## 2 排便困难 积分  对照组 实验组    30    30    0.0890  57.4 0.929\n## 3 排便时间 积分  对照组 实验组    30    30   -0.273   58.0 0.786\n## 4 生活质量 积分  对照组 实验组    30    30   -0.101   52.4 0.92\n\n非常方便，结果也是一目了然，再也不用羡慕SPSS的这一点了！\n你又多了一个用R语言进行医学统计学的理由！",
    "crumbs": [
      "基础统计分析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>“tidy”流统计分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html",
    "href": "1003-dysanova.html",
    "title": "14  多因素方差分析",
    "section": "",
    "text": "14.1 2 x 2 两因素析因设计资料的方差分析\n使用课本例11-1的数据，自己手动摘录：\ndf11_1 &lt;- data.frame(\n  x1 = rep(c(\"外膜缝合\",\"束膜缝合\"), each = 10),\n  x2 = rep(c(\"缝合1个月\",\"缝合2个月\"), each = 5),\n  y = c(10,10,40,50,10,30,30,70,60,30,10,20,30,50,30,50,50,70,60,30)\n)\n\nstr(df11_1)\n## 'data.frame':    20 obs. of  3 variables:\n##  $ x1: chr  \"外膜缝合\" \"外膜缝合\" \"外膜缝合\" \"外膜缝合\" ...\n##  $ x2: chr  \"缝合1个月\" \"缝合1个月\" \"缝合1个月\" \"缝合1个月\" ...\n##  $ y : num  10 10 40 50 10 30 30 70 60 30 ...\n数据一共3列，第1列是缝合方法，第2列是时间，第3列是轴突通过率。试比较不同缝合方法和缝合后时间对轴突通过率的影响，做析因设计的方差分析。\n进行析因设计资料的方差分析（考虑所有因素的主效应和交互作用）：\nf1 &lt;- aov(y ~ x1 * x2, data = df11_1)\n\nsummary(f1)\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## x1           1    180     180   0.600 0.4499  \n## x2           1   2420    2420   8.067 0.0118 *\n## x1:x2        1     20      20   0.067 0.7995  \n## Residuals   16   4800     300                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n结果是一个方差分析表。分别给出了A因素、B因素、AB交互作用、个体间的自由度、离均差平方和、均方误差、F值、P值，可以看到结果和课本是一致的！\n简单介绍一下可视化两因素析因设计的方法：\ninteraction.plot(df11_1$x2, df11_1$x1, df11_1$y, type = \"b\", \n                 col = c(\"red\",\"blue\"), pch = c(12,15),\n                 xlab = \"缝合时间\", ylab = \"轴突通过率\")\n另外一种可视化方法：\nlibrary(gplots)\n\nattach(df11_1)\n\nplotmeans(y ~ interaction(x1,x2),\n          connect = list(c(1,3), c(2,4)),\n          col = c(\"red\",\"darkgreen\"),\n          main = \"两因素析因设计\",\n          xlab = \"时间和方法的交互\")\n再介绍一种方法：\nlibrary(HH)\n\ninteraction2wt(y ~ x1 * x2)\n\n\n\n\n\n\n\ndetach(df11_1)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#x-2-两因素析因设计资料的方差分析",
    "href": "1003-dysanova.html#x-2-两因素析因设计资料的方差分析",
    "title": "14  多因素方差分析",
    "section": "",
    "text": "注释\n\n\n\naov(y ~ x1 * x2, data = df11_1)等价于aov(y ~ x1 + x2 + x1:x2, data = df11_1)，表示x1的主效应、x2的主效应、x1和x2的交互效应。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#i-x-j-两因素析因设计资料的方差分析",
    "href": "1003-dysanova.html#i-x-j-两因素析因设计资料的方差分析",
    "title": "14  多因素方差分析",
    "section": "14.2 I x J 两因素析因设计资料的方差分析",
    "text": "14.2 I x J 两因素析因设计资料的方差分析\n使用课本例11-2的数据，自己手动摘录：\n\ndf11_2 &lt;- data.frame(\n  druga = rep(c(\"1mg\",\"2.5mg\",\"5mg\"), each = 3),\n  drugb = rep(c(\"5微克\",\"15微克\",\"30微克\"),each = 9),\n  y = c(105,80,65,75,115,80,85,120,125,115,105,80,125,130,90,65,\n        120,100,75,95,85,135,120,150,180,190,160)\n)\n\nstr(df11_2)\n## 'data.frame':    27 obs. of  3 variables:\n##  $ druga: chr  \"1mg\" \"1mg\" \"1mg\" \"2.5mg\" ...\n##  $ drugb: chr  \"5微克\" \"5微克\" \"5微克\" \"5微克\" ...\n##  $ y    : num  105 80 65 75 115 80 85 120 125 115 ...\n\n数据一共3列，第1列是a药物的剂量（3种剂量，代表3个水平），第2列是b药物的剂量（3种剂量），第3列是镇痛时间。\n进行两因素三水平的析因设计资料方差分析（考虑所有因素的主效应和交互作用）：\n\nf2 &lt;- aov(y ~ druga * drugb, data = df11_2)\n\nsummary(f2)\n##             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \n## druga        2   6572    3286   8.470 0.00256 **\n## drugb        2   7022    3511   9.050 0.00190 **\n## druga:drugb  4   7872    1968   5.073 0.00647 **\n## Residuals   18   6983     388                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果和课本也是一模一样的哦！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#i-x-j-x-k-三因素析因设计资料的方差分析",
    "href": "1003-dysanova.html#i-x-j-x-k-三因素析因设计资料的方差分析",
    "title": "14  多因素方差分析",
    "section": "14.3 I x J x K 三因素析因设计资料的方差分析",
    "text": "14.3 I x J x K 三因素析因设计资料的方差分析\n使用课本例11-3的数据，\n\ndf11_3 &lt;- foreign::read.spss(\"datasets/例11-03-5种军装热感觉5-2-2.sav\", \n                             to.data.frame = T,reencode=\"UTF-8\")\n\ndf11_3$a &lt;- factor(df11_3$a)\n\nstr(df11_3)\n## 'data.frame':    100 obs. of  4 variables:\n##  $ b: Factor w/ 2 levels \"干燥\",\"潮湿\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ c: Factor w/ 2 levels \"静坐\",\"活动\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ a: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 2 2 2 2 2 ...\n##  $ x: num  0.25 -0.25 1.25 -0.75 0.4 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:4] \"活动环境\" \"活动状态\" \"军装类型\" \"主观热感觉\"\n##   ..- attr(*, \"names\")= chr [1:4] \"b\" \"c\" \"a\" \"x\"\n##  - attr(*, \"codepage\")= int 65001\n\n进行3因素析因设计资料的方差分析（考虑所有的主效应和交互作用）：\n\nf3 &lt;- aov(x ~ a * b * c, data = df11_3)\n\nsummary(f3)\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## a            4   5.20    1.30   3.024   0.0224 *  \n## b            1   9.94    9.94  23.138 6.98e-06 ***\n## c            1 283.35  283.35 659.485  &lt; 2e-16 ***\n## a:b          4   1.94    0.48   1.128   0.3491    \n## a:c          4   1.48    0.37   0.862   0.4905    \n## b:c          1  12.68   12.68  29.514 5.82e-07 ***\n## a:b:c        4   1.61    0.40   0.937   0.4472    \n## Residuals   80  34.37    0.43                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果也是和课本一模一样。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#正交设计资料的方差分析",
    "href": "1003-dysanova.html#正交设计资料的方差分析",
    "title": "14  多因素方差分析",
    "section": "14.4 正交设计资料的方差分析",
    "text": "14.4 正交设计资料的方差分析\n正交设计是残缺不全版本的析因设计，注意指定交互作用即可。\n使用课本例11-4的数据。\n\ndf11_4 &lt;- data.frame(\n  a = rep(c(\"5度\",\"25度\"),each = 4),\n  b = rep(c(0.5, 5.0), each = 2),\n  c = c(10, 30),\n  d = c(6.0, 8.0,8.0,6.0,8.0,6.0,6.0,8.0),\n  x = c(86,95,91,94,91,96,83,88)\n)\n\ndf11_4$a &lt;- factor(df11_4$a)\ndf11_4$b &lt;- factor(df11_4$b)\ndf11_4$c &lt;- factor(df11_4$c)\ndf11_4$d &lt;- factor(df11_4$d)\n\nstr(df11_4)\n## 'data.frame':    8 obs. of  5 variables:\n##  $ a: Factor w/ 2 levels \"25度\",\"5度\": 2 2 2 2 1 1 1 1\n##  $ b: Factor w/ 2 levels \"0.5\",\"5\": 1 1 2 2 1 1 2 2\n##  $ c: Factor w/ 2 levels \"10\",\"30\": 1 2 1 2 1 2 1 2\n##  $ d: Factor w/ 2 levels \"6\",\"8\": 1 2 2 1 2 1 1 2\n##  $ x: num  86 95 91 94 91 96 83 88\n\n进行正交设计资料的方差分析，只考虑4个因素的主效应以及a和b的一阶交互作用：\n\nf4 &lt;- aov(x ~ a + b + c + d + a:b, data = df11_4)\n\nsummary(f4)\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## a            1    8.0     8.0     3.2 0.2155  \n## b            1   18.0    18.0     7.2 0.1153  \n## c            1   60.5    60.5    24.2 0.0389 *\n## d            1    4.5     4.5     1.8 0.3118  \n## a:b          1   50.0    50.0    20.0 0.0465 *\n## Residuals    2    5.0     2.5                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果和课本一模一样，用R语言进行方差分析真是太简单了！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#嵌套设计资料的方差分析",
    "href": "1003-dysanova.html#嵌套设计资料的方差分析",
    "title": "14  多因素方差分析",
    "section": "14.5 嵌套设计资料的方差分析",
    "text": "14.5 嵌套设计资料的方差分析\n嵌套设计也是残缺不全版本的析因设计，使用也是要注意指定主效应和交互效应。\n使用课本例11-6的数据。\n\ndf11_6 &lt;- data.frame(\n  factor1 = factor(rep(c(\"A\",\"B\",\"C\"),each=6)),\n  factor2 = factor(rep(c(70,80,90,55,65,75,90,95,100),each=2)),\n  y = c(82,84,91,88,85,83,65,61,62,59,56,60,71,67,75,78,85,89)\n  )\nstr(df11_6)\n## 'data.frame':    18 obs. of  3 variables:\n##  $ factor1: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 1 2 2 2 2 ...\n##  $ factor2: Factor w/ 8 levels \"55\",\"65\",\"70\",..: 3 3 5 5 6 6 1 1 2 2 ...\n##  $ y      : num  82 84 91 88 85 83 65 61 62 59 ...\n\ndf11_6\n##    factor1 factor2  y\n## 1        A      70 82\n## 2        A      70 84\n## 3        A      80 91\n## 4        A      80 88\n## 5        A      90 85\n## 6        A      90 83\n## 7        B      55 65\n## 8        B      55 61\n## 9        B      65 62\n## 10       B      65 59\n## 11       B      75 56\n## 12       B      75 60\n## 13       C      90 71\n## 14       C      90 67\n## 15       C      95 75\n## 16       C      95 78\n## 17       C     100 85\n## 18       C     100 89\n\nfactor1是一级实验因素（不同的催化剂），factor2是二级实验因素（不同的温度），y是因变量。\n进行嵌套实验设计的方差分析：\n\n# “/”表示factor2嵌套在factor1里\nf &lt;- aov(y ~ factor1 / factor2, data = df11_6)\n\n# 等价于以下写法，所以“/”在R中的公式中也是有特殊含义的！\n#f &lt;- aov(y ~ factor1 + factor1:factor2, data = df11_6)\n\nsummary(f)\n##                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## factor1          2 1956.0   978.0  177.82 5.83e-08 ***\n## factor1:factor2  6  401.0    66.8   12.15 0.000716 ***\n## Residuals        9   49.5     5.5                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果和课本相同。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1003-dysanova.html#裂区设计资料的方差分析",
    "href": "1003-dysanova.html#裂区设计资料的方差分析",
    "title": "14  多因素方差分析",
    "section": "14.6 裂区设计资料的方差分析",
    "text": "14.6 裂区设计资料的方差分析\n使用课本例11-7的数据。这是一个完全随机的2*2裂区设计，家兔为一级实验单位，注射部位为二级实验单位。\n\ndf11_7 &lt;- data.frame(\n  factorA = factor(rep(c(\"a1\",\"a2\"),each=10)),\n  factorB = factor(rep(c(\"b1\",\"b2\"),10)),\n  id = factor(rep(c(1:10),each=2)),\n  y = c(15.75,19.00,15.50,20.75,15.50,18.50,17.00,20.50,16.50,20.00,\n        18.25,22.25,18.50,21.50,19.75,23.50,21.50,24.75,20.75,23.75)\n  )\nstr(df11_7)\n## 'data.frame':    20 obs. of  4 variables:\n##  $ factorA: Factor w/ 2 levels \"a1\",\"a2\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ factorB: Factor w/ 2 levels \"b1\",\"b2\": 1 2 1 2 1 2 1 2 1 2 ...\n##  $ id     : Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 2 3 3 4 4 5 5 ...\n##  $ y      : num  15.8 19 15.5 20.8 15.5 ...\n\ndf11_7\n##    factorA factorB id     y\n## 1       a1      b1  1 15.75\n## 2       a1      b2  1 19.00\n## 3       a1      b1  2 15.50\n## 4       a1      b2  2 20.75\n## 5       a1      b1  3 15.50\n## 6       a1      b2  3 18.50\n## 7       a1      b1  4 17.00\n## 8       a1      b2  4 20.50\n## 9       a1      b1  5 16.50\n## 10      a1      b2  5 20.00\n## 11      a2      b1  6 18.25\n## 12      a2      b2  6 22.25\n## 13      a2      b1  7 18.50\n## 14      a2      b2  7 21.50\n## 15      a2      b1  8 19.75\n## 16      a2      b2  8 23.50\n## 17      a2      b1  9 21.50\n## 18      a2      b2  9 24.75\n## 19      a2      b1 10 20.75\n## 20      a2      b2 10 23.75\n\n裂区设计的A因素只作用于一级实验单位，B因素只作用于二级实验单位，所以其方差分析也是由两部分组成（课本P183）。\n该例题中每个家兔对应着B因素（毒素浓度）的两个水平（每只家兔会注射两种浓度的毒素），但每只家兔只对应A因素的1个水平（每只家兔只会注射一种药物，不会同时注射两种药物），所以需要为B因素指定误差项。\n\n# factorB is nested in id，每个id对应多个factorB\n# factorA和factorB有交叉，但是id只和factorB有交叉\nf &lt;- aov(y ~ factorA * factorB + Error(id/factorB), data = df11_7)\nsummary(f)\n## \n## Error: id\n##           Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## factorA    1  63.01   63.01   28.01 0.000735 ***\n## Residuals  8  18.00    2.25                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: id:factorB\n##                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## factorB          1  63.01   63.01  252.05 2.48e-07 ***\n## factorA:factorB  1   0.11    0.11    0.45    0.521    \n## Residuals        8   2.00    0.25                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果同课本相同。第一部分是A因素主效应和误差，第二部分是：B因素主效应、A和B的交互效应、误差。\n裂区设计和嵌套设计R方差分析实现的参考链接：\n\nCrossed and Nested Factors\naov() error term in R: what’s the difference bw Error(id) and Error(id/timevar) specification?\nFormulae in R\nR and Analysis of Variance",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>多因素方差分析</span>"
    ]
  },
  {
    "objectID": "1008-mauchly.html",
    "href": "1008-mauchly.html",
    "title": "15  球对称检验",
    "section": "",
    "text": "15.1 课本表12-3的数据\n这是一个只有1组的！\n读取数据：\ndf &lt;- foreign::read.spss(\"datasets/表12-3重复测量ANOVA.sav\",\n                         to.data.frame = T, reencode = \"utf-8\")\n\nstr(df)\n## 'data.frame':    8 obs. of  4 variables:\n##  $ t0  : num  5.32 5.32 5.94 5.49 5.71 6.27 5.88 5.32\n##  $ t45 : num  5.32 5.26 5.88 5.43 5.49 6.27 5.77 5.15\n##  $ t90 : num  4.98 4.93 5.43 5.32 5.43 5.66 5.43 5.04\n##  $ t135: num  4.65 4.7 5.04 5.04 4.93 5.26 4.93 4.48\n##  - attr(*, \"variable.labels\")= Named chr(0) \n##   ..- attr(*, \"names\")= chr(0) \n##  - attr(*, \"codepage\")= int 936\nhead(df)\n##     t0  t45  t90 t135\n## 1 5.32 5.32 4.98 4.65\n## 2 5.32 5.26 4.93 4.70\n## 3 5.94 5.88 5.43 5.04\n## 4 5.49 5.43 5.32 5.04\n## 5 5.71 5.49 5.43 4.93\n## 6 6.27 6.27 5.66 5.26\n数据一共4列，就是4个时间点的血糖值。\n首先将数据变为矩阵：\ndf &lt;- as.matrix(df)\n然后进行球对称检验（球形检验）：\nmauchly.test(lm(df ~ 1), X = ~ 1)\n## \n##  Mauchly's test of sphericity\n##  Contrasts orthogonal to\n##  ~1\n## \n## \n## data:  SSD matrix from lm(formula = df ~ 1)\n## W = 0.06273, p-value = 0.008207\n结果就有了，就是这么简单直接，网上很多资料都是直接复制粘贴帮助文档里的内容，非常费脑子！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>球对称检验</span>"
    ]
  },
  {
    "objectID": "1008-mauchly.html#课本例12-3的数据",
    "href": "1008-mauchly.html#课本例12-3的数据",
    "title": "15  球对称检验",
    "section": "15.2 课本例12-3的数据",
    "text": "15.2 课本例12-3的数据\n这个数据有2组！\n直接读取：\n\ndf1 &lt;- foreign::read.spss(\"datasets/例12-03.sav\",to.data.frame = T)\n\nstr(df1)\n## 'data.frame':    15 obs. of  7 variables:\n##  $ No   : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ group: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 2 2 2 2 2 ...\n##  $ t0   : num  120 118 119 121 127 121 122 128 117 118 ...\n##  $ t1   : num  108 109 112 112 121 120 121 129 115 114 ...\n##  $ t2   : num  112 115 119 119 127 118 119 126 111 116 ...\n##  $ t3   : num  120 126 124 126 133 131 129 135 123 123 ...\n##  $ t4   : num  117 123 118 120 126 137 133 142 131 133 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:7] \"\\xd0\\xf2\\xba\\xc5\" \"\\xd7\\xe9\\xb1\\xf0\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:7] \"No\" \"group\" \"t0\" \"t1\" ...\nhead(df1)\n##   No group  t0  t1  t2  t3  t4\n## 1  1     A 120 108 112 120 117\n## 2  2     A 118 109 115 126 123\n## 3  3     A 119 112 119 124 118\n## 4  4     A 121 112 119 126 120\n## 5  5     A 127 121 127 133 126\n## 6  6     B 121 120 118 131 137\n\n数据一共7列，第1列是患者编号，第2列是诱导方法（3种），第3-7列是5个时间点的血压。\n首先将数据变为矩阵，转换数据格式：\n\ndf2 &lt;- as.matrix(cbind(df1[1:5,3:7], df1[6:10,3:7], df1[11:15,3:7]))\n\n把测量点和分组单独建立，注意要和上面的顺序一致：\n\ntimes = ordered(rep(1:5,3))\ngroup = factor(rep(c(\"A\",\"B\",\"C\"),each = 5))\n\n然后进行球对称检验（球形检验）：\n\nmauchly.test(lm(df2 ~ 1), M = ~ group + times, X = ~ times)\n## \n##  Mauchly's test of sphericity\n##  Contrasts orthogonal to\n##  ~times\n## \n##  Contrasts spanned by\n##  ~group + times\n## \n## \n## data:  SSD matrix from lm(formula = df2 ~ 1)\n## W = 0.427, p-value = 0.279\n\n真的是有点费事儿！不过现在很多R包都可以在进行重复测量方差分析时自动给出球形检验的结果，已经方便多了。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>球对称检验</span>"
    ]
  },
  {
    "objectID": "1004-repeatedanova.html",
    "href": "1004-repeatedanova.html",
    "title": "16  重复测量方差分析",
    "section": "",
    "text": "16.1 两因素两水平\n使用课本例12-1的数据，直接读取：\ndf12_1 &lt;- foreign::read.spss(\"datasets/12-1.sav\", to.data.frame = T)\n\nstr(df12_1)\n## 'data.frame':    20 obs. of  5 variables:\n##  $ n    : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : num  130 124 136 128 122 118 116 138 126 124 ...\n##  $ x2   : num  114 110 126 116 102 100 98 122 108 106 ...\n##  $ group: Factor w/ 2 levels \"处理组\",\"对照组\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ d    : num  16 14 10 12 20 18 18 16 18 18 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:5] \"编号\" \"治疗前血压\" \"治疗后血压\" \"组别\" ...\n##   ..- attr(*, \"names\")= chr [1:5] \"n\" \"x1\" \"x2\" \"group\" ...\n##  - attr(*, \"codepage\")= int 936\nhead(df12_1)\n##   n  x1  x2  group  d\n## 1 1 130 114 处理组 16\n## 2 2 124 110 处理组 14\n## 3 3 136 126 处理组 10\n## 4 4 128 116 处理组 12\n## 5 5 122 102 处理组 20\n## 6 6 118 100 处理组 18\n数据一共5列（第5列是自己算出来的，其实原始数据只有4列），第1列是编号，第2列是治疗前血压，第3例是治疗后血压，第4列是分组，第5列是血压前后差值。\n进行重复测量数据两因素两水平的方差分析前，先把数据转换一下格式：\nlibrary(tidyverse)\n\n# 变成长数据\ndf12_11 &lt;- \n  df12_1[,1:4] %&gt;% \n  pivot_longer(cols = 2:3,names_to = \"time\",values_to = \"hp\") %&gt;% \n  mutate_if(is.character, as.factor)\n\ndf12_11$n &lt;- factor(df12_11$n)\n\nstr(df12_11)\n## tibble [40 × 4] (S3: tbl_df/tbl/data.frame)\n##  $ n    : Factor w/ 20 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 2 3 3 4 4 5 5 ...\n##  $ group: Factor w/ 2 levels \"处理组\",\"对照组\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ time : Factor w/ 2 levels \"x1\",\"x2\": 1 2 1 2 1 2 1 2 1 2 ...\n##  $ hp   : num [1:40] 130 114 124 110 136 126 128 116 122 102 ...\nhead(df12_11)\n## # A tibble: 6 × 4\n##   n     group  time     hp\n##   &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt;\n## 1 1     处理组 x1      130\n## 2 1     处理组 x2      114\n## 3 2     处理组 x1      124\n## 4 2     处理组 x2      110\n## 5 3     处理组 x1      136\n## 6 3     处理组 x2      126\n转换后的数据格式如上。\n进行重复测量数据两因素两水平的方差分析:\nhp是因变量，time是测量时间（治疗前和治疗后各测量一次），group是分组因素（两种治疗方法），n是受试者编号。\n# time和group是有交叉的，每个受试者（n）只和time有交叉，和group没有交叉\nf1 &lt;- aov(hp ~ time * group + Error(n/time), data = df12_11)\n\nsummary(f1)\n## \n## Error: n\n##           Df Sum Sq Mean Sq F value Pr(&gt;F)\n## group      1  202.5   202.5   1.574  0.226\n## Residuals 18 2315.4   128.6               \n## \n## Error: n:time\n##            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## time        1 1020.1  1020.1   55.01 7.08e-07 ***\n## time:group  1  348.1   348.1   18.77 0.000401 ***\n## Residuals  18  333.8    18.5                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n结果输出了两张表，第二个是测量前后比较与交互作用的方差分析表，第一个是处理组与对照组比较的方差分析表，可以看到结果和课本是一样的！\n用图形方式展示重复测量的结果：\nwith(df12_11,\n     interaction.plot(time, group, hp, type = \"b\", col = c(\"red\",\"blue\"), \n                      pch = c(12,16), main = \"两因素两水平重复测量方差分析\"))\n或者用箱线图展示结果：\nboxplot(hp ~ group*time, data = df12_11, col = c(\"gold\",\"green\"),\n        main = \"两因素两水平重复测量方差分析\")",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>重复测量方差分析</span>"
    ]
  },
  {
    "objectID": "1004-repeatedanova.html#两因素多水平的分析",
    "href": "1004-repeatedanova.html#两因素多水平的分析",
    "title": "16  重复测量方差分析",
    "section": "16.2 两因素多水平的分析",
    "text": "16.2 两因素多水平的分析\n使用课本例12-3的数据，直接读取：\n\ndf12_3 &lt;- foreign::read.spss(\"datasets/例12-03.sav\",to.data.frame = T,\n                             reencode = \"utf-8\"\n                             )\nstr(df12_3)\n## 'data.frame':    15 obs. of  7 variables:\n##  $ No   : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ group: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 2 2 2 2 2 ...\n##  $ t0   : num  120 118 119 121 127 121 122 128 117 118 ...\n##  $ t1   : num  108 109 112 112 121 120 121 129 115 114 ...\n##  $ t2   : num  112 115 119 119 127 118 119 126 111 116 ...\n##  $ t3   : num  120 126 124 126 133 131 129 135 123 123 ...\n##  $ t4   : num  117 123 118 120 126 137 133 142 131 133 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:7] \"....\" \"....\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:7] \"No\" \"group\" \"t0\" \"t1\" ...\nhead(df12_3)\n##   No group  t0  t1  t2  t3  t4\n## 1  1     A 120 108 112 120 117\n## 2  2     A 118 109 115 126 123\n## 3  3     A 119 112 119 124 118\n## 4  4     A 121 112 119 126 120\n## 5  5     A 127 121 127 133 126\n## 6  6     B 121 120 118 131 137\n\n数据一共7列，第1列是患者编号，第2列是诱导方法（3种），第3-7列是5个时间点的血压。\n首先转换数据格式：\n\nlibrary(tidyverse)\n\n# 变为长数据\ndf12_31 &lt;- df12_3 %&gt;% \n  pivot_longer(cols = 3:7, names_to = \"times\", values_to = \"hp\")\n\ndf12_31$No &lt;- factor(df12_31$No)\ndf12_31$times &lt;- factor(df12_31$times)\n\nstr(df12_31)\n## tibble [75 × 4] (S3: tbl_df/tbl/data.frame)\n##  $ No   : Factor w/ 15 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 2 2 2 2 2 ...\n##  $ group: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ times: Factor w/ 5 levels \"t0\",\"t1\",\"t2\",..: 1 2 3 4 5 1 2 3 4 5 ...\n##  $ hp   : num [1:75] 120 108 112 120 117 118 109 115 126 123 ...\n\n转换后的格式见上图。\n进行方差分析（和两因素两水平没有任何区别）：\n\nf2 &lt;- aov(hp ~ times * group + Error(No/(times)), data = df12_31)\n\nsummary(f2)\n## \n## Error: No\n##           Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## group      2  912.2   456.1   5.783 0.0174 *\n## Residuals 12  946.5    78.9                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: No:times\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## times        4 2336.5   584.1   106.6  &lt; 2e-16 ***\n## times:group  8  837.6   104.7    19.1 1.62e-12 ***\n## Residuals   48  263.1     5.5                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n输出结果是两张表格，第1个是不同诱导方法患者血压比较的方差分析表，第2个是麻醉诱导时相及其与诱导方法交互作用的方差分析表。\n结果和课本是一样的！具体意义解读请认真学习医学统计学相关知识。\n用图形方式展示重复测量的结果：\n\nwith(df12_31,\n     interaction.plot(times, group, hp, type = \"b\", \n                      col = c(\"red\",\"blue\",\"green\"), \n                      pch = c(12,16,20), \n                      main = \"两因素多水平重复测量方差分析\"))\n\n\n\n\n\n\n\n\n或者用箱线图展示结果：\n\nboxplot(hp ~ group*times, data = df12_31, col = c(\"gold\",\"green\",\"black\"),\n        main = \"两因素多水平重复测量方差分析\")",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>重复测量方差分析</span>"
    ]
  },
  {
    "objectID": "1004-repeatedanova.html#多重比较",
    "href": "1004-repeatedanova.html#多重比较",
    "title": "16  重复测量方差分析",
    "section": "16.3 多重比较",
    "text": "16.3 多重比较\n使用课本例12-1的数据，直接读取：\n\ndf12_3 &lt;- foreign::read.spss(\"datasets/例12-03.sav\",to.data.frame = T)\n\nstr(df12_3)\n## 'data.frame':    15 obs. of  7 variables:\n##  $ No   : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ group: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 2 2 2 2 2 ...\n##  $ t0   : num  120 118 119 121 127 121 122 128 117 118 ...\n##  $ t1   : num  108 109 112 112 121 120 121 129 115 114 ...\n##  $ t2   : num  112 115 119 119 127 118 119 126 111 116 ...\n##  $ t3   : num  120 126 124 126 133 131 129 135 123 123 ...\n##  $ t4   : num  117 123 118 120 126 137 133 142 131 133 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:7] \"\\xd0\\xf2\\xba\\xc5\" \"\\xd7\\xe9\\xb1\\xf0\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:7] \"No\" \"group\" \"t0\" \"t1\" ...\n\n数据一共7列，第1列是患者编号，第2列是诱导方法（3种），第3-7列是5个时间点的血压。\n首先转换数据格式：\n\nlibrary(reshape2) # 换个R包用用\n\ndf.l &lt;- melt(df12_3, id.vars = c(\"No\",\"group\"), \n             variable.name = \"times\", \n             value.name = \"hp\")\ndf.l$No &lt;- factor(df.l$No)\n\nstr(df.l)\n## 'data.frame':    75 obs. of  4 variables:\n##  $ No   : Factor w/ 15 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  $ group: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 2 2 2 2 2 ...\n##  $ times: Factor w/ 5 levels \"t0\",\"t1\",\"t2\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ hp   : num  120 118 119 121 127 121 122 128 117 118 ...\nhead(df.l)\n##   No group times  hp\n## 1  1     A    t0 120\n## 2  2     A    t0 118\n## 3  3     A    t0 119\n## 4  4     A    t0 121\n## 5  5     A    t0 127\n## 6  6     B    t0 121\n\n进行重复测量方差分析，默认方法不能输出球形检验的结果，所以我更推荐rstatix提供的方法：\n\n# 默认\nf &lt;- aov(hp ~ group*times + Error(No/times), data = df.l)\nsummary(f)\n## \n## Error: No\n##           Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## group      2  912.2   456.1   5.783 0.0174 *\n## Residuals 12  946.5    78.9                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: No:times\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## times        4 2336.5   584.1   106.6  &lt; 2e-16 ***\n## group:times  8  837.6   104.7    19.1 1.62e-12 ***\n## Residuals   48  263.1     5.5                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n# rstatix\nlibrary(rstatix)\n\nanova_test(data = df.l,\n           dv = hp,\n           wid = No,\n           within = times,\n           between = group\n           )\n## ANOVA Table (type II tests)\n## \n## $ANOVA\n##        Effect DFn DFd       F        p p&lt;.05   ges\n## 1       group   2  12   5.783 1.70e-02     * 0.430\n## 2       times   4  48 106.558 3.02e-23     * 0.659\n## 3 group:times   8  48  19.101 1.62e-12     * 0.409\n## \n## $`Mauchly's Test for Sphericity`\n##        Effect     W     p p&lt;.05\n## 1       times 0.293 0.178      \n## 2 group:times 0.293 0.178      \n## \n## $`Sphericity Corrections`\n##        Effect   GGe      DF[GG]    p[GG] p[GG]&lt;.05   HFe      DF[HF]    p[HF]\n## 1       times 0.679 2.71, 32.58 1.87e-16         * 0.896 3.59, 43.03 4.65e-21\n## 2 group:times 0.679 5.43, 32.58 4.26e-09         * 0.896 7.17, 43.03 2.04e-11\n##   p[HF]&lt;.05\n## 1         *\n## 2         *\n\n画图展示：\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndf.l |&gt; \n  group_by(times,group) |&gt; \n  summarise(mm=mean(hp)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(times,mm))+\n  geom_line(aes(group=group,color=group),linewidth=1.2)+\n  theme_bw()\n\n\n\n\n\n\n\n\n接下来是重复测量数据的多重比较，课本中分成了3个方面。\n\n16.3.1 组间差别多重比较\n首先也计算下各组的均值，和课本对比下（一样的）：\n\ndf.l |&gt; \n  group_by(group) |&gt; \n  summarise(mm=mean(hp))\n## # A tibble: 3 × 2\n##   group    mm\n##   &lt;fct&gt; &lt;dbl&gt;\n## 1 A      120.\n## 2 B      124.\n## 3 C      128.\n\nLSD/SNK/Tukey/Dunnett/Bonferroni等方法都可以，和多个均数比较的多重检验一样。\n\nlibrary(PMCMRplus)\n\nsummary(lsdTest(hp ~ group, data = df.l))\n##            t value  Pr(&gt;|t|)    \n## B - A == 0   2.175 0.0329218   *\n## C - A == 0   3.860 0.0002446 ***\n## C - B == 0   1.686 0.0962097   .\n\nP值和课本不太一样，但是结论是一样的，A组和B组之间，A组和C组之间有差别，B组和C组之间没有差别。\n\n\n16.3.2 时间趋势比较\n重复测量方差分析可以采取正交多项式来探索时间变化趋势，具体的内涵解读可以参考冯国双老师的这篇文章：重复测量数据探索时间变化趋势\n在R里面进行正交多项式的探索略显复杂，需要对时间变量（这里是times）进行正交多项式转换，我们这里有5个时间点，所以是1次方到4次方：\n\n# 给大家展示下正交多项式转换\ncontrasts(df.l$times) &lt;- contr.poly(5)\ncontrasts(df.l$times)\n##               .L         .Q            .C         ^4\n## t0 -6.324555e-01  0.5345225 -3.162278e-01  0.1195229\n## t1 -3.162278e-01 -0.2672612  6.324555e-01 -0.4780914\n## t2 -3.510833e-17 -0.5345225  1.755417e-16  0.7171372\n## t3  3.162278e-01 -0.2672612 -6.324555e-01 -0.4780914\n## t4  6.324555e-01  0.5345225  3.162278e-01  0.1195229\n\n下面进行方差分析，此时是单纯探索时间对因变量的影响，所以注意formula的形式：\n\n# A组\nf1 &lt;- aov(hp ~ times, data = df.l[df.l$group==\"A\",])\n\n# 分别看不同次方的结果\nsummary(f1, \n        split=list(times=list(liner=1,quadratic=2,cubic=3,biquadrate=4)))\n##                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## times                4  475.4   118.9   5.580 0.003486 ** \n##   times: liner       1   84.5    84.5   3.967 0.060229 .  \n##   times: quadratic   1   26.4    26.4   1.240 0.278655    \n##   times: cubic       1  364.5   364.5  17.113 0.000511 ***\n##   times: biquadrate  1    0.0     0.0   0.001 0.972627    \n## Residuals           20  426.0    21.3                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n# B组\nf2 &lt;- aov(hp ~ times, data = df.l[df.l$group==\"B\",])\nsummary(f2, split=list(times=list(liner=1,quadratic=2,cubic=3,biquadrate=4)))\n##                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## times                4 1017.0   254.3   9.757 0.000152 ***\n##   times: liner       1  662.5   662.5  25.421 6.24e-05 ***\n##   times: quadratic   1  296.2   296.2  11.367 0.003034 ** \n##   times: cubic       1    3.9     3.9   0.150 0.702229    \n##   times: biquadrate  1   54.4    54.4   2.088 0.163954    \n## Residuals           20  521.2    26.1                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n# C组\nf3 &lt;- aov(hp ~ times+Error(No/times), data = df.l[df.l$group==\"C\",])\nsummary(f3, split=list(times=list(liner=1,quadratic=2,cubic=3,biquadrate=4)))\n## \n## Error: No\n##           Df Sum Sq Mean Sq F value Pr(&gt;F)\n## Residuals  4     98    24.5               \n## \n## Error: No:times\n##                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## times                4 1681.6   420.4  40.915 3.28e-08 ***\n##   times: liner       1  403.3   403.3  39.249 1.13e-05 ***\n##   times: quadratic   1   41.7    41.7   4.054   0.0612 .  \n##   times: cubic       1  605.5   605.5  58.931 9.43e-07 ***\n##   times: biquadrate  1  631.1   631.1  61.425 7.23e-07 ***\n## Residuals           16  164.4    10.3                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n以上3组结果中，第2组和第3组结果的的离均差平方和、自由度、均方都是和课本一样的，但是F值和P值相差很多。第1组的所有结果都和课本相差很大。关于这方面的资料较少，如果有大神知道，欢迎指教！\n\n\n16.3.3 时间点多重比较\n课本说因为事后检验重复次数太多难以承受，但是我们用计算机很快，所以用事后检验也没什么问题。\n事后检验可以参考组间比较，根据组别进行分组，分组比较不同时间点的差别。\n事前检验课本采用配对t检验，全都和t0的数据进行比较。\n事前检验使用rstatix包解决:\n\nlibrary(rstatix)\n\ndf.l |&gt; \n  group_by(group) |&gt; \n  t_test(hp ~ times, ref.group = \"t0\",paired = T)\n## # A tibble: 12 × 11\n##    group .y.   group1 group2    n1    n2 statistic    df         p    p.adj\n##  * &lt;fct&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n##  1 A     hp    t0     t1         5     5     8.35      4 0.001     0.004   \n##  2 A     hp    t0     t2         5     5     1.77      4 0.152     0.304   \n##  3 A     hp    t0     t3         5     5    -3.64      4 0.022     0.066   \n##  4 A     hp    t0     t4         5     5     0.147     4 0.89      0.89    \n##  5 B     hp    t0     t1         5     5     1.72      4 0.16      0.16    \n##  6 B     hp    t0     t2         5     5     4.35      4 0.012     0.024   \n##  7 B     hp    t0     t3         5     5    -8.37      4 0.001     0.003   \n##  8 B     hp    t0     t4         5     5   -16.7       4 0.0000747 0.000299\n##  9 C     hp    t0     t1         5     5     1.44      4 0.223     0.292   \n## 10 C     hp    t0     t2         5     5     4.75      4 0.009     0.028   \n## 11 C     hp    t0     t3         5     5    -5.12      4 0.007     0.028   \n## 12 C     hp    t0     t4         5     5    -1.80      4 0.146     0.292   \n## # ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n直接给出3组的结果，和课本一模一样~",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>重复测量方差分析</span>"
    ]
  },
  {
    "objectID": "1005-ancova.html",
    "href": "1005-ancova.html",
    "title": "17  协方差分析",
    "section": "",
    "text": "17.1 完全随机设计资料的协方差分析\n使用课本例13-1的例子。\n首先是读取数据，本次数据手动录入：\ndf13_1 &lt;- data.frame(x1=c(10.8,11.6,10.6,9.0,11.2,9.9,10.6,10.4,9.6,10.5,\n                          10.6,9.9,9.5,9.7,10.7,9.2,10.5,11.0,10.1,10.7,8.5,\n                          10.0, 10.4,9.7,9.4,9.2,10.5,11.2,9.6,8.0),\n                     y1=c(9.4,9.7,8.7,7.2,10.0,8.5,8.3,8.1,8.5,9.1,9.2,8.4,\n                          7.6,7.9,8.8,7.4,8.6,9.2,8.0,8.5,7.3,8.3,\n                          8.6,8.7,7.6,8.0,8.8,9.5,8.2,7.2),\n                     x2=c(10.4,9.7,9.9,9.8,11.1,8.2,8.8,10.0,9.0,9.4,8.9,\n                          10.3,9.3,9.2,10.9,9.2,9.2,10.4,11.2,11.1,11.0,\n                          8.6,9.3,10.3,10.3,9.8,10.5,10.7,10.4,9.4),\n                     y2=c(9.2,9.1,8.9,8.6,9.9,7.1,7.8,7.9,8.0,9.0,7.9,8.9,\n                          8.9,8.1,10.2,8.5,9.0,8.9,9.8,10.1,8.5,8.1,8.6,\n                          8.9,9.6,8.1,9.9,9.3,8.7,8.7),\n                     x3=c(9.8,11.2,10.7,9.6,10.1,9.8,10.1,10.3,11.0,10.5,\n                          9.2,10.1,10.4,10.0,8.4,10.1,9.3,10.5,11.1,10.5,\n                          9.7,9.2,9.3,10.4,10.0,10.3,9.9,9.4,8.3,9.2),\n                     y3=c(7.6,7.9,9.0,7.8,8.5,7.5,8.3,8.2,8.4,8.1,7.0,7.7,\n                          8.0,6.6,6.1,8.1,7.8,8.4,8.2,8.0,7.6,6.9,6.7,\n                          8.1,7.4,8.2,7.6,7.8,6.6,7.2)\n                     )\n看一下数据结构：\nstr(df13_1)\n## 'data.frame':    30 obs. of  6 variables:\n##  $ x1: num  10.8 11.6 10.6 9 11.2 9.9 10.6 10.4 9.6 10.5 ...\n##  $ y1: num  9.4 9.7 8.7 7.2 10 8.5 8.3 8.1 8.5 9.1 ...\n##  $ x2: num  10.4 9.7 9.9 9.8 11.1 8.2 8.8 10 9 9.4 ...\n##  $ y2: num  9.2 9.1 8.9 8.6 9.9 7.1 7.8 7.9 8 9 ...\n##  $ x3: num  9.8 11.2 10.7 9.6 10.1 9.8 10.1 10.3 11 10.5 ...\n##  $ y3: num  7.6 7.9 9 7.8 8.5 7.5 8.3 8.2 8.4 8.1 ...\n可以看到一共6列，和课本上面的一模一样，分别是x1,y1,x2,y2,x3,y3。\n接下来为了进行方差分析，需要变为长数据，把所有的x放在1列，所有的y放在1列，还有一列是组别：\n如果大家还对长宽数据转换不了解的，可以翻看之前的历史推文：\n这是一个非常重要且使用频率极高的技能！\nsuppressPackageStartupMessages(library(tidyverse))\n\ndf13_11 &lt;- df13_1 %&gt;% \n  pivot_longer(cols = everything(), # 变长\n               names_to = c(\".value\",\"group\"),\n               names_pattern = \"(.)(.)\"\n               ) %&gt;% \n  mutate(group = as.factor(group)) # 组别变为因子型\n\nglimpse(df13_11) # 查看数据结构，神奇！\n## Rows: 90\n## Columns: 3\n## $ group &lt;fct&gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1…\n## $ x     &lt;dbl&gt; 10.8, 10.4, 9.8, 11.6, 9.7, 11.2, 10.6, 9.9, 10.7, 9.0, 9.8, 9.6…\n## $ y     &lt;dbl&gt; 9.4, 9.2, 7.6, 9.7, 9.1, 7.9, 8.7, 8.9, 9.0, 7.2, 8.6, 7.8, 10.0…\n所有的x放在1列，所有的y放在1列，还有一列是组别！\n然后就是进行单因素协方差分析：\n# 注意公式的写法，一定是把协变量放在主变量前面！\nfit &lt;- aov(y ~ x + group, data = df13_11) \nsummary(fit)\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## x            1  29.06  29.057  171.20 &lt;2e-16 ***\n## group        2  19.85   9.925   58.48 &lt;2e-16 ***\n## Residuals   86  14.60   0.170                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n得到的结果和课本是一模一样的，组内ss=14.60, ms=0.170, v=86, 修正均数ss=19.85, ms=9.925, v=2, F=58.48，拒绝H0，接受H1，可以认为在扣除初始（基线）糖化血红蛋白含量的影响后，3组患者的总体降糖均数有差别。\n结果的可视化可以使用HH包：\nlibrary(HH)\n一行代码即可：\nancovaplot(y ~ x + group, data = df13_11)\n但其实我们也可以用ggplot2来画，可能更好看一点：\ntheme_set(theme_bw())\n\np1 &lt;- ggplot(df13_11, aes(x=x,y=y))+\n  geom_point(aes(color=group,shape=group))+\n  geom_smooth(method = \"lm\",se=F,aes(color=group))+\n  labs(y=NULL)\n\np2 &lt;- ggplot(df13_11, aes(x=x,y=y))+\n  geom_point(aes(color=group,shape=group))+\n  geom_smooth(method = \"lm\",se=F,aes(color=group))+\n  facet_wrap(~group)\n\nlibrary(patchwork)\np2 + p1 + plot_layout(guides = 'collect',widths = c(3, 1))\n好看是好看，但是很明显不如HH简洁啊！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>协方差分析</span>"
    ]
  },
  {
    "objectID": "1005-ancova.html#完全随机设计资料的协方差分析",
    "href": "1005-ancova.html#完全随机设计资料的协方差分析",
    "title": "17  协方差分析",
    "section": "",
    "text": "宽数据变为长数据的5种情况！\n长数据变为宽数据的7种情况！\n长宽数据转换的特殊情况",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>协方差分析</span>"
    ]
  },
  {
    "objectID": "1005-ancova.html#使用rstatix进行优雅的协方差分析",
    "href": "1005-ancova.html#使用rstatix进行优雅的协方差分析",
    "title": "17  协方差分析",
    "section": "17.2 使用rstatix进行优雅的协方差分析",
    "text": "17.2 使用rstatix进行优雅的协方差分析\n\nlibrary(rstatix)\n\nres &lt;- anova_test(y ~ x + group, data = df13_11, type = 1)#不同类型可选\nget_anova_table(res)\n## ANOVA Table (type I tests)\n## \n##   Effect DFn DFd       F        p p&lt;.05   ges\n## 1      x   1  86 171.199 3.64e-22     * 0.666\n## 2  group   2  86  58.480 9.22e-17     * 0.576\n\n结果也是一样的！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>协方差分析</span>"
    ]
  },
  {
    "objectID": "1005-ancova.html#随机区组设计资料的协方差分析",
    "href": "1005-ancova.html#随机区组设计资料的协方差分析",
    "title": "17  协方差分析",
    "section": "17.3 随机区组设计资料的协方差分析",
    "text": "17.3 随机区组设计资料的协方差分析\n使用课本例13-2的数据。\n\ndf &lt;- foreign::read.spss(\"datasets/例13-02.sav\",to.data.frame = T,\n                         reencode = \"utf-8\")\ndf$block &lt;- factor(df$block)\n\nstr(df)\n## 'data.frame':    36 obs. of  4 variables:\n##  $ x    : num  257 272 210 300 262 ...\n##  $ y    : num  27 41.7 25 52 14.5 48.8 48 9.5 37 56.5 ...\n##  $ group: Factor w/ 3 levels \"A....\",\"B....\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ block: Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:4] \"..ʳ..\" \"..........\" \"........\" \"......\"\n##   ..- attr(*, \"names\")= chr [1:4] \"x\" \"y\" \"group\" \"block\"\nhead(df)\n##       x    y group block\n## 1 256.9 27.0 A....     1\n## 2 271.6 41.7 A....     2\n## 3 210.2 25.0 A....     3\n## 4 300.1 52.0 A....     4\n## 5 262.2 14.5 A....     5\n## 6 304.4 48.8 A....     6\n\n进行随机区组设计的协方差分析：\n\nfit &lt;- aov(y ~ x + block + group, data = df) # 注意顺序\nsummary(fit)\n##             Df Sum Sq Mean Sq F value  Pr(&gt;F)    \n## x            1  69073   69073 651.823 &lt; 2e-16 ***\n## block       11   4024     366   3.452 0.00711 ** \n## group        2    464     232   2.189 0.13692    \n## Residuals   21   2225     106                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(fit)\n## Anova Table (Type II tests)\n## \n## Response: y\n##           Sum Sq Df F value    Pr(&gt;F)    \n## x         6174.2  1 58.2643 1.733e-07 ***\n## block     3765.3 11  3.2302   0.01009 *  \n## group      463.9  2  2.1891   0.13692    \n## Residuals 2225.4 21                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果和课本一致。\n或者用rstatix：\n\nfit &lt;- anova_test(y ~ x + block + group, data = df, type = 3)\nget_anova_table(fit)\n## ANOVA Table (type III tests)\n## \n##   Effect DFn DFd      F        p p&lt;.05   ges\n## 1      x   1  21 58.264 1.73e-07     * 0.735\n## 2  block  11  21  3.230 1.00e-02     * 0.629\n## 3  group   2  21  2.189 1.37e-01       0.173",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>协方差分析</span>"
    ]
  },
  {
    "objectID": "1010-anovaattention.html",
    "href": "1010-anovaattention.html",
    "title": "18  方差分析注意事项",
    "section": "",
    "text": "18.1 均衡设计和非均衡设计\n均衡设计是指不同组别之间的样本量相等，非均衡设计自然就是指不同组别之间样本量不相同。\n医学研究中大部分都是均衡设计。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>方差分析注意事项</span>"
    ]
  },
  {
    "objectID": "1010-anovaattention.html#方差分析的3种类型",
    "href": "1010-anovaattention.html#方差分析的3种类型",
    "title": "18  方差分析注意事项",
    "section": "18.2 方差分析的3种类型",
    "text": "18.2 方差分析的3种类型\n在计算方差分析中的平方和时，有3种类型（你可以简单理解为方差分析有3种类型），SPSS/SAS在做方差分析的时候，默认是类型Ⅲ，但是R语言中的aov()函数做方差分析时，默认是类型Ⅰ。\nR语言中做方差分析是用公式表示的，比如：aov(y ~ A + B + A:B, data = df)。\n\n表达式中效应的顺序在两种情况下会造成影响：(a)因子不止一个，并且是非平衡设计；(b)存在协变量。出现任意一种情况时，等式右边的变量都与其他每个变量相关。此时，我们无法清晰地划分它们对因变量的影响。一般来说，越基础性的效应越需要放在表达式前面。具体来讲，首先是协变量，然后是主效应，接着是双因素的交互项，再接着是三因素的交互项，以此类推。对于主效应，越基础性的变量越应放在表达式前面，因此性别要放在处理方式之前。有一个基本的准则：若研究设计不是正交的（也就是说，因子和/或协变量相关），一定要谨慎设置效应的顺序。–《R语言实战》\n\n也就是说：\n\n如果是均衡设计，3种类型的方差分析没有差别，这也是为什么之前的演示全都和SPSS结果一样的原因！\n如果是非均衡设计，但是只存在组别因素（比如完全随机设计的方差分析），结果也是没有差别的！\n如果是非均衡设计并且有多个因素，或者存在协变量时，3种类型方差分析的结果是不一样的！\n\n3种类型的区别可以参考下面这张图：\n\n\n\n\n\n\n\n\n\nR语言的aov()函数不能更改类型，但是我们通过其他R包实现更改类型。比如car::Anova()或者rstatix包。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>方差分析注意事项</span>"
    ]
  },
  {
    "objectID": "1010-anovaattention.html#示例",
    "href": "1010-anovaattention.html#示例",
    "title": "18  方差分析注意事项",
    "section": "18.3 示例",
    "text": "18.3 示例\n使用3个简单的小例子进行演示。\n\n18.3.1 示例一\n首先是一个单因素均衡设计的例子，来自课本例4-2。\n\ntrt&lt;-c(rep(\"group1\",30),rep(\"group2\",30),rep(\"group3\",30),rep(\"group4\",30))\n\nweight&lt;-c(3.53,4.59,4.34,2.66,3.59,3.13,3.30,4.04,3.53,3.56,3.85,4.07,1.37,\n          3.93,2.33,2.98,4.00,3.55,2.64,2.56,3.50,3.25,2.96,4.30,3.52,3.93,\n          4.19,2.96,4.16,2.59,2.42,3.36,4.32,2.34,2.68,2.95,2.36,2.56,2.52,\n          2.27,2.98,3.72,2.65,2.22,2.90,1.98,2.63,2.86,2.93,2.17,2.72,1.56,\n          3.11,1.81,1.77,2.80,3.57,2.97,4.02,2.31,2.86,2.28,2.39,2.28,2.48,\n          2.28,3.48,2.42,2.41,2.66,3.29,2.70,2.66,3.68,2.65,2.66,2.32,2.61,\n          3.64,2.58,3.65,3.21,2.23,2.32,2.68,3.04,2.81,3.02,1.97,1.68,0.89,\n          1.06,1.08,1.27,1.63,1.89,1.31,2.51,1.88,1.41,3.19,1.92,0.94,2.11,\n          2.81,1.98,1.74,2.16,3.37,2.97,1.69,1.19,2.17,2.28,1.72,2.47,1.02,\n          2.52,2.10,3.71)\n\ndf &lt;- data.frame(trt,weight)\n\nstr(df)\n## 'data.frame':    120 obs. of  2 variables:\n##  $ trt   : chr  \"group1\" \"group1\" \"group1\" \"group1\" ...\n##  $ weight: num  3.53 4.59 4.34 2.66 3.59 3.13 3.3 4.04 3.53 3.56 ...\n\nR语言中的方差分析结果比较：\n\n# 1型\nfit &lt;- aov(weight ~ trt, data = df)\nsummary(fit)\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## trt           3  32.16  10.719   24.88 1.67e-12 ***\n## Residuals   116  49.97   0.431                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2型\ncar::Anova(fit, type=2)\n## Anova Table (Type II tests)\n## \n## Response: weight\n##           Sum Sq  Df F value    Pr(&gt;F)    \n## trt       32.156   3  24.884 1.674e-12 ***\n## Residuals 49.967 116                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 3型\ncar::Anova(fit, type=3)\n## Anova Table (Type III tests)\n## \n## Response: weight\n##             Sum Sq  Df F value    Pr(&gt;F)    \n## (Intercept) 353.02   1 819.537 &lt; 2.2e-16 ***\n## trt          32.16   3  24.884 1.674e-12 ***\n## Residuals    49.97 116                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n可以看到3种类型的结果是完全一样的，没有差别~\n下面我们更改一下样本数量，使其变成非均衡设计：\n\n# 每组样本数量改一下\ntrt&lt;-c(rep(\"group1\",10),rep(\"group2\",50),rep(\"group3\",35),rep(\"group4\",25))\n\nweight&lt;-c(3.53,4.59,4.34,2.66,3.59,3.13,3.30,4.04,3.53,3.56,3.85,4.07,1.37,\n          3.93,2.33,2.98,4.00,3.55,2.64,2.56,3.50,3.25,2.96,4.30,3.52,3.93,\n          4.19,2.96,4.16,2.59,2.42,3.36,4.32,2.34,2.68,2.95,2.36,2.56,2.52,\n          2.27,2.98,3.72,2.65,2.22,2.90,1.98,2.63,2.86,2.93,2.17,2.72,1.56,\n          3.11,1.81,1.77,2.80,3.57,2.97,4.02,2.31,2.86,2.28,2.39,2.28,2.48,\n          2.28,3.48,2.42,2.41,2.66,3.29,2.70,2.66,3.68,2.65,2.66,2.32,2.61,\n          3.64,2.58,3.65,3.21,2.23,2.32,2.68,3.04,2.81,3.02,1.97,1.68,0.89,\n          1.06,1.08,1.27,1.63,1.89,1.31,2.51,1.88,1.41,3.19,1.92,0.94,2.11,\n          2.81,1.98,1.74,2.16,3.37,2.97,1.69,1.19,2.17,2.28,1.72,2.47,1.02,\n          2.52,2.10,3.71)\n\ndf1 &lt;- data.frame(trt,weight)\n\n然后再来看一下3种类型方差分析的结果：\n\n# 1型\nfit1 &lt;- aov(weight ~ trt, data = df1)\nsummary(fit1)\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## trt           3  22.03   7.343   14.17 6.23e-08 ***\n## Residuals   116  60.09   0.518                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2型\ncar::Anova(fit1, type = 2)\n## Anova Table (Type II tests)\n## \n## Response: weight\n##           Sum Sq  Df F value    Pr(&gt;F)    \n## trt       22.029   3  14.174 6.228e-08 ***\n## Residuals 60.094 116                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 3型\ncar::Anova(fit1, type = 3)\n## Anova Table (Type III tests)\n## \n## Response: weight\n##              Sum Sq  Df F value    Pr(&gt;F)    \n## (Intercept) 131.551   1 253.933 &lt; 2.2e-16 ***\n## trt          22.029   3  14.174 6.228e-08 ***\n## Residuals    60.094 116                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n可以看到3种类型的结果也是一样的哦！\n\n\n18.3.2 示例二\n使用一个随机区组设计的方差分析进行演示，示例数据来自课本例4-3的数据。\n首先也是均衡设计的情况：\n\nweight &lt;- c(0.82,0.65,0.51,0.73,0.54,0.23,0.43,0.34,0.28,0.41,0.21,\n            0.31,0.68,0.43,0.24)\nblock &lt;- c(rep(c(\"1\",\"2\",\"3\",\"4\",\"5\"),each=3))\ngroup &lt;- c(rep(c(\"A\",\"B\",\"C\"),5))\n\ndata4_4 &lt;- data.frame(weight,block,group)\n\nstr(data4_4)\n## 'data.frame':    15 obs. of  3 variables:\n##  $ weight: num  0.82 0.65 0.51 0.73 0.54 0.23 0.43 0.34 0.28 0.41 ...\n##  $ block : chr  \"1\" \"1\" \"1\" \"2\" ...\n##  $ group : chr  \"A\" \"B\" \"C\" \"A\" ...\n\n下面是3种类型方差分析的结果，由于是均衡设计，3种类型没有任何区别，并且即使你把区组因素和分组因素的位置互换，也不会有任何差别哦！\n\n# 1型\nfit &lt;- aov(weight ~ block + group, data = data4_4)\nsummary(fit)\n##             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \n## block        4 0.2284 0.05709   5.978 0.01579 * \n## group        2 0.2280 0.11400  11.937 0.00397 **\n## Residuals    8 0.0764 0.00955                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2型\ncar::Anova(fit, type = 2)\n## Anova Table (Type II tests)\n## \n## Response: weight\n##            Sum Sq Df F value   Pr(&gt;F)   \n## block     0.22836  4   5.978 0.015787 * \n## group     0.22800  2  11.937 0.003968 **\n## Residuals 0.07640  8                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 3型\ncar::Anova(fit, type = 3)\n## Anova Table (Type III tests)\n## \n## Response: weight\n##              Sum Sq Df F value    Pr(&gt;F)    \n## (Intercept) 1.44086  1 150.875 1.794e-06 ***\n## block       0.22836  4   5.978  0.015787 *  \n## group       0.22800  2  11.937  0.003968 ** \n## Residuals   0.07640  8                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n下面给它改成非均衡设计：\n\nweight &lt;- c(0.82,0.65,0.51,0.73,0.54,0.23,0.43,0.34,0.28,0.41,0.21,\n            0.31,0.68,0.43,0.24)\nblock &lt;- c(rep(c(\"1\",\"2\",\"3\",\"4\",\"5\"),each=3))\n\n# 每组样本量不一样\ngroup &lt;- c(rep(c(\"A\"),2),rep(\"B\",9),rep(\"C\",4))\n\ndata4_4 &lt;- data.frame(weight,block,group)\n\n下面再看一下3种类型方差分析的结果：\n\n# 1型\nfit1 &lt;- aov(weight ~ block + group, data = data4_4)\nsummary(fit)\n##             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \n## block        4 0.2284 0.05709   5.978 0.01579 * \n## group        2 0.2280 0.11400  11.937 0.00397 **\n## Residuals    8 0.0764 0.00955                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2型\ncar::Anova(fit1, type = 2)\n## Anova Table (Type II tests)\n## \n## Response: weight\n##             Sum Sq Df F value Pr(&gt;F)\n## block     0.079789  4  0.5896 0.6798\n## group     0.033750  2  0.4988 0.6250\n## Residuals 0.270650  8\n\n# 3型\ncar::Anova(fit1, type = 3)\n## Anova Table (Type III tests)\n## \n## Response: weight\n##              Sum Sq Df F value    Pr(&gt;F)    \n## (Intercept) 1.08045  1 31.9364 0.0004807 ***\n## block       0.07979  4  0.5896 0.6798021    \n## group       0.03375  2  0.4988 0.6249619    \n## Residuals   0.27065  8                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n可以看到结果完全不一样了哦！\n\n\n18.3.3 协方差分析\n就用一个简单的完全随机设计资料的协方差分析进行演示，示例数据来自课本例13-1。\n\ndf13_1 &lt;- data.frame(x1=c(10.8,11.6,10.6,9.0,11.2,9.9,10.6,10.4,9.6,10.5,\n                          10.6,9.9,9.5,9.7,10.7,9.2,10.5,11.0,10.1,10.7,8.5,10.0,\n                          10.4,9.7,9.4,9.2,10.5,11.2,9.6,8.0),\n                     y1=c(9.4,9.7,8.7,7.2,10.0,8.5,8.3,8.1,8.5,9.1,9.2,8.4,\n                          7.6,7.9,8.8,7.4,8.6,9.2,8.0,8.5,7.3,\n                          8.3,8.6,8.7,7.6,8.0,8.8,9.5,8.2,7.2),\n                     x2=c(10.4,9.7,9.9,9.8,11.1,8.2,8.8,10.0,9.0,9.4,\n                          8.9,10.3,9.3,9.2,10.9,9.2,9.2,10.4,11.2,\n                          11.1,11.0,8.6,9.3,10.3,10.3,9.8,10.5,10.7,10.4,9.4),\n                     y2=c(9.2,9.1,8.9,8.6,9.9,7.1,7.8,7.9,8.0,9.0,7.9,8.9,\n                          8.9,8.1,10.2,8.5,9.0,8.9,9.8,10.1,8.5,\n                          8.1,8.6,8.9,9.6,8.1,9.9,9.3,8.7,8.7),\n                     x3=c(9.8,11.2,10.7,9.6,10.1,9.8,10.1,10.3,11.0,10.5,9.2,\n                          10.1,10.4,10.0,8.4,10.1,9.3,10.5,11.1,10.5,\n                          9.7,9.2,9.3,10.4,10.0,10.3,9.9,9.4,8.3,9.2),\n                     y3=c(7.6,7.9,9.0,7.8,8.5,7.5,8.3,8.2,8.4,8.1,7.0,7.7,8.0,\n                          6.6,6.1,8.1,7.8,8.4,8.2,8.0,7.6,6.9,\n                          6.7,8.1,7.4,8.2,7.6,7.8,6.6,7.2)\n                     )\n\nsuppressPackageStartupMessages(library(tidyverse))\n\ndf13_11 &lt;- df13_1 %&gt;% \n  pivot_longer(cols = everything(), # 变长\n               names_to = c(\".value\",\"group\"),\n               names_pattern = \"(.)(.)\"\n               ) %&gt;% \n  mutate(group = as.factor(group)) # 组别变为因子型\n\nglimpse(df13_11)\n## Rows: 90\n## Columns: 3\n## $ group &lt;fct&gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1…\n## $ x     &lt;dbl&gt; 10.8, 10.4, 9.8, 11.6, 9.7, 11.2, 10.6, 9.9, 10.7, 9.0, 9.8, 9.6…\n## $ y     &lt;dbl&gt; 9.4, 9.2, 7.6, 9.7, 9.1, 7.9, 8.7, 8.9, 9.0, 7.2, 8.6, 7.8, 10.0…\n\ngroup是分组因素，x是协变量，y是因变量。\n下面进行3种类型的协方差分析：\n\n# 1型\nfit &lt;- aov(y ~ x + group, data = df13_11) \nsummary(fit)\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)    \n## x            1  29.06  29.057  171.20 &lt;2e-16 ***\n## group        2  19.85   9.925   58.48 &lt;2e-16 ***\n## Residuals   86  14.60   0.170                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2型\ncar::Anova(fit, type = 2)\n## Anova Table (Type II tests)\n## \n## Response: y\n##           Sum Sq Df F value    Pr(&gt;F)    \n## x         30.183  1  177.83 &lt; 2.2e-16 ***\n## group     19.851  2   58.48 &lt; 2.2e-16 ***\n## Residuals 14.596 86                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 3型\ncar::Anova(fit, type = 3)\n## Anova Table (Type III tests)\n## \n## Response: y\n##              Sum Sq Df  F value Pr(&gt;F)    \n## (Intercept)  0.3818  1   2.2493 0.1373    \n## x           30.1830  1 177.8346 &lt;2e-16 ***\n## group       19.8510  2  58.4798 &lt;2e-16 ***\n## Residuals   14.5963 86                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n可以看到在有协变量的情况下，即使是均衡设计，1型和2型、3型之间的平方和及F值也是有差异的。\n如果你很细心，你可能会发现R中进行单因素协方差分析（ANCOVA）的公式写法和随机区组的anova一模一样！那R语言怎么知道我们是要进行ancova还是随机区组的anova呢？\n很简单，在这里x作为协变量，是数值型，所以R默认会进行ancova，如果是因子型或者字符型，R会默认进行随机区组的anova，比如上面那个随机区组的例子！你会不会觉得这种识别方法过于粗糙了？还真不是，因为协方差分析的应用条件中也有规定：\n\n协方差分析要求协变量是连续变量，而且不能是影响处理的变量。–《医学统计学》P201\n\n所以，一切规定，皆有缘由，都是细节！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>方差分析注意事项</span>"
    ]
  },
  {
    "objectID": "多变量数据hotelling_t2检验.html",
    "href": "多变量数据hotelling_t2检验.html",
    "title": "19  多变量数据的统计描述和统计推断",
    "section": "",
    "text": "19.1 统计描述\n多变量数据主要使用3个指标进行描述：\n在R语言中都很好计算，有现成的函数，无需自己计算。\n以孙振球《医学统计学》第4版（蓝色封面）例14-1为例进行介绍。\n在一项健康调查中，随机抽取某单位15名正常成年男性测量血脂，记录甘油三酯（mmol/L）、总胆固醇（mmol/L）和高密度脂蛋白胆固醇（mmol/L），结果见表14-1。试对这3个反应变量进行多变量描述。\n先读取数据：\ndata14_1 &lt;- haven::read_sav(\"datasets/例14-01.sav\")\n\nstr(data14_1)\n## tibble [15 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ x1: num [1:15] 1.06 0.98 0.85 0.96 0.98 0.99 1.01 1.02 1.02 1.1 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.2\"\n##  $ x2: num [1:15] 2.56 2.42 2.35 2.55 2.65 2.6 2.35 2.89 2.54 2.64 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.2\"\n##  $ x3: num [1:15] 1.93 1.8 1.68 1.34 2.55 2.33 1.93 1.8 1.68 1.34 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.2\"\nhead(data14_1)\n## # A tibble: 6 × 3\n##      x1    x2    x3\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  1.06  2.56  1.93\n## 2  0.98  2.42  1.8 \n## 3  0.85  2.35  1.68\n## 4  0.96  2.55  1.34\n## 5  0.98  2.65  2.55\n## 6  0.99  2.6   2.33\nx1是甘油三酯，x2是总胆固醇，x3是高密度脂蛋白。\n下面就是对这个数据进行统计描述，首先是各列（各个变量）的均值：\ncolMeans(data14_1)\n##       x1       x2       x3 \n## 1.020000 2.728667 2.043333\n然后是协方差矩阵：\ncov(data14_1)\n##             x1         x2          x3\n## x1 0.005757143 0.01029286 0.009314286\n## x2 0.010292857 0.08864095 0.080211905\n## x3 0.009314286 0.08021190 0.186838095\n最后是相关矩阵：\ncor(data14_1) # 相关矩阵\n##           x1        x2        x3\n## x1 1.0000000 0.4556331 0.2839967\n## x2 0.4556331 1.0000000 0.6232882\n## x3 0.2839967 0.6232882 1.0000000\n如果要计算相关系数的P值，可以使用Hmics包（方法非常多，以后再慢慢介绍）：\nlibrary(Hmisc)\n\n# 结果中的P就是P值矩阵\nrcorr(as.matrix(data14_1))$P # 需要矩阵格式\n##            x1         x2        x3\n## x1         NA 0.08785736 0.3049796\n## x2 0.08785736         NA 0.0130474\n## x3 0.30497964 0.01304740        NA\n以上3个多元描述统计量，均值向量描述3个测量指标的平均水平，协方差矩阵描述这3个指标的变异程度，相关矩阵描述这3个指标的相关性。\n由相关矩阵可知，甘油三酯与总胆固醇的相关系数为0.456（P=0.088），甘油三酯与高密度脂蛋白胆固醇的相关系数为0.284（P=0.305），总胆固醇与高密度脂蛋白胆固醇的相关系数为0.623（P=0.013)。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>多变量数据的统计描述和统计推断</span>"
    ]
  },
  {
    "objectID": "多变量数据hotelling_t2检验.html#统计描述",
    "href": "多变量数据hotelling_t2检验.html#统计描述",
    "title": "19  多变量数据的统计描述和统计推断",
    "section": "",
    "text": "均值向量\n协方差矩阵\n相关矩阵",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>多变量数据的统计描述和统计推断</span>"
    ]
  },
  {
    "objectID": "多变量数据hotelling_t2检验.html#多元正态分布",
    "href": "多变量数据hotelling_t2检验.html#多元正态分布",
    "title": "19  多变量数据的统计描述和统计推断",
    "section": "19.2 多元正态分布",
    "text": "19.2 多元正态分布\n在单变量统计描述和推断中，通常假定数据服从正态分布。同理，在多变量统计描述和推断中，也是通常假定数据服从多元正态分布。\n单变量数据可以使用Shapiro-Wilk检验数据的正态性，多变量数据也有多种方法可以检验正态性。\n可以借助MVN包中的mvn函数，或者mvnormtest包中的mshapiro.test函数等实现多变量数据的正态性检验。\n\nlibrary(mvnormtest)\nmshapiro.test(t(data14_1)) # 注意转置\n## \n##  Shapiro-Wilk normality test\n## \n## data:  Z\n## W = 0.90331, p-value = 0.1069\n\nlibrary(MVN)\nmvn(data14_1)$multivariateNormality # 默认Henze-Zirkler法\n##            Test        HZ   p value MVN\n## 1 Henze-Zirkler 0.5079275 0.5592683 YES\n\n结果P&gt;0.05，可以认为符合正态性。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>多变量数据的统计描述和统计推断</span>"
    ]
  },
  {
    "objectID": "多变量数据hotelling_t2检验.html#组间差别的比较",
    "href": "多变量数据hotelling_t2检验.html#组间差别的比较",
    "title": "19  多变量数据的统计描述和统计推断",
    "section": "19.3 组间差别的比较",
    "text": "19.3 组间差别的比较\n多变量数据的假设检验和单变量数据非常相似，单变量数据常用的方法有t检验、方差分析、秩和检验等，多变量数据则是hotelling-T2检验、多变量方差分析、多变量非参数检验等。\n\n19.3.1 单组资料\n孙振球《医学统计学》第4版（蓝色封面）例14-2：随机抽取某单位5名怀疑有冠心病的成年男性，测量其甘油三酯（mmol/L）、总胆固醇mmol/L）和高密度脂蛋白胆固醇（mmol/L）含量。根据例14-1已知，该单位正常成年男性的甘油三酯、总胆固醇和高密度脂蛋白胆固醇均数分别为1.02mmol/L、2.73mmol/L和2.04mmol/L。问该单位怀疑冠心病成年男性的血脂与正常成年男性的血脂有无差别？\n读取数据：\n\ndata14_2 &lt;- haven::read_sav(\"datasets/例14-02.sav\")\n\nhead(data14_2)\n## # A tibble: 5 × 3\n##      x1    x2    x3\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  1.78 0.83  -1.01\n## 2  0.67 0.96  -0.84\n## 3  0.56 0.83  -0.39\n## 4  0.66 1.12  -1.03\n## 5  0.21 0.160  0.4\n\nx1是甘油三酯，x2是总胆固醇，x3是高密度脂蛋白。\n\n# 查看下数据的均值和协方差矩阵\ncolMeans(data14_2)\n##     x1     x2     x3 \n##  0.776  0.780 -0.574\ncov(data14_2)\n##          x1       x2       x3\n## x1  0.34993  0.08295 -0.23692\n## x2  0.08295  0.13435 -0.20485\n## x3 -0.23692 -0.20485  0.36283\n\n单样本的hotelling-T2检验可通过ICSNP包中的HotellingsT2函数实现（在进行hotelling-T2检验前需要检验正态性，略）：\n\nlibrary(ICSNP)\n\nICSNP::HotellingsT2(X = data14_2, mu=c(1.02, 2.73, 2.04), test = \"f\")\n## \n##  Hotelling's one sample T2-test\n## \n## data:  data14_2\n## T.2 = 2389.8, df1 = 3, df2 = 2, p-value = 0.0004183\n## alternative hypothesis: true location is not equal to c(1.02,2.73,2.04)\n\n结果显示P值小于0.01，可认为怀疑冠心病成年男性的血脂与正常成年男性的血脂有差别。根据3个样本均值可知，该单位怀疑冠性病成年男性甘油三酯和总胆固醇高于正常成年男性，高密度脂蛋白低于正常成年男性。\n但是要注意，ICSNP结果中的T.2的值其实是F值，不是T2的值（由于计算过程的差异，单样本检验的F值和书中不同）。\n除了ICSNP外，还可以使用DescTools::HotellingsT2Test实现，但是该函数完全来自于ICSNP包，基本完全一样，所以不再介绍，还有一个rrcov包也可以实现hotelling-T2检验，使用方法如下：\n\nrrcov::T2.test(x = data14_2, mu=c(1.02, 2.73, 2.04))\n## \n##  One-sample Hotelling test\n## \n## data:  data14_2\n## T2 = 14338.9, F = 2389.8, df1 = 3, df2 = 2, p-value = 0.0004183\n## alternative hypothesis: true mean vector is not equal to (1.02, 2.73, 2.04)' \n## \n## sample estimates:\n##                  x1   x2     x3\n## mean x-vector 0.776 0.78 -0.574\n\n结果和ICSNP的结果是一样的，其中的F值就是ICSNP结果中的T.2的值。\n\n\n19.3.2 两组资料\n以孙振球《医学统计学》第4版（蓝色封面）例14-3为例。\n某妇幼保健院将孕妇随机分为两组，一组接受孕期保健教育，另一组作为对照。表14-3是同一日出生的13名顺产婴儿的体重和身长，问孕期保健教育对婴儿生长发育有无促进作用。\n先读取数据：\n\ndata14_3 &lt;- haven::read_sav(\"datasets/例14-03.sav\")\n\ndata14_3\n## # A tibble: 13 × 3\n##    group weight height\n##    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n##  1     1   3.05     50\n##  2     1   4.1      50\n##  3     1   3.5      53\n##  4     1   3.64     50\n##  5     1   3.6      52\n##  6     1   4        55\n##  7     2   3.2      50\n##  8     2   3        46\n##  9     2   3        45\n## 10     2   3.35     47\n## 11     2   2.6      50\n## 12     2   3.55     52\n## 13     2   3.34     50\nstr(data14_3)\n## tibble [13 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ group : num [1:13] 1 1 1 1 1 1 2 2 2 2 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\n##  $ weight: num [1:13] 3.05 4.1 3.5 3.64 3.6 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.2\"\n##  $ height: num [1:13] 50 50 53 50 52 55 50 46 45 47 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\n\n计算下各组的身高和体重的均值：\n\ntapply(data14_3[,-1], data14_3$group, FUN = colMeans)\n## $`1`\n##   weight   height \n##  3.64750 51.66667 \n## \n## $`2`\n##    weight    height \n##  3.148571 48.571429\n\n下面进行多变量的统计推断，也是使用hotelling-T2检验：\n\nwith(data14_3,\n  ICSNP::HotellingsT2(cbind(weight,height) ~ group) # 注意使用格式\n  )\n## \n##  Hotelling's two sample T2-test\n## \n## data:  cbind(weight, height) by group\n## T.2 = 4.3119, df1 = 2, df2 = 10, p-value = 0.04463\n## alternative hypothesis: true location difference is not equal to c(0,0)\n\n再次提醒，这个R包结果中的T.2是F值，这里的P值小于0.05，再结合均值，可得结论：孕期保健教育组出生的婴儿的身高和体重均大于对照组，孕期的保健教育对婴儿生长发育有促进作用。\n也可以使用DescTools或者rrcov实现，完全一样的用法（注意函数的名字略有不同），结果也完全一样，这里就不再重复了。\n但是以上方法的结果和SPSS不一样（P值是一样的，但是F值不一样，不影响结论），如果你想得到和SPSS一样的结果，可以使用Hotelling::hotelling.test实现（这个R包只能用于两样本的比较，不能用于单样本的比较），这个结果和课本（即SPSS）一样的：\n\nprint(with(data14_3,\n  Hotelling::hotelling.test(cbind(weight,height) ~ group)\n  ))\n## Test stat:  9.4862 \n## Numerator df:  2 \n## Denominator df:  10 \n## P-value:  0.04463\n\n\n\n19.3.3 多组资料\n单组资料用单样本的hotelling-T2检验，两组资料用成组的hotelling-T2检验检验，是不是和t检验非常类似呢？大于两组的单变量数据是使用方差分析，同理，大于两组的多变量数据也是使用方差分析，此时是多变量方差分析（multivariate analysis of variance，MANOVA）。\n以孙振球《医学统计学》第4版（蓝色封面）例14-4和利14-5为例，这两个例题用的同一个数据。\n患慢性胃炎的儿童随机分为3组，其中I组、Ⅱ组为治疗组，另一组作为对照，试比较治疗药物对T细胞免疫功能（外周血T3、T4、T8细胞百分比）的影响。\n\ndata14_4 &lt;- haven::read_sav(\"datasets/例14-04.sav\")\ndata14_4\n## # A tibble: 12 × 4\n##       t3    t4    t8 group\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  63.6  30.2  31.2     1\n##  2  60    30    33.4     1\n##  3  63.2  35.3  27.9     1\n##  4  53.4  22.5  25       2\n##  5  46.5  20    14.6     2\n##  6  38.1  25.9  18.1     2\n##  7  32.1  12.1  11.8     2\n##  8  72.4  42.5  29.9     3\n##  9  75    49.5  29.3     3\n## 10  75.9  30    40       3\n## 11  70    32    36.4     3\n## 12  72.8  36.7  33.1     3\n\ngroup是组别，剩余列分别是外周血T3、T4、T8细胞百分比。\n描述一下这个数据：\n\ntapply(data14_4[,1:3], data14_4$group, colMeans) # 均值\n## $`1`\n##       t3       t4       t8 \n## 62.26667 31.83333 30.83333 \n## \n## $`2`\n##     t3     t4     t8 \n## 42.525 20.125 17.375 \n## \n## $`3`\n##    t3    t4    t8 \n## 73.22 38.14 33.74\ntapply(data14_4[,1:3], data14_4$group, cov) # 协方差矩阵\n## $`1`\n##           t3        t4        t8\n## t3  3.893333  2.606667 -4.033333\n## t4  2.606667  9.023333 -7.736667\n## t8 -4.033333 -7.736667  7.663333\n## \n## $`2`\n##         t3       t4       t8\n## t3 87.4425 27.81250 42.26750\n## t4 27.8125 34.46917 22.46083\n## t8 42.2675 22.46083 32.48250\n## \n## $`3`\n##        t3       t4       t8\n## t3 5.3920   3.8015   0.9315\n## t4 3.8015  63.5230 -33.3870\n## t8 0.9315 -33.3870  20.2830\ntapply(data14_4[,1:3], data14_4$group, cor) # 相关矩阵\n## $`1`\n##            t3         t4         t8\n## t3  1.0000000  0.4397858 -0.7384047\n## t4  0.4397858  1.0000000 -0.9303825\n## t8 -0.7384047 -0.9303825  1.0000000\n## \n## $`2`\n##           t3        t4        t8\n## t3 1.0000000 0.5065979 0.7930865\n## t4 0.5065979 1.0000000 0.6712522\n## t8 0.7930865 0.6712522 1.0000000\n## \n## $`3`\n##            t3         t4          t8\n## t3 1.00000000  0.2054067  0.08907213\n## t4 0.20540668  1.0000000 -0.93013460\n## t8 0.08907213 -0.9301346  1.00000000\n\n进行多变量方差分析，直接使用R语言自带的manova函数即可：\n\nfit &lt;- with(data14_4,\n            manova(cbind(t3,t4,t8) ~ as.factor(group))\n            )\n\nsummary(fit, test = \"Wilks\") \n##                  Df    Wilks approx F num Df den Df   Pr(&gt;F)   \n## as.factor(group)  2 0.088735   5.4997      6     14 0.004104 **\n## Residuals         9                                            \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWilks Lambda统计量的值为0.088735，F值为5.4997，P值小于0.01，结果与课本一致。可以认为3组慢性胃炎儿童T细胞免疫功能有差别。进一步比较治疗I组、治疗Ⅱ组和对照组的3个均数向量，结论为：慢性胃炎儿童经药物治疗后，对T细胞免疫功能（%）有影响，治疗组的T细胞免疫功能（%)均比对照组低，其中治疗Ⅱ组已降低到接近正常参考值的下限（注：正常参考值T3：66.0±9.9%，T4：43.8±9.0%，T8：31.3±7.0%）。\n如果要查看每个变量的方差分析表可以使用：\n\n# 单变量方差分析表\nsummary.aov(fit) \n##  Response t3 :\n##                  Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \n## as.factor(group)  2 2110.01 1055.00  32.553 7.582e-05 ***\n## Residuals         9  291.68   32.41                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  Response t4 :\n##                  Df Sum Sq Mean Sq F value   Pr(&gt;F)   \n## as.factor(group)  2 727.70  363.85  8.7197 0.007833 **\n## Residuals         9 375.55   41.73                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  Response t8 :\n##                  Df Sum Sq Mean Sq F value   Pr(&gt;F)   \n## as.factor(group)  2 638.04  319.02  14.807 0.001425 **\n## Residuals         9 193.91   21.55                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n除此之外，除了Wilks Lambda统计量，还有其他几个常用的统计量：\n\n# test = c(\"Pillai\", \"Wilks\", \"Hotelling-Lawley\", \"Roy\")\nsummary(fit, test = \"Pillai\") # 通过参数更改即可\n##                  Df Pillai approx F num Df den Df  Pr(&gt;F)  \n## as.factor(group)  2 1.0492   2.9424      6     16 0.03942 *\n## Residuals         9                                        \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n19.3.4 多变量与单变量分析\n多变量分析是将m个反应变量作为一个整体，进行一次假设检验（Hotelling-T2检验或MANOVA），对组间差别作出推断。虽然在大多数情况下，多变量假设检验结论与对m个反应变量进行m次单变量假设检验（t检验或ANOVA）的结论是一致的，即多变量假设检验拒绝H0，m次单变量假设检验至少有次拒绝H0，SPSS、SAS等统计软件也是先给出多变量假设检验结果，再给出单变量假设检验结果，作为多变量分析的补充。但单变量假设检验不能代替多变量假设检验，主要理由：①m次单变量假设检验增加假阳性错误的概率，设每次单变量假设检验的检验水准定为α，做完m次检验I类错误的概率会增加。②单变量假设检验只说明某一变量在数轴分布上的组间差别，不能反映多个变量在平面或空间上的差别，两者的意义不同，各自说明各自的问题，不能相互代替。\n以孙振球《医学统计学》第4版（蓝色封面）例14-6为例。下面是两组新生儿出生时的身高和体重数据，试做单变量与多变量分析。\n\ndata14_6 &lt;- haven::read_sav(\"datasets/例14-06.sav\")\nstr(data14_6)\n## tibble [16 × 4] (S3: tbl_df/tbl/data.frame)\n##  $ weight: num [1:16] 3.1 3.2 3.5 3 3.85 3.15 3 3.5 4.1 3.5 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.2\"\n##  $ height: num [1:16] 46 50 62 46 67 48 46 55 60 48 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\n##  $ group : num [1:16] 1 1 1 1 1 1 1 1 2 2 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\n##  $ no    : num [1:16] 1 2 3 4 5 6 7 8 1 2 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\nhead(data14_6)\n## # A tibble: 6 × 4\n##   weight height group    no\n##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1   3.1      46     1     1\n## 2   3.2      50     1     2\n## 3   3.5      62     1     3\n## 4   3        46     1     4\n## 5   3.85     67     1     5\n## 6   3.15     48     1     6\n\n#data14_6$group &lt;- factor(data14_6$group)\n\n对体重做单变量t检验（注意，只是单变量，不是单组，其实是两独立样本t检验）：\n\nt.test(weight ~ group, data = data14_6)\n## \n##  Welch Two Sample t-test\n## \n## data:  weight by group\n## t = -1.6219, df = 13.822, p-value = 0.1274\n## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n## 95 percent confidence interval:\n##  -0.53742421  0.07492421\n## sample estimates:\n## mean in group 1 mean in group 2 \n##         3.28750         3.51875\n\n对身高做单变量t检验：\n\nt.test(height ~ group, data = data14_6)\n## \n##  Welch Two Sample t-test\n## \n## data:  height by group\n## t = -0.036693, df = 11.938, p-value = 0.9713\n## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n## 95 percent confidence interval:\n##  -7.551649  7.301649\n## sample estimates:\n## mean in group 1 mean in group 2 \n##          52.500          52.625\n\n做双变量Hotelling-T2检验：和课本结果一致\n\nprint(with(data14_6,\n  Hotelling::hotelling.test(cbind(weight,height) ~ group)\n  ))\n## Test stat:  9.8669 \n## Numerator df:  2 \n## Denominator df:  13 \n## P-value:  0.0312\n\nwith(data14_6,\n  ICSNP::HotellingsT2(cbind(weight,height) ~ group) \n  )\n## \n##  Hotelling's two sample T2-test\n## \n## data:  cbind(weight, height) by group\n## T.2 = 4.5811, df1 = 2, df2 = 13, p-value = 0.0312\n## alternative hypothesis: true location difference is not equal to c(0,0)\n\n分别对两组新生儿出生时的体重与身长做单变量t检验：体重t=1.62，P=0.13，身长t=0.04，P=0.97，都不能拒绝H0。但双变量的HotellingT检验：T2=9.87，F=4.58，P=0.03，拒绝H0。\n两组在平面分布上的差别如下图所示：\n\nlibrary(ggplot2)\n\nggplot(data14_6, aes(height, weight))+\n  geom_point(aes(shape=as.factor(group)),size=5)+\n  labs(x=\"身长(cm)\", y=\"体重(cm)\")+\n  scale_shape_discrete(name = \"组别\",labels=c(\"A组\",\"B组\"))+\n  theme_classic()+\n  theme(legend.position.inside = c(0.8,0.2))",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>多变量数据的统计描述和统计推断</span>"
    ]
  },
  {
    "objectID": "多变量数据hotelling_t2检验.html#重复测量资料的多变量分析",
    "href": "多变量数据hotelling_t2检验.html#重复测量资料的多变量分析",
    "title": "19  多变量数据的统计描述和统计推断",
    "section": "19.4 重复测量资料的多变量分析",
    "text": "19.4 重复测量资料的多变量分析\n如果不考虑重复测量数据是否满足“球对称”假设，可将每个观察对象的m次重复测量结果看作一个向量，直接采用多变量的Hotelling-T2检验。\n孙振球《医学统计学》第4版（蓝色封面）例14-7：10名肥胖患者在医生指导下服用药物减肥，按统一标准记录服药前和服药后1～4周的体重，试分析减肥效果。\n\ndata14_7 &lt;- haven::read_sav(\"datasets/例14-07.sav\")\nstr(data14_7)\n## tibble [10 × 6] (S3: tbl_df/tbl/data.frame)\n##  $ n0: num [1:10] 1 2 3 4 5 6 7 8 9 10\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\n##  $ t1: num [1:10] 132 155 147 163 129 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.1\"\n##  $ t2: num [1:10] 128 153 146 162 125 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.1\"\n##  $ t3: num [1:10] 127 151 144 158 124 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.1\"\n##  $ t4: num [1:10] 125 148 140 154 123 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.1\"\n##  $ t5: num [1:10] 125 146 140 153 121 ...\n##   ..- attr(*, \"format.spss\")= chr \"F8.1\"\nhead(data14_7)\n## # A tibble: 6 × 6\n##      n0    t1    t2    t3    t4    t5\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     1  132.  128.  127.  125.  125.\n## 2     2  155.  153.  151.  148.  146.\n## 3     3  147.  146.  144.  140.  140.\n## 4     4  163.  162.  158.  154.  153.\n## 5     5  129.  125.  124.  123.  121.\n## 6     6  134.  133.  130.  129.  125.\n\n以服药前的体重为基线，计算服药后各时间点的改变值：\n\ndata14_7 &lt;- as.data.frame(data14_7) # 变成`纯血的`data.frame，不然下一步报错\nweight_change &lt;- as.matrix(data14_7[,3:6] - data14_7[,2])\nweight_change\n##         t2   t3   t4    t5\n##  [1,] -3.1 -4.1 -6.2  -6.6\n##  [2,] -1.8 -4.0 -6.5  -8.8\n##  [3,] -1.2 -3.1 -6.2  -6.9\n##  [4,] -1.6 -4.8 -9.0  -9.8\n##  [5,] -3.3 -4.5 -5.8  -7.7\n##  [6,] -1.6 -3.8 -4.8  -9.4\n##  [7,] -1.1 -2.9 -3.3  -5.2\n##  [8,] -1.4 -3.9 -5.2  -7.4\n##  [9,] -3.8 -7.7 -9.8 -11.0\n## [10,] -1.2 -2.8 -4.4  -7.1\n\n建立检验假设：如果减肥药物无效，各时间点体重的总体均数相等，即μ1=μ2=μ3=μ4=μ5。\n进行单样本的HotellingsT2检验，结果和课本一致：\n\nICSNP::HotellingsT2(X=weight_change ,mu=c(0,0,0,0))\n## \n##  Hotelling's one sample T2-test\n## \n## data:  weight_change\n## T.2 = 41.308, df1 = 4, df2 = 6, p-value = 0.0001676\n## alternative hypothesis: true location is not equal to c(0,0,0,0)\n\nF值为41.308，拒绝零假设，可认为服药后体重比服药前降低了。\n然后是分析服药后1~4周的体重变化趋势。先把数据变成长数据。\n\nlibrary(tidyr)\nlibrary(dplyr)\n\ndata14_7_long &lt;- data14_7 %&gt;% \n  pivot_longer(cols = 2:6, names_to = \"times\",values_to = \"weights\")\ndata14_7_long\n## # A tibble: 50 × 3\n##       n0 times weights\n##    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n##  1     1 t1       132.\n##  2     1 t2       128.\n##  3     1 t3       127.\n##  4     1 t4       125.\n##  5     1 t5       125.\n##  6     2 t1       155.\n##  7     2 t2       153.\n##  8     2 t3       151.\n##  9     2 t4       148.\n## 10     2 t5       146.\n## # ℹ 40 more rows\n\n下面是做方差分析，然后比较多个时间点的趋势，但是这种做法我不知道对不对，我也没找到相似的参考资料（欢迎大家赐教，感激）：\n\nf &lt;- aov(weights ~ times, data = data14_7_long)\n\n# 分别看不同次方的结果\nsummary(f, \n        split=list(times=list(liner=1,quadratic=2,cubic=3,biquadrate=4))\n        ,intercept = T\n        )\n##                     Df Sum Sq Mean Sq  F value Pr(&gt;F)    \n## (Intercept)          1 842843  842843 3185.481 &lt;2e-16 ***\n## times                4    404     101    0.382  0.821    \n##   times: liner       1     52      52    0.198  0.659    \n##   times: quadratic   1      2       2    0.008  0.928    \n##   times: cubic       1     30      30    0.114  0.737    \n##   times: biquadrate  1    319     319    1.206  0.278    \n## Residuals           45  11907     265                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n和SPSS的结果相差还是蛮大的：\n\n画图展示：\n\nlibrary(ggplot2)\n\nggplot(data14_7_long, aes(times, weights))+\n  geom_line(aes(group=n0))+\n  geom_point(aes(shape=times))",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>多变量数据的统计描述和统计推断</span>"
    ]
  },
  {
    "objectID": "多变量数据hotelling_t2检验.html#轮廓分析",
    "href": "多变量数据hotelling_t2检验.html#轮廓分析",
    "title": "19  多变量数据的统计描述和统计推断",
    "section": "19.5 轮廓分析",
    "text": "19.5 轮廓分析\n轮廓分析是比较两组或多组多变量均数向量的轮廊是否相等。\n孙振球《医学统计学》第4版（蓝色封面）例14-8。\n分别对50名硕士生和30名博士生进行健康状况抽样调查。调查问卷设计了如下7个问题：\n\n对自已健康状况的满意程度（X1）\n是否需要调养身体（X2）\n身体有不适或不舒服的感觉（X3）\n有生病的感觉（X4）\n有紧张情绪和压力感（X5）\n晚间休息感到不能很快入睡（X6）\n吃饭有时觉得胃口不好（X7）\n\n每个问题的回答从好到差按4个等级记分（分别赋值1、2、3、4），7个问题的平均得分的轮廓图见下图，问每个问题硕士生和博士生的回答结果是否相同？\n\n先读取数据：\n\ndata14_8 &lt;- haven::read_sav(\"datasets/例14-08.sav\")\ndata14_8 &lt;- haven::zap_formats(data14_8) # 去除格式\nstr(data14_8)\n## tibble [80 × 9] (S3: tbl_df/tbl/data.frame)\n##  $ no   : num [1:80] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ group: num [1:80] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ x1   : num [1:80] 2 2 2 2 2 2 1 2 2 1 ...\n##  $ x2   : num [1:80] 2 2 2 2 2 3 2 1 2 2 ...\n##  $ x3   : num [1:80] 3 2 2 2 3 2 2 3 2 2 ...\n##  $ x4   : num [1:80] 3 2 2 2 2 2 2 2 2 1 ...\n##  $ x5   : num [1:80] 3 3 2 3 2 2 2 1 1 3 ...\n##  $ x6   : num [1:80] 2 1 2 1 3 2 2 1 1 3 ...\n##  $ x7   : num [1:80] 2 3 2 3 3 2 2 2 3 4 ...\nhead(data14_8)\n## # A tibble: 6 × 9\n##      no group    x1    x2    x3    x4    x5    x6    x7\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     1     1     2     2     3     3     3     2     2\n## 2     2     1     2     2     2     2     3     1     3\n## 3     3     1     2     2     2     2     2     2     2\n## 4     4     1     2     2     2     2     3     1     3\n## 5     5     1     2     2     3     2     2     3     3\n## 6     6     1     2     3     2     2     2     2     2\n\n先计算下均值：\n\n# 分组均值\ntapply(data14_8[,3:9], data14_8$group, colMeans)\n## $`1`\n##   x1   x2   x3   x4   x5   x6   x7 \n## 2.02 2.32 2.18 1.98 2.44 2.06 2.16 \n## \n## $`2`\n##       x1       x2       x3       x4       x5       x6       x7 \n## 2.033333 2.300000 2.266667 1.900000 2.266667 1.900000 2.133333\n# 不分组均值\ncolMeans(data14_8[,3:9])\n##     x1     x2     x3     x4     x5     x6     x7 \n## 2.0250 2.3125 2.2125 1.9500 2.3750 2.0000 2.1500\n\n轮廓分析可使用profileR包实现，先安装一下：\n\ninstall.packages(\"profileR\")\n\ndevtools::install_github(repo = \"cddesja/profileR\", build_vignettes = TRUE) # with the vignette\n\n两个组的轮廓分析使用pbg函数实现：\n\nlibrary(profileR)\n\nmod &lt;- pbg(data = data14_8[,3:9], group = data14_8$group)\n#print(mod)\n\nsummary(mod) # 查看结果\n## Call:\n## pbg(data = data14_8[, 3:9], group = data14_8$group)\n## \n## Hypothesis Tests:\n## $`Ho: Profiles are parallel`\n##   Multivariate.Test  Statistic  Approx.F num.df den.df   p.value\n## 1             Wilks 0.96198449 0.4807999      6     73 0.8205697\n## 2            Pillai 0.03801551 0.4807999      6     73 0.8205697\n## 3  Hotelling-Lawley 0.03951780 0.4807999      6     73 0.8205697\n## 4               Roy 0.03951780 0.4807999      6     73 0.8205697\n## \n## $`Ho: Profiles have equal levels`\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)\n## group        1   0.05 0.04959   0.276  0.601\n## Residuals   78  13.99 0.17936               \n## \n## $`Ho: Profiles are flat`\n##          F df1 df2      p-value\n## 1 10.83958   6  73 1.355666e-08\n\n轮廓分析主要从3个方面评价：\n\n平行检验：检验两个总体的轮廓是否平行（parallel profile）\n相合检验：检验两个总体的轮廓是否重合（coincident profile），只有两组轮廓平行是才有意义\n水平检验：检验两个总体的轮廓是否为水平直线（level profile），只有在两组轮廓重合时才有意义\n\n先看平行检验的结果，也就是：H0: Profiles are parallel：\n用了4种方法计算hotelling-T2和F值，分别是Wilks、Pillai、Hotelling-Lawley、Roy。P值都是大于0.05的，可以认为两个总体的轮廓互相平行。\n然后看相合检验的结果，也就是：H0: Profiles have equal levels，P值是0.601，可以认为两个总体的轮廓重合。\n最后看水平检验的结果：H0: Profiles are flat，P值小于0.01，说明两样本合并后总体的轮廓不是一条水平线，即健康问卷的7个问题的回答的分支有高有低，其中第5个问题平均得分最高，为2.38，第4个问题的得分最低，为1.95。\n结合轮廓分析平行检验和相合检验的结果，例14-8数据分析结果的总结论：对健康调查问卷7个问题的回答，硕士生和博士生没有差别。健康调查问卷每个问题回答的分值不同，其中第5个问题（有紧张情绪和压力感）的平均得分最高，第4个问题（有生病的感觉）平均得分最低。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>多变量数据的统计描述和统计推断</span>"
    ]
  },
  {
    "objectID": "多变量数据hotelling_t2检验.html#参考资料",
    "href": "多变量数据hotelling_t2检验.html#参考资料",
    "title": "19  多变量数据的统计描述和统计推断",
    "section": "19.6 参考资料",
    "text": "19.6 参考资料\n\nhttps://friendly.github.io/Vis-MLM-book/09-hotelling.html\n多元正态分布检验的R实现方法\nR语言多元统计分析，赵军，戴静毅\nVisualizing Multivariate Data and Models in R",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>多变量数据的统计描述和统计推断</span>"
    ]
  },
  {
    "objectID": "1017-multireg.html",
    "href": "1017-multireg.html",
    "title": "20  多元线性回归",
    "section": "",
    "text": "20.1 多元线性回归\n使用课本例15-1的数据，手动摘录：\ndf15_1 &lt;- data.frame(\n  cho = c(5.68,3.79,6.02,4.85,4.60,6.05,4.90,7.08,3.85,4.65,4.59,4.29,7.97,\n      6.19,6.13,5.71,6.40,6.06,5.09,6.13,5.78,5.43,6.50,7.98,11.54,5.84,\n      3.84),\n  tg = c(1.90,1.64,3.56,1.07,2.32,0.64,8.50,3.00,2.11,0.63,1.97,1.97,1.93,\n      1.18,2.06,1.78,2.40,3.67,1.03,1.71,3.36,1.13,6.21,7.92,10.89,0.92,\n      1.20),\n  ri = c(4.53, 7.32,6.95,5.88,4.05,1.42,12.60,6.75,16.28,6.59,3.61,6.61,7.57,\n      1.42,10.35,8.53,4.53,12.79,2.53,5.28,2.96,4.31,3.47,3.37,1.20,8.61,\n      6.45),\n  hba = c(8.2,6.9,10.8,8.3,7.5,13.6,8.5,11.5,7.9,7.1,8.7,7.8,9.9,6.9,10.5,8.0,\n      10.3,7.1,8.9,9.9,8.0,11.3,12.3,9.8,10.5,6.4,9.6),\n  fpg = c(11.2,8.8,12.3,11.6,13.4,18.3,11.1,12.1,9.6,8.4,9.3,10.6,8.4,9.6,10.9,\n     10.1,14.8,9.1,10.8,10.2,13.6,14.9,16.0,13.2,20.0,13.3,10.4)\n  )\n\nstr(df15_1)\n## 'data.frame':    27 obs. of  5 variables:\n##  $ cho: num  5.68 3.79 6.02 4.85 4.6 6.05 4.9 7.08 3.85 4.65 ...\n##  $ tg : num  1.9 1.64 3.56 1.07 2.32 0.64 8.5 3 2.11 0.63 ...\n##  $ ri : num  4.53 7.32 6.95 5.88 4.05 ...\n##  $ hba: num  8.2 6.9 10.8 8.3 7.5 13.6 8.5 11.5 7.9 7.1 ...\n##  $ fpg: num  11.2 8.8 12.3 11.6 13.4 18.3 11.1 12.1 9.6 8.4 ...\nhead(df15_1)\n##    cho   tg   ri  hba  fpg\n## 1 5.68 1.90 4.53  8.2 11.2\n## 2 3.79 1.64 7.32  6.9  8.8\n## 3 6.02 3.56 6.95 10.8 12.3\n## 4 4.85 1.07 5.88  8.3 11.6\n## 5 4.60 2.32 4.05  7.5 13.4\n## 6 6.05 0.64 1.42 13.6 18.3\n数据一共5列，第1列是总胆固醇，第2列是甘油三酯，第3列是胰岛素，第4列是糖化血红蛋白，第5列是空腹血糖（因变量）。\n在建立回归方程前，先简单探索下数据：\nlibrary(GGally)\n\nggpairs(df15_1) + theme_bw()\n从这幅图来看，血糖和糖化血红蛋白相关性最大，和甘油三酯关系最小。\n接下来建立回归方程：\nf &lt;- lm(fpg ~ cho + tg + ri + hba, data = df15_1)\n\nsummary(f)\n## \n## Call:\n## lm(formula = fpg ~ cho + tg + ri + hba, data = df15_1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.6268 -1.2004 -0.2276  1.5389  4.4467 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept)   5.9433     2.8286   2.101   0.0473 *\n## cho           0.1424     0.3657   0.390   0.7006  \n## tg            0.3515     0.2042   1.721   0.0993 .\n## ri           -0.2706     0.1214  -2.229   0.0363 *\n## hba           0.6382     0.2433   2.623   0.0155 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.01 on 22 degrees of freedom\n## Multiple R-squared:  0.6008, Adjusted R-squared:  0.5282 \n## F-statistic: 8.278 on 4 and 22 DF,  p-value: 0.0003121\n这个结果信息很丰富，给出了截距，各自变量的系数以及标准误、t值、P值，最下方给出了决定系数R2，调整后的R2，F值，总体方程的P值等。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "1017-multireg.html#模型评价",
    "href": "1017-multireg.html#模型评价",
    "title": "20  多元线性回归",
    "section": "20.2 模型评价",
    "text": "20.2 模型评价\n回归模型可以通过R2、AIC、BIC、RMSE等评价，R2范围在0~1之间，越接近1说明结果越好。AIC、BIC、RMSE是越小越好。\n\nlibrary(performance)\nr2(f)\n## # R2 for Linear Regression\n##        R2: 0.601\n##   adj. R2: 0.528\nAIC(f)\n## [1] 120.78\nBIC(f)\n## [1] 128.5551\nrmse(f)\n## [1] 1.81395\n\n或者直接输出所有结果：\n\nmodel_performance(f)\n## # Indices of model performance\n## \n## AIC   |  AICc |   BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n## ---------------------------------------------------------\n## 120.8 | 125.0 | 128.6 | 0.601 |     0.528 | 1.814 | 2.010",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "1017-multireg.html#回归诊断",
    "href": "1017-multireg.html#回归诊断",
    "title": "20  多元线性回归",
    "section": "20.3 回归诊断",
    "text": "20.3 回归诊断\n判断数据是否满足多元线性回归的条件，也就是4个条件：\n\n正态性\n独立性\n等方差性\n线性\n\n\n20.3.1 看图判断\n\nopar &lt;- par(mfrow = c(2,2))\nplot(f)\n\n\n\n\n\n\n\npar(opar)\n\n\n第1幅图（左上）是残差拟合图，展示真实残差和拟合残差的关系，判读是否满足线性这个条件，如果满足，则应该为一条直线，但是本图明显是一条曲线，说明不是很满足线性这个条件，可能需要加二次项。\n第2幅图（右上）是正态Q-Q图，判断是否满足正态性这个条件，通过这个图来看，基本满足。\n第3幅图（左下）是位置尺度图，判读是否满足同方差性，如果满足，水平线两侧的点应该随机分布，从此图来看基本满足。\n第4幅图（右下）是残差杠杆图，用于识别离群点等。\n\n上面是比较原始的方法，下面介绍一个非常现代化的R包，用于实现以上图形：\n\nlibrary(performance)\ncheck_model(f)\n\n\n\n\n\n\n\n\n是不是更加好看了呢？\n这几个图也可以单独画出来，使用以下代码即可：\n\ndiagnostic_plots &lt;- plot(check_model(f, panel = FALSE))\n\n首先看第一个图。这个图是基于check_predictions()函数的，属于事后检验，是检查真实数据和模型数据的拟合情况的。下图中绿色粗线是真实的预测变量的分布情况，蓝色线条表示模拟的分布，理想的情况应该是完全重合的。从下图来看，其实是有些问题的，这说明我们用的模型可能不太合适。\n\ndiagnostic_plots[[1]]\n\n\n\n\n\n\n\n\n下面看第2张图。这张图是检查预测变量和结果变量是否符合线性关系的。合理的情况是残差完全随机地分布在参考线两侧。从这张图来看我们的数据其实不太完美。\n\ndiagnostic_plots[[2]]\n\n\n\n\n\n\n\n\n下面是第3幅图，是用来检查方差齐性的，同上面介绍过的位置尺度图。\n\ndiagnostic_plots[[3]]\n\n\n\n\n\n\n\n\n第4幅图是用来观察强影响点或者离群值、异常值的。使用的是库克距离（cook’s-distance）来计算的，图中在虚线（库克距离）外的点可被认为是异常值。\n\ndiagnostic_plots[[4]]\n\n\n\n\n\n\n\n\n第5幅图是关于多重共线性的。是通过方差膨胀因子来评价的，下图中展示了4个变量的VIF，基本都在3以下，可认为不存在多重共线性：\n\ndiagnostic_plots[[5]]\n\n\n\n\n\n\n\n\n第6幅图是看正态性的。理想情况下数据点应该均匀的分布在横线上，最好是和横线重合，尤其是尾部，我们这个数据还算可以。\n\ndiagnostic_plots[[6]]\n\n\n\n\n\n\n\n\n\n\n20.3.2 统计方法验证\n也可以通过统计方法判断，比如gvlma包可以实现对线性模型的综合判断：\n\nlibrary(gvlma)\ngvmodel&lt;-gvlma(f)\nsummary(gvmodel)\n## \n## Call:\n## lm(formula = fpg ~ cho + tg + ri + hba, data = df15_1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.6268 -1.2004 -0.2276  1.5389  4.4467 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept)   5.9433     2.8286   2.101   0.0473 *\n## cho           0.1424     0.3657   0.390   0.7006  \n## tg            0.3515     0.2042   1.721   0.0993 .\n## ri           -0.2706     0.1214  -2.229   0.0363 *\n## hba           0.6382     0.2433   2.623   0.0155 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.01 on 22 degrees of freedom\n## Multiple R-squared:  0.6008, Adjusted R-squared:  0.5282 \n## F-statistic: 8.278 on 4 and 22 DF,  p-value: 0.0003121\n## \n## \n## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\n## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\n## Level of Significance =  0.05 \n## \n## Call:\n##  gvlma(x = f) \n## \n##                      Value  p-value                   Decision\n## Global Stat        9.68910 0.046003 Assumptions NOT satisfied!\n## Skewness           0.65344 0.418886    Assumptions acceptable.\n## Kurtosis           0.04015 0.841193    Assumptions acceptable.\n## Link Function      7.68064 0.005582 Assumptions NOT satisfied!\n## Heteroscedasticity 1.31487 0.251515    Assumptions acceptable.\n\n\n全局统计量：粗略估计结果变量和预测变量是否符合线性关系，结果是不符合\n偏度和峰度：检验残差分布，结果是符合\n连接函数：检测结果变量是连续型还是二分类，结果是不符合\n异方差：检验方差齐性，结果是符合\n\n以上是多个条件一起输出判断，也可以针对单独的条件进行判断。\n首先看下正态性的判断。\n\nlibrary(car)\n\n# 验证正态性\nqqPlot(f,labels = row.names(df15_1), id.method = \"identify\",simulate = T,\n       main = \"Q-Q plot\")   \n\n\n\n\n\n\n\n## [1] 13 26\n\n从图中可看出正态性基本满足。\n当然也可以使用非常好用的performance包实现：\n\ncheck_normality(f)\n## OK: residuals appear as normally distributed (p = 0.671).\n\n检测离群值，基于cook距离：\n\ncheck_outliers(f)\n## 1 outlier detected: case 25.\n## - Based on the following method and threshold: cook (0.9).\n## - For variable: (Whole model).\n\n检测残差（或者因变量）独立性：\n\nset.seed(123)\ncheck_autocorrelation(f)\n## OK: Residuals appear to be independent and not autocorrelated (p = 0.296).\n\n或者通过car包：\n\nset.seed(123)\n# 验证因变量独立性\ndurbinWatsonTest(f)   \n##  lag Autocorrelation D-W Statistic p-value\n##    1       0.1778885      1.634654   0.296\n##  Alternative hypothesis: rho != 0\n\nP值大于0.05，满足条件。\n\n# 验证线性\ncrPlots(f)\n\n\n\n\n\n\n\n\n通过观察成分残差图，线性基本满足。\n下面是检测方差齐性：\n\n# 验证方差齐性\nncvTest(f)   \n## Non-constant Variance Score Test \n## Variance formula: ~ fitted.values \n## Chisquare = 0.0004274839, Df = 1, p = 0.9835\n\nP值大于0.05，方差齐性满足。\n方差齐性检验也可以通过performance包实现：\n\n# performance检测方差齐性\ncheck_heteroscedasticity(f)\n## OK: Error variance appears to be homoscedastic (p = 0.984).\n\n\n\n20.3.3 多重共线性的检验\n下面是多重共线性的检验，通过计算方差膨胀因子检验。\n\nvif(f)\n##      cho       tg       ri      hba \n## 2.185539 1.779862 1.278364 1.266730\nvif(f)&gt;4\n##   cho    tg    ri   hba \n## FALSE FALSE FALSE FALSE\n\n都小于4（标准有争议），基本不存在多重共线性。\n或者通过performance实现：\n\ncheck_collinearity(f)\n## # Check for Multicollinearity\n## \n## Low Correlation\n## \n##  Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n##   cho 2.19 [1.54, 3.62]         1.48      0.46     [0.28, 0.65]\n##    tg 1.78 [1.31, 2.95]         1.33      0.56     [0.34, 0.76]\n##    ri 1.28 [1.06, 2.32]         1.13      0.78     [0.43, 0.94]\n##   hba 1.27 [1.05, 2.32]         1.13      0.79     [0.43, 0.95]\n\n下面是两种自变量选择的方法，介绍的较为简单，更加详细的介绍请参考R语言实战临床预测模型中的变量选择合集，其中详细介绍了多种方法，如：\n\n逐步回归法\n最优子集法\n先单后多法\n……",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "1017-multireg.html#逐步选择法",
    "href": "1017-multireg.html#逐步选择法",
    "title": "20  多元线性回归",
    "section": "20.4 逐步选择法",
    "text": "20.4 逐步选择法\n向后回归：\n\nlibrary(MASS)\nstepAIC(f, direction = \"backward\")\n## Start:  AIC=42.16\n## fpg ~ cho + tg + ri + hba\n## \n##        Df Sum of Sq     RSS    AIC\n## - cho   1    0.6129  89.454 40.343\n## &lt;none&gt;               88.841 42.157\n## - tg    1   11.9627 100.804 43.568\n## - ri    1   20.0635 108.905 45.655\n## - hba   1   27.7939 116.635 47.507\n## \n## Step:  AIC=40.34\n## fpg ~ tg + ri + hba\n## \n##        Df Sum of Sq     RSS    AIC\n## &lt;none&gt;               89.454 40.343\n## - ri    1    25.690 115.144 45.159\n## - tg    1    26.530 115.984 45.356\n## - hba   1    32.269 121.723 46.660\n## \n## Call:\n## lm(formula = fpg ~ tg + ri + hba, data = df15_1)\n## \n## Coefficients:\n## (Intercept)           tg           ri          hba  \n##      6.4996       0.4023      -0.2870       0.6632\n\n向前回归：\n\nstepAIC(f, direction = \"forward\")\n## Start:  AIC=42.16\n## fpg ~ cho + tg + ri + hba\n## \n## Call:\n## lm(formula = fpg ~ cho + tg + ri + hba, data = df15_1)\n## \n## Coefficients:\n## (Intercept)          cho           tg           ri          hba  \n##      5.9433       0.1424       0.3515      -0.2706       0.6382\n\n逐步回归：\n\nstepAIC(f, direction = \"both\")\n## Start:  AIC=42.16\n## fpg ~ cho + tg + ri + hba\n## \n##        Df Sum of Sq     RSS    AIC\n## - cho   1    0.6129  89.454 40.343\n## &lt;none&gt;               88.841 42.157\n## - tg    1   11.9627 100.804 43.568\n## - ri    1   20.0635 108.905 45.655\n## - hba   1   27.7939 116.635 47.507\n## \n## Step:  AIC=40.34\n## fpg ~ tg + ri + hba\n## \n##        Df Sum of Sq     RSS    AIC\n## &lt;none&gt;               89.454 40.343\n## + cho   1     0.613  88.841 42.157\n## - ri    1    25.690 115.144 45.159\n## - tg    1    26.530 115.984 45.356\n## - hba   1    32.269 121.723 46.660\n## \n## Call:\n## lm(formula = fpg ~ tg + ri + hba, data = df15_1)\n## \n## Coefficients:\n## (Intercept)           tg           ri          hba  \n##      6.4996       0.4023      -0.2870       0.6632",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "1017-multireg.html#全局择优法",
    "href": "1017-multireg.html#全局择优法",
    "title": "20  多元线性回归",
    "section": "20.5 全局择优法",
    "text": "20.5 全局择优法\n也就是全子集回归法。\n\nlibrary(leaps)\nleaps &lt;- regsubsets(fpg ~ cho + tg + ri + hba, data = df15_1, nbest=4)\n\nplot(leaps, scale = \"Cp\") # 通过Cp判断\n\n\n\n\n\n\n\n\nCp是越小越好的，从上面这幅图来看，纳入3个自变量（tg/ri/hba）时最好。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "1018-logistic.html",
    "href": "1018-logistic.html",
    "title": "21  Logistic回归",
    "section": "",
    "text": "21.1 二项逻辑回归\nR语言中的factor()函数可以把变量变为因子类型，默认是没有等级之分的（可以理解为无序分类变量nominal）！当然也可以通过添加参数ordered=T变成有序因子（等级资料，有序分类ordinal）。\n因变量是二分类变量时，可以使用二项逻辑回归（binomial logistic regression），自变量可以是数值变量、无序多分类变量、有序多分类变量。\n本次数据使用孙振球版《医学统计学》第4版例16-2的数据，直接读取。\n为了探讨冠心病发生的危险因素，对26例冠心病患者和28例对照者进行病例-对照研究，试用逻辑回归筛选危险因素。\ndf16_2 &lt;- foreign::read.spss(\"datasets/例16-02.sav\", \n                             to.data.frame = T,\n                             use.value.labels = F,\n                             reencode  = \"utf-8\")\n\nstr(df16_2)\n## 'data.frame':    54 obs. of  11 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : num  3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : num  1 0 1 0 0 0 0 0 0 0 ...\n##  $ x3   : num  0 1 0 0 0 1 1 1 0 0 ...\n##  $ x4   : num  1 1 1 1 1 1 0 1 0 1 ...\n##  $ x5   : num  0 0 0 0 0 0 0 1 0 0 ...\n##  $ x6   : num  0 0 0 0 1 0 0 0 0 0 ...\n##  $ x7   : num  1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : num  1 0 0 0 1 1 0 0 1 0 ...\n##  $ y    : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n数据一共11列，第1列是编号，第2-9列是自变量，第10列是因变量。\n具体说明：\n这里的x1~y虽然是数值型，但并不是真的代表数字大小，只是为了方便标识，进行了转换，因此在进行logistic回归之前，我们要把数值型变量变成无序分类或有序分类变量，在R语言中可以通过factor()函数变成因子型实现。\n# 变成因子型\ndf16_2[,c(2:10)] &lt;- lapply(df16_2[,c(2:10)], factor)\nstr(df16_2)\n## 'data.frame':    54 obs. of  11 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 1 1 1 ...\n##  $ x3   : Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 2 2 2 1 1 ...\n##  $ x4   : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 1 2 1 2 ...\n##  $ x5   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n##  $ x6   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 1 1 1 ...\n##  $ x7   : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 2 1 1 2 1 ...\n##  $ y    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n需要注意的是自变量x1和x7，这两个应该是有序分类变量，这种自变量在进行逻辑回归时，可以进行哑变量设置，即给定一个参考，让其他所有组都和参考相比，比如这里，我们把x1变成因子型后，R语言在进行logistic回归时，会默认选择第一个为参考。\n接下来进行二项逻辑回归，在R语言中，默认是以因子的第一个为参考的，不仅是自变量，因变量也是如此！ 和SPSS的默认方式不太一样。\nf &lt;- glm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, \n         data = df16_2, \n         family = binomial())\n\nsummary(f)\n## \n## Call:\n## glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, family = binomial(), \n##     data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -5.46026    2.07370  -2.633  0.00846 **\n## x12          0.85285    1.54399   0.552  0.58070   \n## x13          0.47754    1.59320   0.300  0.76438   \n## x14          3.44227    2.10985   1.632  0.10278   \n## x21          1.14905    0.93176   1.233  0.21750   \n## x31          1.66039    1.16857   1.421  0.15535   \n## x41          0.85994    1.32437   0.649  0.51613   \n## x51          0.73600    0.97088   0.758  0.44840   \n## x61          3.92067    1.57004   2.497  0.01252 * \n## x72         -0.03467    1.13363  -0.031  0.97560   \n## x73         -0.38230    1.61710  -0.236  0.81311   \n## x81          2.46322    1.10484   2.229  0.02578 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 40.028  on 42  degrees of freedom\n## AIC: 64.028\n## \n## Number of Fisher Scoring iterations: 6\n结果详解：\nDeviance Residuals:表示偏差残差统计量。在理想情况下应服从正态分布，均值应为0。\n在此例中，中位数的符号为负（-0.01406），表明整体向左偏移，中位数的大小表明偏移的程度。第一个四分位数（1Q）和第三个四分位数（3Q）为两侧“尾巴”分布的幅度。这里3Q大于1Q（绝对值），表明这个曲线是向右倾斜的。最大和最小残差可用来检验数据中的离群值。\n结果中Estimate是回归系数和截距，Std. Error表示回归系数的标准误，z value是统计量值（z的平方就是Wald值），Pr(&gt;|z|)是P值。\nβ值（这里就是Estimate）是指回归系数和截距（常数项），可以是负数（负相关时回归系数出现负值）；\nOR是比值比（odds ratio），OR = exp(β)，其取值范围是0至正无穷，不可能是负数；\nWald是一个卡方值，等于β除以它的标准误（这里是Std. Error），然后取平方（也就是z值的平方），因此也不可能是负数。Wald用于对β值进行检验，考察β值是否等于0。若β值等于0，其对应的OR值，也就是Exp(β)为1，表明两组没有显著差异。Wald值越大，β值越不可能等于0。\n结果中出现了x12/x13/x14这种，这是因为R语言在做回归时，如果设置了哑变量，默认是以第一个为参考的，其余都是和第一个进行比较，这也是R中自动进行哑变量编码的方式。\nNull deviance:无效偏差（零偏差），Residual deviance:残差偏差，无效偏差和残差偏差之间的差异越大越好，用来评价模型与实际数据的吻合情况。\nAIC：赤池信息准则，表示模型拟合程度的好坏，AIC越低表示模型拟合越好。\n最后还有一个Fisher Scoring的迭代次数，这个和最大似然函数的计算有关。\n我们可以通过函数的方式分别获取模型信息。\n# β值\ncoefficients(f)\n## (Intercept)         x12         x13         x14         x21         x31 \n## -5.46025547  0.85285212  0.47754497  3.44227124  1.14905003  1.66039360 \n##         x41         x51         x61         x72         x73         x81 \n##  0.85994185  0.73600239  3.92067487 -0.03467497 -0.38230011  2.46321944\n\n# β值\ncoef(f)\n## (Intercept)         x12         x13         x14         x21         x31 \n## -5.46025547  0.85285212  0.47754497  3.44227124  1.14905003  1.66039360 \n##         x41         x51         x61         x72         x73         x81 \n##  0.85994185  0.73600239  3.92067487 -0.03467497 -0.38230011  2.46321944\n\n# β值的95%可信区间\nconfint(f)\n##                   2.5 %    97.5 %\n## (Intercept) -10.3696980 -1.983104\n## x12          -2.0317236  4.405067\n## x13          -2.5429244  4.085370\n## x14          -0.2319302  8.343123\n## x21          -0.6458070  3.099838\n## x31          -0.5431686  4.205175\n## x41          -1.6713365  3.801261\n## x51          -1.1846658  2.725051\n## x61           1.3290533  7.677657\n## x72          -2.3618580  2.224863\n## x73          -3.8303437  2.725470\n## x81           0.5105394  4.997352\n\n# OR值\nexp(coef(f))\n##  (Intercept)          x12          x13          x14          x21          x31 \n##  0.004252469  2.346329320  1.612111759 31.257871683  3.155194147  5.261381340 \n##          x41          x51          x61          x72          x73          x81 \n##  2.363023282  2.087573511 50.434470096  0.965919321  0.682290259 11.742555242\n\n# OR值的95%的可信区间\nexp(confint(f))\n##                    2.5 %       97.5 %\n## (Intercept) 3.136876e-05    0.1376413\n## x12         1.311093e-01   81.8646261\n## x13         7.863610e-02   59.4639513\n## x14         7.930015e-01 4201.1887167\n## x21         5.242393e-01   22.1943589\n## x31         5.809047e-01   67.0323349\n## x41         1.879956e-01   44.7576059\n## x51         3.058484e-01   15.2571993\n## x61         3.777465e+00 2159.5535363\n## x72         9.424495e-02    9.2522177\n## x73         2.170216e-02   15.2635868\n## x81         1.666190e+00  148.0206875\n这里x12的OR值是2.346329320，代表：45~55岁的人群患冠心病的风险是小于45岁人群的2.346329320倍，但是这个结果并没有统计学意义！\n# Wald值\nsummary(f)$coefficients[,3]^2\n## (Intercept)         x12         x13         x14         x21         x31 \n## 6.933188870 0.305111544 0.089843733 2.661883233 1.520790277 2.018903576 \n##         x41         x51         x61         x72         x73         x81 \n## 0.421615676 0.574682148 6.235929079 0.000935592 0.055890396 4.970577395\n\n# P值\nsummary(f)$coefficients[,4]\n## (Intercept)         x12         x13         x14         x21         x31 \n##  0.00846107  0.58069555  0.76437591  0.10277898  0.21749994  0.15535128 \n##         x41         x51         x61         x72         x73         x81 \n##  0.51613195  0.44840433  0.01251839  0.97559855  0.81311338  0.02578204\n\n# 预测值\nfitted(f) # 或者 predict(f,type = \"response\")\n##           1           2           3           4           5           6 \n## 0.375076515 0.110360122 0.069240725 0.023034427 0.905605901 0.491543165 \n##           7           8           9          10          11          12 \n## 0.049878030 0.151052146 0.104875967 0.009948712 0.208062753 0.046013662 \n##          13          14          15          16          17          18 \n## 0.009879122 0.497751927 0.500211100 0.074509703 0.023034427 0.105543397 \n##          19          20          21          22          23          24 \n## 0.359548891 0.441102099 0.048627400 0.734770361 0.366272916 0.009879122 \n##          25          26          27          28          29          30 \n## 0.049878030 0.366272916 0.009879122 0.101665098 0.995553588 0.950848767 \n##          31          32          33          34          35          36 \n## 0.712839656 0.995611072 0.216828996 0.984826081 0.543195397 0.905612594 \n##          37          38          39          40          41          42 \n## 0.868286980 0.993760333 0.868286980 0.034813473 0.902606657 0.966930037 \n##          43          44          45          46          47          48 \n## 0.375076515 0.964725296 0.840087511 0.818110300 0.881331876 0.676305952 \n##          49          50          51          52          53          54 \n## 0.780828686 0.555921773 0.986103872 0.816157300 0.466253375 0.655579178\n\n# 偏差\ndeviance(f)\n## [1] 40.02758\n\n# 残差自由度\ndf.residual(f)\n## [1] 42\n\n# 伪R^2\nDescTools::PseudoR2(f, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n##   McFadden   CoxSnell Nagelkerke \n##  0.4647704  0.4746397  0.6331426\n这里需要说明以下fitted(f)，也就是predict(f,type = \"response\")得到的结果是预测概率，范围是0-1之间的。\n对于logistic回归来说，如果不使用type函数，默认是type = \"link\"，返回的是logit(P)的值。\n# 默认返回logit(P)的值\npredict(f)\n\n# 返回概率\npredict(f, type = \"response\")\n模型整体的假设检验：\n# 先构建一个只有截距的模型\nf0 &lt;- glm(y ~ 1, data = df16_2, family = binomial())\n\n# 原模型和这个空模型进行比较\nanova(f0,f,test=\"Chisq\")\n## Analysis of Deviance Table\n## \n## Model 1: y ~ 1\n## Model 2: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\n##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n## 1        53     74.786                          \n## 2        42     40.028 11   34.758 0.0002716 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nP&lt;0.001，说明我们的模型是有意义的。\n逐步回归法的logistic回归，可以使用step()函数：\n# 向前\nf1 &lt;- step(f, direction = \"forward\")\n## Start:  AIC=64.03\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\nsummary(f1)\n## \n## Call:\n## glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, family = binomial(), \n##     data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -5.46026    2.07370  -2.633  0.00846 **\n## x12          0.85285    1.54399   0.552  0.58070   \n## x13          0.47754    1.59320   0.300  0.76438   \n## x14          3.44227    2.10985   1.632  0.10278   \n## x21          1.14905    0.93176   1.233  0.21750   \n## x31          1.66039    1.16857   1.421  0.15535   \n## x41          0.85994    1.32437   0.649  0.51613   \n## x51          0.73600    0.97088   0.758  0.44840   \n## x61          3.92067    1.57004   2.497  0.01252 * \n## x72         -0.03467    1.13363  -0.031  0.97560   \n## x73         -0.38230    1.61710  -0.236  0.81311   \n## x81          2.46322    1.10484   2.229  0.02578 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 40.028  on 42  degrees of freedom\n## AIC: 64.028\n## \n## Number of Fisher Scoring iterations: 6\n\n# 向后\nf2 &lt;- step(f, direction = \"backward\")\n## Start:  AIC=64.03\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\n## \n##        Df Deviance    AIC\n## - x7    2   40.086 60.086\n## - x1    3   43.933 61.933\n## - x4    1   40.466 62.466\n## - x5    1   40.605 62.605\n## - x2    1   41.600 63.600\n## &lt;none&gt;      40.028 64.028\n## - x3    1   42.196 64.196\n## - x8    1   46.365 68.365\n## - x6    1   50.469 72.469\n## \n## Step:  AIC=60.09\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x1    3   44.541 58.541\n## - x5    1   40.611 58.611\n## - x4    1   40.814 58.814\n## - x2    1   41.616 59.616\n## &lt;none&gt;      40.086 60.086\n## - x3    1   42.747 60.747\n## - x8    1   47.255 65.255\n## - x6    1   51.415 69.415\n## \n## Step:  AIC=58.54\n## y ~ x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x5    1   45.746 57.746\n## - x4    1   45.779 57.779\n## - x3    1   45.853 57.853\n## &lt;none&gt;      44.541 58.541\n## - x2    1   46.763 58.763\n## - x8    1   50.136 62.136\n## - x6    1   54.588 66.588\n## \n## Step:  AIC=57.75\n## y ~ x2 + x3 + x4 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x4    1   47.537 57.537\n## &lt;none&gt;      45.746 57.746\n## - x2    1   48.470 58.470\n## - x3    1   49.083 59.083\n## - x8    1   51.976 61.976\n## - x6    1   56.634 66.634\n## \n## Step:  AIC=57.54\n## y ~ x2 + x3 + x6 + x8\n## \n##        Df Deviance    AIC\n## &lt;none&gt;      47.537 57.537\n## - x3    1   50.276 58.276\n## - x2    1   51.418 59.418\n## - x8    1   53.869 61.869\n## - x6    1   59.649 67.649\nsummary(f2)\n## \n## Call:\n## glm(formula = y ~ x2 + x3 + x6 + x8, family = binomial(), data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -3.0314     0.8965  -3.381 0.000722 ***\n## x21           1.4715     0.7656   1.922 0.054617 .  \n## x31           1.2251     0.7543   1.624 0.104359    \n## x61           3.6124     1.3391   2.698 0.006985 ** \n## x81           1.8639     0.8045   2.317 0.020505 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 47.537  on 49  degrees of freedom\n## AIC: 57.537\n## \n## Number of Fisher Scoring iterations: 5\n\n# 步进法\nf3 &lt;- step(f, direction = \"both\")\n## Start:  AIC=64.03\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\n## \n##        Df Deviance    AIC\n## - x7    2   40.086 60.086\n## - x1    3   43.933 61.933\n## - x4    1   40.466 62.466\n## - x5    1   40.605 62.605\n## - x2    1   41.600 63.600\n## &lt;none&gt;      40.028 64.028\n## - x3    1   42.196 64.196\n## - x8    1   46.365 68.365\n## - x6    1   50.469 72.469\n## \n## Step:  AIC=60.09\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x1    3   44.541 58.541\n## - x5    1   40.611 58.611\n## - x4    1   40.814 58.814\n## - x2    1   41.616 59.616\n## &lt;none&gt;      40.086 60.086\n## - x3    1   42.747 60.747\n## + x7    2   40.028 64.028\n## - x8    1   47.255 65.255\n## - x6    1   51.415 69.415\n## \n## Step:  AIC=58.54\n## y ~ x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x5    1   45.746 57.746\n## - x4    1   45.779 57.779\n## - x3    1   45.853 57.853\n## &lt;none&gt;      44.541 58.541\n## - x2    1   46.763 58.763\n## + x1    3   40.086 60.086\n## + x7    2   43.933 61.933\n## - x8    1   50.136 62.136\n## - x6    1   54.588 66.588\n## \n## Step:  AIC=57.75\n## y ~ x2 + x3 + x4 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x4    1   47.537 57.537\n## &lt;none&gt;      45.746 57.746\n## - x2    1   48.470 58.470\n## + x5    1   44.541 58.541\n## + x1    3   40.611 58.611\n## - x3    1   49.083 59.083\n## + x7    2   44.697 60.697\n## - x8    1   51.976 61.976\n## - x6    1   56.634 66.634\n## \n## Step:  AIC=57.54\n## y ~ x2 + x3 + x6 + x8\n## \n##        Df Deviance    AIC\n## &lt;none&gt;      47.537 57.537\n## + x1    3   41.625 57.625\n## + x4    1   45.746 57.746\n## + x5    1   45.779 57.779\n## - x3    1   50.276 58.276\n## - x2    1   51.418 59.418\n## + x7    2   46.792 60.792\n## - x8    1   53.869 61.869\n## - x6    1   59.649 67.649\nsummary(f3)\n## \n## Call:\n## glm(formula = y ~ x2 + x3 + x6 + x8, family = binomial(), data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -3.0314     0.8965  -3.381 0.000722 ***\n## x21           1.4715     0.7656   1.922 0.054617 .  \n## x31           1.2251     0.7543   1.624 0.104359    \n## x61           3.6124     1.3391   2.698 0.006985 ** \n## x81           1.8639     0.8045   2.317 0.020505 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 47.537  on 49  degrees of freedom\n## AIC: 57.537\n## \n## Number of Fisher Scoring iterations: 5\n按照步进法最终纳入的自变量是x2,x6,x8。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Logistic回归</span>"
    ]
  },
  {
    "objectID": "1018-logistic.html#二项逻辑回归",
    "href": "1018-logistic.html#二项逻辑回归",
    "title": "21  Logistic回归",
    "section": "",
    "text": "x1：年龄，小于45岁是1,45-55是2,55-65是3,65以上是4；\nx2：高血压病史，1代表有，0代表无；\nx3：高血压家族史，1代表有，0代表无；\nx4：吸烟，1代表吸烟，0代表不吸烟；\nx5：高血脂病史，1代表有，0代表无；\nx6：动物脂肪摄入，0表示低，1表示高\nx7：BMI，小于24是1,24-26是2，大于26是3；\nx8：A型性格，1代表是，0代表否；\ny：是否是冠心病，1代表是，0代表否",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Logistic回归</span>"
    ]
  },
  {
    "objectID": "1018-logistic.html#多项逻辑回归",
    "href": "1018-logistic.html#多项逻辑回归",
    "title": "21  Logistic回归",
    "section": "21.2 多项逻辑回归",
    "text": "21.2 多项逻辑回归\n因变量是无序多分类资料（＞2）时，可使用多分类逻辑回归（multinomial logistic regression）。\n使用课本例16-5的数据，课本电子版及数据已上传到QQ群，自行下载即可。\n某研究人员欲了解不同社区和性别之间居民获取健康知识的途径是否相同，对2个社区的314名成人进行了调查，其中X1是社区，社区1用0表示，社区2用1表示；X2是性别，0是男，1是女，Y是获取健康知识途径，1是传统大众传媒，2是网络，3是社区宣传。\n\ndf &lt;- read.csv(\"datasets/例16-05.csv\",header = T)\n\npsych::headtail(df)\n## Warning in psych::headtail(df): headtail is deprecated.  Please use the\n## headTail function\n##      X1  X2   Y\n## 1     0   0   1\n## 2     0   0   1\n## 3     0   0   1\n## 4     0   0   1\n## ... ... ... ...\n## 311   1   1   3\n## 312   1   1   3\n## 313   1   1   3\n## 314   1   1   3\n\n首先变为因子型，无需多分类的logistic回归需要对因变量设置参考，我们这里直接用factor()函数变为因子，这样在进行无序多分类的logistic时默认是以第一个为参考。也可以使用relevel()重新设置参考。\n\ndf$X1 &lt;- factor(df$X1,levels = c(0,1),labels = c(\"社区1\",\"社区2\"))\ndf$X2 &lt;- factor(df$X2,levels = c(0,1),labels = c(\"男\",\"女\"))\n\n# 因变量设置参考，这里选择第1个（传统大众传媒）为参考\ndf$Y &lt;- factor(df$Y,levels = c(1,2,3),labels = c(\"传统大众传媒\",\"网络\",\"社区宣传\"))\n\nstr(df)\n## 'data.frame':    314 obs. of  3 variables:\n##  $ X1: Factor w/ 2 levels \"社区1\",\"社区2\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ X2: Factor w/ 2 levels \"男\",\"女\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Y : Factor w/ 3 levels \"传统大众传媒\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n使用nnet::multinom进行无序多分类的logistic回归：\n\nlibrary(nnet)\n\nfit &lt;- multinom(Y ~ X1 + X2, data = df, model = T)\n## # weights:  12 (6 variable)\n## initial  value 344.964259 \n## iter  10 value 316.575399\n## iter  10 value 316.575399\n## iter  10 value 316.575399\n## final  value 316.575399 \n## converged\n\nsummary(fit)\n## Call:\n## multinom(formula = Y ~ X1 + X2, data = df, model = T)\n## \n## Coefficients:\n##          (Intercept)    X1社区2      X2女\n## 网络       0.5484998 -1.3743147 0.4321069\n## 社区宣传   0.3940422 -0.9933526 1.2266459\n## \n## Std. Errors:\n##          (Intercept)   X1社区2      X2女\n## 网络       0.2583299 0.3201514 0.3265384\n## 社区宣传   0.2574175 0.2952083 0.2991714\n## \n## Residual Deviance: 633.1508 \n## AIC: 645.1508\n\n可以看到结果比二项逻辑回归的结果简洁多了，少了很多信息，只给出了Coefficients/Std. Errors/Residual Deviance/AIC。\n不过也是两个模型的结果，分别是 社区宣传 和 传统大众传媒 比，网络 和 传统大众传媒 比。\n自变量的Z值（wald Z, Z-score）和P值需要手动计算:\n\nz_stats &lt;- summary(fit)$coefficients/summary(fit)$standard.errors\n\np_values &lt;- (1 - pnorm(abs(z_stats)))*2\n\np_values\n##          (Intercept)      X1社区2         X2女\n## 网络      0.03373263 1.765117e-05 1.857371e-01\n## 社区宣传  0.12583082 7.656564e-04 4.128929e-05\n\n但其实可以调包解决：\n\nres &lt;- broom::tidy(fit)\nres\n## # A tibble: 6 × 6\n##   y.level  term        estimate std.error statistic   p.value\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 网络     (Intercept)    0.548     0.258      2.12 0.0337   \n## 2 网络     X1社区2       -1.37      0.320     -4.29 0.0000177\n## 3 网络     X2女           0.432     0.327      1.32 0.186    \n## 4 社区宣传 (Intercept)    0.394     0.257      1.53 0.126    \n## 5 社区宣传 X1社区2       -0.993     0.295     -3.36 0.000766 \n## 6 社区宣传 X2女           1.23      0.299      4.10 0.0000413\n\nOR值及其95%的可信区间也没有给出来，需要手动计算OR值和可信区间：\n\n# 计算OR值\nOR &lt;- exp(coef(fit))\nOR\n##          (Intercept)   X1社区2     X2女\n## 网络        1.730655 0.2530129 1.540500\n## 社区宣传    1.482963 0.3703330 3.409774\n\n# 计算OR值的95%的可信区间\nOR.confi &lt;- exp(confint(fit))\nOR.confi\n## , , 网络\n## \n##                 2.5 %    97.5 %\n## (Intercept) 1.0430848 2.8714501\n## X1社区2     0.1350919 0.4738666\n## X2女        0.8122910 2.9215386\n## \n## , , 社区宣传\n## \n##                 2.5 %    97.5 %\n## (Intercept) 0.8953982 2.4560912\n## X1社区2     0.2076398 0.6605021\n## X2女        1.8970133 6.1288742\n\n模型整体的假设检验：\n\n# 先构建一个只有截距的模型\nfit0 &lt;- multinom(Y ~ 1, data = df, model = T)\n## # weights:  6 (2 variable)\n## initial  value 344.964259 \n## final  value 338.603448 \n## converged\n\n# 两个模型比较,Likelihood ratio tests\nanova(fit0, fit)\n## Likelihood ratio tests of Multinomial Models\n## \n## Response: Y\n##     Model Resid. df Resid. Dev   Test    Df LR stat.      Pr(Chi)\n## 1       1       626   677.2069                                   \n## 2 X1 + X2       622   633.1508 1 vs 2     4  44.0561 6.245931e-09\n\nP&lt;0.001，模型具有统计学意义。\n获取模型预测的类别：\n\npred &lt;- predict(fit, df, type = \"class\")\nhead(pred)\n## [1] 网络 网络 网络 网络 网络 网络\n## Levels: 传统大众传媒 网络 社区宣传\n\n获取模型预测的概率：\n\nprob &lt;- predict(fit, df, type = \"probs\") # 或者使用 fitted(fit)\nhead(prob)\n##   传统大众传媒      网络  社区宣传\n## 1    0.2373257 0.4107289 0.3519453\n## 2    0.2373257 0.4107289 0.3519453\n## 3    0.2373257 0.4107289 0.3519453\n## 4    0.2373257 0.4107289 0.3519453\n## 5    0.2373257 0.4107289 0.3519453\n## 6    0.2373257 0.4107289 0.3519453\n\n模型拟合优度的检验，这里使用卡方检验：\n\nchisq.test(df$Y, pred)\n## \n##  Pearson's Chi-squared test\n## \n## data:  df$Y and pred\n## X-squared = 39.521, df = 4, p-value = 5.436e-08\n\n计算伪R2：\n\nDescTools::PseudoR2(fit, which = \"all\")\n##        McFadden     McFaddenAdj        CoxSnell      Nagelkerke   AldrichNelson \n##      0.06505559      0.04733575      0.13090778      0.14803636              NA \n## VeallZimmermann           Efron McKelveyZavoina            Tjur             AIC \n##              NA              NA              NA              NA    645.15079819 \n##             BIC          logLik         logLik0              G2 \n##    667.64715610   -316.57539909   -338.60344772     44.05609725\n\n不仅给出了伪R^2，还给出了超多的值，每一项的意义可以参考下面这张图：\n\n\n\n\n\n\n\n\n\n结果解读可以参考二项逻辑回归。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Logistic回归</span>"
    ]
  },
  {
    "objectID": "1018-logistic.html#有序逻辑回归",
    "href": "1018-logistic.html#有序逻辑回归",
    "title": "21  Logistic回归",
    "section": "21.3 有序逻辑回归",
    "text": "21.3 有序逻辑回归\nordinal logistic regression适用于因变量为等级资料。使用课本例16-4的数据。\n随机选取84例患者做临床试验，探讨性别和治疗方法对该病的影响。变量赋值为：性别（X1，男=0，女=1），治疗方法（X2，传统疗法=0，新型疗法=1），疗效（Y，无效=1，有效=2，痊愈=3）。\n\ndf &lt;- read.csv(\"datasets/例16-04.csv\",header = T)\npsych::headtail(df)\n## Warning in psych::headtail(df): headtail is deprecated.  Please use the\n## headTail function\n##      X1  X2   Y\n## 1     0   0   1\n## 2     0   0   1\n## 3     0   0   1\n## 4     0   0   1\n## ... ... ... ...\n## 81    1   1   3\n## 82    1   1   3\n## 83    1   1   3\n## 84    1   1   3\n\n变为因子型：\n\n# 因变量变为有序因子\ndf$Y &lt;- factor(df$Y, levels = c(1,2,3),\n               labels = c(\"无效\",\"有效\",\"痊愈\"),\n               ordered = T)\n\n# 自变量变为无序因子\ndf$X1 &lt;- factor(df$X1,levels = c(0,1),labels = c(\"男\",\"女\"))\ndf$X2 &lt;- factor(df$X2,levels = c(0,1),labels = c(\"传统疗法\",\"新型疗法\"))\n\nstr(df)\n## 'data.frame':    84 obs. of  3 variables:\n##  $ X1: Factor w/ 2 levels \"男\",\"女\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ X2: Factor w/ 2 levels \"传统疗法\",\"新型疗法\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Y : Ord.factor w/ 3 levels \"无效\"&lt;\"有效\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n\n使用MASS::polr拟合有序逻辑回归：\n\nlibrary(MASS)\n\nfit &lt;- polr(Y ~ X1 + X2, data = df,Hess = TRUE,method = \"logistic\")\nsummary(fit)\n## Call:\n## polr(formula = Y ~ X1 + X2, data = df, Hess = TRUE, method = \"logistic\")\n## \n## Coefficients:\n##            Value Std. Error t value\n## X1女       1.319     0.5381   2.451\n## X2新型疗法 1.797     0.4718   3.809\n## \n## Intercepts:\n##           Value  Std. Error t value\n## 无效|有效 1.8128 0.5654     3.2061 \n## 有效|痊愈 2.6672 0.6065     4.3979 \n## \n## Residual Deviance: 150.0294 \n## AIC: 158.0294\n\n结果也是没有给出P值，手动计算P值：\n\np &lt;- pnorm(abs(coef(summary(fit))[, \"t value\"]),lower.tail = F)*2\np\n##         X1女   X2新型疗法    无效|有效    有效|痊愈 \n## 1.425572e-02 1.392807e-04 1.345300e-03 1.092866e-05\n\n这次broom::tidy(fit)并没有直接给出P值：\n\nbroom::tidy(fit)\n## # A tibble: 4 × 5\n##   term       estimate std.error statistic coef.type  \n##   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n## 1 X1女           1.32     0.538      2.45 coefficient\n## 2 X2新型疗法     1.80     0.472      3.81 coefficient\n## 3 无效|有效      1.81     0.565      3.21 scale      \n## 4 有效|痊愈      2.67     0.606      4.40 scale\n\nOR值：\n\nOR &lt;- exp(coef(fit))\nOR\n##       X1女 X2新型疗法 \n##   3.738765   6.033338\n\n平行线检验(Brant-Wald test)：\n\nbrant::brant(fit)\n## -------------------------------------------- \n## Test for X2  df  probability \n## -------------------------------------------- \n## Omnibus      1.83    2   0.4\n## X1女      1.59    1   0.21\n## X2新型疗法       0.01    1   0.94\n## -------------------------------------------- \n## \n## H0: Parallel Regression Assumption holds\n## Warning in brant::brant(fit): 1 combinations in table(dv,ivs) do not occur.\n## Because of that, the test results might be invalid.\n\nP值&gt;0.05，平行线检验通过，可以使用有序逻辑回归。\n模型整体的显著性检验：\n\n# 先构建一个只有截距的模型\nfit0 &lt;- polr(Y ~ 1, data = df,Hess = TRUE,method = \"logistic\")\n\n# 两个模型比较\nanova(fit0, fit)\n## Likelihood ratio tests of ordinal regression models\n## \n## Response: Y\n##     Model Resid. df Resid. Dev   Test    Df LR stat.     Pr(Chi)\n## 1       1        82   169.9159                                  \n## 2 X1 + X2        80   150.0294 1 vs 2     2  19.8865 4.80508e-05\n\nP值＜0.01，模型是有意义的。\n获取模型预测的类别：\n\npred &lt;- predict(fit, df, type = \"class\")\nhead(pred)\n## [1] 无效 无效 无效 无效 无效 无效\n## Levels: 无效 有效 痊愈\n\n获取模型预测的概率：\n\nprob &lt;- predict(fit, df, type = \"probs\") # 或者使用 fitted(fit)\nhead(prob)\n##        无效       有效       痊愈\n## 1 0.8597003 0.07536263 0.06493706\n## 2 0.8597003 0.07536263 0.06493706\n## 3 0.8597003 0.07536263 0.06493706\n## 4 0.8597003 0.07536263 0.06493706\n## 5 0.8597003 0.07536263 0.06493706\n## 6 0.8597003 0.07536263 0.06493706\n\n模型拟合优度的检验，这里使用卡方检验：\n\nchisq.test(df$Y, pred)\n## Warning in chisq.test(df$Y, pred): Chi-squared approximation may be incorrect\n## \n##  Pearson's Chi-squared test\n## \n## data:  df$Y and pred\n## X-squared = 14.246, df = 2, p-value = 0.0008065\n\n计算伪R2：\n\nDescTools::PseudoR2(fit, which = \"all\")\n##        McFadden        CoxSnell      Nagelkerke   AldrichNelson VeallZimmermann \n##       0.1170373       0.2108068       0.2429443              NA              NA \n##           Efron McKelveyZavoina            Tjur             AIC             BIC \n##              NA              NA              NA     158.0294131     167.7526803 \n##          logLik         logLik0              G2 \n##     -75.0147065     -84.9579583      19.8865036",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Logistic回归</span>"
    ]
  },
  {
    "objectID": "1018-logistic.html#条件逻辑回归",
    "href": "1018-logistic.html#条件逻辑回归",
    "title": "21  Logistic回归",
    "section": "21.4 条件逻辑回归",
    "text": "21.4 条件逻辑回归\nconditional logistic regression是针对配对数据资料分析的一种方法。在一些病例-对照研究中，把病例和对照按照年龄、性别等进行配对，形成多个匹配组，各匹配组的病例数和对照数是任意的，并不是1个对1个，常用的是每组中有一个病例和多个对照，即1：M配对研究。\n使用课本例16-3的数据。某北方城市研究喉癌发病的危险因素，用1:2配对研究，现选取了6个可能的危险因素并记录了25对数据，试做条件logistic回归。\n\n\n\n\n\n\n\n\n\n\ndf &lt;- foreign::read.spss(\"datasets/例16-03.sav\",to.data.frame = T)\npsych::headtail(df)\n## Warning in psych::headtail(df): headtail is deprecated.  Please use the\n## headTail function\n##       i   y  x1  x2  x3  x4  x5  x6\n## 1     1   1   3   5   1   1   1   0\n## 2     1   0   1   1   1   3   3   0\n## 3     1   0   1   1   1   3   3   0\n## 4     2   1   1   3   1   1   3   0\n## ... ... ... ... ... ... ... ... ...\n## 72   24   0   1   1   2   3   2   0\n## 73   25   1   1   4   1   1   1   1\n## 74   25   0   1   1   1   3   2   0\n## 75   25   0   1   1   1   3   3   0\nstr(df)\n## 'data.frame':    75 obs. of  8 variables:\n##  $ i : num  1 1 1 2 2 2 3 3 3 4 ...\n##  $ y : num  1 0 0 1 0 0 1 0 0 1 ...\n##  $ x1: num  3 1 1 1 1 1 1 1 1 1 ...\n##  $ x2: num  5 1 1 3 1 2 4 5 4 4 ...\n##  $ x3: num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ x4: num  1 3 3 1 3 3 3 3 3 2 ...\n##  $ x5: num  1 3 3 3 2 2 2 2 2 1 ...\n##  $ x6: num  0 0 0 0 0 0 0 0 0 1 ...\n\ni是配对的对子数。\n不需要变成因子型。\n使用survival::clogit进行条件逻辑回归：\n\nlibrary(survival)\n\nfit &lt;- clogit(y ~ x1+x2+x3+x4+x5+x6+strata(i), data = df, method = \"exact\")\n\nsummary(fit)\n## Call:\n## coxph(formula = Surv(rep(1, 75L), y) ~ x1 + x2 + x3 + x4 + x5 + \n##     x6 + strata(i), data = df, method = \"exact\")\n## \n##   n= 75, number of events= 25 \n## \n##        coef exp(coef) se(coef)      z Pr(&gt;|z|)  \n## x1  2.58880  13.31380  2.50172  1.035   0.3008  \n## x2  1.68796   5.40843  0.68545  2.463   0.0138 *\n## x3  2.31944  10.16995  1.26096  1.839   0.0659 .\n## x4 -3.88886   0.02047  1.90656 -2.040   0.0414 *\n## x5 -0.49102   0.61200  1.19020 -0.413   0.6799  \n## x6  3.50899  33.41447  2.13723  1.642   0.1006  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##    exp(coef) exp(-coef) lower .95 upper .95\n## x1  13.31380    0.07511 0.0988170 1793.7921\n## x2   5.40843    0.18490 1.4112830   20.7266\n## x3  10.16995    0.09833 0.8589963  120.4056\n## x4   0.02047   48.85506 0.0004878    0.8589\n## x5   0.61200    1.63399 0.0593818    6.3074\n## x6  33.41447    0.02993 0.5066653 2203.6771\n## \n## Concordance= 0.91  (se = 0.064 )\n## Likelihood ratio test= 42.21  on 6 df,   p=2e-07\n## Wald test            = 7.71  on 6 df,   p=0.3\n## Score (logrank) test = 29.13  on 6 df,   p=6e-05\n\n结果非常齐全，β值，OR值，P值等信息都有了。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Logistic回归</span>"
    ]
  },
  {
    "objectID": "1018-logistic.html#参考资料",
    "href": "1018-logistic.html#参考资料",
    "title": "21  Logistic回归",
    "section": "21.5 参考资料",
    "text": "21.5 参考资料\n\nhttps://blog.csdn.net/weixin_41744624/article/details/105506951\nhttps://zhuanlan.zhihu.com/p/113403422\nhttps://duanku.pai-hang-bang.cn/kuzi_1046977453210716059\nhttps://bookdown.org/chua/ber642_advanced_regression/\nhttps://peopleanalytics-regression-book.org/",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Logistic回归</span>"
    ]
  },
  {
    "objectID": "对数线性模型.html",
    "href": "对数线性模型.html",
    "title": "22  多维列联表的对数线性模型",
    "section": "",
    "text": "22.1 对数线性模型和卡方检验\n孙振球医学统计学第4版例17-1。研究产前护理量对婴儿死亡率的影响，收集了甲乙两家医院的资料，此资料由分类变量X（产前护理量）、Y（婴儿是否存活）、Z（接受护理地点）构成的2×2×2三维列联表，X为行变量、Y为列变量、Z为分层变量。资料见表17-1，试对此资料中3个变量的关系进行分析。\n对表17-1资料中的产前护理量与接受护理地点和婴儿存活情况进行卡方检验，如果不考虑接受护理地点的影响，对产前护理量和婴儿存活情况分别合并后得到（Χ2=5.255，P=0.022），即产前护理量与婴儿存活情况有关联：\n# 不考虑护理地点的影响，就是2*2列联表\nM &lt;- matrix(c(20,373,6,316),nrow = 2, byrow = T,\n            dimnames = list(`产前护理量`=c(\"少\",\"多\"),\n                            `存活情况`=c(\"死\",\"活\")))\nM\n##           存活情况\n## 产前护理量 死  活\n##         少 20 373\n##         多  6 316\n\n# 进行卡方检验\nchisq.test(M,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  M\n## X-squared = 5.2555, df = 1, p-value = 0.02188\n如果考虑接受护理地点的影响，分别对两个诊所进行假设检验时却发现产前护理量与婴儿存活情况是没有关联的，即基于接受护理地点的条件下产前护理量与婴儿存活情况没有关联：\n# 诊所甲\nM1 &lt;- matrix(c(3,176,4,293),nrow = 2, byrow = T,\n            dimnames = list(`产前护理量`=c(\"少\",\"多\"),\n                            `存活情况`=c(\"死\",\"活\")))\nM1\n##           存活情况\n## 产前护理量 死  活\n##         少  3 176\n##         多  4 293\n\n# 进行卡方检验\nchisq.test(M1,correct = F)\n## Warning in chisq.test(M1, correct = F): Chi-squared approximation may be\n## incorrect\n## \n##  Pearson's Chi-squared test\n## \n## data:  M1\n## X-squared = 0.083522, df = 1, p-value = 0.7726\n# 诊所乙\nM2 &lt;- matrix(c(17,197,2,23),nrow = 2, byrow = T,\n            dimnames = list(`产前护理量`=c(\"少\",\"多\"),\n                            `存活情况`=c(\"死\",\"活\")))\nM2\n##           存活情况\n## 产前护理量 死  活\n##         少 17 197\n##         多  2  23\n\n# 进行卡方检验\nchisq.test(M2,correct = F)\n## Warning in chisq.test(M2, correct = F): Chi-squared approximation may be\n## incorrect\n## \n##  Pearson's Chi-squared test\n## \n## data:  M2\n## X-squared = 9.6186e-05, df = 1, p-value = 0.9922\n这是因为产前护理量和接受护理地点之间不是独立的（Χ2=173.372，P&lt;0.001），这个例子说明由于接受护理量不是一个独立的变量，故对二维以上的列联表资料不能通过简单合并的方法进行卡方检验。\n# 产前护理量和护理地点之间的独立性检验\nM3 &lt;- matrix(c(179,297,214,25),nrow = 2, byrow = T,\n            dimnames = list(`产前护理量`=c(\"少\",\"多\"),\n                            `护理地点`=c(\"甲\",\"乙\")))\nM3\n##           护理地点\n## 产前护理量  甲  乙\n##         少 179 297\n##         多 214  25\n\n# 进行卡方检验\nchisq.test(M3,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  M3\n## X-squared = 173.37, df = 1, p-value &lt; 2.2e-16\n在列联表17-1中有8个格子，总频数715分布于这8个格子中，假设在列联表资料中，这种分布是随机变量，那么在总样本量、行合计、列合计、层合计均固定时，我们可以通过这种多项分布建立模型，描述变量之间的关系。这就是我们要着重介绍的列联表对数线性模型，将每个格子的期望频数的自然对数与分类变量之间建立线性联系，分析分类变量之间的关系。具体统计分析过程包括建立对数线性模型、拟合优度检验及参数估计。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>多维列联表的对数线性模型</span>"
    ]
  },
  {
    "objectID": "对数线性模型.html#对数线性模型的概念",
    "href": "对数线性模型.html#对数线性模型的概念",
    "title": "22  多维列联表的对数线性模型",
    "section": "22.2 对数线性模型的概念",
    "text": "22.2 对数线性模型的概念\n对数线性模型为层次模型，如果模型中包含了某几个变量的高阶交互效应项时，这几个变量的低阶交互效应项与主效应项也一定包含在模型中。对数线性模型的建立一般以饱和模型（saturated model）)开始，饱和模型包含了所有变量的主效应、低阶交互效应和高阶交互效应项。通过后退法逐渐排除没有统计学意义的作用项，最后拟合最优的简化模型即不饱和模型（unsaturated model）。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>多维列联表的对数线性模型</span>"
    ]
  },
  {
    "objectID": "对数线性模型.html#列联表",
    "href": "对数线性模型.html#列联表",
    "title": "22  多维列联表的对数线性模型",
    "section": "22.3 2*2列联表",
    "text": "22.3 2*2列联表\n孙振球医学统计学第4版例17-2。一项蒙古族高血压危险因素的研究中得到性别和血压的关系数据，此数据由分类变量X（性别）、Y（是否高血压）构成的2×2二维列联表。试分析蒙古族高血压危险因素研究中性别和血压的关系。\n\ndata17_2 &lt;- haven::read_sav(\"datasets/例17-02.sav\",encoding = \"GBK\")\n#str(data17_2)\ndata17_2 &lt;- haven::as_factor(data17_2)\nstr(data17_2)\n## tibble [4 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ 性别: Factor w/ 2 levels \"男性\",\"女性\": 1 1 2 2\n##  $ 血压: Factor w/ 2 levels \"正常血压\",\"高血压\": 1 2 1 2\n##  $ 频数: num [1:4] 579 485 1032 483\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\ndata17_2\n## # A tibble: 4 × 3\n##   性别  血压      频数\n##   &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt;\n## 1 男性  正常血压   579\n## 2 男性  高血压     485\n## 3 女性  正常血压  1032\n## 4 女性  高血压     483\n\n先改变一下格式，变成矩阵：\n\nM &lt;- matrix(data17_2$频数,nrow = 2,byrow = T,\n            dimnames = list(trt = c(\"男性\", \"女性\"),\n                            effect = c(\"正常\",\"高血压\")))\n\nM\n##       effect\n## trt    正常 高血压\n##   男性  579    485\n##   女性 1032    483\n\n这个其实是2维列联表，你用卡方检验也可以的：\n\nchisq.test(M,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  M\n## X-squared = 50.046, df = 1, p-value = 1.502e-12\n\n从卡方检验的结果来看性别和血压是独立的。\n下面做对数线性模型，直接使用R语言自带的loglin函数：\n\nfm &lt;- loglin(M, margin=list(1,2), fit=T, param=T)\n## 2 iterations: deviation 0\nfm$lrt # 查看似然比G^2^\n## [1] 49.84297\nfm$pearson # 查看卡方值\n## [1] 50.04632\n1 - pchisq(fm$lrt, fm$df) # 计算似然比G^2^的P值\n## [1] 1.665557e-12\n1 - pchisq(fm$pearson, fm$df) # 计算卡方的P值\n## [1] 1.501577e-12\n\n似然比统计量G2和pearson-x2都和课本一样，P值小于0.0001。\n也可以使用MASS包中的loglm拟合对数线性模型，可以使用公式的形式，结果也更加简洁易懂：\n\nlibrary(MASS)\n\n# 不饱和模型（没有交互项）\nf &lt;- loglm(`频数` ~ `性别` + `血压`, data = data17_2)\nf\n## Call:\n## loglm(formula = 频数 ~ 性别 + 血压, data = data17_2)\n## \n## Statistics:\n##                       X^2 df     P(&gt; X^2)\n## Likelihood Ratio 49.84297  1 1.665557e-12\n## Pearson          50.04632  1 1.501577e-12\n\n如果要对饱和模型进行拟合优度检验，可以使用以下代码：\n\nf1 &lt;- update(f, ~ .^2) # 直接更新模型\nanova(f, f1) # 比较饱和模型和不饱和模型\n## LR tests for hierarchical log-linear models\n## \n## Model 1:\n##  频数 ~ 性别 + 血压 \n## Model 2:\n##  频数 ~ 性别 + 血压 + 性别:血压 \n## \n##           Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev)\n## Model 1   49.84297  1                                    \n## Model 2    0.00000  0   49.84297         1              0\n## Saturated  0.00000  0    0.00000         0              1\n\n当然你不更新模型，直接拟合一个饱和模型也是可以的：\n\n# 或者重新拟合一个饱和模型\nf2 &lt;- loglm(`频数` ~ `性别` * `血压`, data = data17_2)\nanova(f,f2) # 比较饱和模型和不饱和模型\n## LR tests for hierarchical log-linear models\n## \n## Model 1:\n##  频数 ~ 性别 + 血压 \n## Model 2:\n##  频数 ~ 性别 * 血压 \n## \n##           Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev)\n## Model 1   49.84297  1                                    \n## Model 2    0.00000  0   49.84297         1              0\n## Saturated  0.00000  0    0.00000         0              1\n\n结果和课本是一致的。G2=49.843，P值&lt;0.0001，说明饱和模型和简化模型（没有交互项的模型）有显著性差异，简化模型不能取代饱和模型，确定最终模型为饱和模型。\n如何计算OR值？可以使用glm函数拟合模型，然后根据系数计算：\n\nf3 &lt;- glm(`频数` ~ `性别` * `血压`, data = data17_2, family = poisson())\n#f3\ncoef(f3) # 查看系数\n##         (Intercept)            性别女性          血压高血压 性别女性:血压高血压 \n##           6.3613025           0.5779515          -0.1771536          -0.5820837\nexp(-0.5820837) # 计算OR值\n## [1] 0.5587329\n\n除此之外，还可以使用Crosstabs.Loglinear包中的LOGLINEAR函数拟合对数线性模型，给出的信息超级全面，而且格式和SPSS类似，更容易观看：\nlibrary(Crosstabs.Loglinear)\n\nLOGLINEAR(data = data17_2,\n          data_type = \"counts\",\n          variables = c(\"性别\",\"血压\"),\n          Freq = \"频数\")\n\n# 给出的结果，太全面了！\nThe input data:\n\n      血压\n性别    正常血压    高血压\n  男性         579       485\n  女性        1032       483\n\n\nK - Way and Higher-Order Effects\n\n    K    df    LR Chi-Square    p    Pearson Chi-Square    p        AIC\n    1     3          291.135    0               319.455    0    326.153\n    2     1           49.843    0                50.046    0     88.860\n    0     0            0.000    1                 0.000    1     41.017\n\n    These are tests that K - Way and Higher-Order Effects are zero, i.e., tests\n    of the hypothesis that Kth-order and higher interactions are zero\n    If these effects and all higher order effects are removed from the model,\n    then here are the consequences.\n    \n    The df values indicate the number of effects (model terms) that are removed.\n    \n    The first row, labeled as 1, shows the consequences of removing all of the main\n    effects and all higher order effects (i.e., everything) from the model. This\n    usually results in poor fit. A statistically significant chi-square indicates   \n    that the prediction of the cell frequencies is significantly worse than the \n    prediction that is provided by the saturated model. It would suggest that at\n    least one of the removed effects needs to be included in the model.\n    \n    The second row, labeled as 2, shows the consequences of removing all of the\n    two-way and higher order effects from the model, while keeping the main effects.\n    A statistically significant chi-square indicates a reduction in prediction success\n    compared to the saturated model and that at least one of the removed effects needs\n    to be included in the model.\n    \n    The same interpretation process applies if there is a K = 3 row, and so on.\n    A K = 3 row in the table would show the consequences of removing all of the\n    three-way and higher order effects from the model, while keeping the two-way\n    interactions and main effects.\n    \n    A nonsignificant chi-square for a row would indicate that removing the\n    model term(s) does not significantly worsen the prediction of the cell\n    frequencies and the term(s) is nonessential and can be dropped from the model.\n    \n    The bottom row in the table, labeled as 0, is for the saturated mode. It\n    includes all possible model terms and therefore provides perfect prediction\n    of the cell frequencies. The AIC values for this model can be helpful in\n    gaging the relative fit of models with fewer terms.\n\n\nK-Way Effects\n\n    K    df    LR Chi-Square    p    Pearson Chi-Square    p    AIC diff.\n    1     2          241.292    0               269.409    0      237.292\n    2     1           49.843    0                50.046    0       47.843\n\n    These are tests that the K - Way Effects are zero, i.e., tests whether\n    interactions of a particular order are zero. The tests are for model\n    comparisons/differences. For each K-way test, a model is fit with and then\n    without the interactions and the change/difference in chi-square and\n    likelihood ratio chi-square values are computed.\n        \n    For example, the K = 1 test is for the comparison of the model with\n    all main effects and the intercept with the model with only the intercept.\n    A statistically significant K = 1 test is (conventionally) considered to\n    mean that the main effects are not zero and that they are needed in the model.\n        \n    The K = 2 test is for the comparison of the model with all two-way\n    interactions, all main effects, and the intercept with the model with\n    the main effects, and the intercept. A statistically significant K = 2 test\n    is (conventionally) considered to mean that the two-way interactions are\n    not zero and that they are needed in the model.\n        \n    The K = 3 test (if there is one) is for the comparison of the model\n    with all three-way interactions, all two-way interactions, all main\n    effects, and the intercept with the model with all two-way interactions,\n    all main effects, and the intercept. A statistically significant K = 3 test\n    is (conventionally) considered to mean that the three-way interactions\n    are not zero and that they are needed in the model, and so on.\n        \n    The df values for the model comparisons are the df values associated\n    with the K-way terms.\n        \n    The above \"K - Way and Higher-Order Effects\" and \"K - Way\" tests are for the\n    ncollective importance of the effects at each value of K. There are not tests\n    nof individual terms. For example, a significant K = 2 test means that the set\n    nof two-way terms is important, but it does not mean that every two-way term is\n    significant.\n\n\nPartial Associations:\n\n     Effect    LR.Chi.Square    df    p    AIC.diff.\n1                                                   \n2      性别           79.275     1    0       77.275\n3      血压          162.017     1    0      160.017\n\n    These are tests of individual terms in the model, with the restriction that\n    higher-order terms at each step are excluded. The tests are for differences\n    between models. For example, the tests of 2-way interactions are for the\n    differences between the model with all 2-way interactions (and no higher-order\n    interactions) and the model when each individual 2-way interaction is removed in turn.\n\n\n\nParameter Estimates (SPSS \"Model Selection\", not \"General\", Parameter Estimates):\n\n    For saturated models, .500 has been added to all observed cells:\n\n               Estimate    Std. Error    z value    p     CI_lb     CI_ub\n(Intercept)       6.417         0.021    310.767    0     6.377     6.458\n性别1            -0.143         0.021     -6.943    0    -0.184    -0.103\n血压1             0.234         0.021     11.328    0     0.193     0.274\n性别1:血压1      -0.145         0.021     -7.043    0    -0.186    -0.105\n\n\nBackward Elimination Statistics:\n\n    Step              GenDel      Effects    LR_Chi_Square    df    p       AIC\n                                                                               \n       0    Generating Class    性别:血压                0     0    1    41.017\n                                                                               \n              Deleted Effect    性别:血压           49.843     1    0     88.86\n\n    The hierarchical backward elimination procedure begins with all possible\n    terms in the model and then removes, one at a time, terms that do not\n    satisfy the criteria for remaining in the model.\n    A term is dropped only when it is determined that removing the term does\n    not result in a reduction in model fit AND if the term is not involved in any\n    higher order interaction. On each Step above, the focus is on the term that results\n    in the least-significant change in the likelihood ratio chi-squre if removed.\n    If the change is not significant, then the term is removed.\n\n\n\nThe Final Model Formula:\n\nFreq ~ 性别 + 血压 + 性别:血压\n\n\nThe Final Model Goodness-of-Fit Tests:\n\n    df    LR Chi-Square    p    Pearson Chi-Square    p       AIC\n     0                0    1                     0    0    41.017\n\n\nGeneralized Linear Model Coefficients for the Final Model:\n\n                       Estimate    Std. Error    z value    Pr(&gt;|z|)\n(Intercept)               6.361         0.042    153.068       0.000\n性别女性                  0.578         0.052     11.131       0.000\n血压高血压               -0.177         0.062     -2.878       0.004\n性别女性:血压高血压      -0.582         0.083     -7.044       0.000\n\n\nCell Counts and Residuals:\n\n     性别        血压    Obsd. Freq.    Exp. Freq.    Residuals    Std. Resid.    Adjusted Resid.\n1    男性    正常血压            579           579            0              0                  0\n3    男性      高血压            485           485            0              0                  0\n2    女性    正常血压           1032          1032            0              0                  0\n4    女性      高血压            483           483            0              0                  0",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>多维列联表的对数线性模型</span>"
    ]
  },
  {
    "objectID": "对数线性模型.html#rc表",
    "href": "对数线性模型.html#rc表",
    "title": "22  多维列联表的对数线性模型",
    "section": "22.4 R*C表",
    "text": "22.4 R*C表\n孙振球医学统计学第4版例17-3。比较3种方剂治疗胃溃疡的效果，将200名病情类似的患者随机分到3个治疗组，疗效如下。试分析3个方剂的治疗效果有无差别。\n\ndata17_3 &lt;- haven::read_sav(\"datasets/例17-03.sav\",encoding = \"GBK\")\ndata17_3 &lt;- haven::as_factor(data17_3)\nstr(data17_3)\n## tibble [6 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ 治疗方法: Factor w/ 3 levels \"甲方剂\",\"乙方剂\",..: 1 1 2 2 3 3\n##  $ 治疗效果: Factor w/ 2 levels \"有效\",\"无效\": 1 2 1 2 1 2\n##  $ 频数    : num [1:6] 42 18 38 27 56 19\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\ndata17_3\n## # A tibble: 6 × 3\n##   治疗方法 治疗效果  频数\n##   &lt;fct&gt;    &lt;fct&gt;    &lt;dbl&gt;\n## 1 甲方剂   有效        42\n## 2 甲方剂   无效        18\n## 3 乙方剂   有效        38\n## 4 乙方剂   无效        27\n## 5 丙方剂   有效        56\n## 6 丙方剂   无效        19\n\n直接使用loglm函数拟合对数线性模型：\n\n# 拟合不饱和模型\nf &lt;- loglm(`频数` ~ `治疗方法`+`治疗效果`,data = data17_3)\nf\n## Call:\n## loglm(formula = 频数 ~ 治疗方法 + 治疗效果, data = data17_3)\n## \n## Statistics:\n##                       X^2 df  P(&gt; X^2)\n## Likelihood Ratio 4.310314  2 0.1158850\n## Pearson          4.359917  2 0.1130462\n\n进行拟合优度检验：\n\nf1 &lt;- update(f, ~ .^2) # 饱和模型\nanova(f,f1) # 拟合优度检验\n## LR tests for hierarchical log-linear models\n## \n## Model 1:\n##  频数 ~ 治疗方法 + 治疗效果 \n## Model 2:\n##  频数 ~ 治疗方法 + 治疗效果 + 治疗方法:治疗效果 \n## \n##           Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev)\n## Model 1   4.310314  2                                    \n## Model 2   0.000000  0   4.310314         2        0.11588\n## Saturated 0.000000  0   0.000000         0        1.00000\n\n这个结果也是和课本一致的，似然比G2=4.31，p值=0.1159，不拒绝H0，简化模型可以取代饱和模型。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>多维列联表的对数线性模型</span>"
    ]
  },
  {
    "objectID": "对数线性模型.html#三维列联表",
    "href": "对数线性模型.html#三维列联表",
    "title": "22  多维列联表的对数线性模型",
    "section": "22.5 三维列联表",
    "text": "22.5 三维列联表\n孙振球医学统计学第4版例17-4。Vandenbroucke等人采用病例对照研究，研究避孕药与Fcator-V-Leiden等位基因在静脉血栓发生中的作用。共调查324人，其中病例155人，对照169人。试对避孕药与基因的交互作用进行分析。\n\ndata17_4 &lt;- haven::read_sav(\"datasets/例17-04.sav\",encoding = \"GBK\")\ndata17_4 &lt;- haven::as_factor(data17_4)\nstr(data17_4)\n## tibble [8 × 4] (S3: tbl_df/tbl/data.frame)\n##  $ 人群分组          : Factor w/ 2 levels \"病例组\",\"对照组\": 1 1 1 1 2 2 2 2\n##  $ 口服避孕药暴露水平: Factor w/ 2 levels \"暴露\",\"未暴露\": 1 1 2 2 1 1 2 2\n##  $ 基因型            : Factor w/ 2 levels \"突变型\",\"野生型\": 1 2 1 2 1 2 1 2\n##  $ 频数              : num [1:8] 25 84 10 36 2 63 4 100\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\ndata17_4\n## # A tibble: 8 × 4\n##   人群分组 口服避孕药暴露水平 基因型  频数\n##   &lt;fct&gt;    &lt;fct&gt;              &lt;fct&gt;  &lt;dbl&gt;\n## 1 病例组   暴露               突变型    25\n## 2 病例组   暴露               野生型    84\n## 3 病例组   未暴露             突变型    10\n## 4 病例组   未暴露             野生型    36\n## 5 对照组   暴露               突变型     2\n## 6 对照组   暴露               野生型    63\n## 7 对照组   未暴露             突变型     4\n## 8 对照组   未暴露             野生型   100\n\n下面就是建立饱和模型和多个不饱和模型，书中介绍了4种不饱和模型：\n\n无二阶交互效应的模型\n条件独立模型\n联合独立模型\n完全独立模型\n\n然后借助SAS对饱和模型进行逐步筛选，并根据BIC和G2选择最终模型。\n我们先建立饱和模型和几个不饱和模型（并没有完全按照书中来，太麻烦了，不嫌麻烦的可以自己逐个列出来）。\n\nlibrary(MASS)\n\n# 无交互项，只有主效应，完全独立模型\nf1 &lt;- loglm(`频数` ~ `人群分组`+`口服避孕药暴露水平`+`基因型`,data = data17_4)\n# 添加1阶交互效应，即只有两个变量的交互，没有3变量交互\nf2 &lt;- update(f1, ~ .^2)\n# 添加2阶交互效应，即饱和模型，既有两变量交互，又有3变量交互\nf3 &lt;- update(f1, ~ .^3)\n\n下面使用逐步回归法对饱和模型进行筛选，使用的指标的AIC(AIC和BIC类似，也是值越小说明模型拟合效果越好)：\n\nf &lt;- step(f3, direction = \"both\")\n## Start:  AIC=16\n## 频数 ~ 人群分组 + 口服避孕药暴露水平 + 基因型 + \n##     人群分组:口服避孕药暴露水平 + 人群分组:基因型 + \n##     口服避孕药暴露水平:基因型 + 人群分组:口服避孕药暴露水平:基因型\n## \n##                                      Df    AIC\n## - 人群分组:口服避孕药暴露水平:基因型  1 14.096\n## &lt;none&gt;                                  16.000\n## \n## Step:  AIC=14.1\n## 频数 ~ 人群分组 + 口服避孕药暴露水平 + 基因型 + \n##     人群分组:口服避孕药暴露水平 + 人群分组:基因型 + \n##     口服避孕药暴露水平:基因型\n## \n##                                      Df    AIC\n## - 口服避孕药暴露水平:基因型           1 12.097\n## &lt;none&gt;                                  14.096\n## + 人群分组:口服避孕药暴露水平:基因型  1 16.000\n## - 人群分组:基因型                     1 37.909\n## - 人群分组:口服避孕药暴露水平         1 42.920\n## \n## Step:  AIC=12.1\n## 频数 ~ 人群分组 + 口服避孕药暴露水平 + 基因型 + \n##     人群分组:口服避孕药暴露水平 + 人群分组:基因型\n## \n##                               Df    AIC\n## &lt;none&gt;                           12.097\n## + 口服避孕药暴露水平:基因型    1 14.096\n## - 人群分组:基因型              1 38.751\n## - 人群分组:口服避孕药暴露水平  1 43.762\nf # AIC最小的模型\n## Call:\n## loglm(formula = 频数 ~ 人群分组 + 口服避孕药暴露水平 + \n##     基因型 + 人群分组:口服避孕药暴露水平 + 人群分组:基因型, \n##     data = data17_4, evaluate = FALSE)\n## \n## Statistics:\n##                         X^2 df  P(&gt; X^2)\n## Likelihood Ratio 0.09702652  2 0.9526447\n## Pearson          0.09561789  2 0.9533159\n\n结果得到的最终模型和书中是一样的：\n频数 ~ 人群分组 + 口服避孕药暴露水平 + 基因型 + 人群分组:口服避孕药暴露水平 + 人群分组:基因型\n可以再比较一下饱和模型和AIC最小的这个模型：\n\nanova(f3,f)\n## LR tests for hierarchical log-linear models\n## \n## Model 1:\n##  频数 ~ 人群分组 + 口服避孕药暴露水平 + 基因型 + 人群分组:口服避孕药暴露水平 + 人群分组:基因型 \n## Model 2:\n##  频数 ~ 人群分组 + 口服避孕药暴露水平 + 基因型 + 人群分组:口服避孕药暴露水平 + 人群分组:基因型 + 口服避孕药暴露水平:基因型 + 人群分组:口服避孕药暴露水平:基因型 \n## \n##             Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev)\n## Model 1   0.09702652  2                                    \n## Model 2   0.00000000  0 0.09702652         2        0.95264\n## Saturated 0.00000000  0 0.00000000         0        1.00000\n\n结果显示似然比G2=0.097，P值=0.095（就是上面f的结果），简化模型可以取代饱和模型。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>多维列联表的对数线性模型</span>"
    ]
  },
  {
    "objectID": "泊松回归和负二项回归.html",
    "href": "泊松回归和负二项回归.html",
    "title": "23  泊松回归和负二项回归",
    "section": "",
    "text": "23.1 泊松回归简介\n人类稀有疾病或一些卫生事件，如恶性肿瘤、非遗传性先天性疾病、癫痫患者在两周内癫痫病发作次数、某地在一个月内因交通事故死亡人数、某病患者在一年内住院次数、1ml水中大肠杆菌数、1L空气中粉尘粒子数和放射性物质在一定时间内放射性质点数等计数资料（count data），具有发病率低或者不像二项分布资料有分母能计算比例（proportion）等特点；因此，这些事件数的多少除了取决于事件的实际发生数外，还取决于计数时研究者所观察的范围，即观察多长时间、多大人群、多少体积或面积等。使用发病密度（incidence density，ID）等密度指标描述这些事件的群体特征比较合适。对于上述罕见事件的发生，如果事件之间彼此相互独立，观察样本含量较大时，则具有平均计数等于方差的特点；这类事件的发生次数往往服从Poisson分布。\nPoisson回归（Poisson regression）主要用于单位时间、单位面积、单位空间内某事件发生数的影响因素分析。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>泊松回归和负二项回归</span>"
    ]
  },
  {
    "objectID": "泊松回归和负二项回归.html#泊松回归应用",
    "href": "泊松回归和负二项回归.html#泊松回归应用",
    "title": "23  泊松回归和负二项回归",
    "section": "23.2 泊松回归应用",
    "text": "23.2 泊松回归应用\n孙振球《医学统计学》第4版例18-1。研究者为检查某冶炼厂的砷暴露与因呼吸道疾病死亡之间的关系，对该厂1978一2009年的职工进行了回顾性队列研究，其结果如下。请对该资料进行分析。\n\ndata18_1 &lt;- haven::read_sav(\"datasets/例18-01.sav\",encoding = \"GBK\")\ndata18_1 &lt;- haven::as_factor(data18_1)\nstr(data18_1)\n## tibble [8 × 7] (S3: tbl_df/tbl/data.frame)\n##  $ X1     : Factor w/ 2 levels \"有暴露\",\"无暴露\": 1 2 1 2 1 2 1 2\n##   ..- attr(*, \"label\")= chr \"砷暴露情况\"\n##  $ YEARGRP: Factor w/ 4 levels \"40-49岁组\",\"50-59岁组\",..: 1 1 2 2 3 3 4 4\n##   ..- attr(*, \"label\")= chr \"年龄分组\"\n##  $ X2     : num [1:8] 1 1 0 0 1 1 1 1\n##   ..- attr(*, \"label\")= chr \"50-59岁组\"\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\n##  $ X3     : num [1:8] 1 1 1 1 0 0 1 1\n##   ..- attr(*, \"label\")= chr \"60-69岁组\"\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\n##  $ X4     : num [1:8] 1 1 1 1 1 1 0 0\n##   ..- attr(*, \"label\")= chr \"&gt;=70岁组\"\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\n##  $ Y      : num [1:8] 7 14 42 38 59 58 17 41\n##   ..- attr(*, \"label\")= chr \"死亡数\"\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\n##  $ N      : num [1:8] 11026 38337 10792 31019 6898 ...\n##   ..- attr(*, \"label\")= chr \"观察单位数\"\n##   ..- attr(*, \"format.spss\")= chr \"F8.2\"\ndata18_1\n## # A tibble: 8 × 7\n##   X1     YEARGRP      X2    X3    X4     Y      N\n##   &lt;fct&gt;  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1 有暴露 40-49岁组     1     1     1     7 11026.\n## 2 无暴露 40-49岁组     1     1     1    14 38337.\n## 3 有暴露 50-59岁组     0     1     1    42 10792.\n## 4 无暴露 50-59岁组     0     1     1    38 31019.\n## 5 有暴露 60-69岁组     1     0     1    59  6898.\n## 6 无暴露 60-69岁组     1     0     1    58 17496.\n## 7 有暴露 &gt;=70岁组      1     1     0    17  2581.\n## 8 无暴露 &gt;=70岁组      1     1     0    41  6842.\n\nY是因变量，X1和YEARGRP是自变量，X2/X3/X4是YEARGRP的哑变量编码形式，建模时用不到，因为软件会自动进行这一步，详情可见：分类变量进行回归分析时的编码方式。\nX1的因子水平需要修改一下，让无暴露作为reference：\n\n# 查看下因子水平\nlevels(data18_1$X1)\n## [1] \"有暴露\" \"无暴露\"\nlevels(data18_1$YEARGRP)\n## [1] \"40-49岁组\" \"50-59岁组\" \"60-69岁组\" \"&gt;=70岁组\"\n\n# 修改下X1的因子水平\ndata18_1$X1 &lt;- factor(data18_1$X1,levels = c(\"无暴露\",\"有暴露\"))\n\n下面就是建立泊松回归模型即可。本例资料的观察单位为人年数，事件数（因变量）为因呼吸道疾病死亡人数；影响因素有两个，砷暴露情况和年龄。由于观察单位不同，所以在建模时需要设置偏移量，也就是offset：\n\nf &lt;- glm(Y ~ X1 + YEARGRP,data = data18_1,family = poisson(),offset = log(N))\n# 或者也可以写成\n# glm(Y ~ X1 + YEARGRP+offset(log(N)), data = data18_1, family = poisson())\n#f\nsummary(f)\n## \n## Call:\n## glm(formula = Y ~ X1 + YEARGRP, family = poisson(), data = data18_1, \n##     offset = log(N))\n## \n## Coefficients:\n##                  Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)       -8.0086     0.2233 -35.859  &lt; 2e-16 ***\n## X1有暴露           0.8109     0.1210   6.699 2.09e-11 ***\n## YEARGRP50-59岁组   1.4702     0.2453   5.994 2.04e-09 ***\n## YEARGRP60-69岁组   2.3661     0.2372   9.976  &lt; 2e-16 ***\n## YEARGRP&gt;=70岁组    2.6238     0.2548  10.297  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 260.9304  on 7  degrees of freedom\n## Residual deviance:   9.9303  on 3  degrees of freedom\n## AIC: 61.342\n## \n## Number of Fisher Scoring iterations: 4\n\n常数项和各变量的回归系数和书中是一样的，但是标准误有所不同(导致可信区间也不同)，推测是由于各种计算过程中的细节差异导致，并不影响结果。z值是系数除以标准误得到的。\n\n# 单独查看结果以及其他结果\ncoef(f) # 查看回归系数\n##      (Intercept)         X1有暴露 YEARGRP50-59岁组 YEARGRP60-69岁组 \n##       -8.0086495        0.8108698        1.4701505        2.3661111 \n##  YEARGRP&gt;=70岁组 \n##        2.6237532\nconfint(f) # 回归系数的标准误\n##                       2.5 %    97.5 %\n## (Intercept)      -8.4780952 -7.598323\n## X1有暴露          0.5724818  1.047494\n## YEARGRP50-59岁组  1.0094838  1.976026\n## YEARGRP60-69岁组  1.9239329  2.858522\n## YEARGRP&gt;=70岁组   2.1411250  3.145495\nexp(coef(f)) # 相对危险度RR值（或者叫发病率比值，IRR）\n##      (Intercept)         X1有暴露 YEARGRP50-59岁组 YEARGRP60-69岁组 \n##     3.325736e-04     2.249864e+00     4.349890e+00     1.065587e+01 \n##  YEARGRP&gt;=70岁组 \n##     1.378737e+01\nexp(confint(f)) # RR值的可信区间\n##                         2.5 %       97.5 %\n## (Intercept)      0.0002079745 5.012913e-04\n## X1有暴露         1.7726609281 2.850500e+00\n## YEARGRP50-59岁组 2.7441841472 7.214017e+00\n## YEARGRP60-69岁组 6.8478373075 1.743573e+01\n## YEARGRP&gt;=70岁组  8.5090045641 2.323118e+01\n\n结果解释：在控制年龄因素后，砷暴露组因呼吸道疾病死亡的风险是非暴露组的exp(0.8109)=2.25倍（95%CI：1.77～2.85）。在控制砷暴露因素后，因呼吸道疾病死亡风险随着年龄增加越来越大，其中“50～59岁”年龄组因呼吸道疾病死亡风险是“40～49岁”年龄组的exp(1.4702)=4.35倍（95%CI：2.747.21)，“60～69岁”年龄组因呼吸道疾病死亡风险是“40～49岁”年龄组的exp(2.366)=10.66倍（95%CI：6.85～17.44），70岁以上年龄组因呼吸道疾病死亡风险是“40～49”岁年龄组的exp(2.6238)=13.79倍（95%CI:8.5123.23)。\n基线发病密度（即所有自变量均为0时的发病密度）或40～49岁无砷暴露史人群因呼吸道疾病的死亡密度为exp(-8.0086)=33.260/10万人年。40～49岁有砷暴露史的人群因呼吸道疾病的死亡密度为exp(-8.0086+0.8109×1+1.4702×0+2.3661x0+2.6238x0)=74.831/10万人年；其他各组的死亡密度计算与此类似。\n下面对模型进行拟合优度检验，采用pearson卡方统计量和残差偏差统计量进行评价，参考资料见（https://rstudio-pubs-static.s3.amazonaws.com/1047952_9306ae04c1de4543812af559d777dd72.html）。\n首先是残差偏差法：\n\n# 计算P值\n1 - pchisq(deviance(f), df = f$df.residual)\n## [1] 0.01916785\n\n结果小于0.05，说明拟合不好。\n再看pearson卡方的P值：\n\nPearson &lt;- sum((data18_1$Y - f$fitted.values)^2 / f$fitted.values)\n1 - pchisq(Pearson, df = f$df.residual)\n## [1] 0.02137005\n\n结果也是小于0.05，说明拟合不好。书中的方法与此不同，其结果都是P值大于0.25，但其实个人感觉这个模型确实不太好。\n下面我们对这个模型进行过度离散（overdispersion）检验，如果存在过度离散，说明使用该方法是不太合适的。\n\nlibrary(performance)\n\ncheck_overdispersion(f)\n## # Overdispersion test\n## \n##        dispersion ratio = 3.231\n##   Pearson's Chi-Squared = 9.692\n##                 p-value = 0.021\n\n结果表明该数据确实存在过度离散，因此使用poisson()是不太合适的，我们可以使用quasipoisson()试试（或者用下一节介绍的负二项回归）。\n泊松回归假设方差等于均值，也就是所谓的等离散（equidispersion）。而实际数据中经常出现过度离散（overdispersion），也就是方差大于均值的情况。这时候，使用poisson族可能不合适，标准误会被低估，导致p值过小，可能得出错误的显著性结论。quasipoisson就是用来处理这种情况的，它引入了一个离散参数来调整方差和标准误，但不改变系数估计值。\n\nf1 &lt;- glm(Y ~ X1 + YEARGRP,data = data18_1,family = quasipoisson(),offset = log(N))\nsummary(f1)\n## \n## Call:\n## glm(formula = Y ~ X1 + YEARGRP, family = quasipoisson(), data = data18_1, \n##     offset = log(N))\n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)       -8.0086     0.4014 -19.950 0.000275 ***\n## X1有暴露           0.8109     0.2176   3.727 0.033640 *  \n## YEARGRP50-59岁组   1.4702     0.4408   3.335 0.044555 *  \n## YEARGRP60-69岁组   2.3661     0.4263   5.550 0.011534 *  \n## YEARGRP&gt;=70岁组    2.6238     0.4580   5.729 0.010558 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for quasipoisson family taken to be 3.23081)\n## \n##     Null deviance: 260.9304  on 7  degrees of freedom\n## Residual deviance:   9.9303  on 3  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 4\n\n系数没变，P值和标准误变大了，结果解读同上，不再重复。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>泊松回归和负二项回归</span>"
    ]
  },
  {
    "objectID": "泊松回归和负二项回归.html#负二项回归简介",
    "href": "泊松回归和负二项回归.html#负二项回归简介",
    "title": "23  泊松回归和负二项回归",
    "section": "23.3 负二项回归简介",
    "text": "23.3 负二项回归简介\nPoisson回归的应用条件之一是计数资料服从Poisson分布，即满足计数值的平均值等于方差。但是，有许多事件的发生是非独立的（如传染性疾病、遗传性疾病、地方病、致病生物的分布和一些原因不明疾病的空间聚集现象等），它们的计数资料会发生方差远远大于平均值，即存在过度离散现象（over-dispersion）；若用Poisson回归来分析这些事件的影响因素，会导致模型参数估计值的标准误偏小，参数检验的假阳性率增加。此时，宜选用负二项回归模型（negativebinomialregression）来分析这些资料。\n负二项回归模型是基于计数资料服从负二项分布的。负二项分布实际上是当Poisson分布中强度参数λ服从γ分布（Gamma distribution）时所得到的复合分布。在Poisson分布中，λ是一个常数：在负二项分布中，λ是一个随机变量，并服从γ分布；因此，负二项分布又称γ-Poisson分布（Gamma-Poisson distribution）。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>泊松回归和负二项回归</span>"
    ]
  },
  {
    "objectID": "泊松回归和负二项回归.html#负二项回归应用",
    "href": "泊松回归和负二项回归.html#负二项回归应用",
    "title": "23  泊松回归和负二项回归",
    "section": "23.4 负二项回归应用",
    "text": "23.4 负二项回归应用\n孙振球《医学统计学》第4版例18-2。某学者为了检查居住地类型与蚊虫幼虫滋生的关系，对299个不同居住地的家庭进行调查，结果如下。请对该资料选择合适的统计方法进行分析。\n\ndata18_2 &lt;- haven::read_sav(\"datasets/例18-02.sav\",encoding = \"GBK\")\ndata18_2 &lt;- haven::as_factor(data18_2)\nstr(data18_2)\n## tibble [24 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ x    : Factor w/ 3 levels \"农村\",\"城市贫民区\",..: 1 1 1 1 1 1 1 1 2 2 ...\n##   ..- attr(*, \"label\")= chr \"居住地情况\"\n##  $ y    : num [1:24] 0 1 2 3 4 5 6 11 0 1 ...\n##   ..- attr(*, \"label\")= chr \"受蚊子幼虫滋生的容器数\"\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\n##  $ count: num [1:24] 136 23 10 5 2 1 1 1 38 8 ...\n##   ..- attr(*, \"label\")= chr \"频数\"\n##   ..- attr(*, \"format.spss\")= chr \"F8.0\"\nhead(data18_2)\n## # A tibble: 6 × 3\n##   x         y count\n##   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 农村      0   136\n## 2 农村      1    23\n## 3 农村      2    10\n## 4 农村      3     5\n## 5 农村      4     2\n## 6 农村      5     1\n\n这是频数资料的格式，不是原始数据格式，我们先把这个数据变成原始数据的格式：\n\nx1 &lt;- rep(data18_2$x,data18_2$count)\ny1 &lt;- rep(data18_2$y,data18_2$count)\ndf &lt;- data.frame(x1,y1)\nstr(df) # 原始数据的结构\n## 'data.frame':    299 obs. of  2 variables:\n##  $ x1: Factor w/ 3 levels \"农村\",\"城市贫民区\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ y1: num  0 0 0 0 0 0 0 0 0 0 ...\nxtabs(~x1+y1,data = df) # 检查下和原来的数据是否一样\n##             y1\n## x1             0   1   2   3   4   5   6  11\n##   农村       136  23  10   5   2   1   1   1\n##   城市贫民区  38   8   2   0   0   0   0   0\n##   城市        67   5   0   0   0   0   0   0\n\n使用glm.nb()函数（来自MASS包）来拟合负二项回归模型：\n\nlibrary(MASS)\nf &lt;- glm.nb(y1~x1, data = df)\nsummary(f)\n## \n## Call:\n## glm.nb(formula = y1 ~ x1, data = df, init.theta = 0.3002652205, \n##     link = log)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)   -0.7100     0.1731  -4.102  4.1e-05 ***\n## x1城市贫民区  -0.6762     0.4274  -1.582 0.113612    \n## x1城市        -1.9572     0.5256  -3.724 0.000196 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for Negative Binomial(0.3003) family taken to be 1)\n## \n##     Null deviance: 174.95  on 298  degrees of freedom\n## Residual deviance: 156.37  on 296  degrees of freedom\n## AIC: 426.23\n## \n## Number of Fisher Scoring iterations: 1\n## \n## \n##               Theta:  0.3003 \n##           Std. Err.:  0.0764 \n## \n##  2 x log-likelihood:  -418.2280\n\n结果与书中一致。输出结果和泊松回归的结果基本一样，其中有一个Theta：0.3003是负二项分布的过度离散参数，表示数据的过度离散程度。如果theta接近1，表明数据接近泊松分布。\n结果显示城市家庭和贫民区家庭滋生蚊虫幼虫机会都低于农村家庭，但是只有城市家庭才有统计学意义。或者，农村家庭滋生蚊虫幼虫机会是城市家庭的exp（1.9572）=7.08倍（95%CI：2.527～19.834），而与贫民区家庭之间没有差别。\n\n# 演示下这个95%CI的计算方法。\n# 先计算系数的可信区间：均值±1.96*标准误\ncbind(-1.9572+1.96*0.5256,-1.9572-1.96*0.5256) # 系数的可信区间\n##           [,1]      [,2]\n## [1,] -0.927024 -2.987376\ncbind(exp(0.9270),exp(2.9874)) # RR值的可信区间\n##          [,1]     [,2]\n## [1,] 2.526917 19.83405",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>泊松回归和负二项回归</span>"
    ]
  },
  {
    "objectID": "泊松回归和负二项回归.html#参考资料",
    "href": "泊松回归和负二项回归.html#参考资料",
    "title": "23  泊松回归和负二项回归",
    "section": "23.5 参考资料",
    "text": "23.5 参考资料\n\nWhen to use an offset in a Poisson regression?\nhttps://rstudio-pubs-static.s3.amazonaws.com/1047952_9306ae04c1de4543812af559d777dd72.html",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>泊松回归和负二项回归</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html",
    "href": "1019-codescheme.html",
    "title": "24  分类变量重编码",
    "section": "",
    "text": "24.1 演示数据\n使用hsb2数据集进行演示。其中write是数值型因变量，race是其中一个自变量，是无序分类变量，有4个类别：1 = Hispanic, 2 = Asian, 3 = African American and 4 = Caucasian。\nload(file = \"datasets/codingSchemes.rdata\")\n\n# 把race变为因子型，并放到新的一列中\nhsb2$race.f &lt;- factor(hsb2$race, labels=c(\"Hispanic\", \"Asian\", \"African-Am\", \"Caucasian\"))\n\n# 根据race.f进行分组，计算分组后因变量write的均值\ntapply(hsb2$write, hsb2$race.f, mean)\n##   Hispanic      Asian African-Am  Caucasian \n##   46.45833   58.00000   48.20000   54.05517\n记住这个均值，很重要，后面要用的！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>分类变量重编码</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#dummy-coding",
    "href": "1019-codescheme.html#dummy-coding",
    "title": "24  分类变量重编码",
    "section": "24.2 Dummy Coding",
    "text": "24.2 Dummy Coding\n哑变量是最常见的分类变量编码方式，它以其中一个类别为参考，其他所有类别都和参考进行比较。比如，我们设定race中的1为参考，2,3,4都和1进行比较，也就是race=1时，write的均值。\n\n\n\n\n\n\n\n\n\n这也是R语言中数值型和无序因子型变量的默认编码方式，如果要手动设置，可以使用函数contr.treatment()，这个函数就是手动进行哑变量设置的函数。\n\ncontr.treatment(4)\n##   2 3 4\n## 1 0 0 0\n## 2 1 0 0\n## 3 0 1 0\n## 4 0 0 1\n\n这是一个比较矩阵，如果有4个类别就是4行，每行代表一个类别，比如第一行，2,3,4都是0，那么就代表类别1,；第二行，2是1,3,4都是0，就是代表类别2，以此类推。\n如果有K个类别，就会有K-1个哑变量，比如我们这个例子，有4个类别，就有3个哑变量，在R中的其他编码方式也是这样的。\nR语言中对不同类型的变量都会有默认的编码方式，可以通过contrasts()函数查看，比如对于race.f默认的就是哑变量编码：\n\n# 和上面的比较矩阵一模一样的形式\ncontrasts(hsb2$race.f)\n##            Asian African-Am Caucasian\n## Hispanic       0          0         0\n## Asian          1          0         0\n## African-Am     0          1         0\n## Caucasian      0          0         1\n\n在前面介绍logistic回归时，多次使用过哑变量编码的方式，因为是默认的，所以并不用手动设置！\n手动设置也可以，和默认是一样的结果，现在我们定义race.f这个变量进入回归模型后使用哑变量编码的方式：\n\n# 手动定义变量的编码方式\ncontrasts(hsb2$race.f) &lt;- contr.treatment(4)\n\n# 进行回归\nsummary(lm(write ~ race.f, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   46.458      1.842  25.218  &lt; 2e-16 ***\n## race.f2       11.542      3.286   3.512 0.000552 ***\n## race.f3        1.742      2.732   0.637 0.524613    \n## race.f4        7.597      1.989   3.820 0.000179 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\n可以看到结果中race.f2/race.f3/race.f4，都是和race.f1进行比较！和不设置的是一模一样的结果。\n\n请注意它们的系数，比如race.f2的系数是11.542，这个系数就是根据race.f进行分组后，race.f=2时write的均值 减去 race.f=1时write的均值，也就是58-46.45833！intercept（截距）的系数就是参考组write的平均值46.45833！\n\n哑变量编码后的数据进入回归分析时的具体操作可以这么理解，比如现在是race.f这个变量设置了哑变量编码的方式，那当它进入回归分析时，这一列就被我们设置的另外3列替代了，也就是原数据中的race.f这一列被另外3列哑变量替代了，当race.f这列的值是Hispanic时，3列哑变量就分别是0,0,0，如果race.f这列的值是Asian时，3列哑变量就分别是1,0,0，不知道大家理解了没有。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>分类变量重编码</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#simple-coding",
    "href": "1019-codescheme.html#simple-coding",
    "title": "24  分类变量重编码",
    "section": "24.3 simple coding",
    "text": "24.3 simple coding\n简单编码和哑变量编码的唯一不同之处是截距！哑变量的截距是参考组的因变量平均值，简单编码的截距是总的平均值。\n下面这张图是简单编码的方式。简单编码的所有类别编码相加需要等于1。\n\n\n\n\n\n\n\n\n\nlevel 1还是参考组，对于race.f1，是类别2比类别1，类别2就被编码为3/4，其他类别都是 -1/4，对于race.f2是类别3比类别1，类别3就别编码为为3/4，其他类别被编码为 -1/4，对于race.f3是类别4比类别1，类别4别编码为3/4，其他类别被编码为 -1/4。\n也就是说，参考组肯定是被编码为 -1/4（和类别个数有关，这里race.f是4个类别），谁和参考组比谁就被编码为 3/4。\n如果变量有K个类别的话，那就是参考组被编码为 -1/k，谁和参考组比谁就被编码为(k-1)/k。\nsimple coding在R中并没有提供直接的函数，但是可以通过哑变量进行转换：\n\n# 设置simple coding\nc&lt;-contr.treatment(4)\nmy.coding&lt;-matrix(rep(1/4, 12), ncol=3)\nmy.simple&lt;-c-my.coding\nmy.simple\n##       2     3     4\n## 1 -0.25 -0.25 -0.25\n## 2  0.75 -0.25 -0.25\n## 3 -0.25  0.75 -0.25\n## 4 -0.25 -0.25  0.75\n\n\n# 把race.f变成simple coding\ncontrasts(hsb2$race.f) &lt;- my.simple\n\n# 进行回归\nsummary(lm(write ~ race.f, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f2      11.5417     3.2861   3.512 0.000552 ***\n## race.f3       1.7417     2.7325   0.637 0.524613    \n## race.f4       7.5968     1.9889   3.820 0.000179 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\n可以看到除了截距不一样，其他的系数都是一样的，这里的截距51.6784 = (46.45833 + 58 + 48.2 + 54.05517)/4 。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>分类变量重编码</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#deviation-coding",
    "href": "1019-codescheme.html#deviation-coding",
    "title": "24  分类变量重编码",
    "section": "24.4 Deviation coding",
    "text": "24.4 Deviation coding\n这种编码方式比较的是某个类别的因变量的均值和其他所有类别的因变量的均值。它的比较矩阵是下面这种的。\n\n\n\n\n\n\n\n\n\n第1组是比较类别1和其它所有类别，第2组是比较类别2和其它所有类别，第3组是比较类别3和其它所有类别。\n这种比较矩阵（编码方式）通过赋值1，0，-1实现。第1组中，类别1被设为1（因为是类别1和其他所有类别比较），第2组是类别2被设为1，第3组是类别3倍设为1，而类别4一直是 -1，因为它没和别人比过。\n在R中通过函数contr.sum()实现这种编码方式（比较矩阵）：\n\ncontr.sum(4)\n##   [,1] [,2] [,3]\n## 1    1    0    0\n## 2    0    1    0\n## 3    0    0    1\n## 4   -1   -1   -1\n\n现在我们把race.f这个变量设置为deviation coding，再次进行回归分析：\n\ncontrasts(hsb2$race.f) &lt;- contr.sum(4)\n\nsummary(lm(write ~ race.f, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f1      -5.2200     1.6314  -3.200  0.00160 ** \n## race.f2       6.3216     2.1603   2.926  0.00384 ** \n## race.f3      -3.4784     1.7323  -2.008  0.04602 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\n此时的截距是总的均值，也就是(46.4583 + 58 + 48.2 + 54.0552) / 4 = 51.678375，回归系数是相应类别的均值减去总的均值，比如race.f1的回归系数 -5.2200 = 46.4583 – 51.678375。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>分类变量重编码</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#orthogonal-polynomial-coding",
    "href": "1019-codescheme.html#orthogonal-polynomial-coding",
    "title": "24  分类变量重编码",
    "section": "24.5 Orthogonal Polynomial Coding",
    "text": "24.5 Orthogonal Polynomial Coding\n正交多项式编码，在重复测量方差分析的多重比较中用过，这种编码方式常用于线性趋势检验，检测某自变量的1次项、2次项、3次项等和因变量之间有无线性关系。只用在有序分类变量（有序因子）且不同类别间对因变量影响相同的情况下。\n在R语言中中通过函数contr.poly()实现对某个变量的正交多项式编码，对于有序因子变量来说，这种编码方式是默认的，不需要手动指定。\n我们使用hsb2数据集中的read这一个变量，把它变成一个有序因子：\n\n# 新建一列，根据read转换为有序因子\nhsb2$readcat &lt;- cut(hsb2$read, 4, ordered=T)\ntable(hsb2$readcat)\n## \n## (28,40] (40,52] (52,64] (64,76] \n##      22      93      55      30\n\ntapply(hsb2$write, hsb2$readcat, mean)\n##  (28,40]  (40,52]  (52,64]  (64,76] \n## 42.77273 49.97849 56.56364 61.83333\n\n\n\n\n\n\n\n\n\n\n\ncontr.poly(4)\n##              .L   .Q         .C\n## [1,] -0.6708204  0.5 -0.2236068\n## [2,] -0.2236068 -0.5  0.6708204\n## [3,]  0.2236068 -0.5 -0.6708204\n## [4,]  0.6708204  0.5  0.2236068\n\n目前我们还未对readcat这个变量进行任何编码设置，但是R语言是有默认编码方式的，可以通过contrasts()查看：\n\n# 发现默认就是正交多项式编码\ncontrasts(hsb2$readcat)\n##              .L   .Q         .C\n## [1,] -0.6708204  0.5 -0.2236068\n## [2,] -0.2236068 -0.5  0.6708204\n## [3,]  0.2236068 -0.5 -0.6708204\n## [4,]  0.6708204  0.5  0.2236068\n\n当然也可以手动设置：\n\ncontrasts(hsb2$readcat) &lt;- contr.poly(4)\n\nsummary(lm(write ~ readcat, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ readcat, data = hsb2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -18.979  -5.824   1.227   5.436  17.021 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  52.7870     0.6339  83.268   &lt;2e-16 ***\n## readcat.L    14.2587     1.4841   9.607   &lt;2e-16 ***\n## readcat.Q    -0.9680     1.2679  -0.764    0.446    \n## readcat.C    -0.1554     1.0062  -0.154    0.877    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.726 on 196 degrees of freedom\n## Multiple R-squared:  0.3456, Adjusted R-squared:  0.3356 \n## F-statistic: 34.51 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n回归分析结果显示readcat的一次项和因变量有很强的的线性关系，二次项和三次项没有明显的线性关系。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>分类变量重编码</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#helmert-coding",
    "href": "1019-codescheme.html#helmert-coding",
    "title": "24  分类变量重编码",
    "section": "24.6 Helmert Coding",
    "text": "24.6 Helmert Coding\nHelmert Coding比较的是当前类别下的因变量平均值和后面的类别的因变量平均值。\n\n\n\n\n\n\n\n\n\n如上图所示，race.f1比较的是类别1的因变量平均值和类别2,3,4的因变量平均值，race.f2比较的是类别2的因变量平均值和类别3,4的因变量平均值，race.f3比较的是类别3的因变量平均值和类别4的因变量平均值。\nR中提供了contr.helmert，但是对应的是 反helmert编码方式。所以如果想要对一个变量使用Helmert coding，需要手动设置：\n\nmy.helmert &lt;- matrix(c(3/4, -1/4, -1/4, -1/4, 0, 2/3, -1/3, -1/3, 0, 0, 1/\n    2, -1/2), ncol = 3)\nmy.helmert\n##       [,1]       [,2] [,3]\n## [1,]  0.75  0.0000000  0.0\n## [2,] -0.25  0.6666667  0.0\n## [3,] -0.25 -0.3333333  0.5\n## [4,] -0.25 -0.3333333 -0.5\n\n现在把race.f设置为Helmert coding：\n\ncontrasts(hsb2$race.f) &lt;- my.helmert\n\nsummary(lm(write ~ race.f, hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f1      -6.9601     2.1752  -3.200  0.00160 ** \n## race.f2       6.8724     2.9263   2.348  0.01985 *  \n## race.f3      -5.8552     2.1528  -2.720  0.00712 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\nrace.f1的回归系数 -6.9601 = 46.4583 – [(58 + 48.2 + 54.0552) / 3]，也就是类别1的因变量平均值减去类别2,3,4的因变量平均值；race.f2和race.f3的系数同理。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>分类变量重编码</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#reverse-helmert-coding",
    "href": "1019-codescheme.html#reverse-helmert-coding",
    "title": "24  分类变量重编码",
    "section": "24.7 Reverse Helmert Coding",
    "text": "24.7 Reverse Helmert Coding\n和Helmert Coding刚好完全相反，每一个类别和它前面的类别比较。\n如下图所示，race.f1比较的是类别2的因变量平均值和类别1的因变量平均值，race.f2比较的是类别3的因变量平均值和类别1,2的因变量平均值，race.f3比较的是类别4的因变量平均值和类别1,2,3的因变量平均值。\n\n\n\n\n\n\n\n\n\nR中提供了contr.helmert，可以进行Reverse Helmert Coding：\n\ncontr.helmert(4)\n##   [,1] [,2] [,3]\n## 1   -1   -1   -1\n## 2    1   -1   -1\n## 3    0    2   -1\n## 4    0    0    3\n\n把race.f设置为这种编码方式再次进行回归分析：\n\ncontrasts(hsb2$race.f) &lt;- contr.helmert(4)\n\nsummary(lm(write ~ race.f, hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f1       5.7708     1.6431   3.512 0.000552 ***\n## race.f2      -1.3431     0.8675  -1.548 0.123170    \n## race.f3       0.7923     0.3720   2.130 0.034439 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\n看完了上面那么多例子，这个回归系数的解读大家应该明白了吧，这里就不再赘述了。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>分类变量重编码</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#forward-difference-coding",
    "href": "1019-codescheme.html#forward-difference-coding",
    "title": "24  分类变量重编码",
    "section": "24.8 Forward Difference Coding",
    "text": "24.8 Forward Difference Coding\n这种编码方式比较的是当前类别的因变量均值和它相邻的下一个类别的因变量均值，注意是下一个，Helmert coding是多个。\n如下图所示，race.f1比较的是类别1和类别2，race.f2比较的是类别2和3，race.f3`比较的是类别3和类别4。\n\n\n\n\n\n\n\n\n\nR语言中并没有默认函数提供此类编码方式，需要手动设置。\n\nmy.forward.diff = matrix(c(3/4, -1/4, -1/4, -1/4, 1/2, 1/2, -1/2, -1/2, 1/\n4, 1/4, 1/4, -3/4), ncol = 3)\nmy.forward.diff\n##       [,1] [,2]  [,3]\n## [1,]  0.75  0.5  0.25\n## [2,] -0.25  0.5  0.25\n## [3,] -0.25 -0.5  0.25\n## [4,] -0.25 -0.5 -0.75\n\n把race.f设置为这种编码方式：\n\ncontrasts(hsb2$race.f) &lt;- my.forward.diff\n\nsummary(lm(write ~ race.f, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f1     -11.5417     3.2861  -3.512 0.000552 ***\n## race.f2       9.8000     3.3878   2.893 0.004251 ** \n## race.f3      -5.8552     2.1528  -2.720 0.007118 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\nrace.f1的截距是类别2和类别1的因变量差值， -11.5417 = 46.4583 – 58，race.f2的截距 是类别3和类别2的因变量差值，9.9 = 58 – 48.2，race.f3的截距是类别4和类别3的因变量差值，-5.8552 = 48.2 – 54.0552。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>分类变量重编码</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#backward-difference-coding",
    "href": "1019-codescheme.html#backward-difference-coding",
    "title": "24  分类变量重编码",
    "section": "24.9 Backward Difference Coding",
    "text": "24.9 Backward Difference Coding\n这种编码方式和前一种刚好相反，比较的是当前类别和它相邻的前一个类别的因变量均值。\n\n\n\n\n\n\n\n\n\n在R语言中实现这种编码方式也是需要手动设置的：\n\nmy.backward.diff = matrix(c(-3/4, 1/4, 1/4, 1/4, -1/2, -1/2, 1/2, 1/2, -1/4, -1/4, -1/4, 3/4), ncol = 3)\nmy.backward.diff\n##       [,1] [,2]  [,3]\n## [1,] -0.75 -0.5 -0.25\n## [2,]  0.25 -0.5 -0.25\n## [3,]  0.25  0.5 -0.25\n## [4,]  0.25  0.5  0.75\n\n\ncontrasts(hsb2$race.f) &lt;- my.backward.diff\n\nsummary(lm(write ~ race.f, data = hsb2))\n## \n## Call:\n## lm(formula = write ~ race.f, data = hsb2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -23.0552  -5.4583   0.9724   7.0000  18.8000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  51.6784     0.9821  52.619  &lt; 2e-16 ***\n## race.f1      11.5417     3.2861   3.512 0.000552 ***\n## race.f2      -9.8000     3.3878  -2.893 0.004251 ** \n## race.f3       5.8552     2.1528   2.720 0.007118 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.025 on 196 degrees of freedom\n## Multiple R-squared:  0.1071, Adjusted R-squared:  0.0934 \n## F-statistic: 7.833 on 3 and 196 DF,  p-value: 5.785e-05\n\n此时的截距正负号刚好和上面一种情况相反。\n这几种就是常见的R语言中分类变量的编码方式，除了这几个，大家还可以根据自己需要灵活手动设置。\n大家以为这套规则只是R语言中独有的吗？并不是，在SPSS、SAS等软件中，分类变量的编码方式也是类似的！\n这里只演示了线性回归的，logistic回归、cox回归也是类似的编码方案！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>分类变量重编码</span>"
    ]
  },
  {
    "objectID": "1019-codescheme.html#参考资料",
    "href": "1019-codescheme.html#参考资料",
    "title": "24  分类变量重编码",
    "section": "24.10 参考资料",
    "text": "24.10 参考资料\n\nhttps://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>分类变量重编码</span>"
    ]
  },
  {
    "objectID": "1032-survival.html",
    "href": "1032-survival.html",
    "title": "25  生存分析",
    "section": "",
    "text": "25.1 生存过程的描述\nlibrary(survival)\nlibrary(survminer)\n使用survival包中的lung数据集用于演示，这是一份关于肺癌患者的生存数据。time是生存时间，以天为单位，status是生存状态，1代表删失，2代表死亡。但是一般在生存分析中我们喜欢用1代表死亡，用0代表删失，所以我们更改一下（其实不改也可以，你记住就行）。\ndf &lt;- lung\ndf$status &lt;- ifelse(df$status == 2,1,0)\nstr(df)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  1 1 0 1 1 0 1 1 1 1 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n首先把生存时间和生存状态用Surv()放到一起，可以看到有+的就是截尾数据。\nSurv(time = lung$time, event = lung$status)\n##   [1]  306   455  1010+  210   883  1022+  310   361   218   166   170   654 \n##  [13]  728    71   567   144   613   707    61    88   301    81   624   371 \n##  [25]  394   520   574   118   390    12   473    26   533   107    53   122 \n##  [37]  814   965+   93   731   460   153   433   145   583    95   303   519 \n##  [49]  643   765   735   189    53   246   689    65     5   132   687   345 \n##  [61]  444   223   175    60   163    65   208   821+  428   230   840+  305 \n##  [73]   11   132   226   426   705   363    11   176   791    95   196+  167 \n##  [85]  806+  284   641   147   740+  163   655   239    88   245   588+   30 \n##  [97]  179   310   477   166   559+  450   364   107   177   156   529+   11 \n## [109]  429   351    15   181   283   201   524    13   212   524   288   363 \n## [121]  442   199   550    54   558   207    92    60   551+  543+  293   202 \n## [133]  353   511+  267   511+  371   387   457   337   201   404+  222    62 \n## [145]  458+  356+  353   163    31   340   229   444+  315+  182   156   329 \n## [157]  364+  291   179   376+  384+  268   292+  142   413+  266+  194   320 \n## [169]  181   285   301+  348   197   382+  303+  296+  180   186   145   269+\n## [181]  300+  284+  350   272+  292+  332+  285   259+  110   286   270    81 \n## [193]  131   225+  269   225+  243+  279+  276+  135    79    59   240+  202+\n## [205]  235+  105   224+  239   237+  173+  252+  221+  185+   92+   13   222+\n## [217]  192+  183   211+  175+  197+  203+  116   188+  191+  105+  174+  177+\n如果只是想要描述一下这份生存数据，可以使用寿命表法或者K-M曲线，在R中可以通过survfit()实现。\n# 构建生存曲线\nfit &lt;- survfit(Surv(time, status) ~ 1, data = df)\n\n# 寿命表，surv_summary比默认的summary()更好\nsurv_summary(fit)\n##     time n.risk n.event n.censor       surv     std.err     upper      lower\n## 1      5    228       1        0 0.99561404 0.004395615 1.0000000 0.98707342\n## 2     11    227       3        0 0.98245614 0.008849904 0.9996460 0.96556190\n## 3     12    224       1        0 0.97807018 0.009916654 0.9972662 0.95924368\n## 4     13    223       2        0 0.96929825 0.011786516 0.9919508 0.94716300\n## 5     15    221       1        0 0.96491228 0.012628921 0.9890941 0.94132171\n## 6     26    220       1        0 0.96052632 0.013425540 0.9861367 0.93558107\n## 7     30    219       1        0 0.95614035 0.014184183 0.9830945 0.92992527\n## 8     31    218       1        0 0.95175439 0.014910735 0.9799794 0.92434234\n## 9     53    217       2        0 0.94298246 0.016284897 0.9735659 0.91335978\n## 10    54    215       1        0 0.93859649 0.016939076 0.9702809 0.90794671\n## 11    59    214       1        0 0.93421053 0.017574720 0.9669508 0.90257880\n## 12    60    213       2        0 0.92543860 0.018798180 0.9601711 0.89196244\n## 13    61    211       1        0 0.92105263 0.019389168 0.9567281 0.88670745\n## 14    62    210       1        0 0.91666667 0.019968077 0.9532533 0.88148430\n## 15    65    209       2        0 0.90789474 0.021093908 0.9462168 0.87112471\n## 16    71    207       1        0 0.90350877 0.021642644 0.9426590 0.86598451\n## 17    79    206       1        0 0.89912281 0.022182963 0.9390770 0.86086855\n## 18    81    205       2        0 0.89035088 0.023240987 0.9318456 0.85070391\n## 19    88    203       2        0 0.88157895 0.024272607 0.9245323 0.84062118\n## 20    92    201       1        1 0.87719298 0.024779731 0.9208475 0.83560803\n## 21    93    199       1        0 0.87278498 0.025286647 0.9171308 0.83058337\n## 22    95    198       2        0 0.86396897 0.026285933 0.9096467 0.82058489\n## 23   105    196       1        1 0.85956096 0.026778995 0.9058807 0.81560966\n## 24   107    194       2        0 0.85069951 0.027763443 0.8982733 0.80564534\n## 25   110    192       1        0 0.84626878 0.028250266 0.8944478 0.80068492\n## 26   116    191       1        0 0.84183806 0.028733836 0.8906085 0.79573831\n## 27   118    190       1        0 0.83740733 0.029214392 0.8867559 0.79080503\n## 28   122    189       1        0 0.83297660 0.029692160 0.8828904 0.78588462\n## 29   131    188       1        0 0.82854588 0.030167350 0.8790125 0.78097668\n## 30   132    187       2        0 0.81968442 0.031110783 0.8712208 0.77119665\n## 31   135    185       1        0 0.81525370 0.031579392 0.8673077 0.76632386\n## 32   142    184       1        0 0.81082297 0.032046159 0.8633836 0.76146212\n## 33   144    183       1        0 0.80639224 0.032511243 0.8594487 0.75661112\n## 34   145    182       2        0 0.79753079 0.033436970 0.8515479 0.74694024\n## 35   147    180       1        0 0.79310006 0.033897900 0.8475824 0.74211984\n## 36   153    179       1        0 0.78866934 0.034357720 0.8436072 0.73730913\n## 37   156    178       2        0 0.77980788 0.035274546 0.8356287 0.72771592\n## 38   163    176       3        0 0.76651570 0.036644539 0.8235936 0.71339353\n## 39   166    173       2        0 0.75765425 0.037555674 0.8155273 0.70388809\n## 40   167    171       1        0 0.75322352 0.038010898 0.8114818 0.69914771\n## 41   170    170       1        0 0.74879280 0.038466026 0.8074284 0.69441536\n## 42   173    169       0        1 0.74879280 0.038466026 0.8074284 0.69441536\n## 43   174    168       0        1 0.74879280 0.038466026 0.8074284 0.69441536\n## 44   175    167       1        1 0.74430901 0.038932090 0.8033269 0.68962694\n## 45   176    165       1        0 0.73979804 0.039403839 0.7991969 0.68481391\n## 46   177    164       1        1 0.73528708 0.039875693 0.7950587 0.68000904\n## 47   179    162       2        0 0.72620946 0.040831745 0.7867159 0.67035655\n## 48   180    160       1        0 0.72167065 0.041310284 0.7825326 0.66554231\n## 49   181    159       2        0 0.71259304 0.042268879 0.7741425 0.65593716\n## 50   182    157       1        0 0.70805423 0.042749126 0.7699360 0.65114603\n## 51   183    156       1        0 0.70351542 0.043230132 0.7657221 0.64636237\n## 52   185    155       0        1 0.70351542 0.043230132 0.7657221 0.64636237\n## 53   186    154       1        0 0.69894713 0.043718251 0.7614780 0.64155115\n## 54   188    153       0        1 0.69894713 0.043718251 0.7614780 0.64155115\n## 55   189    152       1        0 0.69434880 0.044213739 0.7572033 0.63671178\n## 56   191    151       0        1 0.69434880 0.044213739 0.7572033 0.63671178\n## 57   192    150       0        1 0.69434880 0.044213739 0.7572033 0.63671178\n## 58   194    149       1        0 0.68968874 0.044723618 0.7528734 0.63180684\n## 59   196    148       0        1 0.68968874 0.044723618 0.7528734 0.63180684\n## 60   197    147       1        1 0.68499698 0.045241530 0.7485112 0.62687218\n## 61   199    145       1        0 0.68027286 0.045767770 0.7441162 0.62190715\n## 62   201    144       2        0 0.67082463 0.046824116 0.7353020 0.61200115\n## 63   202    142       1        1 0.66610051 0.047354439 0.7308831 0.60705997\n## 64   203    140       0        1 0.66610051 0.047354439 0.7308831 0.60705997\n## 65   207    139       1        0 0.66130842 0.047901723 0.7264037 0.60204649\n## 66   208    138       1        0 0.65651633 0.048450680 0.7219163 0.59704112\n## 67   210    137       1        0 0.65172424 0.049001423 0.7174208 0.59204373\n## 68   211    136       0        1 0.65172424 0.049001423 0.7174208 0.59204373\n## 69   212    135       1        0 0.64689665 0.049562270 0.7128898 0.58701260\n## 70   218    134       1        0 0.64206907 0.050125134 0.7083507 0.58198951\n## 71   221    133       0        1 0.64206907 0.050125134 0.7083507 0.58198951\n## 72   222    132       1        1 0.63720491 0.050698710 0.7037752 0.57693155\n## 73   223    130       1        0 0.63230333 0.051283424 0.6991623 0.57183791\n## 74   224    129       0        1 0.63230333 0.051283424 0.6991623 0.57183791\n## 75   225    128       0        2 0.63230333 0.051283424 0.6991623 0.57183791\n## 76   226    126       1        0 0.62728505 0.051898763 0.6944504 0.56661573\n## 77   229    125       1        0 0.62226677 0.052516642 0.6897296 0.56140253\n## 78   230    124       1        0 0.61724849 0.053137208 0.6849999 0.55619818\n## 79   235    123       0        1 0.61724849 0.053137208 0.6849999 0.55619818\n## 80   237    122       0        1 0.61724849 0.053137208 0.6849999 0.55619818\n## 81   239    121       2        0 0.60704604 0.054428498 0.6753847 0.54562217\n## 82   240    119       0        1 0.60704604 0.054428498 0.6753847 0.54562217\n## 83   243    118       0        1 0.60704604 0.054428498 0.6753847 0.54562217\n## 84   245    117       1        0 0.60185761 0.055101203 0.6704957 0.54024596\n## 85   246    116       1        0 0.59666918 0.055777281 0.6655969 0.53487943\n## 86   252    115       0        1 0.59666918 0.055777281 0.6655969 0.53487943\n## 87   259    114       0        1 0.59666918 0.055777281 0.6655969 0.53487943\n## 88   266    113       0        1 0.59666918 0.055777281 0.6655969 0.53487943\n## 89   267    112       1        0 0.59134178 0.056493740 0.6605811 0.52935986\n## 90   268    111       1        0 0.58601437 0.057214008 0.6555547 0.52385081\n## 91   269    110       1        1 0.58068697 0.057938291 0.6505179 0.51835217\n## 92   270    108       1        0 0.57531024 0.058680326 0.6454326 0.51280626\n## 93   272    107       0        1 0.57531024 0.058680326 0.6454326 0.51280626\n## 94   276    106       0        1 0.57531024 0.058680326 0.6454326 0.51280626\n## 95   279    105       0        1 0.57531024 0.058680326 0.6454326 0.51280626\n## 96   283    104       1        0 0.56977841 0.059470446 0.6402172 0.50708954\n## 97   284    103       1        1 0.56424658 0.060265393 0.6349901 0.50138454\n## 98   285    101       2        0 0.55307338 0.061902647 0.6244165 0.48988160\n## 99   286     99       1        0 0.54748678 0.062729652 0.6191120 0.48414791\n## 100  288     98       1        0 0.54190018 0.063562614 0.6137958 0.47842592\n## 101  291     97       1        0 0.53631358 0.064401818 0.6084680 0.47271553\n## 102  292     96       0        2 0.53631358 0.064401818 0.6084680 0.47271553\n## 103  293     94       1        0 0.53060812 0.065283876 0.6030365 0.46687880\n## 104  296     93       0        1 0.53060812 0.065283876 0.6030365 0.46687880\n## 105  300     92       0        1 0.53060812 0.065283876 0.6030365 0.46687880\n## 106  301     91       1        1 0.52477726 0.066212421 0.5974962 0.46090869\n## 107  303     89       1        1 0.51888089 0.067169680 0.5918922 0.45487570\n## 108  305     87       1        0 0.51291674 0.068157318 0.5862225 0.44877769\n## 109  306     86       1        0 0.50695259 0.069153590 0.5805384 0.44269407\n## 110  310     85       2        0 0.49502429 0.071173772 0.5691277 0.43056952\n## 111  315     83       0        1 0.49502429 0.071173772 0.5691277 0.43056952\n## 112  320     82       1        0 0.48898741 0.072223700 0.5633452 0.42444435\n## 113  329     81       1        0 0.48295053 0.073284268 0.5575481 0.41833381\n## 114  332     80       0        1 0.48295053 0.073284268 0.5575481 0.41833381\n## 115  337     79       1        0 0.47683723 0.074383257 0.5516775 0.41214973\n## 116  340     78       1        0 0.47072393 0.075494166 0.5457918 0.40598083\n## 117  345     77       1        0 0.46461064 0.076617562 0.5398911 0.39982704\n## 118  348     76       1        0 0.45849734 0.077754031 0.5339753 0.39368826\n## 119  350     75       1        0 0.45238404 0.078904180 0.5280446 0.38756443\n## 120  351     74       1        0 0.44627074 0.080068634 0.5220991 0.38145549\n## 121  353     73       2        0 0.43404415 0.082443090 0.5101637 0.36928207\n## 122  356     71       0        1 0.43404415 0.082443090 0.5101637 0.36928207\n## 123  361     70       1        0 0.42784352 0.083689321 0.5041055 0.36311858\n## 124  363     69       2        0 0.41544226 0.086235271 0.4919424 0.35083836\n## 125  364     67       1        1 0.40924162 0.087536643 0.4858376 0.34472158\n## 126  371     65       2        0 0.39664957 0.090283246 0.4734305 0.33232098\n## 127  376     63       0        1 0.39664957 0.090283246 0.4734305 0.33232098\n## 128  382     62       0        1 0.39664957 0.090283246 0.4734305 0.33232098\n## 129  384     61       0        1 0.39664957 0.090283246 0.4734305 0.33232098\n## 130  387     60       1        0 0.39003875 0.091834363 0.4669574 0.32579034\n## 131  390     59       1        0 0.38342792 0.093411868 0.4604644 0.31927978\n## 132  394     58       1        0 0.37681710 0.095017143 0.4539514 0.31278928\n## 133  404     57       0        1 0.37681710 0.095017143 0.4539514 0.31278928\n## 134  413     56       0        1 0.37681710 0.095017143 0.4539514 0.31278928\n## 135  426     55       1        0 0.36996588 0.096772712 0.4472339 0.30604732\n## 136  428     54       1        0 0.36311466 0.098561472 0.4404935 0.29932852\n## 137  429     53       1        0 0.35626344 0.100385300 0.4337299 0.29263289\n## 138  433     52       1        0 0.34941222 0.102246185 0.4269433 0.28596045\n## 139  442     51       1        0 0.34256100 0.104146240 0.4201335 0.27931128\n## 140  444     50       1        1 0.33570978 0.106087711 0.4133006 0.27268545\n## 141  450     48       1        0 0.32871582 0.108156668 0.4063345 0.26592397\n## 142  455     47       1        0 0.32172187 0.110274202 0.3993431 0.25918807\n## 143  457     46       1        0 0.31472792 0.112443281 0.3923261 0.25247790\n## 144  458     45       0        1 0.31472792 0.112443281 0.3923261 0.25247790\n## 145  460     44       1        0 0.30757501 0.114769476 0.3851616 0.24561738\n## 146  473     43       1        0 0.30042210 0.117156914 0.3779689 0.23878538\n## 147  477     42       1        0 0.29326919 0.119609626 0.3707476 0.23198214\n## 148  511     41       0        2 0.29326919 0.119609626 0.3707476 0.23198214\n## 149  519     39       1        0 0.28574947 0.122397820 0.3632207 0.22480203\n## 150  520     38       1        0 0.27822975 0.125269565 0.3556585 0.21765764\n## 151  524     37       2        0 0.26319030 0.131289244 0.3404266 0.20347744\n## 152  529     35       0        1 0.26319030 0.131289244 0.3404266 0.20347744\n## 153  533     34       1        0 0.25544941 0.134640748 0.3325916 0.19619977\n## 154  543     33       0        1 0.25544941 0.134640748 0.3325916 0.19619977\n## 155  550     32       1        0 0.24746662 0.138333639 0.3245387 0.18869779\n## 156  551     31       0        1 0.24746662 0.138333639 0.3245387 0.18869779\n## 157  558     30       1        0 0.23921773 0.142427599 0.3162481 0.18095008\n## 158  559     29       0        1 0.23921773 0.142427599 0.3162481 0.18095008\n## 159  567     28       1        0 0.23067424 0.146997865 0.3076975 0.17293157\n## 160  574     27       1        0 0.22213075 0.151765851 0.2990832 0.16497774\n## 161  583     26       1        0 0.21358726 0.156752465 0.2904045 0.15708959\n## 162  588     25       0        1 0.21358726 0.156752465 0.2904045 0.15708959\n## 163  613     24       1        0 0.20468779 0.162428228 0.2814175 0.14887877\n## 164  624     23       1        0 0.19578832 0.168401942 0.2723521 0.14074818\n## 165  641     22       1        0 0.18688885 0.174710378 0.2632068 0.13269961\n## 166  643     21       1        0 0.17798938 0.181396440 0.2539797 0.12473524\n## 167  654     20       1        0 0.16908991 0.188510603 0.2446686 0.11685766\n## 168  655     19       1        0 0.16019044 0.196112784 0.2352708 0.10906995\n## 169  687     18       1        0 0.15129097 0.204274810 0.2257834 0.10137573\n## 170  689     17       1        0 0.14239151 0.213083712 0.2162028 0.09377928\n## 171  705     16       1        0 0.13349204 0.222646211 0.2065248 0.08628565\n## 172  707     15       1        0 0.12459257 0.233094916 0.1967446 0.07890080\n## 173  728     14       1        0 0.11569310 0.244597108 0.1868568 0.07163182\n## 174  731     13       1        0 0.10679363 0.257367445 0.1768548 0.06448724\n## 175  735     12       1        0 0.09789416 0.271686878 0.1667313 0.05747732\n## 176  740     11       0        1 0.09789416 0.271686878 0.1667313 0.05747732\n## 177  765     10       1        0 0.08810474 0.291418720 0.1559751 0.04976720\n## 178  791      9       1        0 0.07831533 0.314346559 0.1450170 0.04229358\n## 179  806      8       0        1 0.07831533 0.314346559 0.1450170 0.04229358\n## 180  814      7       1        0 0.06712742 0.350176075 0.1333431 0.03379322\n## 181  821      6       0        1 0.06712742 0.350176075 0.1333431 0.03379322\n## 182  840      5       0        1 0.06712742 0.350176075 0.1333431 0.03379322\n## 183  883      4       1        0 0.05034557 0.453824434 0.1225342 0.02068546\n## 184  965      3       0        1 0.05034557 0.453824434 0.1225342 0.02068546\n## 185 1010      2       0        1 0.05034557 0.453824434 0.1225342 0.02068546\n## 186 1022      1       0        1 0.05034557 0.453824434 0.1225342 0.02068546\n画出生存曲线，横坐标是生存时间，纵坐标是生存率。\nggsurvplot(fit,\n           conf.int = TRUE, # 可信区间\n           palette= 'blue', # 更改配色\n           surv.median.line = \"hv\", # 中位生存时间\n           ggtheme = theme_bw() # 更改主题\n           \n)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>生存分析</span>"
    ]
  },
  {
    "objectID": "1032-survival.html#生存过程的比较",
    "href": "1032-survival.html#生存过程的比较",
    "title": "25  生存分析",
    "section": "25.2 生存过程的比较",
    "text": "25.2 生存过程的比较\n如果通过某个变量把数据分为多组，然后检验不同组别之间的生存时间（生存曲线）有无差别，则可以通过logrank检验或者breslow检验。\n在R语言中通过survdiff()实现logrank检验。\n\nfit &lt;- survdiff(Surv(time, status) ~ sex, data = df)\nfit\n## Call:\n## survdiff(formula = Surv(time, status) ~ sex, data = df)\n## \n##         N Observed Expected (O-E)^2/E (O-E)^2/V\n## sex=1 138      112     91.6      4.55      10.3\n## sex=2  90       53     73.4      5.68      10.3\n## \n##  Chisq= 10.3  on 1 degrees of freedom, p= 0.001\n\n可以用神包broom提取数据：\n\nbroom::tidy(fit)\n## # A tibble: 2 × 4\n##   sex       N   obs   exp\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 1       138   112  91.6\n## 2 2        90    53  73.4\nbroom::glance(fit)\n## # A tibble: 1 × 3\n##   statistic    df p.value\n##       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n## 1      10.3     1 0.00131\n\n对于不同组别之间生存曲线的检验，也可以通过K-M图示的方法：\n\nfit.logrank &lt;- survfit(Surv(time, status) ~ sex, data = df)\n\n# 这一步输出太多，我注释掉了，可以自己运行看看\n# surv_summary(fit.logrank) # 可以查看寿命表\n\n通过ggsurvplot()进行可视化，非常多的细节可以修改，超级详细的教程可以参考我的另一篇推文：R语言生存曲线的可视化(超详细)\n\nggsurvplot(fit.logrank, \n           data = df,\n           surv.median.line = \"hv\", # Add medians survival\n           \n           # Change legends: title & labels\n           legend.title = \"Sex\",\n           legend.labs = c(\"Male\", \"Female\"),\n           \n           # Add p-value and tervals\n           pval = TRUE, # 这里P值直接写数字也行\n           conf.int = TRUE,\n           \n           # Add risk table\n           risk.table = TRUE, \n           tables.height = 0.2,\n           tables.theme = theme_cleantable(),\n           \n           ncensor.plot = TRUE,\n           \n           # Color palettes. Use custom color: c(\"#E7B800\", \"#2E9FDF\"),\n           # or brewer color (e.g.: \"Dark2\"), or ggsci color (e.g.: \"jco\")\n           palette = c(\"#E7B800\", \"#2E9FDF\"),\n           ggtheme = theme_bw(), # Change ggplot2 theme\n           \n           # Change font size, style and color\n           main = \"Survival curve\",\n           font.main = c(16, \"bold\", \"darkblue\"),\n           font.x = c(14, \"bold.italic\", \"red\"),\n           font.y = c(14, \"bold.italic\", \"darkred\"),\n           font.tickslab = c(12, \"plain\", \"darkgreen\")\n)\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n自带的surv_cutpoint()可用于寻找最佳切点，但是只能用于连续性数据。\n使用myeloma数据进行演示。\n\nrm(list = ls())\n\n# 0. Load some data\ndata(myeloma)\nhead(myeloma)\n##          molecular_group chr1q21_status treatment event  time   CCND1 CRIM1\n## GSM50986      Cyclin D-1       3 copies       TT2     0 69.24  9908.4 420.9\n## GSM50988      Cyclin D-2       2 copies       TT2     0 66.43 16698.8  52.0\n## GSM50989           MMSET       2 copies       TT2     0 66.50   294.5 617.9\n## GSM50990           MMSET       3 copies       TT2     1 42.67   241.9  11.9\n## GSM50991             MAF           &lt;NA&gt;       TT2     0 65.00   472.6  38.8\n## GSM50992    Hyperdiploid       2 copies       TT2     0 65.20   664.1  16.9\n##          DEPDC1    IRF4   TP53   WHSC1\n## GSM50986  523.5 16156.5   10.0   261.9\n## GSM50988   21.1 16946.2 1056.9   363.8\n## GSM50989  192.9  8903.9 1762.8 10042.9\n## GSM50990  184.7 11894.7  946.8  4931.0\n## GSM50991  212.0  7563.1  361.4   165.0\n## GSM50992  341.6 16023.4 2096.3   569.2\n\n寻找最佳切点：\n\n# 1. Determine the optimal cutpoint of variables\nres.cut &lt;- surv_cutpoint(myeloma, time = \"time\", event = \"event\",\n                         variables = c(\"DEPDC1\", \"WHSC1\", \"CRIM1\") # 找这3个变量的最佳切点\n                         )\n\nsummary(res.cut)\n##        cutpoint statistic\n## DEPDC1    279.8  4.275452\n## WHSC1    3205.6  3.361330\n## CRIM1      82.3  1.968317\n\n查看根据最佳切点进行分组后的数据分布情况：\n\n# 2. Plot cutpoint for DEPDC1\nplot(res.cut, \"DEPDC1\", palette = \"npg\")\n## $DEPDC1\n\n\n\n\n\n\n\n\n根据最佳切点重新划分数据，这样数据就根据最佳切点变成了高表达/低表达组。\n\n# 3. Categorize variables\nres.cat &lt;- surv_categorize(res.cut)\nhead(res.cat)\n##           time event DEPDC1 WHSC1 CRIM1\n## GSM50986 69.24     0   high   low  high\n## GSM50988 66.43     0    low   low   low\n## GSM50989 66.50     0    low  high  high\n## GSM50990 42.67     1    low  high   low\n## GSM50991 65.00     0    low   low   low\n## GSM50992 65.20     0   high   low   low\n\n根据最佳切点绘制生存曲线：\n\n# 4. Fit survival curves and visualize\nlibrary(\"survival\")\nfit &lt;- survfit(Surv(time, event) ~DEPDC1, data = res.cat)\nggsurvplot(fit, data = res.cat, risk.table = TRUE, conf.int = TRUE)\n\n\n\n\n\n\n\n\n确定最佳切点的R包还有非常多，其他的后续会再介绍。\n下次继续介绍Cox回归。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>生存分析</span>"
    ]
  },
  {
    "objectID": "1032-survival.html#cox回归",
    "href": "1032-survival.html#cox回归",
    "title": "25  生存分析",
    "section": "25.3 Cox回归",
    "text": "25.3 Cox回归\n上次介绍了生存分析中的寿命表、K-M曲线、logrank检验、最佳切点的寻找等，本次主要介绍Cox回归。\n本推文不涉及理论，只有实操，想要了解生存分析的理论的请自行学习。\n使用survival包中的lung数据集用于演示，这是一份关于肺癌患者的生存数据。time是生存时间，以天为单位，status是生存状态，1代表删失，2代表死亡。\n\nrm(list = ls())\nlibrary(survival)\nlibrary(survminer)\n\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\n可以使用cox回归探索危险因素。分类变量需要变为因子型，这样在进行回归时会自动进行哑变量设置。\n\nlung$sex &lt;- factor(lung$sex, labels = c(\"female\",\"male\"))\nlung$ph.ecog &lt;- factor(lung$ph.ecog, labels = c(\"asymptomatic\", \"symptomatic\",\n                                                'in bed &lt;50%','in bed &gt;50%'))\n\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : Factor w/ 4 levels \"asymptomatic\",..: 2 1 1 2 1 2 3 3 2 3 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\n拟合多因素Cox回归模型，这里我们只用sex/age两个变量做演示：\n\nfit.cox &lt;- coxph(Surv(time, status) ~ sex + age + ph.karno, data = lung)\n\n# 查看结果\nsummary(fit.cox)\n## Call:\n## coxph(formula = Surv(time, status) ~ sex + age + ph.karno, data = lung)\n## \n##   n= 227, number of events= 164 \n##    (1 observation deleted due to missingness)\n## \n##               coef exp(coef)  se(coef)      z Pr(&gt;|z|)   \n## sexmale  -0.497170  0.608249  0.167713 -2.964  0.00303 **\n## age       0.012375  1.012452  0.009405  1.316  0.18821   \n## ph.karno -0.013322  0.986767  0.005880 -2.266  0.02348 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##          exp(coef) exp(-coef) lower .95 upper .95\n## sexmale     0.6082     1.6441    0.4378    0.8450\n## age         1.0125     0.9877    0.9940    1.0313\n## ph.karno    0.9868     1.0134    0.9755    0.9982\n## \n## Concordance= 0.637  (se = 0.025 )\n## Likelihood ratio test= 18.81  on 3 df,   p=3e-04\n## Wald test            = 18.73  on 3 df,   p=3e-04\n## Score (logrank) test = 19.05  on 3 df,   p=3e-04\n\n结果解读和logistic回归的结果解读类似：R语言logistic回归的细节解读\n\ncoef是回归系数，\nexp(coef)是HR值，\nse(coef)是回归系数的标准误，\nz是Wald检验的z值，\nPr(&gt;|z|)是回归系数的P值，\nlower .95/upper .95是HR值的95%可信区间。\n\nConcordance= 0.645是Cox回归的C-index，最后给出了Likelihood ratio test似然比检验的统计量、自由度、P值；Wald test的统计量、自由度、P值；Score (logrank) test的统计量、自由度、P值。\n想获得整洁的结果不需要自己提取，只要用神包broom即可：\n\nbroom::tidy(fit.cox, exponentiate = T, conf.int = T)\n## # A tibble: 3 × 7\n##   term     estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 sexmale     0.608   0.168       -2.96 0.00303    0.438     0.845\n## 2 age         1.01    0.00940      1.32 0.188      0.994     1.03 \n## 3 ph.karno    0.987   0.00588     -2.27 0.0235     0.975     0.998\n\n\nestimate：HR值（exp(coef)）\nstd.error：回归系数的标准误（se(coef)）\nstatistic：Wald检验的z值\np.value：回归系数的P值\nconf.low/conf.high：HR的95%的可信区间\n\n构建好Cox回归后，也可以用函数单独提取想要的结果，以下图片展示了可用于提取模型信息的函数，和logistic回归差不多：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n进行Cox回归必须要符合等比例风险假设，关于什么是等比例风险假设，可以参考郑老师的这篇文章：生存分析COX回归，小心你的数据不符合应用条件\n等比例风险的检验可以通过很多方法进行，比如K-M曲线，一般如果有交叉，那么可能不符合等比例风险假设，还可以通过各种残差分布来检验。\n下面是Cox回归的等比例风险假设检验，检验方法是基于Schoenfeld残差：\n\nftest &lt;- cox.zph(fit.cox)\nftest\n##           chisq df      p\n## sex       3.085  1 0.0790\n## age       0.478  1 0.4892\n## ph.karno  8.017  1 0.0046\n## GLOBAL   10.359  3 0.0157\n\n可以看到ph.karno的P值是小于0.05的，其实是不满足等比例风险假设的，下一篇推文会说到不符合等比例风险假设时该怎么办。\n这种方法是基于Schoenfeld残差，检验结果可以通过图示画出来：\n\nlibrary(survminer)\n\nggcoxzph(ftest)\n\n\n\n\n\n\n\n\n可以看到sex和age的回归系数随着时间变化基本没啥变化，稳定在0水平线上，和上面的检验结果一样。\n还可以通过以下方式查看残差的变化：\n\nggcoxdiagnostics(fit.cox, type = \"schoenfeld\")\n\n\n\n\n\n\n\n\n这张图反映的也是回归系数随时间的变化趋势，和上面的图意思一样，如果符合比例风险假设，那么结果应该是一条水平线，从图示来看，这3个变量都是有点问题的，但是真实数据往往不可能是完美的，很少有完全符合要求的数据。\n除了Schoenfeld残差外，ggcoxdiagnostics()还支持其他类型，比如：“martingale”, “deviance”, “score”,“dfbetas” and “scaledsch”在，只需要在type参数中提供合适的类型即可。\ncox回归也是回归分析的一种，可以计算出回归系数和95%的可信区间，因此结果可以通过森林图展示：\n\n# 为了森林图好看点，多选几个变量\nfit.cox &lt;- coxph(Surv(time, status) ~ . , data = lung)\n\nggforest(fit.cox, data = lung,\n         main = \"Hazard ratio\",\n         cpositions = c(0.01, 0.15, 0.35), # 更改前三列的相对位置\n         fontsize = 0.7,\n         refLabel = \"reference\",\n         noDigits = 2\n         )\n\n\n\n\n\n\n\n\n这个结果如果你觉得不好看，或者你还有其他的森林图想做到统一的样式，可以考虑我公众号中介绍的画森林图的方法进行个性化定制:\n\n画一个好看的森林图\n用更简单的方式画森林图\nR语言画森林图系列3\nR语言画森林图系列4\nggplot2绘制森林图(有亚组和没亚组)\n\n以上是Cox回归的主要内容，大家有问题可以加群或者评论区留言。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>生存分析</span>"
    ]
  },
  {
    "objectID": "1032-survival.html#时间依存协变量的cox回归和时间依存系数cox回归",
    "href": "1032-survival.html#时间依存协变量的cox回归和时间依存系数cox回归",
    "title": "25  生存分析",
    "section": "25.4 时间依存协变量的Cox回归和时间依存系数Cox回归",
    "text": "25.4 时间依存协变量的Cox回归和时间依存系数Cox回归\n之前分别介绍了生存分析中的寿命表法、K-M曲线、logrank检验，以及Cox回归的构建、可视化以及比例风险检验的内容。\n本次主要介绍如果数据不符合PH假设时采取的方法。\n关于时依协变量、时依系数的基础知识，大家可以参考这几篇文章：\n\nsurvival包的案例介绍：Using Time Dependent Covariates and Time Dependent Coefcients in the Cox Model\n医咖会：一文详解时依协变量\n7code：含时依协变量的Cox回归\n\n如果不能满足PH假设，可以考虑使用时依协变量或者时依系数Cox回归，时依协变量和时依系数是两个概念，简单来说就是如果一个协变量本身会随着时间而改变，这种叫时依协变量，如果是协变量的系数随着时间改变，这种叫时依系数。\n这里以survival包的veteran数据集为例，演示如何处理此类不符合PH检验的情况。\n\nrm(list = ls())\nlibrary(survival)\nstr(veteran)\n## 'data.frame':    137 obs. of  8 variables:\n##  $ trt     : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ celltype: Factor w/ 4 levels \"squamous\",\"smallcell\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ time    : num  72 411 228 126 118 10 82 110 314 100 ...\n##  $ status  : num  1 1 1 1 1 1 1 1 1 0 ...\n##  $ karno   : num  60 70 60 60 70 20 40 80 50 70 ...\n##  $ diagtime: num  7 5 3 9 11 5 10 29 18 6 ...\n##  $ age     : num  69 64 38 63 65 49 69 68 43 70 ...\n##  $ prior   : num  0 10 0 10 10 0 10 0 0 0 ...\n\n这个数据集中的变量解释如下图：\n\n\n\n\n\n\n\n\n\n首先构建普通的Cox回归，进行等比例风险假设，这里只选择了trt/prior/karno3个变量，而且trt/prior作为分类变量并没有转换为因子型，因为二分类变量数值型和因子型的结果是一样的，转不转换没啥影响！\n\nfit &lt;- coxph(Surv(time, status) ~ trt + prior + karno, data = veteran)\n\n# 进行PH检验\nzp &lt;- cox.zph(fit)\nzp\n##         chisq df       p\n## trt     0.288  1 0.59125\n## prior   2.168  1 0.14087\n## karno  12.138  1 0.00049\n## GLOBAL 18.073  3 0.00042\n\n可以看到变量karno的P值小于0.05，是不满足PH假设的。\n通过图形化方法查看PH检验的结果：\n\n#op &lt;- par(mfrow=c(1,3))\n#plot(zp)\n#par(op)\n#ggcoxdiagnostics(fit, type = \"schoenfeld\")\nplot(zp[3])\nabline(0,0, col=\"red\") # 0水平线\nabline(h=fit$coef[3], col=\"green\", lwd=2, lty=2) \n\n\n\n\n\n\n\n\n黑色实线以及两侧的虚线是karno的系数随着时间变化的曲线，绿色虚线是假设karno符合PH检验时的总体估计线，红色实线是参考线。\n这张图反映了karno变量的系数随着时间的改变，karno偏离的比较厉害（上面注释掉的代码可以都运行看看其他变量的情况），系数最开始接近-0.05，然后逐渐趋于0，最后又开始趋向-0.05，所以它的系数是一致在随着时间改变的，不符合比例风险假设。\n\n25.4.1 对时间分层\n这种情况下一个比较简单的解决方式是对时间使用分层函数。根据上面的图示我们知道karno的系数大概分为3层（3段），可以根据两个拐点进行分层，通过survival中的survSplit()实现。\n\nvet2 &lt;- survSplit(Surv(time, status) ~ ., data= veteran, \n                  cut=c(90, 180), # 两个拐点把时间分为3层（3段）\n                  episode= \"tgroup\", \n                  id=\"id\")\nvet2[1:7, c(\"id\", \"tstart\", \"time\", \"status\", \"tgroup\", \"age\", \"karno\")]\n##   id tstart time status tgroup age karno\n## 1  1      0   72      1      1  69    60\n## 2  2      0   90      0      1  64    70\n## 3  2     90  180      0      2  64    70\n## 4  2    180  411      1      3  64    70\n## 5  3      0   90      0      1  38    60\n## 6  3     90  180      0      2  38    60\n## 7  3    180  228      1      3  38    60\n\n结果多了两列：tstart/tgroup。\n受试者1（id编号为1）在第72天的时候死了，所以数据和之前一样。受试者2和3（id为2和3）虽然时间在变，但是直到第3层才死去，karno的值没有变化。\n重新拟合Cox模型，此时tgroup是分好的层，所以要用strata()，另外karno会随着时间变化，和时间有交互，所以用karno:strata(tgroup)。\n\n# 注意此时Surv()的用法！\nfit2 &lt;- coxph(Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), data = vet2)\nfit2\n## Call:\n## coxph(formula = Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), \n##     data = vet2)\n## \n##                                   coef exp(coef)  se(coef)      z        p\n## trt                          -0.011025  0.989035  0.189062 -0.058    0.953\n## prior                        -0.006107  0.993912  0.020355 -0.300    0.764\n## karno:strata(tgroup)tgroup=1 -0.048755  0.952414  0.006222 -7.836 4.64e-15\n## karno:strata(tgroup)tgroup=2  0.008050  1.008083  0.012823  0.628    0.530\n## karno:strata(tgroup)tgroup=3 -0.008349  0.991686  0.014620 -0.571    0.568\n## \n## Likelihood ratio test=63.04  on 5 df, p=2.857e-12\n## n= 225, number of events= 128\n\n结果表明karno这个变量只有在tgroup=1（第1层，前3个月）才有意义，后面两层是没有意义的。\n再次进行PH检验：\n\ncox.zph(fit2)\n##                      chisq df     p\n## trt                   1.72  1 0.189\n## prior                 3.81  1 0.051\n## karno:strata(tgroup)  3.04  3 0.385\n## GLOBAL                8.03  5 0.154\n\n这时karno:strata(tgroup)就满足了等比例风险假设。\n\n\n25.4.2 连续性时依系数变换\n除了对时间进行分层外，还有一种解决方法。\n上面的图中我们可以看出karno系数随时间变化的曲线明显不是线性的，我们可以通过数据变换把它变成类似线性的，比如取log，这种变换通过tt(time transform)函数实现。\n这种方法实际上是通过tt()函数构建了一个时依协变量，但是这样做是为了解决系数随着时间改变的问题（也就是为了解决时依系数的问题）。\n\nfit3 &lt;- coxph(Surv(time, status) ~ trt + prior + karno + tt(karno), # 对karno进行变换\n              data = veteran, \n              tt = function(x, t, ...) x * log(t+20) # 具体变换方式\n              )\nfit3\n## Call:\n## coxph(formula = Surv(time, status) ~ trt + prior + karno + tt(karno), \n##     data = veteran, tt = function(x, t, ...) x * log(t + 20))\n## \n##                coef exp(coef)  se(coef)      z        p\n## trt        0.016478  1.016614  0.190707  0.086  0.93115\n## prior     -0.009317  0.990726  0.020296 -0.459  0.64619\n## karno     -0.124662  0.882795  0.028785 -4.331 1.49e-05\n## tt(karno)  0.021310  1.021538  0.006607  3.225  0.00126\n## \n## Likelihood ratio test=53.84  on 4 df, p=5.698e-11\n## n= 137, number of events= 128\n\n此时karno的时依系数估计为：-0.124662 * log(t + 20)。\n在构建时依协变量时，可以选择x * t、x * log(t)、x * log(t + 20)、x * log(t + 200)等等，没有明确的规定，要结合结果和图示进行选择，可以参考冯国双老师的文章：一文详解时依协变量。\n我们可以把现在的时依系数估计和经过变换后的的PH检验画在一起，看看变换后的效果：\n\n# 变换后的PH检验\nzp &lt;- cox.zph(fit, transform = function(time) log(time + 20))\n\n# 画图\nplot(zp[3])\nabline(0,0, col=\"red\") # 0水平线\nabline(h=fit$coef[3], col=\"green\", lwd=2, lty=2) # 整体估计\nabline(coef(fit3)[3:4],lwd=2,lty=3,col=\"blue\") # 现在的估计\n\n\n\n\n\n\n\n\n可以看到变换后结果好多了（蓝色虚线，和黑色曲线相比较），虽然还是有一点倾斜。\n以上是两种处理不满足PH假设的方法，实际还有很多种方法，比较常用的是对时间进行分层，其他方法有机会继续介绍。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>生存分析</span>"
    ]
  },
  {
    "objectID": "1032-survival.html#参考资料",
    "href": "1032-survival.html#参考资料",
    "title": "25  生存分析",
    "section": "25.5 参考资料",
    "text": "25.5 参考资料\n\nhttp://www.sthda.com/english/wiki/survival-analysis-basics\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nsurvival包帮助文档\nhttps://mp.weixin.qq.com/s/2rwxeaF_M0UnqPi2F9JNxA",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>生存分析</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html",
    "href": "1033-survivalvis.html",
    "title": "26  生存曲线可视化",
    "section": "",
    "text": "26.1 演示数据\n使用survival包中的lung数据集用于演示，这是一份关于肺癌患者的生存数据。time是生存时间，以天为单位，status是生存状态，1代表删失，2代表死亡。\nlibrary(survival)\nlibrary(survminer)\n\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\nfit &lt;- survfit(Surv(time, status) ~ sex, data = lung)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#基本的生存曲线",
    "href": "1033-survivalvis.html#基本的生存曲线",
    "title": "26  生存曲线可视化",
    "section": "26.2 基本的生存曲线",
    "text": "26.2 基本的生存曲线\n最基本的生存曲线：\n\nggsurvplot(fit, data = lung)\n\n\n\n\n\n\n\n\n删失数据的形状可以更改，默认是+，我们可以改成自己喜欢的：\n\n# 更改删失数据的形状、大小\nggsurvplot(fit, data = lung, censor.shape=\"|\", censor.size = 4)\n\n\n\n\n\n\n\n\n字体都是可以进行更改的！\n\nggsurvplot(fit, data = lung,\n           surv.median.line = \"hv\", # 中位生存时间\n   title = \"Survival curves\", \n   subtitle = \"Based on Kaplan-Meier estimates\",\n   caption = \"created with survminer\",\n   font.title = c(16, \"bold\", \"darkblue\"), # 大小、粗细、颜色\n   font.subtitle = c(15, \"bold.italic\", \"purple\"),\n   font.caption = c(14, \"plain\", \"orange\"),\n   font.x = c(14, \"bold.italic\", \"red\"),\n   font.y = c(14, \"bold.italic\", \"darkred\"),\n   font.tickslab = c(12, \"plain\", \"darkgreen\"))\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n累积风险曲线：\n\nggsurvplot(fit,\n           fun = \"cumhaz\", \n           conf.int = TRUE, # 可信区间\n           palette = \"lancet\", # 支持ggsci配色，自定义颜色，brewer palettes中的配色，等\n           ggtheme = theme_bw() # 支持ggplot2及其扩展包的主题\n)\n\n\n\n\n\n\n\n\n累积事件曲线：\n\nggsurvplot(fit,\n           fun = \"event\", \n           conf.int = TRUE, # 可信区间\n           palette = \"grey\",\n           ggtheme = theme_pubclean() \n)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#增加-risk-table",
    "href": "1033-survivalvis.html#增加-risk-table",
    "title": "26  生存曲线可视化",
    "section": "26.3 增加 risk table",
    "text": "26.3 增加 risk table\n增加多种自定义选项：\n\nggsurvplot(\n  fit,\n  data = lung,\n  size = 1,                 # 更改线条粗细\n  # 配色方案，支持ggsci配色，自定义颜色，brewer palettes中的配色，等\n  palette = \"lancet\",\n  conf.int = TRUE,          # 可信区间\n  pval = TRUE,              # log-rank P值，也可以提供一个数值\n  #计算P值的方法，\n  pval.method = TRUE,       \n  log.rank.weights = \"1\",\n  risk.table = TRUE,        # 增加risk table\n  risk.table.col = \"strata\",# risk table根据分组使用不同颜色\n  legend.labs = c(\"Male\", \"Female\"),    # 图例标签\n  risk.table.height = 0.25, # risk table高度\n  ggtheme = theme_classic2()      # 主题，支持ggplot2及其扩展包的主题\n)\n\n\n\n\n\n\n\n\n计算P值的方法，可参考https://rpkgs.datanovia.com/survminer/articles/Specifiying_weights_in_log-rank_comparisons.html\n\nggsurvplot(\n   fit,                     \n   data = lung,             \n   risk.table = TRUE,       \n   pval = TRUE,             \n   conf.int = TRUE,         \n   xlim = c(0,500),         # 横坐标轴范围，相当于局部放大\n   xlab = \"Time in days\",   # 横坐标标题\n   break.time.by = 100,     # 横坐标刻度\n   ggtheme = theme_light(), \n   risk.table.y.text.col = T, # risk table文字注释颜色\n   risk.table.y.text = FALSE # risk table显示条形而不是文字\n)\n\n\n\n\n\n\n\n\nrisk table的各种字体也都是可以更改的！\n\nggsurvplot(fit, data = lung,\n   title = \"Survival curves\", subtitle = \"Based on Kaplan-Meier estimates\",\n   caption = \"created with survminer\",\n   font.title = c(16, \"bold\", \"darkblue\"),\n   font.subtitle = c(15, \"bold.italic\", \"purple\"),\n   font.caption = c(14, \"plain\", \"orange\"),\n   font.x = c(14, \"bold.italic\", \"red\"),\n   font.y = c(14, \"bold.italic\", \"darkred\"),\n   font.tickslab = c(12, \"plain\", \"darkgreen\"),\n   ########## risk table #########,\n   risk.table = TRUE,\n   risk.table.title = \"Note the risk set sizes\",\n   risk.table.subtitle = \"and remember about censoring.\",\n   risk.table.caption = \"source code: website.com\",\n   risk.table.height = 0.45)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#增加删失时间表ncensor-plot",
    "href": "1033-survivalvis.html#增加删失时间表ncensor-plot",
    "title": "26  生存曲线可视化",
    "section": "26.4 增加删失时间表ncensor plot",
    "text": "26.4 增加删失时间表ncensor plot\n\nggsurvplot(fit, data = lung, risk.table = TRUE, ncensor.plot = TRUE)\n\n\n\n\n\n\n\n\nncensor plot的字体也是支持各种设置的。\n\nggsurvplot(fit, data = lung,\n   title = \"Survival curves\", subtitle = \"Based on Kaplan-Meier estimates\",\n   caption = \"created with survminer\",\n   font.title = c(16, \"bold\", \"darkblue\"),\n   font.subtitle = c(15, \"bold.italic\", \"purple\"),\n   font.caption = c(14, \"plain\", \"orange\"),\n   font.x = c(14, \"bold.italic\", \"red\"),\n   font.y = c(14, \"bold.italic\", \"darkred\"),\n   font.tickslab = c(12, \"plain\", \"darkgreen\"),\n   ########## risk table #########,\n   risk.table = TRUE,\n   risk.table.title = \"Note the risk set sizes\",\n   risk.table.subtitle = \"and remember about censoring.\",\n   risk.table.caption = \"source code: website.com\",\n   risk.table.height = 0.2,\n   ## ncensor plot ##\n   ncensor.plot = TRUE,\n   ncensor.plot.title = \"Number of censorings\",\n   ncensor.plot.subtitle = \"over the time.\",\n   ncensor.plot.caption = \"data available at data.com\",\n   ncensor.plot.height = 0.25)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#超级无敌精细化自定设置",
    "href": "1033-survivalvis.html#超级无敌精细化自定设置",
    "title": "26  生存曲线可视化",
    "section": "26.5 超级无敌精细化自定设置",
    "text": "26.5 超级无敌精细化自定设置\n首先设置好自己的默认样式：\n\nggsurv &lt;- ggsurvplot(\n           fit,                     \n           data = lung,             \n           risk.table = TRUE,       \n           pval = TRUE,             \n           conf.int = TRUE,         \n           palette = c(\"#E7B800\", \"#2E9FDF\"),\n           xlim = c(0,500),         \n           xlab = \"Time in days\",   \n           break.time.by = 100,     \n           ggtheme = theme_light(), \n          risk.table.y.text.col = T,\n          risk.table.height = 0.25, \n          risk.table.y.text = FALSE,\n          ncensor.plot = TRUE,      \n          ncensor.plot.height = 0.25,\n          conf.int.style = \"step\",  # customize style of confidence intervals\n          surv.median.line = \"hv\",  \n          legend.labs = c(\"Male\", \"Female\")    \n        )\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\nggsurv\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n自定义一个函数，用来更改各种样式：\n\ncustomize_labels &lt;- function (p, font.title = NULL,\n                              font.subtitle = NULL, font.caption = NULL,\n                              font.x = NULL, font.y = NULL, font.xtickslab = NULL, font.ytickslab = NULL)\n{\n  original.p &lt;- p\n  if(is.ggplot(original.p)) list.plots &lt;- list(original.p)\n  else if(is.list(original.p)) list.plots &lt;- original.p\n  else stop(\"Can't handle an object of class \", class (original.p))\n  .set_font &lt;- function(font){\n    font &lt;- ggpubr:::.parse_font(font)\n    ggtext::element_markdown (size = font$size, face = font$face, colour = font$color)\n  }\n  for(i in 1:length(list.plots)){\n    p &lt;- list.plots[[i]]\n    if(is.ggplot(p)){\n      if (!is.null(font.title)) p &lt;- p + theme(plot.title = .set_font(font.title))\n      if (!is.null(font.subtitle)) p &lt;- p + theme(plot.subtitle = .set_font(font.subtitle))\n      if (!is.null(font.caption)) p &lt;- p + theme(plot.caption = .set_font(font.caption))\n      if (!is.null(font.x)) p &lt;- p + theme(axis.title.x = .set_font(font.x))\n      if (!is.null(font.y)) p &lt;- p + theme(axis.title.y = .set_font(font.y))\n      if (!is.null(font.xtickslab)) p &lt;- p + theme(axis.text.x = .set_font(font.xtickslab))\n      if (!is.null(font.ytickslab)) p &lt;- p + theme(axis.text.y = .set_font(font.ytickslab))\n      list.plots[[i]] &lt;- p\n    }\n  }\n  if(is.ggplot(original.p)) list.plots[[1]]\n  else list.plots\n}\n\n然后分别对上面图形的3个部分（生存曲线、risk table、ncensor plot）进行个性化自定义\n\n# 更改生存曲线的标签\nggsurv$plot &lt;- ggsurv$plot + labs(\n  title    = \"Survival curves\",\n  subtitle = \"Based on Kaplan-Meier estimates\",\n  caption  = \"created with survminer\"\n  )\n\n# 更改risk table的标签\nggsurv$table &lt;- ggsurv$table + labs(\n  title    = \"Note the risk set sizes\",\n  subtitle = \"and remember about censoring.\",\n  caption  = \"source code: website.com\"\n  )\n\n# 更改ncensor plot的标签 \nggsurv$ncensor.plot &lt;- ggsurv$ncensor.plot + labs(\n  title    = \"Number of censorings\",\n  subtitle = \"over the time.\",\n  caption  = \"source code: website.com\"\n  )\n\n# 更改生存曲线，risk table，ncensor plot的字体大小、类型、颜色\n\nggsurv &lt;- customize_labels(\n  ggsurv,\n  font.title    = c(16, \"bold\", \"darkblue\"),\n  font.subtitle = c(15, \"bold.italic\", \"purple\"),\n  font.caption  = c(14, \"plain\", \"orange\"),\n  font.x        = c(14, \"bold.italic\", \"red\"),\n  font.y        = c(14, \"bold.italic\", \"darkred\"),\n  font.xtickslab = c(12, \"plain\", \"darkgreen\")\n)\n## Warning: `is.ggplot()` was deprecated in ggplot2 3.5.2.\n## ℹ Please use `is_ggplot()` instead.\n\nggsurv\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#多个组的生存曲线",
    "href": "1033-survivalvis.html#多个组的生存曲线",
    "title": "26  生存曲线可视化",
    "section": "26.6 多个组的生存曲线",
    "text": "26.6 多个组的生存曲线\n如果你的分类变量是多个组别的（常见的都是两组比较的），会自动画出多条生存曲线。如果你有多个分类自变量，会自动画出所有组合的生存曲线。\n使用colon数据集，其中time是时间，status是生存状态，1为发生终点事件，0为删失，rx是治疗方式，有三种：observation、Levamisole、Levamisole+5-FU，obstruct是肿瘤是否阻塞结肠，有为1，无为0，adhere是肿瘤是否粘附附近器官，有为1，无为0。\n\nrm(list = ls())\nlibrary(survival)\nlibrary(survminer)\n\npsych::headTail(colon)\n##       id study      rx sex age obstruct perfor adhere nodes status differ\n## 1      1     1 Lev+5FU   1  43        0      0      0     5      1      2\n## 2      1     1 Lev+5FU   1  43        0      0      0     5      1      2\n## 3      2     1 Lev+5FU   1  63        0      0      0     1      0      2\n## 4      2     1 Lev+5FU   1  63        0      0      0     1      0      2\n## ...  ...   ...    &lt;NA&gt; ... ...      ...    ...    ...   ...    ...    ...\n## 1855 928     1 Lev+5FU   0  48        1      0      0     4      0      2\n## 1856 928     1 Lev+5FU   0  48        1      0      0     4      0      2\n## 1857 929     1     Lev   0  66        1      0      0     1      0      2\n## 1858 929     1     Lev   0  66        1      0      0     1      0      2\n##      extent surg node4 time etype\n## 1         3    0     1 1521     2\n## 2         3    0     1  968     1\n## 3         3    0     0 3087     2\n## 4         3    0     0 3087     1\n## ...     ...  ...   ...  ...   ...\n## 1855      3    1     1 2072     2\n## 1856      3    1     1 2072     1\n## 1857      3    0     0 1820     2\n## 1858      3    0     0 1820     1\n\n# 两个分类变量\nfit2 &lt;- survfit( Surv(time, status) ~ rx + obstruct, data = colon )\n\n# 结果会给出所有组合的生存曲线\nggsurvplot(fit2, pval = TRUE, \n           risk.table = TRUE,\n           risk.table.height = 0.3\n           )",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#多个分类变量分面绘制",
    "href": "1033-survivalvis.html#多个分类变量分面绘制",
    "title": "26  生存曲线可视化",
    "section": "26.7 多个分类变量分面绘制",
    "text": "26.7 多个分类变量分面绘制\n还是以colon数据集为例，这次我们用3个变量：sex/rx/adhere，这3个都是分类变量。\n首先构建生存函数：\n\nfit3 &lt;- survfit(Surv(time, status) ~ sex + rx + adhere, data = colon )\n\n然后把生存曲线保存为一个对象：\n\nggsurv &lt;- ggsurvplot(fit3, data = colon,\n  fun = \"cumhaz\", conf.int = TRUE,\n  risk.table = TRUE, risk.table.col=\"strata\",\n  ggtheme = theme_bw())\n\n接下来就可以分别提取生存曲线（这里是cumhaz，累积风险曲线）、risk table、删失事件表，根据不同的变量进行分面即可：\n\n# 分面累积风险曲线\ncurv_facet &lt;- ggsurv$plot + facet_grid(rx ~ adhere)\ncurv_facet\n\n\n\n\n\n\n\n\n\n# 分面risk table，和上面的累积风险曲线分面方法一样\nggsurv$table + facet_grid(rx ~ adhere, scales = \"free\")+\n theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n# risk table另一种分面方法，由于有3个分类变量，可以选择自己需要的分面方法\ntbl_facet &lt;- ggsurv$table + facet_grid(.~ adhere, scales = \"free\")\ntbl_facet + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n# 重新安排下布局，把生存曲线和risk table画在一起\ng2 &lt;- ggplotGrob(curv_facet)\ng3 &lt;- ggplotGrob(tbl_facet)\nmin_ncol &lt;- min(ncol(g2), ncol(g3))\ng &lt;- gridExtra::gtable_rbind(g2[, 1:min_ncol], g3[, 1:min_ncol], size=\"last\")\ng$widths &lt;- grid::unit.pmax(g2$widths, g3$widths)\ngrid::grid.newpage()\ngrid::grid.draw(g)\n\n\n\n\n\n\n\n\n如果想根据某个变量进行分组绘制生存曲线，然后分面展示，也可以用ggsurvplot_facet()实现：\n\nfit &lt;- survfit( Surv(time, status) ~ sex, data = colon )\n\n# 根据rx进行分组，展示每个组内的生存曲线\nggsurvplot_facet(fit, colon, \n                 facet.by = \"rx\",\n                 palette = \"jco\", \n                 pval = TRUE)\n\n\n\n\n\n\n\n\n还可以根据多个变量进行分面展示：\n\nggsurvplot_facet(fit, colon, facet.by = c(\"rx\", \"adhere\"),\n                palette = \"jco\", pval = TRUE)\n\n\n\n\n\n\n\n\n\nfit2 &lt;- survfit( Surv(time, status) ~ sex + rx, data = colon )\nggsurvplot_facet(fit2, colon, facet.by = \"adhere\",\n                palette = \"jco\", pval = TRUE)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#同时绘制多个生存函数",
    "href": "1033-survivalvis.html#同时绘制多个生存函数",
    "title": "26  生存曲线可视化",
    "section": "26.8 同时绘制多个生存函数",
    "text": "26.8 同时绘制多个生存函数\n\ndata(colon)\n## Warning in data(colon): data set 'colon' not found\nf1 &lt;- survfit(Surv(time, status) ~ adhere, data = colon)\nf2 &lt;- survfit(Surv(time, status) ~ rx, data = colon)\nfits &lt;- list(sex = f1, rx = f2)\n\n# 一下子画好！在循环出图时有用处\nlegend.title &lt;- list(\"sex\", \"rx\")\nggsurvplot_list(fits, colon, legend.title = legend.title)\n## $sex\n\n\n\n\n\n\n\n## \n## $rx\n\n\n\n\n\n\n\n## \n## attr(,\"class\")\n## [1] \"list\"            \"ggsurvplot_list\"",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#根据某一个变量分组绘制",
    "href": "1033-survivalvis.html#根据某一个变量分组绘制",
    "title": "26  生存曲线可视化",
    "section": "26.9 根据某一个变量分组绘制",
    "text": "26.9 根据某一个变量分组绘制\n比如以colon数据为例，我们想以rx（治疗方式）进行分组，然后看每个组内的生存曲线，可以通过ggsurvplot_group_by()实现。\n\nrm(list = ls())\n\nfit &lt;- survfit( Surv(time, status) ~ sex, data = colon )\n\n# Visualize: grouped by treatment rx\n#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\nggsurv.list &lt;- ggsurvplot_group_by(fit, colon, group.by = \"rx\", risk.table = TRUE,\n                                 pval = TRUE, conf.int = TRUE, palette = \"jco\")\nnames(ggsurv.list)\n## [1] \"rx.Obs::sex\"     \"rx.Lev::sex\"     \"rx.Lev+5FU::sex\"\n\n这个图形和上面的分面展示中的ggsurvplot_facet画出来的图形是一样的，区别就是一个是分面，这个是分开多个图形！\n可以根据多个变量进行分组，比如下面这个情况，会分别绘制6张生存曲线图：\n\n# Visualize: grouped by treatment rx and adhere\nggsurv.list &lt;- ggsurvplot_group_by(fit, colon, group.by = c(\"rx\", \"adhere\"),\n                                 risk.table = TRUE,\n                                 pval = TRUE, conf.int = TRUE, palette = \"jco\")\n\n# 6张图的名字，图没有画出来，感兴趣的可以自己试试看\nnames(ggsurv.list)\n## [1] \"rx:Obs, adhere:0::sex\"     \"rx:Obs, adhere:1::sex\"    \n## [3] \"rx:Lev, adhere:0::sex\"     \"rx:Lev, adhere:1::sex\"    \n## [5] \"rx:Lev+5FU, adhere:0::sex\" \"rx:Lev+5FU, adhere:1::sex\"",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#在原有生存曲线的基础上增加",
    "href": "1033-survivalvis.html#在原有生存曲线的基础上增加",
    "title": "26  生存曲线可视化",
    "section": "26.10 在原有生存曲线的基础上增加",
    "text": "26.10 在原有生存曲线的基础上增加\n先画好一个生存曲线图，然后在原图的基础上添加新的生存曲线图，类似于base r中常用的add = T，比如在这篇推文中介绍的：多个时间点和多指标生存曲线\n\nlibrary(survival)\n\n# 注意这里的surv_fit，是survfit的封装\nfit &lt;- surv_fit(Surv(time, status) ~ sex, data = lung)\n\n# Visualize survival curves\nggsurvplot(fit, data = lung,\n          risk.table = TRUE, pval = TRUE,\n          surv.median.line = \"hv\", palette = \"jco\")\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 2 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n在上面图形的基础上添加所有人的总的生存曲线：\n\n# Add survival curves of pooled patients (Null model)\n# Use add.all = TRUE option\nggsurvplot(fit, data = lung,\n          risk.table = TRUE, pval = TRUE,\n          surv.median.line = \"hv\", palette = \"jco\",\n          add.all = TRUE)\n## Warning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 3 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 3 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 3 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 3 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#多个生存函数画在一起",
    "href": "1033-survivalvis.html#多个生存函数画在一起",
    "title": "26  生存曲线可视化",
    "section": "26.11 多个生存函数画在一起",
    "text": "26.11 多个生存函数画在一起\n比如把PFS和OS的生存曲线画在一张图上。\n\nrm(list = ls())\n# 构建一个示例数据集\nset.seed(123)\ndemo.data &lt;- data.frame(\n   os.time = colon$time,\n   os.status = colon$status,\n   pfs.time = sample(colon$time),\n   pfs.status = colon$status,\n   sex = colon$sex, rx = colon$rx, adhere = colon$adhere\n )\n\n# 总体的PFS和OS生存曲线\npfs &lt;- survfit( Surv(pfs.time, pfs.status) ~ 1, data = demo.data)\nos &lt;- survfit( Surv(os.time, os.status) ~ 1, data = demo.data)\n\n# Combine on the same plot\nfit &lt;- list(PFS = pfs, OS = os)\nggsurvplot_combine(fit, demo.data)\n\n\n\n\n\n\n\n\n这个情况你用ggsurvplot_list也能画，不过就是分开的两个图形了！\n如果是分类变量会自动画出多条生存曲线：\n\npfs &lt;- survfit( Surv(pfs.time, pfs.status) ~ rx, data = demo.data)\nos &lt;- survfit( Surv(os.time, os.status) ~ rx, data = demo.data)\n# Combine on the same plot\nfit &lt;- list(PFS = pfs, OS = os)\nggsurvplot_combine(fit, demo.data)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1033-survivalvis.html#参考资料",
    "href": "1033-survivalvis.html#参考资料",
    "title": "26  生存曲线可视化",
    "section": "26.12 参考资料",
    "text": "26.12 参考资料\n\nsurvminer包帮助文档",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>生存曲线可视化</span>"
    ]
  },
  {
    "objectID": "1020-discriminant.html",
    "href": "1020-discriminant.html",
    "title": "27  判别分析",
    "section": "",
    "text": "27.1 Fisher判别分析\nFisher判别又称为典型判别（canonical discriminant）分析，适用于两类和多分类判别。\nFisher判别使用贝叶斯定理确定每个观测属于某个类别的概率。如果你有两个类别，比如良性和恶性，判别分析会分别计算属于两个类别的概率，然后选择概率大的类别作为正确的类别。\n线性判别分析假设每个类中的观测服从多元正态分布，并且不同类别之间的协方差相等。二次判别假设观测服从正态分布，每种类别都有自己的协方差。\n使用孙振球版《医学统计学》第4版例20-1的数据。电子版及配套数据已上传到QQ群，需要的加群下载即可。\n收集了22例肝硬化患者的3个指标，其中早期患者（用1表示）12例，晚期患者（用2表示），试做判别分析。\ndf &lt;- read.csv(\"datasets/例20-1.csv\")\n\npsych::headTail(df)\n##      id  x1  x2  x3   y\n## 1     1  23   8   0   1\n## 2     2  -1   9  -2   1\n## 3     3 -10   5   0   1\n## 4     4  -7  -2   1   1\n## ... ... ... ... ... ...\n## 19   19  -9 -20   3   2\n## 20   20  -7  -2   3   2\n## 21   21  -9   6   0   2\n## 22   22  12   0   0   2\n这个数据集中id是编号，x1,x2,x3是自变量，y是因变量。\n线性判别分析可以通过MASS包中的lda函数实现：\nlibrary(MASS)\n\nfit &lt;- lda(y ~ x1+x2+x3, data = df)\nfit\n## Call:\n## lda(y ~ x1 + x2 + x3, data = df)\n## \n## Prior probabilities of groups:\n##         1         2 \n## 0.5454545 0.4545455 \n## \n## Group means:\n##   x1 x2 x3\n## 1 -3  4 -1\n## 2  4 -5  1\n## \n## Coefficients of linear discriminants:\n##           LD1\n## x1  0.0395150\n## x2 -0.1265698\n## x3  0.1792631\nPrior probabilities of groups是先验概率，类别1的概率是0.5454545，类别2是0.4545455。\n然后给出了每个组在不同类别中的均值。\n最下面给出了线性判别系数，如果你的结果变量是3个类别，会给出两组判别系数，这里我的结果变量只有2分类，所以结果只有1组。\n结果可以画出来：\nplot(fit,type=\"both\")\n上图是判别分析结果的直方图和密度图，可以看出组间有重合，说明有些分组分错了。\n下面用predict提取判别分析的分类结果。\npredict用于判别分析可以得到3种类型的结果，class是类别，posterior是概率，x是线性判别评分。\npred &lt;- predict(fit)$class\ntable(df$y, pred)\n##    pred\n##      1  2\n##   1 11  1\n##   2  2  8\n可以看到有3个分类分错了，结果还是可以的。\n可以查看每个患者的后验概率：\n# 查看概率\npredict(fit)$posterior\n##             1           2\n## 1  0.62566758 0.374332416\n## 2  0.95508370 0.044916302\n## 3  0.89600449 0.103995511\n## 4  0.51330556 0.486694443\n## 5  0.95464457 0.045355435\n## 6  0.88314148 0.116858515\n## 7  0.77454260 0.225457398\n## 8  0.99508599 0.004914013\n## 9  0.89391137 0.106088634\n## 10 0.84899794 0.151002059\n## 11 0.31960372 0.680396284\n## 12 0.64144092 0.358559076\n## 13 0.14903037 0.850969632\n## 14 0.57026493 0.429735074\n## 15 0.13106732 0.868932682\n## 16 0.26925350 0.730746503\n## 17 0.03911397 0.960886034\n## 18 0.04332382 0.956676176\n## 19 0.01115243 0.988847571\n## 20 0.35826933 0.641730670\n## 21 0.90954200 0.090457999\n## 22 0.37480490 0.625195100\n上面的图我们也可以用ggplot2画出来。\ndf.plot &lt;- data.frame(LD1 = predict(fit)$x[,1],\n                      y = factor(df$y,labels = c(\"早期患者\",\"晚期患者\"))\n                      )\n\nlibrary(ggplot2)\n\nggplot(df.plot, aes(x=LD1, fill=y))+\n  geom_histogram()+\n  facet_wrap(~ y, ncol = 1)\n如果你想用这个模型预测新的数据，只需要predict(fit, newdata = xxx)即可。比如我们新建一个数据：\ntmp &lt;- data.frame(x1 = c(-9,-7,-9),\n                  x2 = c(-18,-2,6),\n                  x3 = c(3,3,1)\n                  )\n\npredict(fit, newdata = tmp)\n## $class\n## [1] 2 2 1\n## Levels: 1 2\n## \n## $posterior\n##            1         2\n## 1 0.01736557 0.9826344\n## 2 0.35826933 0.6417307\n## 3 0.87974275 0.1202573\n## \n## $x\n##          LD1\n## 1  2.4580167\n## 2  0.5119296\n## 3 -0.9381851\n这样就得到新的结果。\n我们再用一个iris鸢尾花数据集演示下线性判别分析的结果可视化，这个结果变量是3分类的。\nstr(iris)\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n拟合模型：\nlibrary(MASS)\n\nfit &lt;- lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris)\nfit\n## Call:\n## lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, \n##     data = iris)\n## \n## Prior probabilities of groups:\n##     setosa versicolor  virginica \n##  0.3333333  0.3333333  0.3333333 \n## \n## Group means:\n##            Sepal.Length Sepal.Width Petal.Length Petal.Width\n## setosa            5.006       3.428        1.462       0.246\n## versicolor        5.936       2.770        4.260       1.326\n## virginica         6.588       2.974        5.552       2.026\n## \n## Coefficients of linear discriminants:\n##                     LD1         LD2\n## Sepal.Length  0.8293776 -0.02410215\n## Sepal.Width   1.5344731 -2.16452123\n## Petal.Length -2.2012117  0.93192121\n## Petal.Width  -2.8104603 -2.83918785\n## \n## Proportion of trace:\n##    LD1    LD2 \n## 0.9912 0.0088\n可视化结果：\niris$LD1 &lt;- predict(fit)$x[,1]\niris$LD2 &lt;- predict(fit)$x[,2]\n\nlibrary(ggplot2)\n\nggplot(iris, aes(LD1,LD2))+\n  geom_point(aes(color=Species),size=3)\nggplot(iris, aes(x=LD1, fill=Species))+\n  geom_histogram()+\n  facet_wrap(~ Species, ncol = 1)\n二次判别分析和线性判别分析用法一样。\nfit &lt;- qda(y ~ x1+x2+x3, data = df)\nfit\n## Call:\n## qda(y ~ x1 + x2 + x3, data = df)\n## \n## Prior probabilities of groups:\n##         1         2 \n## 0.5454545 0.4545455 \n## \n## Group means:\n##   x1 x2 x3\n## 1 -3  4 -1\n## 2  4 -5  1\n结果不含判别系数，查看分类结果：\npred &lt;- predict(fit)$class\ntable(df$y, pred)\n##    pred\n##      1  2\n##   1 10  2\n##   2  1  9\n也是3个分错了。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>判别分析</span>"
    ]
  },
  {
    "objectID": "1020-discriminant.html#bayes判别分析",
    "href": "1020-discriminant.html#bayes判别分析",
    "title": "27  判别分析",
    "section": "27.2 Bayes判别分析",
    "text": "27.2 Bayes判别分析\n贝叶斯判别也是根据概率大小进行判别，要求各类近似服从多元正态分布。当各类的协方差相等时，可得到线性贝叶斯判别函数，当各类的协方差不相等时，可得到二次贝叶斯判别函数。\n欲用4个标化后的影像学指标鉴别脑囊肿（1）、胶质瘤（2）、转移瘤（3），收集了17个病例，试建立判别贝叶斯函数。\n\ndf &lt;- read.csv(\"datasets/例20-4.csv\")\n\ndf$y &lt;- factor(df$y)\n\npsych::headTail(df)\n##       x1    x2  x3  x4    y\n## 1      6 -11.5  19  90    1\n## 2    -11 -18.5  25 -36    3\n## 3   90.2   -17  17   3    2\n## 4     -4   -15  13  54    1\n## ...  ...   ... ... ... &lt;NA&gt;\n## 14    10   -18  14  50    1\n## 15    -8   -14  16  56    1\n## 16   0.6   -13  26  21    3\n## 17   -40   -20  22 -50    3\n\n使用klaR包实现贝叶斯判别分析：\n\nlibrary(klaR)\n\nfit &lt;- NaiveBayes(y ~ ., data = df)\nfit\n## $apriori\n## grouping\n##         1         2         3 \n## 0.4117647 0.2352941 0.3529412 \n## \n## $tables\n## $tables$x1\n##        [,1]     [,2]\n## 1 -14.42857 38.26163\n## 2   0.80000 78.10779\n## 3  -6.65000 19.78017\n## \n## $tables$x2\n##        [,1]     [,2]\n## 1 -17.34286 4.103599\n## 2 -17.42500 3.085855\n## 3 -17.33333 4.143268\n## \n## $tables$x3\n##       [,1]     [,2]\n## 1 12.71429 4.990467\n## 2 17.50000 2.081666\n## 3 20.16667 6.493587\n## \n## $tables$x4\n##        [,1]     [,2]\n## 1  31.14286 44.03948\n## 2   0.00000 30.75711\n## 3 -15.00000 35.83295\n## \n## \n## $levels\n## [1] \"1\" \"2\" \"3\"\n## \n## $call\n## NaiveBayes.default(x = X, grouping = Y)\n## \n## $x\n##        x1    x2 x3  x4\n## 1     6.0 -11.5 19  90\n## 2   -11.0 -18.5 25 -36\n## 3    90.2 -17.0 17   3\n## 4    -4.0 -15.0 13  54\n## 5     0.0 -14.0 20  35\n## 6     0.5 -11.5 19  37\n## 7   -10.0 -19.0 21 -42\n## 8     0.0 -23.0  5 -35\n## 9    20.0 -22.0  8 -20\n## 10 -100.0 -21.4  7 -15\n## 11 -100.0 -21.5 15 -40\n## 12   13.0 -17.2 18   2\n## 13   -5.0 -18.5 15  18\n## 14   10.0 -18.0 14  50\n## 15   -8.0 -14.0 16  56\n## 16    0.6 -13.0 26  21\n## 17  -40.0 -20.0 22 -50\n## \n## $usekernel\n## [1] FALSE\n## \n## $varnames\n## [1] \"x1\" \"x2\" \"x3\" \"x4\"\n## \n## attr(,\"class\")\n## [1] \"NaiveBayes\"\n\n获取预测结果，并查看混淆矩阵：\n\npred &lt;- predict(fit)$class\n## Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n## observation 10\ntable(pred, df$y)\n##     \n## pred 1 2 3\n##    1 7 0 1\n##    2 0 3 0\n##    3 0 1 5\n\n只有两个分错了。\n如果要预测新的数据，只需要predict(fit, newdata = xxx)即可。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>判别分析</span>"
    ]
  },
  {
    "objectID": "1021-cluster.html",
    "href": "1021-cluster.html",
    "title": "28  聚类分析",
    "section": "",
    "text": "28.1 系统聚类（层次聚类,Hierarchical clustering）\n# 没安装flexclust包的需要先安装\ndata(nutrient, package = \"flexclust\")\nrow.names(nutrient) &lt;- tolower(row.names(nutrient))\n\ndim(nutrient) # 27行5列\n## [1] 27  5\n\npsych::headTail(nutrient)\n##                 energy protein fat calcium iron\n## beef braised       340      20  28       9  2.6\n## hamburger          245      21  17       9  2.7\n## beef roast         420      15  39       7    2\n## beef steak         375      19  32       9  2.6\n## ...                ...     ... ...     ...  ...\n## salmon canned      120      17   5     159  0.7\n## sardines canned    180      22   9     367  2.5\n## tuna canned        170      25   7       7  1.2\n## shrimp canned      110      23   1      98  2.6\n层次聚类在R语言中非常简单，通过hclust实现。\n# 聚类前先进行标准化\nnutrient.scaled &lt;- scale(nutrient)\nh.clust &lt;- hclust(dist(nutrient.scaled,method = \"euclidean\"), # 计算距离有不同方法\n                  method = \"average\" # 层次聚类有不同方法\n                  )\n下面就是画图，简单点可以直接用plot()。\nplot(h.clust,hang = -1,main = \"层次聚类\", sub=\"\", \n     xlab=\"\", cex.lab = 1.0, cex.axis = 1.0, cex.main = 2)\n关于更加精细化的细节修改，下面会介绍。或者可以借助其他R包快速绘制好看的聚类分析图形。\n如何选择聚类的个数呢？\n可以通过R包NbClust实现。\nlibrary(NbClust)\n\nnc &lt;- NbClust(nutrient.scaled, distance = \"euclidean\",\n              min.nc = 2, # 最小聚类数\n              max.nc = 10, # 最大聚类树\n              method = \"average\"\n              )\n\n\n\n\n\n\n\n## *** : The Hubert index is a graphical method of determining the number of clusters.\n##                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n##                 significant increase of the value of the measure i.e the significant peak in Hubert\n##                 index second differences plot. \n## \n\n\n\n\n\n\n\n## *** : The D index is a graphical method of determining the number of clusters. \n##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n##                 second differences plot) that corresponds to a significant increase of the value of\n##                 the measure. \n##  \n## ******************************************************************* \n## * Among all indices:                                                \n## * 5 proposed 2 as the best number of clusters \n## * 5 proposed 3 as the best number of clusters \n## * 2 proposed 4 as the best number of clusters \n## * 4 proposed 5 as the best number of clusters \n## * 1 proposed 8 as the best number of clusters \n## * 1 proposed 9 as the best number of clusters \n## * 5 proposed 10 as the best number of clusters \n## \n##                    ***** Conclusion *****                            \n##  \n## * According to the majority rule, the best number of clusters is  2 \n##  \n##  \n## *******************************************************************\n输出日志里给出了评判准则以及最终结果：Hubert index和D index使用图形的方式判断最佳聚类个数，拐点明显的可视作最佳聚类个数。\n它给出的结论是最佳聚类数是2。我们也可以通过条形图查看这些评判准则的具体数量。\nbarplot(table(nc$Best.nc[1,]),\n        xlab = \"聚类数目\",\n        ylab = \"评判准则个数\"\n        )\n从条形图中可以看出，聚类数目为2,3,5,10时，评判准则个数最多，为5个，这里我们可以选择5个。\n# 把聚类树划分为5类\ncluster &lt;- cutree(h.clust, k=5)\n\n# 查看每一类有多少例\ntable(cluster)\n## cluster\n##  1  2  3  4  5 \n##  7 16  1  2  1\n把最终结果画出来：\nplot(h.clust, hang = -1,main = \"\",xlab = \"\")\nrect.hclust(h.clust, k=5) # 添加矩形，方便观察",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>聚类分析</span>"
    ]
  },
  {
    "objectID": "1021-cluster.html#聚类分析可视化",
    "href": "1021-cluster.html#聚类分析可视化",
    "title": "28  聚类分析",
    "section": "28.2 聚类分析可视化",
    "text": "28.2 聚类分析可视化\n上面使用默认的plot函数进行聚类树的可视化，下面继续扩展聚类树的可视化。\n默认的聚类树可视化函数已经非常好用，有非常多的自定义设置，可以轻松实现好看的聚类树可视化。\n\nop &lt;- par(bg = \"grey90\")\n\nplot(h.clust, main = \"层次聚类\", sub=\"\", xlab = \"\",\n     col = \"#487AA1\", col.main = \"#45ADA8\", col.lab = \"#7C8071\",\n     col.axis = \"#F38630\", lwd = 2, lty = 1, hang = -1, axes = FALSE)\n# add axis\naxis(side = 2, at = 0:5, col = \"#F38630\",\n     labels = FALSE, lwd = 2)\n# add text in margin\nmtext(0:5, side = 2, at = 0:5,\n      line = 1, col = \"#A38630\", las = 2)\n\n\n\n\n\n\n\n\npar(op)\n\n如果对默认的可视化效果不满意，可以先用as.dendrogram()转化一下，再画图可以指定更多细节。\n\ndhc &lt;- as.dendrogram(h.clust)\nplot(dhc,type = \"triangle\") # 比如换个类型\n\n\n\n\n\n\n\n\n可以提取部分树进行查看，使用cut指定某个高度以上或以下的树进行查看。\n\nop &lt;- par(mfrow = c(2, 1))\n\n# 高度在3以上的树\nplot(cut(dhc, h = 3)$upper, main = \"Upper tree of cut at h=3\")\n\n# 高度在3以下的树\nplot(cut(dhc, h = 3)$lower[[2]],\n     main = \"Second branch of lower tree with cut at h=3\")\n\n\n\n\n\n\n\n\npar(op)\n\n每一个节点都有不同的属性，比如颜色、形状等，我们可以用函数修改每个节点的属性。\n比如修改标签的颜色。\n\n# 按照上面画出来的结果，我们可以分为5类，所以准备好5个颜色\nlabelColors = c(\"#CDB380\", \"#036564\", \"#EB6841\", \"#EDC951\", \"#487AA1\")\n\n# 把聚类树分为5个类\nclusMember &lt;- cutree(h.clust,k=5)\n\n# 给标签增加不同的颜色\ncolLab &lt;- function(n) {\n  if (is.leaf(n)) {\n    a &lt;- attributes(n)\n    labCol &lt;- labelColors[clusMember[which(names(clusMember) == a$label)]]\n    attr(n, \"nodePar\") &lt;- c(a$nodePar,\n                            list(cex=1.5, # 节点形状大小\n                                 pch=20, # 节点形状\n                                 col=labCol, # 节点颜色\n                                 lab.col=labCol, # 标签颜色\n                                 lab.font=2, # 标签字体，粗体斜体粗斜体\n                                 lab.cex=1 # 标签大小\n                                 )\n                            )\n  }\n  n\n}\n\n# 把自定义标签颜色应用到聚类树中\ndiyDendro = dendrapply(dhc, colLab)    \n\n# 画图\nplot(diyDendro, main = \"DIY Dendrogram\")  \n\n# 加图例\nlegend(\"topright\", \n     legend = c(\"Cluster 1\",\"Cluster 2\",\"Cluster 3\",\"Cluster 4\",\"Cluster 5\"), \n     col = c(\"#CDB380\", \"#036564\", \"#EB6841\", \"#EDC951\", \"#487AA1\"), \n     pch = c(20,20,20,20,20), bty = \"n\", pt.cex = 2, cex = 1 , \n     text.col = \"black\", horiz = FALSE, inset = c(0, 0.1))\n\n\n\n\n\n\n\n\n如果想要更加精美的聚类分析可视化，可以参考之前的几篇推文：\n\n又是聚类分析可视化\nR语言可视化聚类树\nR语言画好看的聚类树\n\n参考资料：\n\nR帮助文档\nhttps://r-graph-gallery.com/31-custom-colors-in-dendrogram.html\nhttps://www.gastonsanchez.com/visually-enforced/how-to/2012/10/0/Dendrograms/",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>聚类分析</span>"
    ]
  },
  {
    "objectID": "1021-cluster.html#快速聚类划分聚类partitioning-clustering",
    "href": "1021-cluster.html#快速聚类划分聚类partitioning-clustering",
    "title": "28  聚类分析",
    "section": "28.3 快速聚类（划分聚类,partitioning clustering）",
    "text": "28.3 快速聚类（划分聚类,partitioning clustering）\n\n28.3.1 K-means聚类\nK-means聚类，K均值聚类，是快速聚类的一种。比层次聚类更适合大样本的数据。在R语言中可以通过kmeans()实现K均值聚类。\n使用K均值聚类处理178种葡萄酒中13种化学成分的数据集。\n\ndata(wine, package = \"rattle\")\ndf &lt;- scale(wine[,-1])\n\npsych::headTail(df)\n##     Alcohol Malic   Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids\n## 1      1.51 -0.56  0.23      -1.17      1.91    0.81       1.03         -0.66\n## 2      0.25  -0.5 -0.83      -2.48      0.02    0.57       0.73         -0.82\n## 3       0.2  0.02  1.11      -0.27      0.09    0.81       1.21          -0.5\n## 4      1.69 -0.35  0.49      -0.81      0.93    2.48       1.46         -0.98\n## ...     ...   ...   ...        ...       ...     ...        ...           ...\n## 175    0.49  1.41  0.41       1.05      0.16   -0.79      -1.28          0.55\n## 176    0.33  1.74 -0.39       0.15      1.42   -1.13      -1.34          0.55\n## 177    0.21  0.23  0.01       0.15      1.42   -1.03      -1.35          1.35\n## 178    1.39  1.58  1.36        1.5     -0.26   -0.39      -1.27          1.59\n##     Proanthocyanins Color   Hue Dilution Proline\n## 1              1.22  0.25  0.36     1.84    1.01\n## 2             -0.54 -0.29   0.4     1.11    0.96\n## 3              2.13  0.27  0.32     0.79    1.39\n## 4              1.03  1.18 -0.43     1.18    2.33\n## ...             ...   ...   ...      ...     ...\n## 175           -0.32  0.97 -1.13    -1.48    0.01\n## 176           -0.42  2.22 -1.61    -1.48    0.28\n## 177           -0.23  1.83 -1.56     -1.4     0.3\n## 178           -0.42  1.79 -1.52    -1.42   -0.59\n\n进行K均值聚类时，需要在一开始就指定聚类的个数，我们也可以通过NbClust包实现这个过程。\n\nlibrary(NbClust)\n\nset.seed(123)\nnc &lt;- NbClust(df, min.nc = 2, max.nc = 15, method = \"kmeans\")# 方法选择kmeans\n\n\n\n\n\n\n\n## *** : The Hubert index is a graphical method of determining the number of clusters.\n##                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n##                 significant increase of the value of the measure i.e the significant peak in Hubert\n##                 index second differences plot. \n## \n\n\n\n\n\n\n\n## *** : The D index is a graphical method of determining the number of clusters. \n##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n##                 second differences plot) that corresponds to a significant increase of the value of\n##                 the measure. \n##  \n## ******************************************************************* \n## * Among all indices:                                                \n## * 2 proposed 2 as the best number of clusters \n## * 19 proposed 3 as the best number of clusters \n## * 1 proposed 14 as the best number of clusters \n## * 1 proposed 15 as the best number of clusters \n## \n##                    ***** Conclusion *****                            \n##  \n## * According to the majority rule, the best number of clusters is  3 \n##  \n##  \n## *******************************************************************\n\n结果中给出了划分依据以及最佳的聚类数目为3个，可以画图查看结果：\n\ntable(nc$Best.nc[1,])\n## \n##  0  1  2  3 14 15 \n##  2  1  2 19  1  1\n\nbarplot(table(nc$Best.nc[1,]),\n        xlab = \"聚类数目\",\n        ylab = \"评判准则个数\"\n        )\n\n\n\n\n\n\n\n\n可以看到聚类数目为3是最佳的选择。\n确定最佳聚类个数过程也可以通过非常好用的R包factoextra实现。\n\nlibrary(factoextra)\n\nset.seed(123)\nfviz_nbclust(df, kmeans, k.max = 15)\n\n\n\n\n\n\n\n\n这个结果给出的最佳聚类个数也是3个。\n下面进行K均值聚类，聚类数目设为3.\n\nset.seed(123)\nfit.km &lt;- kmeans(df, centers = 3, nstart = 25)\nfit.km\n## K-means clustering with 3 clusters of sizes 51, 62, 65\n## \n## Cluster means:\n##      Alcohol      Malic        Ash Alcalinity   Magnesium     Phenols\n## 1  0.1644436  0.8690954  0.1863726  0.5228924 -0.07526047 -0.97657548\n## 2  0.8328826 -0.3029551  0.3636801 -0.6084749  0.57596208  0.88274724\n## 3 -0.9234669 -0.3929331 -0.4931257  0.1701220 -0.49032869 -0.07576891\n##    Flavanoids Nonflavanoids Proanthocyanins      Color        Hue   Dilution\n## 1 -1.21182921    0.72402116     -0.77751312  0.9388902 -1.1615122 -1.2887761\n## 2  0.97506900   -0.56050853      0.57865427  0.1705823  0.4726504  0.7770551\n## 3  0.02075402   -0.03343924      0.05810161 -0.8993770  0.4605046  0.2700025\n##      Proline\n## 1 -0.4059428\n## 2  1.1220202\n## 3 -0.7517257\n## \n## Clustering vector:\n##   [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [38] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 1 3 3 3 3 3 3 3 3 3 3 3 2\n##  [75] 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 1 3 3 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n## [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n## \n## Within cluster sum of squares by cluster:\n## [1] 326.3537 385.6983 558.6971\n##  (between_SS / total_SS =  44.8 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n\n结果很详细，K均值聚类聚为3类，每一类数量分别是51,62,65。然后还给出了聚类中心，每一个观测分别属于哪一个类。\n不管是哪一种聚类方法，factoextra配合factomineR都可以给出非常好看的可视化结果。\n\nfviz_cluster(fit.km, data = df)\n\n\n\n\n\n\n\n\n有非常多的细节可以调整，大家在使用的时候可以自己尝试，和之前推文中介绍的PCA美化一样，也是支持ggplot2语法的。\n\nfviz_cluster(fit.km, data = df, \n             ellipse = T, # 增加椭圆\n             ellipse.type = \"t\", # 椭圆类型\n             geom = \"point\", # 只显示点不要文字\n             palette = \"lancet\", # 支持超多配色方案\n             ggtheme = theme_bw() # 支持更换主题\n             )\n\n\n\n\n\n\n\n\n\n\n28.3.2 围绕中心点的划分PAM\nK均值聚类是基于均值的，所以对异常值很敏感。一个更稳健的方法是围绕中心点的划分（PAM）。用一个最有代表性的观测值代表这一类(有点类似于主成分)。K均值聚类一般使用欧几里得距离，而PAM可以使用任意的距离来计算。因此，PAM可以容纳混合数据类型，并且不仅限于连续变量。\n我们还是用葡萄酒数据进行演示。PAM聚类可以通过cluster包中的pam()实现。\n\nlibrary(cluster)\n\nset.seed(123)\nfit.pam &lt;- pam(wine[-1,], k=3 # 聚为3类\n               , stand = T # 聚类前进行标准化\n               )\nfit.pam\n## Medoids:\n##      ID Type Alcohol Malic  Ash Alcalinity Magnesium Phenols Flavanoids\n## 36   35    1   13.48  1.81 2.41       20.5       100    2.70       2.98\n## 107 106    2   12.25  1.73 2.12       19.0        80    1.65       2.03\n## 149 148    3   13.32  3.24 2.38       21.5        92    1.93       0.76\n##     Nonflavanoids Proanthocyanins Color  Hue Dilution Proline\n## 36           0.26            1.86  5.10 1.04     3.47     920\n## 107          0.37            1.63  3.40 1.00     3.17     510\n## 149          0.45            1.25  8.42 0.55     1.62     650\n## Clustering vector:\n##   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21 \n##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n##  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41 \n##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n##  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61 \n##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   2 \n##  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81 \n##   2   2   1   2   1   2   2   2   1   2   1   2   1   1   2   2   2   2   1   2 \n##  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 \n##   2   2   3   2   2   2   2   2   2   2   2   2   2   2   1   1   2   1   2   2 \n## 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 \n##   2   2   2   2   2   2   2   2   1   2   2   2   2   2   2   2   2   2   2   2 \n## 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 \n##   1   2   2   2   2   2   2   2   2   3   3   3   3   3   3   3   3   3   3   3 \n## 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 \n##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n## 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 \n##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n## Objective function:\n##    build     swap \n## 3.537365 3.504175 \n## \n## Available components:\n##  [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n##  [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"\n\nMedoids给出了中心点，用第35个观测代表第1类，第107个观测代表第2类，第149个观测代表第3类。Clustering vector给出了每一个观测分别属于哪一个类。结果可以画出来：\n\nclusplot(fit.pam, main = \"PAM cluster\")\n\n\n\n\n\n\n\n\n同样也可以用factoextra包实现可视化。\n\nfviz_cluster(fit.pam, \n             ellipse = T, # 增加椭圆\n             ellipse.type = \"t\", # 椭圆类型\n             geom = \"point\", # 只显示点不要文字\n             palette = \"aaas\", # 支持超多配色方案\n             ggtheme = theme_bw() # 支持更换主题\n             )\n\n\n\n\n\n\n\n\n以后会给大家带来factoextra和factomineR包的详细介绍。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>聚类分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html",
    "href": "1022-pca.html",
    "title": "29  主成分分析",
    "section": "",
    "text": "29.1 加载数据\n使用R语言自带的iris鸢尾花数据进行演示。\nstr(iris)\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\npsych::headTail(iris)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 1            5.1         3.5          1.4         0.2    setosa\n## 2            4.9           3          1.4         0.2    setosa\n## 3            4.7         3.2          1.3         0.2    setosa\n## 4            4.6         3.1          1.5         0.2    setosa\n## ...          ...         ...          ...         ...      &lt;NA&gt;\n## 147          6.3         2.5            5         1.9 virginica\n## 148          6.5           3          5.2           2 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9           3          5.1         1.8 virginica",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>主成分分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html#相关性检验",
    "href": "1022-pca.html#相关性检验",
    "title": "29  主成分分析",
    "section": "29.2 相关性检验",
    "text": "29.2 相关性检验\n在进行PCA之前可以先进行相关性分析，看看相关系数，这样有助于我们查看各个变量之间的相关性，如果各个变量之间的相关性很小，那么这个数据可能不适合做PCA。\n\ncor(iris[,-5])\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\n## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\n## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\n## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>主成分分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html#kmo和bartlett球形检验",
    "href": "1022-pca.html#kmo和bartlett球形检验",
    "title": "29  主成分分析",
    "section": "29.3 KMO和Bartlett球形检验",
    "text": "29.3 KMO和Bartlett球形检验\nKMO（Kaiser-Meyer-Olkin）检验用于衡量变量之间的偏相关性。其取值范围在0到1之间，若KMO值越接近1，表明变量间的偏相关性越强，意味着原始变量之间存在较多的共同信息，适合进行主成分分析等降维操作；若KMO值接近0，则说明变量间的偏相关性较弱，各变量可能相对独立，不太适合做主成分分析。一般来说，KMO值大于0.6时，进行主成分分析的效果较为理想。\nKMO检验可以使用psych实现：\n\npsych::KMO(iris[,-5])\n## Kaiser-Meyer-Olkin factor adequacy\n## Call: psych::KMO(r = iris[, -5])\n## Overall MSA =  0.54\n## MSA for each item = \n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##         0.58         0.27         0.53         0.63\n\nOverall MSA（Measures of Sampling Adequacy）是总体的检验统计量，然后是每个变量的检验统计量。MSA取值范围在0到1之间，越接近1越好。\nBartlett球形检验与KMO检验类似，也是帮助我们确定变量之间是否具有足够的相关性。Bartlett球形检验也可以使用psych实现：\n\npsych::cortest.bartlett(iris[,-5])\n## $chisq\n## [1] 706.9592\n## \n## $p.value\n## [1] 1.92268e-149\n## \n## $df\n## [1] 6\n\n\nchisq：卡方统计量，其值越大，表明变量间的相关性越强。\np.value：p 值，若p值小于设定的显著性水平（通常为0.05），则拒绝原假设，认为变量之间存在显著的相关性，适合进行主成分分析或因子分析等依赖变量相关性的分析方法。\n\n以上两个检验也可以直接用performance包实现：\n\nperformance::check_factorstructure(iris[,-5])\n## # Is the data suitable for Factor Analysis?\n## \n## \n##   - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(6) = 706.96, p &lt; .001).\n##   - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.54). The individual KMO scores are: Sepal.Length (0.58), Sepal.Width (0.27*), Petal.Length (0.53), Petal.Width (0.63).",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>主成分分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html#pca和结果解读",
    "href": "1022-pca.html#pca和结果解读",
    "title": "29  主成分分析",
    "section": "29.4 PCA和结果解读",
    "text": "29.4 PCA和结果解读\n主成分分析可以通过分步计算，主要就是标准化-求相关矩阵-计算特征值和特征向量。R中自带了prcomp()进行主成分分析，这就是工具的魅力，一次完成多步需求。\n使用prcomp()进行主成分分析：\n\n# R自带函数\npca.res &lt;- prcomp(iris[,-5], scale. = T, # 标准化\n                  center = T # 中心化\n                  )\n\n我们可以通过以下代码单独查看主成分的载荷矩阵，载荷矩阵的每一列对应一个主成分，每一行对应一个原始变量。\n\n# 单独查看主成分的载荷矩阵\npca.res$rotation\n##                     PC1         PC2        PC3        PC4\n## Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\n## Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\n## Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\n## Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n载荷的绝对值越大，说明该原始变量对相应主成分的贡献越大（主成分载荷通常可以理解为原始变量与主成分之间的相关系数；或者理解为线性回归中的变量系数）。比如对于我们的PC1，它的计算公式如下：\n\\[\nPC1=0.521*Sepal.Length-0.269*Sepal.Width+0.580*Petal.Length+0.565*Petal.Width\n\\]\n通过观察主成分载荷矩阵，可以了解每个原始变量对各个主成分的贡献程度。例如，如果某个主成分在多个变量上的载荷绝对值都较大，说明该主成分综合反映了这些变量的信息；如果某个主成分只在一个或少数几个变量上有较大载荷，说明该主成分主要代表了这些变量的信息。\n每一列代表一个主成分对应的特征向量，例如，矩阵中第一列表示第一个主成分在各个原始变量上的权重，这些权重构成了第一个主成分的特征向量。\n查看特征值：\n\n# 查看特征值\npca.res$sdev^2\n## [1] 2.91849782 0.91403047 0.14675688 0.02071484\n\n特征值反映了每个主成分所包含的信息量，特征值越大，说明对应的主成分能解释原始数据的方差越多，也就越重要。在选择主成分时，通常会根据特征值的大小，选取特征值大于1或者累积方差贡献率达到一定阈值（如80%或90%）的主成分，以实现数据降维的目的。\n查看主成分得分，主成分的得分是将原始数据投影到主成分上得到的值。\n\n# 样本得分score\nhead(pca.res$x)\n##            PC1        PC2         PC3          PC4\n## [1,] -2.257141 -0.4784238  0.12727962  0.024087508\n## [2,] -2.074013  0.6718827  0.23382552  0.102662845\n## [3,] -2.356335  0.3407664 -0.04405390  0.028282305\n## [4,] -2.291707  0.5953999 -0.09098530 -0.065735340\n## [5,] -2.381863 -0.6446757 -0.01568565 -0.035802870\n## [6,] -2.068701 -1.4842053 -0.02687825  0.006586116\n\n主成分得分可以用于样本的排序和聚类分析。将样本的主成分得分绘制在二维或三维空间中，可以直观地观察样本之间的关系。例如，在主成分得分图中，距离较近的样本可能具有相似的特征。\n查看主成分的标准差、方差贡献率、累积方差贡献率：\n\n# 查看标准差、方差贡献率、累积方差贡献率\nsummary(pca.res)\n## Importance of components:\n##                           PC1    PC2     PC3     PC4\n## Standard deviation     1.7084 0.9560 0.38309 0.14393\n## Proportion of Variance 0.7296 0.2285 0.03669 0.00518\n## Cumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\nStandard deviation:标准差\nProportion of Variance:方差贡献率\nCumulative Proportion:累积方差贡献率\n\n方差贡献率反映了每个主成分所包含的信息量。通常我们会选择方差贡献率较大的前几个主成分，这些主成分能够保留原始数据的大部分信息。通过累积方差贡献率，可以确定需要保留的主成分个数。例如，如果前两个主成分的累积方差贡献率达到了85%（该例中前两个主成分的累积方差贡献率为95.81%），那么可以选择保留这两个主成分，将四维数据降维到二维数据进行分析。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>主成分分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html#默认的结果可视化",
    "href": "1022-pca.html#默认的结果可视化",
    "title": "29  主成分分析",
    "section": "29.5 默认的结果可视化",
    "text": "29.5 默认的结果可视化\n默认的主成分分析结果可视化：\n\n# 双标图 \nbiplot(pca.res)\n\n\n\n\n\n\n\n\n双标图通常展示前两个主成分（第一主成分和第二主成分），横轴一般是第一主成分，纵轴是第二主成分。主成分是原始变量的线性组合，第一主成分通常解释了数据中最大的方差，第二主成分解释了剩余方差中的最大值，且与第一主成分正交（不相关）。\n图中的点代表样本。点在图中的位置反映了样本在主成分空间中的得分。距离较近的点表示这些样本在主成分所代表的特征上具有相似性。例如，在iris数据集的双标图中，如果某些鸢尾花样本点聚集在一起，说明这些鸢尾花在主成分所概括的特征方面比较相似。\n样本点在坐标轴上的投影可以近似看作该样本在相应主成分上的得分。沿着第一主成分轴（通常是横轴）方向的位置变化反映了样本在第一主成分上得分的高低，同理，沿着第二主成分轴（通常是纵轴）方向的位置变化反映了样本在第二主成分上得分的高低。\n图中的箭头代表原始变量。箭头的方向表示该变量与主成分之间的相关性方向。如果箭头与某个主成分轴的夹角较小，说明该变量与这个主成分的相关性较强。例如，若一个变量的箭头几乎与第一主成分轴平行，那么这个变量在第一主成分上的载荷较大，对第一主成分的贡献也较大。\n箭头的长度表示该变量对主成分的重要性。箭头越长，说明该变量对主成分的影响越大。\n通过箭头之间的夹角可以判断变量之间的相关性。夹角较小的箭头表示对应的变量之间正相关；夹角接近180度的箭头表示对应的变量之间负相关；夹角接近90度的箭头表示对应的变量之间相关性较弱。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>主成分分析</span>"
    ]
  },
  {
    "objectID": "1022-pca.html#确定最佳主成分个数",
    "href": "1022-pca.html#确定最佳主成分个数",
    "title": "29  主成分分析",
    "section": "29.6 确定最佳主成分个数",
    "text": "29.6 确定最佳主成分个数\n碎石图可以帮助确认最佳的主成分个数，可以使用默认的screeplot()实现：\n\n# 默认是条形图，我们改为折线图，其实就是方差贡献度的可视化\nscreeplot(pca.res, type = \"lines\")\n\n\n\n\n\n\n\n\n\n一般来说，主成分的保留个数可以按照以下原则确定： 1. 以累积贡献率确定，当前K个主成分的累积贡献率达到某一特定值（一般选70%或者80%、90%都行）时，则保留前K个主成分； 2. 以特征值大小来确定：如果主成分的特征值大于1，就保留这个主成分。\n\n从上图可以看到用2-3个主成分就挺好了。但是保留几个主成分并没有绝对的标准，大家根据自己的实际情况来！\n这里再给大家介绍一个非常实用的函数，使用parameters包实现，可以给出好多种方法的选择：\n\nn &lt;- parameters::n_components(iris[,-5])\nn\n## # Method Agreement Procedure:\n## \n## The choice of 1 dimensions is supported by 6 (46.15%) methods out of 13 (Bentler, Optimal coordinates, Acceleration factor, Parallel analysis, Kaiser criterion, Velicer's MAP).\n\n6个方法都支持选择1个主成分，如果要想查看所有的方法及每个方法选择的主成分个数,可以直接变为数据框查看:\n\nas.data.frame(n) # 一共有12个方法!（选0个因子的不算）\n##    n_Factors              Method       Family\n## 1          0          Scree (R2)     Scree_SE\n## 2          1             Bentler      Bentler\n## 3          1 Optimal coordinates        Scree\n## 4          1 Acceleration factor        Scree\n## 5          1   Parallel analysis        Scree\n## 6          1    Kaiser criterion        Scree\n## 7          1       Velicer's MAP Velicers_MAP\n## 8          2          Scree (SE)     Scree_SE\n## 9          2    VSS complexity 1          VSS\n## 10         2    VSS complexity 2          VSS\n## 11         3            Bartlett      Barlett\n## 12         3            Anderson      Barlett\n## 13         3              Lawley      Barlett\n\n或者也可以查看这十几种结果的汇总表:\n\nsummary(n)\n##   n_Factors n_Methods Variance_Cumulative\n## 1         1         6           0.7213402\n## 2         2         3           0.8667486\n## 3         3         3           0.8912973\n\n当然也可以把结果画出来的,借助see这个包即可:\n\nlibrary(see)\nplot(n)+theme_modern()\n\n\n\n\n\n\n\n\n横坐标是选择的主成分数量,左侧纵坐标是方法的数量（百分比）,右侧纵坐标是解释的方差百分比。先看条形图，选择1个主成分的方法最多，占了12个方法的约50%以上；再看折线图，选择主成分越多，解释的方差百分比越多，不过超过2之后就变化不大了。（这个例子变量太少了）",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>主成分分析</span>"
    ]
  },
  {
    "objectID": "pca可视化.html",
    "href": "pca可视化.html",
    "title": "30  主成分分析可视化",
    "section": "",
    "text": "30.1 进阶的PCA可视化\n网络上很多R语言教程都是基于R语言实战进行修改，今天为大家介绍更好用的R包，在之前聚类分析中也经常用到：factoextra和factoMineR，关于主成分分析的可视化，大家比较常见的可能是ggbiplot，这几个R包都挺不错，大家可以比较下。\n这两个R包的函数可以直接使用prcomp()函数的结果，也可以使用FactoMineR的PCA()函数进行，结果更加详细。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>主成分分析可视化</span>"
    ]
  },
  {
    "objectID": "pca可视化.html#进阶的pca可视化",
    "href": "pca可视化.html#进阶的pca可视化",
    "title": "30  主成分分析可视化",
    "section": "",
    "text": "30.1.1 进行PCA分析\n使用R语言自带的iris鸢尾花数据进行演示。\n\nrm(list = ls())\nlibrary(factoextra)\nlibrary(FactoMineR)\n\npca.res &lt;- PCA(iris[,-5], graph = F, scale.unit = T) # 1行代码实现主成分分析\npca.res\n## **Results for the Principal Component Analysis (PCA)**\n## The analysis was performed on 150 individuals, described by 4 variables\n## *The results are available in the following objects:\n## \n##    name               description                          \n## 1  \"$eig\"             \"eigenvalues\"                        \n## 2  \"$var\"             \"results for the variables\"          \n## 3  \"$var$coord\"       \"coord. for the variables\"           \n## 4  \"$var$cor\"         \"correlations variables - dimensions\"\n## 5  \"$var$cos2\"        \"cos2 for the variables\"             \n## 6  \"$var$contrib\"     \"contributions of the variables\"     \n## 7  \"$ind\"             \"results for the individuals\"        \n## 8  \"$ind$coord\"       \"coord. for the individuals\"         \n## 9  \"$ind$cos2\"        \"cos2 for the individuals\"           \n## 10 \"$ind$contrib\"     \"contributions of the individuals\"   \n## 11 \"$call\"            \"summary statistics\"                 \n## 12 \"$call$centre\"     \"mean of the variables\"              \n## 13 \"$call$ecart.type\" \"standard error of the variables\"    \n## 14 \"$call$row.w\"      \"weights for the individuals\"        \n## 15 \"$call$col.w\"      \"weights for the variables\"\n\n结果信息丰富，可以通过不断的$获取，也可以通过特定函数提取，下面介绍。\n\n\n30.1.2 特征值可视化\n获取特征值、方差贡献率和累积方差贡献率，可以看到和上一节的结果是一样的：\n\nget_eigenvalue(pca.res)\n##       eigenvalue variance.percent cumulative.variance.percent\n## Dim.1 2.91849782       72.9624454                    72.96245\n## Dim.2 0.91403047       22.8507618                    95.81321\n## Dim.3 0.14675688        3.6689219                    99.48213\n## Dim.4 0.02071484        0.5178709                   100.00000\n\n通过这几个值，可以确定主成分个数，当然也可以通过碎石图（就是方差解释度的可视化）直观的观察：\n\nfviz_eig(pca.res,addlabels = T,ylim=c(0,100))\n\n\n\n\n\n\n\n\n\n\n30.1.3 提取变量结果\n通过get_pca_var()`函数实现：\n\nres.var &lt;- get_pca_var(pca.res)\nres.var$cor\n##                   Dim.1      Dim.2       Dim.3       Dim.4\n## Sepal.Length  0.8901688 0.36082989 -0.27565767 -0.03760602\n## Sepal.Width  -0.4601427 0.88271627  0.09361987  0.01777631\n## Petal.Length  0.9915552 0.02341519  0.05444699  0.11534978\n## Petal.Width   0.9649790 0.06399985  0.24298265 -0.07535950\nres.var$coord          \n##                   Dim.1      Dim.2       Dim.3       Dim.4\n## Sepal.Length  0.8901688 0.36082989 -0.27565767 -0.03760602\n## Sepal.Width  -0.4601427 0.88271627  0.09361987  0.01777631\n## Petal.Length  0.9915552 0.02341519  0.05444699  0.11534978\n## Petal.Width   0.9649790 0.06399985  0.24298265 -0.07535950\nres.var$contrib       \n##                  Dim.1       Dim.2     Dim.3     Dim.4\n## Sepal.Length 27.150969 14.24440565 51.777574  6.827052\n## Sepal.Width   7.254804 85.24748749  5.972245  1.525463\n## Petal.Length 33.687936  0.05998389  2.019990 64.232089\n## Petal.Width  31.906291  0.44812296 40.230191 27.415396\nres.var$cos2        \n##                  Dim.1       Dim.2       Dim.3        Dim.4\n## Sepal.Length 0.7924004 0.130198208 0.075987149 0.0014142127\n## Sepal.Width  0.2117313 0.779188012 0.008764681 0.0003159971\n## Petal.Length 0.9831817 0.000548271 0.002964475 0.0133055723\n## Petal.Width  0.9311844 0.004095980 0.059040571 0.0056790544\n\n\nres.var$cor:变量和主成分的相关系数\nres.var$coord: 变量在主成分投影上的坐标，下面会结合图说明，因为进行了标准化，所以和相关系数结果一样，其数值代表了主成分和变量之间的相关性\nres.var$cos2: 是coord的平方，也是表示主成分和变量间的相关性，同一个变量所有cos2的总和是1\nres.var$contrib: 变量对主成分的贡献\n\n这几个结果都可以进行可视化。\n\n\n30.1.4 变量结果可视化\n使用fviz_pca_var()对变量结果进行可视化：\n\nfviz_pca_var(pca.res)\n\n\n\n\n\n\n\n\nres.var$coord是变量在主成分投影上的坐标，Sepal.Width在Dim.1的坐标是-0.4601427，在Dim.2的坐标是0.88271627，根据这两个坐标就画出来Sepal.Width那根线了，以此类推。\n\n30.1.4.1 变量和主成分的cos2可视化\ncos2是coord的平方，也是表示主成分和变量间的相关性，所以首先可以画相关图：\n\nlibrary(\"corrplot\")\ncorrplot(res.var$cos2, is.corr = F)\n\n\n\n\n\n\n\n\n可以看到Petal.Length、Petal.Width和Dim1的相关性比较强，Sepal.Width和Dim2的相关性比较强。\n通过fviz_cos2()查看变量在不同主成分的总和，以下是不同变量在第1和第2主成分的加和，如果把axes = 1:2改成axes = 1:4，就会变成都是1（这个数据最多4个主成分，同一变量的cos2在所有主成分的总和是1）。\n\nfviz_cos2(pca.res, choice = \"var\", axes = 1:2)\n\n\n\n\n\n\n\n\n可以通过col.var = \"cos2\"参数给不同变量按照cos2的数值大小上色：\n\nfviz_pca_var(pca.res, col.var = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE \n             )\n\n\n\n\n\n\n\n\n\n# 黑白版本\nfviz_pca_var(pca.res, alpha.var = \"cos2\")\n\n\n\n\n\n\n\n\n\n\n30.1.4.2 变量对主成分的贡献可视化\n\nres.var$contrib\n##                  Dim.1       Dim.2     Dim.3     Dim.4\n## Sepal.Length 27.150969 14.24440565 51.777574  6.827052\n## Sepal.Width   7.254804 85.24748749  5.972245  1.525463\n## Petal.Length 33.687936  0.05998389  2.019990 64.232089\n## Petal.Width  31.906291  0.44812296 40.230191 27.415396\n\n首先也是可以通过画相关性图进行可视化：\n\nlibrary(\"corrplot\")\ncorrplot(res.var$contrib, is.corr=FALSE) \n\n\n\n\n\n\n\n\n通过fviz_contrib()可视化变量对不同主成分的贡献：\n\n# 对第1主成分的贡献\nfviz_contrib(pca.res, choice = \"var\", axes = 1)\n\n\n\n\n\n\n\n\n\n# 对第1和第2主成分的贡献\nfviz_contrib(pca.res, choice = \"var\", axes = 1:2)\n\n\n\n\n\n\n\n\n通过col.var = \"contrib\"参数给不同变量按照contrib的数值大小上色：\n\nfviz_pca_var(pca.res, col.var = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")\n             )\n\n\n\n\n\n\n\n\n\n\n\n30.1.5 维度描述\n\nres.desc &lt;- dimdesc(pca.res, axes = c(1,2), proba = 0.05)\n# Description of dimension 1\nres.desc$Dim.1\n## \n## Link between the variable and the continuous variables (R-square)\n## =================================================================================\n##              correlation       p.value\n## Petal.Length   0.9915552 3.369916e-133\n## Petal.Width    0.9649790  6.609632e-88\n## Sepal.Length   0.8901688  2.190813e-52\n## Sepal.Width   -0.4601427  3.139724e-09\n\n\n\n30.1.6 提取样本结果\n使用get_pca_ind()提取样本结果，和变量结果类似：\n\nres.ind &lt;- get_pca_ind(pca.res)\n\nhead(res.ind$coord)          \n##       Dim.1      Dim.2       Dim.3       Dim.4\n## 1 -2.264703  0.4800266 -0.12770602 -0.02416820\n## 2 -2.080961 -0.6741336 -0.23460885 -0.10300677\n## 3 -2.364229 -0.3419080  0.04420148 -0.02837705\n## 4 -2.299384 -0.5973945  0.09129011  0.06595556\n## 5 -2.389842  0.6468354  0.01573820  0.03592281\n## 6 -2.075631  1.4891775  0.02696829 -0.00660818\nhead(res.ind$contrib)      \n##       Dim.1      Dim.2       Dim.3       Dim.4\n## 1 1.1715796 0.16806554 0.074085470 0.018798188\n## 2 0.9891845 0.33146674 0.250034006 0.341474919\n## 3 1.2768164 0.08526419 0.008875320 0.025915633\n## 4 1.2077372 0.26029781 0.037858004 0.140000650\n## 5 1.3046313 0.30516562 0.001125175 0.041530572\n## 6 0.9841236 1.61748779 0.003303827 0.001405371\nhead(res.ind$cos2)          \n##       Dim.1      Dim.2        Dim.3        Dim.4\n## 1 0.9539975 0.04286032 0.0030335249 1.086460e-04\n## 2 0.8927725 0.09369248 0.0113475382 2.187482e-03\n## 3 0.9790410 0.02047578 0.0003422122 1.410446e-04\n## 4 0.9346682 0.06308947 0.0014732682 7.690193e-04\n## 5 0.9315095 0.06823959 0.0000403979 2.104697e-04\n## 6 0.6600989 0.33978301 0.0001114335 6.690714e-06\n\n3个概念和变量的解释也是类似的，只不过上面是变量（列）和主成分的关系，现在是样本（观测，行）和主成分的关系。\n\n\n30.1.7 样本结果可视化\n样本的结果可视化可能是更常见的PCA图形，通过fviz_pca_ind()实现：\n\nfviz_pca_ind(pca.res)\n\n\n\n\n\n\n\n\n这个图是通过res.ind$coord里面的坐标实现的，其实就是不同样本在不同主成分的上面的得分score。\n默认的可视化比较简陋，但是可以通过超多参数实现各种精细化的控制，比如把不同的属性映射给点的大小和颜色，实现各种花里胡哨的效果。\n比如通过组别上色，就是大家最常见的PCA可视化图形：\n\n# 经典图形，是不是很熟悉？\nfviz_pca_ind(pca.res,\n             geom.ind = \"point\", # 只显示点，不要文字\n             col.ind = iris$Species, # 按照组别上色\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), # 自己提供颜色，或者使用主题\n             addEllipses = TRUE, # 添加置信椭圆\n             legend.title = \"Groups\"\n             )\n\n\n\n\n\n\n\n\n\n30.1.7.1 样本的cos2可视化\n使用方法和变量的cos2可视化基本一样，通过更改参数值即可实现：\n\nfviz_pca_ind(pca.res,\n             col.ind = \"cos2\", # 按照cos2上色\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE    \n             )\n\n\n\n\n\n\n\n\n可以更改点的大小、颜色等，只要设置合适的参数即可：\n\nfviz_pca_ind(pca.res, \n             pointsize = \"cos2\", # 把cos2的大小映射给点的大小\n             pointshape = 21, \n             fill = \"#E7B800\",\n             repel = TRUE \n             )\n\n\n\n\n\n\n\n\n同时更改点的大小和颜色当然也是支持的：\n\nfviz_pca_ind(pca.res, \n             col.ind = \"cos2\", # 控制颜色\n             pointsize = \"contrib\", # 控制大小\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE \n             )\n\n\n\n\n\n\n\n\n使用参数choice = \"ind\"可视化样本对不同主成分的cos2：\n\n# axes选择主成分\nfviz_cos2(pca.res, choice = \"ind\", axes = 1:2)\n\n\n\n\n\n\n\n\n\n\n30.1.7.2 样本对主成分的贡献可视化\n和变量对主成分的贡献可视化非常类似，简单演示下：\n\nfviz_contrib(pca.res, choice = \"ind\", axes = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n30.1.8 biplot\n双标图…\n同时展示变量和样本和主成分的关系，超级多的自定义可视化细节。\n\n# 同时有箭头和椭圆\nfviz_pca_biplot(pca.res, \n                col.ind = iris$Species, \n                palette = \"jco\", \n                addEllipses = TRUE, \n                label = \"var\",\n                col.var = \"black\", \n                repel = TRUE,\n                legend.title = \"Species\"\n                ) \n\n\n\n\n\n\n\n\n\nfviz_pca_biplot(pca.res, \n                # 组别映射给点的填充色\n                geom.ind = \"point\",\n                pointshape = 21,\n                pointsize = 2.5,\n                fill.ind = iris$Species,\n                col.ind = \"black\",\n                # 通过自定义分组给变量上色\n                col.var = factor(c(\"sepal\", \"sepal\", \"petal\", \"petal\")),\n                # 自定义图例标题\n                legend.title = list(fill = \"Species\", color = \"Clusters\"),\n                repel = TRUE        \n             )+\n  ggpubr::fill_palette(\"jco\")+ # 选择点的填充色的配色\n  ggpubr::color_palette(\"npg\") # 选择变量颜色的配色\n\n\n\n\n\n\n\n\n\nfviz_pca_biplot(pca.res, \n                # 自定义样本部分\n                geom.ind = \"point\",\n                fill.ind = iris$Species, # 填充色\n                col.ind = \"black\", # 边框色\n                pointshape = 21, # 点的形状\n                pointsize = 2, \n                palette = \"jco\",\n                addEllipses = TRUE,\n                # 自定义变量部分\n                alpha.var =\"contrib\", col.var = \"contrib\",\n                gradient.cols = \"RdYlBu\",\n                \n                # 自定义图例标题\n                legend.title = list(fill = \"Species\", color = \"Contrib\",\n                                    alpha = \"Contrib\")\n                )\n\n\n\n\n\n\n\n\nfviz_xxx系列可视化函数底层是ggscatter的封装，这个函数来自ggpubr包，所有ggpubr支持的特性都可以给fviz_xxx函数使用，这也是这几个函数功能强大的原因，毕竟底层都是ggplot2!\n下载会继续给大家介绍如何提取PCA的数据，并使用ggplot2可视化，以及三维PCA图的实现。\nfactoextra和factoMineR在聚类分析、主成分分析、因子分析等方面都可以使用。\n参考资料：http://www.sthda.com/",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>主成分分析可视化</span>"
    ]
  },
  {
    "objectID": "pca可视化.html#pca可视化3d版",
    "href": "pca可视化.html#pca可视化3d版",
    "title": "30  主成分分析可视化",
    "section": "30.2 PCA可视化3d版",
    "text": "30.2 PCA可视化3d版\n之前详细介绍了R语言中的主成分分析，以及超级详细的主成分分析可视化方法，主要是基于factoextra和factoMineR两个神包。\n今天说一下如何提取数据用ggplot2画PCA图，以及三维PCA图。\n\n30.2.1 提取数据\n还是使用鸢尾花数据集。\n\nrm(list = ls())\n\npca.res &lt;- prcomp(iris[,-5], scale. = T, center = T)\npca.res\n## Standard deviations (1, .., p=4):\n## [1] 1.7083611 0.9560494 0.3830886 0.1439265\n## \n## Rotation (n x k) = (4 x 4):\n##                     PC1         PC2        PC3        PC4\n## Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\n## Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\n## Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\n## Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n在上一篇中提到过，经典的PCA图的横纵坐标其实就是不同样本在不同主成分中的得分，只要提取出来就可以用gplot2画了。\n\n# 提取得分\ntmp &lt;- as.data.frame(pca.res$x)\nhead(tmp)\n##         PC1        PC2         PC3          PC4\n## 1 -2.257141 -0.4784238  0.12727962  0.024087508\n## 2 -2.074013  0.6718827  0.23382552  0.102662845\n## 3 -2.356335  0.3407664 -0.04405390  0.028282305\n## 4 -2.291707  0.5953999 -0.09098530 -0.065735340\n## 5 -2.381863 -0.6446757 -0.01568565 -0.035802870\n## 6 -2.068701 -1.4842053 -0.02687825  0.006586116\n\n和原数据拼到一起就可以画图了：\n\ntmp$species &lt;- iris$Species\nhead(tmp)\n##         PC1        PC2         PC3          PC4 species\n## 1 -2.257141 -0.4784238  0.12727962  0.024087508  setosa\n## 2 -2.074013  0.6718827  0.23382552  0.102662845  setosa\n## 3 -2.356335  0.3407664 -0.04405390  0.028282305  setosa\n## 4 -2.291707  0.5953999 -0.09098530 -0.065735340  setosa\n## 5 -2.381863 -0.6446757 -0.01568565 -0.035802870  setosa\n## 6 -2.068701 -1.4842053 -0.02687825  0.006586116  setosa\n\n\nlibrary(ggplot2)\nlibrary(ggsci)\n\nggplot(tmp, aes(PC1, PC2))+\n  geom_point(aes(color = species))+\n  stat_ellipse(aes(fill=species), alpha = 0.2,\n               geom =\"polygon\",type = \"norm\")+\n  scale_fill_aaas()+\n  scale_color_aaas()+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n30.2.2 3d版\n其实就是使用3个主成分，之前介绍过一种：使用R语言美化PCA图，使用方法非常简单，也是在文献中学习到的。\n\n\n\n\n\n\n\n\n\n今天再介绍下scatterplot3d包。\n\nlibrary(scatterplot3d)\n\nscatterplot3d(tmp[,1:3], # 第1-3主成分\n              # 颜色长度要和样本长度一样，且对应！\n              color = rep(c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),each=50),\n              pch = 15,\n              lty.hide = 2\n              )\nlegend(\"topleft\",c('Setosa','Versicolor','Virginica'),\nfill=c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),box.col=NA)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>主成分分析可视化</span>"
    ]
  },
  {
    "objectID": "主成分回归.html",
    "href": "主成分回归.html",
    "title": "31  主成分回归",
    "section": "",
    "text": "31.1 pls\n首先是pls包可直接实现主成分回归。\n# 安装\ninstall.packages(\"pls\")\n加载R包：\nrm(list = ls())\nlibrary(pls)\n## Warning: package 'pls' was built under R version 4.5.1\n我们使用R自带的mtcars数据集进行演示。\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n直接1个函数就可以实现主成分回归了，非常简单：\nset.seed(1)\n\nmodel &lt;- pcr(hp~mpg+disp+drat+wt+qsec, data=mtcars, scale=TRUE, validation=\"CV\")\nsummary(model)\n## Data:    X dimension: 32 5 \n##  Y dimension: 32 1\n## Fit method: svdpc\n## Number of components considered: 5\n## \n## VALIDATION: RMSEP\n## Cross-validated using 10 random segments.\n##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps\n## CV           69.66    43.74    34.58    34.93    36.34    37.40\n## adjCV        69.66    43.65    34.30    34.61    35.95    36.95\n## \n## TRAINING: % variance explained\n##     1 comps  2 comps  3 comps  4 comps  5 comps\n## X     69.83    89.35    95.88    98.96   100.00\n## hp    62.38    81.31    81.96    81.98    82.03\n结果首先告诉你自变量X的维度是32行，5列，因变量Y的维度是32行，1列。\n使用的主成分个数是5。\nVALIDATION: RMSEP是通过10折交叉验证得到的RMSE，先看CV这一行，只用截距项，RMSE为69.66，添加第一主成分之后RMSE是43.74。。。adjCV是调整后的RMSE。\n最后一部分是主成分能够解释的方差百分比。\n可视化主成分个数与各种模型指标的关系，默认指标是RMSE，可选择MAE或者R2：\nvalidationplot(model)\n\n\n\n\n\n\n\nvalidationplot(model, val.type=\"MSEP\") # MAE\n\n\n\n\n\n\n\nvalidationplot(model, val.type=\"R2\")\n对新数据进行预测，可自由选择主成分个数，根据上面的图，我们选择2个主成分：\ntest &lt;- head(mtcars)\npredict(model, test, ncomp = 2)\n## , , 2 comps\n## \n##                         hp\n## Mazda RX4         155.2385\n## Mazda RX4 Wag     146.6904\n## Datsun 710        100.4458\n## Hornet 4 Drive    118.3935\n## Hornet Sportabout 186.7221\n## Valiant           111.2443",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>主成分回归</span>"
    ]
  },
  {
    "objectID": "主成分回归.html#tidymodels",
    "href": "主成分回归.html#tidymodels",
    "title": "31  主成分回归",
    "section": "31.2 tidymodels",
    "text": "31.2 tidymodels\n然后再介绍一下如何使用tidymodels实现主成分回归，这个过程就稍显复杂了。\n\nsuppressMessages(library(tidymodels))\ntidymodels_prefer()\n\n虽然复杂但是很有逻辑，这个也是tidymodels的使用步骤，不管换什么模型，都是这一套步骤，打完一套组合拳即可：\n\n建立模型设定；\n数据划分；\n配方（预处理步骤）；\n工作流；\n超参数调优\n\n\n# 模型设定\nset.seed(994)\nlm_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n# 数据划分\nmtcars_resamples &lt;- vfold_cv(mtcars, v = 10)\n\n# 配方（预处理步骤）\nmtcars_pca_recipe &lt;- recipe(hp~mpg+disp+drat+wt+qsec, data = mtcars) %&gt;%\n  #step_dummy(all_nominal()) %&gt;%\n  step_normalize(all_predictors()) %&gt;% # 肯定是要scale的\n  step_pca(all_predictors(), num_comp = tune())\n\n# 工作流\nmtcars_pca_workflow &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(mtcars_pca_recipe)\n\n# 超参数调优\nnum_comp_grid &lt;- grid_regular(num_comp(range = c(0, 5)), levels = 6)\n\nmtcars_pca_tune &lt;- tune_grid(mtcars_pca_workflow,\n                              resamples = mtcars_resamples,\n                              grid = num_comp_grid)\n\n这里进行超参数调优的目的是为了找出最佳的主成分个数，你可能也发现了，在tidymodels中实现主成分回归其实只是把主成分分析作为一个预处理步骤而已，本质上还是做回归分析。\n结果可视化，从图中可以看出还是2个主成分的模型最好，和上面的结果是一样的：\n\nautoplot(mtcars_pca_tune)\n\n\n\n\n\n\n\n\n查看最佳结果：\n\nshow_best(mtcars_pca_tune)\n## Warning in show_best(mtcars_pca_tune): No value of `metric` was given; \"rmse\"\n## will be used.\n## # A tibble: 5 × 7\n##   num_comp .metric .estimator  mean     n std_err .config             \n##      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n## 1        2 rmse    standard    29.1    10    5.63 Preprocessor3_Model1\n## 2        3 rmse    standard    31.0    10    5.11 Preprocessor4_Model1\n## 3        4 rmse    standard    31.4    10    5.16 Preprocessor5_Model1\n## 4        0 rmse    standard    32.1    10    5.18 Preprocessor1_Model1\n## 5        5 rmse    standard    32.1    10    5.18 Preprocessor6_Model1\n\n使用最佳结果（2个主成分）重新建模，然后重新在训练集训练，然后对测试集预测：\n\nmtcars_pca_workflow_final &lt;-\n  finalize_workflow(mtcars_pca_workflow,\n                    select_best(mtcars_pca_tune, metric = \"rmse\"))\n\nmtcars_pca_fit_final &lt;- fit(mtcars_pca_workflow_final,data = mtcars)\n\npredict(mtcars_pca_fit_final, new_data = head(mtcars))\n## # A tibble: 6 × 1\n##   .pred\n##   &lt;dbl&gt;\n## 1  155.\n## 2  147.\n## 3  100.\n## 4  118.\n## 5  187.\n## 6  111.\n\n公众号后台回复tidymodels即可获取合集。\n两种方法，你喜欢哪种呢？",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>主成分回归</span>"
    ]
  },
  {
    "objectID": "1023-factoranalysis.html",
    "href": "1023-factoranalysis.html",
    "title": "32  探索性因子分析",
    "section": "",
    "text": "32.1 演示数据\n数据来自于孙振球医学统计学第4版例22-2。\n某医院为了评价医疗工作质量，收集了三年的门诊人次、出院人数、病床利用率、病床周转次数、平均住院天数、治愈好转率、病死率、诊断符合率、抢救成功率9个指标，采用因子分析方法，探讨其综合评价体系。\ndf &lt;- foreign::read.spss(\"datasets/例22-02.sav\",to.data.frame = T,\n                         reencode = \"utf-8\")\nnames(df) &lt;- c(\"年\",\"月\",\"门诊人次\",\"出院人数\",\"病床利用率\",\"病床周转次数\",\n               \"平均住院天数\",\"治愈好转率\",\"病死率\",\"诊断符合率\",\"抢救成功率\")\n\nstr(df)\n## 'data.frame':    36 obs. of  11 variables:\n##  $ 年          : num  1991 1991 1991 1991 1991 ...\n##  $ 月          : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ 门诊人次    : num  4.34 3.45 4.38 4.18 4.32 4.13 4.57 4.31 4.06 4.43 ...\n##  $ 出院人数    : num  389 271 385 377 378 349 361 209 425 458 ...\n##  $ 病床利用率  : num  99.1 88.3 104 99.5 102 ...\n##  $ 病床周转次数: num  1.23 0.85 1.21 1.19 1.19 1.1 1.14 0.52 0.93 0.95 ...\n##  $ 平均住院天数: num  25.5 23.6 26.5 26.9 27.6 ...\n##  $ 治愈好转率  : num  93.2 94.3 92.5 93.9 93.2 ...\n##  $ 病死率      : num  3.56 2.44 4.02 2.92 1.99 4.38 2.73 3.65 3.09 4.21 ...\n##  $ 诊断符合率  : num  97.5 97.9 98.5 99.4 99.7 ...\n##  $ 抢救成功率  : num  61.7 73.3 76.8 63.2 80 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \".·.\" \".....˴.\" \"..Ժ....\" ...\npsych::headTail(df)\n##       年  月 门诊人次 出院人数 病床利用率 病床周转次数 平均住院天数 治愈好转率\n## 1   1991   1     4.34      389      99.06         1.23        25.46      93.15\n## 2   1991   2     3.45      271      88.28         0.85        23.55      94.31\n## 3   1991   3     4.38      385     103.97         1.21        26.54      92.53\n## 4   1991   4     4.18      377      99.48         1.19        26.89      93.86\n## ...  ... ...      ...      ...        ...          ...          ...        ...\n## 33  1993   9      3.9      555      80.58          1.1        23.08      94.38\n## 34  1993  10     3.62      554      87.21          1.1         22.5      92.43\n## 35  1993  11     3.75      586      90.31         1.12        23.73      92.47\n## 36  1993  12     3.77      627      86.47         1.24        23.22      91.17\n##     病死率 诊断符合率 抢救成功率\n## 1     3.56      97.51      61.66\n## 2     2.44      97.94      73.33\n## 3     4.02      98.48      76.79\n## 4     2.92      99.41      63.16\n## ...    ...        ...        ...\n## 33    2.06      96.82      91.79\n## 34    3.22      97.16      87.77\n## 35    2.07      97.74      93.89\n## 36     3.4      98.98       89.8",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>探索性因子分析</span>"
    ]
  },
  {
    "objectID": "1023-factoranalysis.html#确定最佳因子个数",
    "href": "1023-factoranalysis.html#确定最佳因子个数",
    "title": "32  探索性因子分析",
    "section": "32.2 确定最佳因子个数",
    "text": "32.2 确定最佳因子个数\n这一步类似于PCA中的确定主成分的个数，使用的规则也是类似的。\n\nCattell的“碎石”检验是解决因子数量问题最简单的检验方法之一。Horn的“平行”分析是同样引人注目的方法。其他确定最优因子数量的方法包括寻找“极简单结构”（Very Simple Structure，VSS）准则（VSS）以Velicer的MAP程序（包含在VSS中）。VSS和MAP准则都包含在nfactors函数中，该函数还会报告每个多重解的平均项目复杂度和贝叶斯信息准则（BIC）。–fa.parallel函数的帮助文档\n\npsych包中的fa.parallel函数可以同时输出碎石检验和平行分析的结果，通过设置参数fa=\"both\"同时给出PCA和因子分析的平行检验结果以及碎石图，fa=\"pc\"只给出主成分分析的结果，fa=\"fa\"只给出因子分析的结果：\n\n# 只用后面9列数据\ndf.use &lt;- df[,-c(1,2)]\n\nlibrary(psych)\n\n# 碎石图和平行检验\nset.seed(1)\nfa.parallel(df.use, fa = \"fa\",fm=\"ml\")\n\n\n\n\n\n\n\n## Parallel analysis suggests that the number of factors =  3  and the number of components =  NA\n\n平行分析建议我们提取3个因子。\n碎石图的横坐标是因子个数，纵坐标是特征值，带三角形（图例有误）的蓝色线是真实数据的结果，红色线是模拟数据的结果和重抽样数据的结果，一般取在红色线以上的因子作为合适的个数或者取明显的转折点处的因子个数，从图中看，也是取3个比较合适。\n但是提取几个因子并没有绝对的标准，我们可以结合多种方法或专业知识，特征值的大小和累积方差贡献率等，这个和提取主成分的方法类似。\n下面给大家简单展示一下nfactors函数的用法：\n\n# 多种方法确定最佳因子或主成分个数\nnfactors(df.use)\n\n\n\n\n\n\n\n## \n## Number of factors\n## Call: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n##     n.obs = n.obs, plot = FALSE, title = title, use = use, cor = cor)\n## VSS complexity 1 achieves a maximimum of 0.68  with  5  factors\n## VSS complexity 2 achieves a maximimum of 0.85  with  6  factors\n## The Velicer MAP achieves a minimum of 0.07  with  2  factors \n## Empirical BIC achieves a minimum of  -41.74  with  2  factors\n## Sample Size adjusted BIC achieves a minimum of  3.18  with  5  factors\n## \n## Statistics by number of factors \n##   vss1 vss2   map dof   chisq    prob sqresid  fit RMSEA     BIC SABIC complex\n## 1 0.48 0.00 0.090  27 7.8e+01 7.8e-07    8.09 0.48 0.227 -18.848  65.5     1.0\n## 2 0.54 0.73 0.073  19 3.2e+01 3.0e-02    4.27 0.73 0.136 -35.955  23.4     1.4\n## 3 0.62 0.81 0.097  12 2.0e+01 7.0e-02    2.34 0.85 0.132 -23.156  14.3     1.4\n## 4 0.66 0.82 0.144   6 7.7e+00 2.6e-01    1.63 0.90 0.084 -13.795   4.9     1.4\n## 5 0.68 0.84 0.205   1 3.6e+00 5.6e-02    0.97 0.94 0.269   0.056   3.2     1.4\n## 6 0.59 0.85 0.290  -3 9.5e-01      NA    0.63 0.96    NA      NA    NA     1.7\n## 7 0.55 0.73 0.486  -6 2.7e-09      NA    0.45 0.97    NA      NA    NA     1.8\n## 8 0.55 0.71 1.000  -8 0.0e+00      NA    0.45 0.97    NA      NA    NA     2.0\n## 9 0.55 0.71    NA  -9 0.0e+00      NA    0.45 0.97    NA      NA    NA     2.0\n##    eChisq    SRMR eCRMS  eBIC\n## 1 9.4e+01 1.9e-01 0.220  -2.7\n## 2 2.6e+01 1.0e-01 0.139 -41.7\n## 3 7.0e+00 5.2e-02 0.090 -36.0\n## 4 2.3e+00 3.0e-02 0.073 -19.2\n## 5 8.6e-01 1.8e-02 0.109  -2.7\n## 6 1.2e-01 6.9e-03    NA    NA\n## 7 1.9e-10 2.7e-07    NA    NA\n## 8 2.3e-16 3.0e-10    NA    NA\n## 9 2.3e-16 3.0e-10    NA    NA\n\n但是这个函数的输出结果有些凌乱了，现在我更推荐大家使用parameters，其实就是对nfactors函数的重新包装：\n\nn &lt;- parameters::n_factors(df.use)\nn\n## # Method Agreement Procedure:\n## \n## The choice of 3 dimensions is supported by 6 (31.58%) methods out of 19 (CNG, Optimal coordinates, Acceleration factor, Parallel analysis, Kaiser criterion, Scree (SE)).\n\n6个方法都支持选择3个因子,如果要想查看所有的方法及每个方法选择的因子个数,可以直接变为数据框查看:\n\nas.data.frame(n) # 一共有18个方法!（选0个因子的不算）\n##    n_Factors              Method              Family\n## 1          0          Scree (R2)            Scree_SE\n## 2          2             Bentler             Bentler\n## 3          2                   t Multiple_regression\n## 4          2                   p Multiple_regression\n## 5          2       Velicer's MAP        Velicers_MAP\n## 6          2                 BIC                 BIC\n## 7          3                 CNG                 CNG\n## 8          3 Optimal coordinates               Scree\n## 9          3 Acceleration factor               Scree\n## 10         3   Parallel analysis               Scree\n## 11         3    Kaiser criterion               Scree\n## 12         3          Scree (SE)            Scree_SE\n## 13         4                beta Multiple_regression\n## 14         5    VSS complexity 1                 VSS\n## 15         5      BIC (adjusted)                 BIC\n## 16         6    VSS complexity 2                 VSS\n## 17         7            Bartlett             Barlett\n## 18         7            Anderson             Barlett\n## 19         7              Lawley             Barlett\n\n或者也可以查看这十几种结果的汇总表:\n\nsummary(n)\n##   n_Factors n_Methods Variance_Cumulative\n## 1         2         5           0.4936863\n## 2         3         6           0.6385558\n## 3         4         1           0.7072393\n## 4         5         2           0.7574222\n## 5         6         1           0.7928020\n## 6         7         3           0.8133083\n\n当然也可以把结果画出来的,借助see这个包即可:\n\nlibrary(see)\nplot(n)+theme_modern()\n\n\n\n\n\n\n\n\n横坐标是选择的因子数量,左侧纵坐标是方法的数量（百分比）,右侧纵坐标是解释的方差百分比。先看条形图，选择3个因子的方法最多，占了18个方法的30%以上；再看折线图，选择因子越多，解释的方差百分比越多，不过超过3之后就变化不大了。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>探索性因子分析</span>"
    ]
  },
  {
    "objectID": "1023-factoranalysis.html#因子分析初体验",
    "href": "1023-factoranalysis.html#因子分析初体验",
    "title": "32  探索性因子分析",
    "section": "32.3 因子分析初体验",
    "text": "32.3 因子分析初体验\n下面我们首先用9个因子进行因子分析，看看结果再说。\nrotate参数确定旋转方法，有多种不同的选择，比如不旋转、正交旋转法（比如最大方差法）、斜交旋转法等，\nfm参数选择提取公共因子的计算方法，比如最大似然法ml、主轴迭代法pa、加权最小二乘wls、广义加权最小二乘gls、最小残差minres（默认），等。\n\n# 进行因子分析，首先9个因子用一下看看结果再说，最大似然法，不旋转\nfa.res &lt;- fa(df.use, nfactors = 9, rotate = \"none\", fm=\"ml\")\nfa.res\n## Factor Analysis using method =  ml\n## Call: fa(r = df.use, nfactors = 9, rotate = \"none\", fm = \"ml\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                ML1   ML2   ML3   ML4   ML5   ML6 ML7 ML8 ML9   h2   u2 com\n## 门诊人次      0.14  0.68  0.09  0.37 -0.06 -0.03   0   0   0 0.63 0.37 1.7\n## 出院人数      0.68 -0.30  0.19  0.32  0.06  0.03   0   0   0 0.69 0.31 2.1\n## 病床利用率    0.58  0.52 -0.09 -0.31  0.04  0.05   0   0   0 0.71 0.29 2.6\n## 病床周转次数  0.90  0.20 -0.01 -0.01  0.01 -0.01   0   0   0 0.85 0.15 1.1\n## 平均住院天数 -0.53  0.43  0.38  0.05  0.02  0.09   0   0   0 0.62 0.38 2.9\n## 治愈好转率   -0.03 -0.07  0.69 -0.18  0.13 -0.02   0   0   0 0.53 0.47 1.2\n## 病死率       -0.33  0.08 -0.46  0.18  0.24  0.09   0   0   0 0.42 0.58 2.9\n## 诊断符合率   -0.31  0.54  0.02  0.02 -0.15  0.07   0   0   0 0.41 0.59 1.8\n## 抢救成功率    0.41 -0.64  0.07 -0.03 -0.12  0.10   0   0   0 0.60 0.40 1.9\n## \n##                        ML1  ML2  ML3  ML4  ML5  ML6  ML7  ML8  ML9\n## SS loadings           2.27 1.75 0.89 0.40 0.12 0.04 0.00 0.00 0.00\n## Proportion Var        0.25 0.19 0.10 0.04 0.01 0.00 0.00 0.00 0.00\n## Cumulative Var        0.25 0.45 0.55 0.59 0.60 0.61 0.61 0.61 0.61\n## Proportion Explained  0.41 0.32 0.16 0.07 0.02 0.01 0.00 0.00 0.00\n## Cumulative Proportion 0.41 0.74 0.90 0.97 0.99 1.00 1.00 1.00 1.00\n## \n## Mean item complexity =  2\n## Test of the hypothesis that 9 factors are sufficient.\n## \n## df null model =  36  with the objective function =  3.82 with Chi Square =  119.03\n## df of  the model are -9  and the objective function was  0.46 \n## \n## The root mean square of the residuals (RMSR) is  0.04 \n## The df corrected root mean square of the residuals is  NA \n## \n## The harmonic n.obs is  36 with the empirical chi square  3.42  with prob &lt;  NA \n## The total n.obs was  36  with Likelihood Chi Square =  11.69  with prob &lt;  NA \n## \n## Tucker Lewis Index of factoring reliability =  2.377\n## Fit based upon off diagonal values = 0.99\n## Measures of factor score adequacy             \n##                                                    ML1  ML2  ML3   ML4   ML5\n## Correlation of (regression) scores with factors   0.94 0.90 0.79  0.70  0.42\n## Multiple R square of scores with factors          0.88 0.80 0.63  0.50  0.18\n## Minimum correlation of possible factor scores     0.77 0.61 0.26 -0.01 -0.65\n##                                                     ML6 ML7 ML8 ML9\n## Correlation of (regression) scores with factors    0.26   0   0   0\n## Multiple R square of scores with factors           0.07   0   0   0\n## Minimum correlation of possible factor scores     -0.86  -1  -1  -1\n\n首先看因子载荷矩阵，即结果中的Standardized loadings：\n在因子载荷矩阵中，每一行是一个原始变量，每一列是一个公共因子，这里我们由于使用了9个因子，所以结果中也显示为9个因子（ML1-ML9）。例如：门诊人次在ML1上的载荷为0.142，在ML2上的载荷为0.679。载荷的绝对值越大，说明该变量与对应因子的相关性越强。正载荷表示正相关，负载荷表示负相关。\nh2是公因子方差，或者叫公共度或者共性方差，英文communality，表示每个原始变量的方差中能够被公共因子所解释的比例，u2是独特性（uniqueness），u2=1-h2，表示不能被公共因子解释的比例。公因子方差越接近1，说明该变量与公共因子的相关性越强，在因子分析中越能被有效解释。\ncom表示每个变量的复杂性（complexity）。变量的复杂性是指该变量在多个公共因子上的载荷分散程度。简单来说，它反映了一个变量与多个公共因子的关联情况。在因子分析中，我们提取公共因子来解释原始变量之间的相关性，如果一个变量只与一个公共因子有较强的相关性，那么它的复杂性就低；如果一个变量与多个公共因子都有一定程度的相关性，那么它的复杂性就高。\n\nSS loadings：每个因子的载荷平方和，也称为特征值（eigenvalues）。它表示每个公共因子所解释的总方差。例如：ML1的SS loadings是2.27，说明ML1解释了约2.27个单位的方差。\nProportion Var：每个因子解释的方差占总方差的比例。例如：ML1解释了总方差的0.25（25%）。\nCumulative Var：多个因子累积解释的方差占总方差的比例。例如：ML1解释了总方差的0.25（25%），ML2和ML1解释了总方差的0.45（0.25+0.19），以此类推。\nProportion Explained：方差解释的相对比例，即Proportion Var/sum(Proportion Var)\nCumulative Proportion：Proportion Explained的加和。\n\n根据结果中的Cumulative Var，累积方差解释，可以看到在使用3个因子时，累计贡献度是0.55，4个因子是0.59，结合碎石图，我们选择用4个因子（前面的方法都是建议选3个，选4个是为了和书中保持一致，你可以选择3个试试看，毕竟这个没有金标准）。\n\nMean item complexity = 2：表示平均复杂性是2。\n\n然后是模型拟合的假设检验结果：\ndf null model =  36  with the objective function =  3.82 with Chi Square =  119.03\ndf of  the model are -9  and the objective function was  0.46 \nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  NA \n零模型（即假设所有变量相互独立）的自由度为36，目标函数值为3.82，卡方值为119.03。提取9个公共因子的模型的自由度为-9，目标函数值为0.46。目标函数值越小，说明模型拟合得越好。\n残差均方根（RMSR）衡量了模型预测值与实际值之间的差异程度。RMSR值越小，说明模型拟合效果越好。经过自由度校正后的残差均方根是NA。\n再往下是观测数量：\nThe harmonic n.obs is  36 with the empirical chi square  3.42  with prob &lt;  NA \nThe total n.obs was  36  with Likelihood Chi Square =  11.69  with prob &lt;  NA \n所有观测对数的调和平均数（the harmonic mean of all the pairwise counts of observations）是36，经验卡方是3.42。总的观测数量是36，似然比卡方是11.69。\n再往下是拟合指数：\nTucker Lewis Index of factoring reliability =  2.377\nFit based upon off diagonal values = 0.99\nTucker Lewis指数一般会在结构方程模型中报告，也被称为非范拟合指数（Non - Normed Fit Index，NNFI），是在因子分析以及结构方程模型等领域用于评估模型拟合优度的一个重要指标，通常希望大于0.9，值越大说明拟合效果越好。这里是2.377。\nFit based upon off diagonal values：可以将其视为1-resid2/cor2，或者是一将应用于相关矩阵而不是原始数据的R2。\n最后是因子得分充分性度量：\nMeasures of factor score adequacy             \n                                                   ML1  ML2  ML3   ML4   ML5   ML6 ML7 ML8 ML9\nCorrelation of (regression) scores with factors   0.94 0.90 0.79  0.70  0.42  0.26   0   0   0\nMultiple R square of scores with factors          0.88 0.80 0.63  0.50  0.18  0.07   0   0   0\nMinimum correlation of possible factor scores     0.77 0.61 0.26 -0.01 -0.65 -0.86  -1  -1  -1\n这些结果告诉我们因子得分估计对潜在结构的代表性，可以被称为不确定性指数。\n\nCorrelation of (regression) scores with factors：Multiple R square的平方根，可认为是因子得分估计的决定系数的上限，本质上是因子与观测数据之间的（多重）相关系数。\nMultiple R square of scores with factors：和回归分析中的R2的意思类似，越大越好。\nMinimum correlation of possible factor scores：2*R2-1\n\n以上结果中的每个部分都可以单独提取，例如：\n\nfa.res$loadings # 查看因子载荷矩阵\nfa.res$communality # 公因子方差\nfa.res$uniquenesses # 独特性\nfa.res$complexity # 复杂性\nfa.res$Vaccounted # 查看载荷平方和、方差解释比等\nfa.res$STATISTIC # 似然比卡方\nfa.res$dof # 公因子模型的自由度\nfa.res$PVAL # P值",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>探索性因子分析</span>"
    ]
  },
  {
    "objectID": "1023-factoranalysis.html#进行因子分析",
    "href": "1023-factoranalysis.html#进行因子分析",
    "title": "32  探索性因子分析",
    "section": "32.4 进行因子分析",
    "text": "32.4 进行因子分析\n下面我们选择4个因子，重新进行因子分析，先不旋转：\n\n# 选择4个因子，不旋转，主因子解：最小残差法\nfa.res &lt;- fa(df.use, nfactors = 4, rotate = \"none\", fm=\"minres\")\n## Warning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, :\n## The estimated weights for the factor scores are probably incorrect.  Try a\n## different factor score estimation method.\n## Warning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, : An\n## ultra-Heywood case was detected.  Examine the results carefully\nfa.res\n## Factor Analysis using method =  minres\n## Call: fa(r = df.use, nfactors = 4, rotate = \"none\", fm = \"minres\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                MR1   MR2   MR3   MR4   h2       u2 com\n## 门诊人次     -0.20  0.86  0.05  0.47 1.00  0.00093 1.7\n## 出院人数      0.72  0.02  0.12  0.34 0.65  0.35197 1.5\n## 病床利用率    0.31  0.65  0.00 -0.46 0.73  0.27437 2.3\n## 病床周转次数  0.80  0.58  0.01 -0.10 1.00  0.00227 1.9\n## 平均住院天数 -0.65  0.18  0.33  0.05 0.56  0.43758 1.7\n## 治愈好转率    0.02 -0.13  0.99 -0.08 1.00 -0.00329 1.1\n## 病死率       -0.32 -0.05 -0.37  0.00 0.24  0.75666 2.0\n## 诊断符合率   -0.49  0.35  0.02 -0.08 0.37  0.63478 1.9\n## 抢救成功率    0.63 -0.39  0.01  0.11 0.56  0.44236 1.7\n## \n##                        MR1  MR2  MR3  MR4\n## SS loadings           2.45 1.82 1.25 0.58\n## Proportion Var        0.27 0.20 0.14 0.06\n## Cumulative Var        0.27 0.48 0.61 0.68\n## Proportion Explained  0.40 0.30 0.20 0.09\n## Cumulative Proportion 0.40 0.70 0.91 1.00\n## \n## Mean item complexity =  1.7\n## Test of the hypothesis that 4 factors are sufficient.\n## \n## df null model =  36  with the objective function =  3.82 with Chi Square =  119.03\n## df of  the model are 6  and the objective function was  0.27 \n## \n## The root mean square of the residuals (RMSR) is  0.03 \n## The df corrected root mean square of the residuals is  0.07 \n## \n## The harmonic n.obs is  36 with the empirical chi square  2.32  with prob &lt;  0.89 \n## The total n.obs was  36  with Likelihood Chi Square =  7.71  with prob &lt;  0.26 \n## \n## Tucker Lewis Index of factoring reliability =  0.859\n## RMSEA index =  0.084  and the 90 % confidence intervals are  0 0.25\n## BIC =  -13.8\n## Fit based upon off diagonal values = 0.99\n\n选择4个因子，最终的累积方差解释是0.68，再看因子载荷矩阵，因子1（MR1）在出院人数、门诊人次、病床周转率、平均住院天数等具有较高的载荷，因子2在门诊人次、病床利用率、病床周转率等具有很大的载荷，因子3在治愈好转率具有较高的载荷，因子4的载荷都较低。\n从专业角度来看，主因子解没有发现什么规律，好像不能很好的解释专业意义。\n下面再试试主成分解的因子分析：\n\n# 选择4个因子，不旋转，主成分解\nfa.res &lt;- fa(df.use, nfactors = 4, rotate = \"none\", fm=\"pa\")\n## Warning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, :\n## The estimated weights for the factor scores are probably incorrect.  Try a\n## different factor score estimation method.\n## Warning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, : An\n## ultra-Heywood case was detected.  Examine the results carefully\nfa.res\n## Factor Analysis using method =  pa\n## Call: fa(r = df.use, nfactors = 4, rotate = \"none\", fm = \"pa\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                PA1   PA2   PA3   PA4   h2     u2 com\n## 门诊人次     -0.20  0.88  0.07  0.47 1.03 -0.034 1.7\n## 出院人数      0.72  0.02  0.11  0.34 0.64  0.357 1.5\n## 病床利用率    0.31  0.63  0.02 -0.45 0.69  0.305 2.3\n## 病床周转次数  0.81  0.59  0.02 -0.13 1.02 -0.023 1.9\n## 平均住院天数 -0.64  0.17  0.32  0.05 0.55  0.453 1.7\n## 治愈好转率    0.02 -0.16  1.04 -0.07 1.12 -0.123 1.1\n## 病死率       -0.32 -0.04 -0.36 -0.01 0.23  0.770 2.0\n## 诊断符合率   -0.49  0.35  0.03 -0.09 0.37  0.635 1.9\n## 抢救成功率    0.63 -0.39 -0.01  0.13 0.56  0.440 1.8\n## \n##                        PA1  PA2  PA3  PA4\n## SS loadings           2.46 1.84 1.34 0.58\n## Proportion Var        0.27 0.20 0.15 0.06\n## Cumulative Var        0.27 0.48 0.63 0.69\n## Proportion Explained  0.40 0.30 0.22 0.09\n## Cumulative Proportion 0.40 0.69 0.91 1.00\n## \n## Mean item complexity =  1.7\n## Test of the hypothesis that 4 factors are sufficient.\n## \n## df null model =  36  with the objective function =  3.82 with Chi Square =  119.03\n## df of  the model are 6  and the objective function was  0.26 \n## \n## The root mean square of the residuals (RMSR) is  0.03 \n## The df corrected root mean square of the residuals is  0.07 \n## \n## The harmonic n.obs is  36 with the empirical chi square  2.25  with prob &lt;  0.9 \n## The total n.obs was  36  with Likelihood Chi Square =  7.46  with prob &lt;  0.28 \n## \n## Tucker Lewis Index of factoring reliability =  0.88\n## RMSEA index =  0.077  and the 90 % confidence intervals are  0 0.246\n## BIC =  -14.04\n## Fit based upon off diagonal values = 0.99\n\n最终的累积方差解释是0.69，再看因子载荷矩阵，因子1（PA1）在出院人数、病床周转、平均住院天数等具有较高的载荷，因子2在门诊人次、病床利用率等具有很大的载荷，因子3在治愈好转率具有较高的载荷，因子4在门诊人次、病床利用率、具有较高的载荷。\n从专业角度来看，因子1可以反应病房的情况，但是其他因子貌似没什么专业意义。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>探索性因子分析</span>"
    ]
  },
  {
    "objectID": "1023-factoranalysis.html#因子旋转",
    "href": "1023-factoranalysis.html#因子旋转",
    "title": "32  探索性因子分析",
    "section": "32.5 因子旋转",
    "text": "32.5 因子旋转\n建立因子分析模型的目的不仅是找出公因子，更重要的是弄清各公因子的专业意义，以便对实际问题进行分析。然而在很多情况下，因子分析的主成分解、主因子解的各公因子的典型代表变量并不是很突出，容易使各公因子的专业意义难以解释，从而达不到因子分析的主要目的。\n通过因子旋转我们可以更容易找到内在规律，使得结果更加容易结合专业背景进行解释。\n常见的因子旋转方法有正交旋转和斜交旋转两大类。正交旋转能保持各个指标的共性方差不变，每个因子的意义相对独立和清晰，解释起来较为简单直观，在一些探索性研究或初步分析中，有助于简化分析过程和理解因子结构。斜交旋转在解释因子结构时相对复杂一些，需要同时考虑因子载荷和因子之间的相关系数。但在一些复杂的研究问题中，能够提供更全面和深入的信息。\n\n# 选择4个因子，最大方差旋转，最大似然法\nfa.res &lt;- fa(df.use, nfactors = 4, rotate = \"varimax\", fm=\"ml\")\nfa.res\n## Factor Analysis using method =  ml\n## Call: fa(r = df.use, nfactors = 4, rotate = \"varimax\", fm = \"ml\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                ML3   ML1   ML2   ML4   h2    u2 com\n## 门诊人次     -0.31  0.23 -0.03  0.92 1.00 0.005 1.4\n## 出院人数      0.75  0.16  0.24  0.27 0.72 0.276 1.6\n## 病床利用率   -0.10  0.83  0.03  0.07 0.71 0.289 1.0\n## 病床周转次数  0.46  0.84  0.09  0.26 1.00 0.005 1.8\n## 平均住院天数 -0.64 -0.23  0.24  0.21 0.57 0.435 1.8\n## 治愈好转率   -0.09 -0.09  0.98 -0.10 1.00 0.005 1.1\n## 病死率       -0.20 -0.18 -0.42 -0.06 0.26 0.743 1.9\n## 诊断符合率   -0.56  0.02 -0.10  0.18 0.36 0.642 1.3\n## 抢救成功率    0.70 -0.04  0.04 -0.21 0.55 0.455 1.2\n## \n##                        ML3  ML1  ML2  ML4\n## SS loadings           2.15 1.58 1.29 1.12\n## Proportion Var        0.24 0.18 0.14 0.12\n## Cumulative Var        0.24 0.41 0.56 0.68\n## Proportion Explained  0.35 0.26 0.21 0.18\n## Cumulative Proportion 0.35 0.61 0.82 1.00\n## \n## Mean item complexity =  1.4\n## Test of the hypothesis that 4 factors are sufficient.\n## \n## df null model =  36  with the objective function =  3.82 with Chi Square =  119.03\n## df of  the model are 6  and the objective function was  0.24 \n## \n## The root mean square of the residuals (RMSR) is  0.04 \n## The df corrected root mean square of the residuals is  0.09 \n## \n## The harmonic n.obs is  36 with the empirical chi square  3.43  with prob &lt;  0.75 \n## The total n.obs was  36  with Likelihood Chi Square =  6.84  with prob &lt;  0.34 \n## \n## Tucker Lewis Index of factoring reliability =  0.931\n## RMSEA index =  0.055  and the 90 % confidence intervals are  0 0.235\n## BIC =  -14.67\n## Fit based upon off diagonal values = 0.99\n## Measures of factor score adequacy             \n##                                                    ML3  ML1  ML2  ML4\n## Correlation of (regression) scores with factors   0.93 0.96 1.00 0.98\n## Multiple R square of scores with factors          0.86 0.92 0.99 0.96\n## Minimum correlation of possible factor scores     0.72 0.85 0.99 0.91\n\n此时我们再看因子载荷阵，因子3在出院人数、平均住院天数、诊断符合率、抢救成功率等多个指标上具有较大的载荷，因子2在治愈好转率、病死率上载荷最大，因子1在病床利用率、病床周转次数这两个指标上载荷最高，因子4在门诊人次的载荷最大。\n因此可以认为因子3反映了反映了医疗工作质量各个方面的情况，称为综合因子；因子1反应病床利用情况，可以称为病床利用因子；因子2反映了医疗水平，称为水平因子；因子4反应了就诊患者数量，称为数量因子。\n由于因子分析的结果并不是唯一的，大家可以多尝试其他方法，可根据具体情况选择不同方法来获得符合客观实际的解。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>探索性因子分析</span>"
    ]
  },
  {
    "objectID": "1023-factoranalysis.html#结果可视化",
    "href": "1023-factoranalysis.html#结果可视化",
    "title": "32  探索性因子分析",
    "section": "32.6 结果可视化",
    "text": "32.6 结果可视化\n展示每个因子在各个变量中的载荷大小。这个图也叫路径图，在结构方程模型和验证性因子分析中非常常见。\n\nfa.diagram(fa.res)\n\n\n\n\n\n\n\n\n下面这张图是另一种展示方法，只看右上三角或者左下三角即可，是一样的。以右上三角为例：以第一行为例，第一行是ML3这个因子，看第一行第二列的这个子图形，它的横坐标是ML1这个因子，纵坐标是ML3这个因子，图中的点（被文字覆盖住了）是不同的变量，每个点的坐标就是该变量在不同因子上的载荷值。\n\n# 这个图实在是有些太花里胡哨了\nfactor.plot(fa.res,\n            labels = colnames(df.use[,c(-1,-2)]),\n            show.points = T)\n\n\n\n\n\n\n\n# 或者plot(fa.res)\n\n由于上图太过于花里胡哨，不便解读，所以我们也可以直接自己提取数据，使用其他可视化R包自己绘制图形，我这里演示下如何使用pheatmap通过热图的形式展示因子载荷：\n\n# 1行代码就行了\npheatmap::pheatmap(t(fa.res$loadings),cluster_rows = F,cluster_cols = F)\n\n\n\n\n\n\n\n\n这个热图就很好解读了，横坐标是不同的变量，纵坐标是不同的因子，颜色深浅表示载荷的绝对值大小，红色表示正相关，蓝色表示负相关。是不是很直观呢？",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>探索性因子分析</span>"
    ]
  },
  {
    "objectID": "1023-factoranalysis.html#参考资料",
    "href": "1023-factoranalysis.html#参考资料",
    "title": "32  探索性因子分析",
    "section": "32.7 参考资料",
    "text": "32.7 参考资料\n\nhttps://m-clark.github.io/posts/2020-04-10-psych-explained/\n孙振球医学统计学第4版和第5版\nhttps://stats.oarc.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/\nR语言医学多元统计分析，赵军、戴静毅",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>探索性因子分析</span>"
    ]
  },
  {
    "objectID": "1016-partialcorrelation.html",
    "href": "1016-partialcorrelation.html",
    "title": "33  偏相关和典型相关分析",
    "section": "",
    "text": "33.1 偏相关（partial correlation）\n使用R包ppcor实现。\n首先是加载数据和R包。\nlibrary(ppcor)\n\ndf &lt;- haven::read_sav(\"datasets/data01.sav\")\ndf1 &lt;- df[,2:4]\nnames(df1) &lt;- c(\"height\",\"weight\",\"vc\")\nhead(df1)\n## # A tibble: 6 × 3\n##   height weight    vc\n##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1   139.   30.4  2   \n## 2   164.   46.2  2.75\n## 3   156.   37.1  2.75\n## 4   156.   35.5  2   \n## 5   150.   31    1.5 \n## 6   145    33    2.5\n这个数据有3列，现在我们要探索身高（height）和体重（weight）的关系，其中vc是需要控制的因素。\n首先进行pearson偏相关分析：\np1 &lt;- pcor(df1,method = \"pearson\")\np1\n## $estimate\n##            height    weight         vc\n## height  1.0000000 0.7941292 -0.2022408\n## weight  0.7941292 1.0000000  0.6166786\n## vc     -0.2022408 0.6166786  1.0000000\n## \n## $p.value\n##              height       weight          vc\n## height 0.0000000000 0.0000491115 0.406351395\n## weight 0.0000491115 0.0000000000 0.004920346\n## vc     0.4063513954 0.0049203462 0.000000000\n## \n## $statistic\n##            height   weight         vc\n## height  0.0000000 5.387551 -0.8514549\n## weight  5.3875507 0.000000  3.2299064\n## vc     -0.8514549 3.229906  0.0000000\n## \n## $n\n## [1] 20\n## \n## $gp\n## [1] 1\n## \n## $method\n## [1] \"pearson\"\n结果中$estimate给出了偏相关系数，可以看到在控制了vc后，height和weight的偏相关系数是0.7941292；$p.value给出了相应的P值，$statistic给出了检验统计量。\n上面演示的是pearson偏相关分析，下面展示一个spearman偏相关分析。\n# 加载数据\ndf2 &lt;- haven::read_sav(\"datasets/data02.sav\")\nnames(df2) &lt;- c(\"id\",\"x\",\"y\",\"z\")\n\nhead(df2)\n## # A tibble: 6 × 4\n##      id x         y             z\n##   &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt;\n## 1     7 1 [矮]    1 [轻]     1.25\n## 2    17 1 [矮]    1 [轻]     1.25\n## 3     1 1 [矮]    1 [轻]     2   \n## 4    11 1 [矮]    1 [轻]     2   \n## 5     5 2 [中]    1 [轻]     1.5 \n## 6    15 2 [中]    1 [轻]     1.5\n现在我们要计算x和y的相关性，z是要控制的因素，由于这两个变量是分类变量，所以要用spearman偏相关分析。\n其实用法是一样的，就是改个参数而已：\npcor(df2[,-1],method = \"spearman\")\n## $estimate\n##            x         y          z\n## x  1.0000000 0.6985577 -0.4212568\n## y  0.6985577 1.0000000  0.8486095\n## z -0.4212568 0.8486095  1.0000000\n## \n## $p.value\n##              x            y            z\n## x 0.0000000000 8.779998e-04 7.245901e-02\n## y 0.0008779998 0.000000e+00 4.386687e-06\n## z 0.0724590110 4.386687e-06 0.000000e+00\n## \n## $statistic\n##           x        y         z\n## x  0.000000 4.025172 -1.915103\n## y  4.025172 0.000000  6.613943\n## z -1.915103 6.613943  0.000000\n## \n## $n\n## [1] 20\n## \n## $gp\n## [1] 1\n## \n## $method\n## [1] \"spearman\"\n结果解读同上。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>偏相关和典型相关分析</span>"
    ]
  },
  {
    "objectID": "1016-partialcorrelation.html#偏相关partial-correlation",
    "href": "1016-partialcorrelation.html#偏相关partial-correlation",
    "title": "33  偏相关和典型相关分析",
    "section": "",
    "text": "33.1.1 偏相关散点图\n还是用df1的数据作为演示，现在是研究weight对height的影响，vc是需要控制的变量。\n所以我们可以分别计算残差，用残差的散点图代表偏相关的散点图。\n\n# 首先计算height为因变量，vc是自变量的残差\nresidX &lt;- resid(lm(height~vc,data = df1))\n\n# 再计算weight为因变量，vc是自变量的残差\nresidY &lt;- resid(lm(weight~vc, data = df1))\n\n# 两个残差的相关系数就是weight和height的偏相关系数！\ncor(residX, residY, method = \"pearson\")\n## [1] 0.7941292\n\n# 画图即可\nplot(residX, residY)\n\n\n\n\n\n\n\n\n但是这个图的横纵坐标取值范围对实际来说是不能解释的，所以我们可以分别加上它们各自的平均值，然后再画散点图，方法借鉴了这篇文章：\n\nresidX1 &lt;- residX + mean(df1$height)\nresidY1 &lt;- residY + mean(df1$weight)\n\nplot(residX1, residY1,xlab = \"身高\",ylab = \"体重\")\n\n\n\n\n\n\n\n\n这个就是偏相关散点图了！",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>偏相关和典型相关分析</span>"
    ]
  },
  {
    "objectID": "1016-partialcorrelation.html#典型相关canonical-correlation",
    "href": "1016-partialcorrelation.html#典型相关canonical-correlation",
    "title": "33  偏相关和典型相关分析",
    "section": "33.2 典型相关（Canonical Correlation）",
    "text": "33.2 典型相关（Canonical Correlation）\n这个数据来自孙振球《医学统计学》第四版的例23-1，探讨小学生的生长发育指标（肺活量、身高、体重、胸围）和身体素质（短跑、跳高、跳远、实心球）的相互关系。\n\ndf &lt;- read.csv(\"datasets/例23-1.csv\",header = T)\npsych::headtail(df)\n## Warning in psych::headtail(df): headtail is deprecated.  Please use the\n## headTail function\n##     肺活量  身高 体重 胸围 短跑 跳高 跳远 实心球\n## 1     1210 120.1 23.8   61 10.2 66.3 2.01   2.73\n## 2     1210 120.7 23.4 59.8 11.3 67.6 1.92   2.71\n## 3     1040 121.2 22.9   59 10.1 66.5 1.92    2.6\n## 4     1620 121.5 24.6 59.5  9.5 67.8 1.95   2.64\n## ...    ...   ...  ...  ...  ...  ...  ...    ...\n## 81    1310 129.7 24.7 61.7 10.1 69.4 2.03    2.8\n## 82    2280 143.6 37.6   70  9.7 88.8 2.17   4.18\n## 83    1580 136.6 32.3 67.2 10.3 87.1 2.66   4.04\n## 84    2370 147.4 38.8   73 10.8 90.7 2.82   4.38\n\n典型相关分析R语言自带了cancor()函数，无需借助第三方R包：\n\n# 前4个变量和后4个变量做相关性，直接提供2个数据框也可以\ncc1 &lt;- cancor(df[,1:4],df[,5:8])\n\ncc1\n## $cor\n## [1] 0.8858445 0.2791523 0.1940486 0.0379654\n## \n## $xcoef\n##                 [,1]          [,2]         [,3]          [,4]\n## 肺活量 -5.267493e-05 -0.0001955795 -0.000407694  0.0002971469\n## 身高   -7.754975e-03 -0.0086910713  0.021599065  0.0079782016\n## 体重   -3.471120e-03 -0.0180620718 -0.015626841 -0.0522321990\n## 胸围   -1.552353e-02  0.0464952778  0.004886088  0.0178728641\n## \n## $ycoef\n##               [,1]        [,2]        [,3]        [,4]\n## 短跑    0.02340474 -0.08458262  0.07017709 -0.13566387\n## 跳高   -0.01068107 -0.02440377  0.01443519  0.01626168\n## 跳远   -0.02867642  0.92500098  0.23862503 -0.29882238\n## 实心球 -0.06884355 -0.07825414 -0.29442851 -0.19118769\n## \n## $xcenter\n##     肺活量       身高       体重       胸围 \n## 1490.47619  131.52024   26.44405   61.51190 \n## \n## $ycenter\n##      短跑      跳高      跳远    实心球 \n## 10.271429 72.805952  2.109048  2.978929\n\n$cor给出了两组数据之间的典型相关系数，$xcoef是第一组的典型相关系数，可以看到计算出了4个虚拟变量，$ycoef是第二组的典型相关系数。\n下面进行典型相关的显著性检验，使用R包CCP实现。\n\nlibrary(CCP)\n\nrho &lt;- cc1$cor\nn &lt;- dim(df[,1:4])[1]\np &lt;- length(df[,1:4])\nq &lt;- length(df[,5:8])\n\np.asym()函数实现典型相关的显著性检验。需要典型相关系数、观测个数、第一组的变量个数、第二组的变量个数。\n\n# 4种典型相关的结果\np.asym(rho,n,p,q, tstat = \"Wilks\")\n## Wilks' Lambda, using F-approximation (Rao's F):\n##               stat     approx df1      df2   p.value\n## 1 to 4:  0.1907537 10.4765088  16 232.8215 0.0000000\n## 2 to 4:  0.8860745  1.0618303   9 187.5484 0.3930330\n## 3 to 4:  0.9609581  0.7843615   4 156.0000 0.5369444\n## 4 to 4:  0.9985586  0.1140327   1  79.0000 0.7364945\np.asym(rho,n,p,q, tstat = \"Hotelling\")\n##  Hotelling-Lawley Trace, using F-approximation:\n##                 stat     approx df1 df2   p.value\n## 1 to 4:  3.770206950 17.5550261  16 298 0.0000000\n## 2 to 4:  0.125083307  1.0632081   9 306 0.3898996\n## 3 to 4:  0.040571670  0.7962190   4 314 0.5283457\n## 4 to 4:  0.001443452  0.1161979   1 322 0.7334177\np.asym(rho,n,p,q, tstat = \"Pillai\")\n##  Pillai-Bartlett Trace, using F-approximation:\n##                 stat    approx df1 df2      p.value\n## 1 to 4:  0.901742684 5.7482049  16 316 5.963363e-11\n## 2 to 4:  0.117022206 1.0849404   9 324 3.733220e-01\n## 3 to 4:  0.039096223 0.8192541   4 332 5.135803e-01\n## 4 to 4:  0.001441371 0.1225607   1 340 7.264904e-01\np.asym(rho,n,p,q, tstat = \"Roy\")\n##  Roy's Largest Root, using F-approximation:\n##               stat   approx df1 df2 p.value\n## 1 to 1:  0.7847205 71.99119   4  79       0\n## \n##  F statistic for Roy's Greatest Root is an upper bound.\n\n我们就看下Wilks结果，可以看到只有第一个典型相关系数是有意义的，后面3个都没有显著性。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>偏相关和典型相关分析</span>"
    ]
  },
  {
    "objectID": "结构方程模型.html",
    "href": "结构方程模型.html",
    "title": "34  结构方程模型",
    "section": "",
    "text": "34.1 理论知识\n结构方程模型和中学时学过的方程组的概念非常类似！其实就是一个解方程的过程。\n有时我们的数据可能有非常多的变量，比如我们有x1x10，y1y10，z1~z10，一共30个变量，不同的x之间可能都有关系，不同的y之间可能都有关系，不同的z之间可能都有关系，x,y,z之间可能也有关系，这时候你只写一个方程（或者叫模型、公式都可以）是不可能说清楚这么多关系的，比如你只写y1~x1+x2+x3+x5，那么剩下的那么多变量的关系你怎么解释？\n这时候你就可以用结构方程模型了，因为结构方程模型可以包括多个模型，每一个变量之间的关系你都可以写一个方程出来，你把这些方程全都放在一起解决（是不是和方程组的概念很像？），就是结构方程模型了，它可以帮你算清楚这么多复杂的关系。\n以下是正式的概念：\n结构方程模型（Structural Equation Modeling，SEM）是一种多元统计分析方法，属于潜变量分析的一种。结构方程模型是一种基于变量之间的协方差的矩阵来分析多变量之间结构关系的多元统计方法，也被称为协方差结构模型。SEM的主要作用是研究可观测变量和不能直接测量的变量之间的结构关系，同时还能考虑到测量误差。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>结构方程模型</span>"
    ]
  },
  {
    "objectID": "结构方程模型.html#理论知识",
    "href": "结构方程模型.html#理论知识",
    "title": "34  结构方程模型",
    "section": "",
    "text": "潜变量分析是用于处理不能直接观测到的变量的统计方法，常见的潜变量分析方法有探索性因子分析(EFA)、验证性因子分析(CFA)、结构方程模型(SEM)、潜在类别分析(LCA)、路径分析、增长曲线分析等。\n\n\n34.1.1 潜变量和显变量\n在结构方程模型中，变量分为观测变量（observed variable）和潜变量（latent variable）。\n\n观测变量是可以直接测量的变量，比如考试成绩、身高、体重等，又被称为显变量（manifest variable）、显在变量、指示变量（indicator）、度量变量（measurement variable）。\n潜变量是无法直接测量，需要通过观测变量来间接反映的变量，像学习能力、幸福感、工作满意度等，又被称为潜在变量、隐变量、潜在因子（latent factor）、潜在特质（latent trait）。\n\n\n\n34.1.2 外生变量和内生变量\n无论是潜变量还是显变量，均可分为内生变量（endogenous variable）与外生变量（exogenous variable)。\n\n内生变量指模型需要解释的变量，在模型中被看作因变量或效应变量，包括内生显变量（一般用Y表示）与内生潜变量（一般用η表示）；\n外生变量指能够对内生变量产生影响的变量，在模型中被看作自变量或解释变量，包括外生显变量（一般用X表示）与外生潜变量（一般用ξ表示）。\n\n\n\n34.1.3 测量模型和结构模型\nSEM中的模型主要包括两种（SEM类似于多个方程组成的方程组，按照使用目的，这些方程大致可以分为两类）：\n\n测量模型(measurement model)：用来描述观测变量和潜变量之间的关系。比如，我们想研究学生的“学习能力”这个潜变量、可能会通过“语文成绩”、“数学成绩”、“英语成绩”等观测变量来衡量。测量模型就是要确定这些观测变量在多大程度上能够准确地反映“学习能力”这个潜变量。\n结构模型(structural model)：用于描述潜变量之间的关系。比如，我们认为“学习能力”会影响“学习成绩”，“学习态度”也会影响“学习成绩”，同时“学习态度”还可能会影响“学习能力”，这些潜变量之间的相互影响关系就构成了结构模型。\n\n所以，SEM其实就是多个模型的结合体，因为你只用一个模型是无法说清楚这么多变量之间的关系的，所以要建立多个方程（模型）。\n当SEM只有测量模型，没有结构模型时，被称为验证性因子分析（confirmatory factor analysis，CFA）。当SEM只有结构模型，没有测量模型时，被称为路径分析（path analysis）。\n\n\n34.1.4 路径图\n路径图是用于描述变量之间关系的示意图，是SEM和验证性因子分析中必须了解的图形。其实就是用图形化的方式展示潜变量和显变量、潜变量和潜变量之间的关系，以及关系的大小（载荷）和方向等。由于SEM和CFA中通常涉及多个变量之间的复杂关系，只看数字或者文字描述很难直接理清他们的关系，但是图形可以直接反映，毕竟一图胜千言！\n其他常用的做SEM的软件，比如AMOS，是以图形界面为核心的，所以在使用这种软件进行SEM第一步就是要建立路径图，使用路径图假设各个变量之间的关系。但是R语言是以代码为核心的，直接使用代码表示多个变量之间的关系即可（即直接建立多个模型（在R中就是建立多个公式）），路径图更多是作为结果呈现的内容，而不是必须第一步就要做的。\n在之前的探索性因子分析中出现过一个简单的路径图：\n\n这个图就是一个简单的路径图，展示了4个潜变量和各自的显变量之间的关系，并且标注了因子载荷以及正负，看起来很直观。\n在CFA和SEM中，变量之间的关系比这个要复杂的多，为了规范，路径图中的圆圈、方框、箭头等都有明确的含义，以下是简单说明：\n\n\n\n34.1.5 SEM和CFA\nSEM与验证性因子分析（CFA）关系密切。根据分析目的，因子分析可分为探索性因子分析（exploratory factor analysis，EFA）和验证性因子分析（confirmatory factor analysis，CFA）。\n\n探索性因子分析：研究者事先并不清楚或不确定潜在因子与可测变量之间的关联，也不清楚可测变量会隐含多少个潜在因子。\n如果研究者根据以往的研究经验或根据探索性因子分析的结果，对所要研究的可测变量与潜在因子之间的内在结构已然清楚，即已知哪些可测变量可能被哪些潜在因子影响，只需进一步确定可测变量在潜在因子上的载荷大小，并验证这种结构与数据的吻合程度，即为验证性因子分析。\n\n与探索性因子分析要求因子之间独立不同的是，验证性因子分析允许潜在因子之间相关。\n验证性因子分析在社会、心理、教育、管理及医学等研究领域，常用于评价某个测验或量表的构念效度（constructure validity，又称结构效度）。验证性因子分析与结构方程模型有着密切的联系，其实质是SEM的测量模型部分。验证性因子分析的数学模型与探索性因子分析的数学模型类似，也是联系潜在因子与可测变量的系列方程组。验证性因子分析从理论模型的设定、参数估计、模型的评价以及模型的修正与解释等一系列的过程皆与结构方程原理类似。\n\n构念效度指的是测量工具能够准确测量理论上的构念或特质的程度。简单来说，就是看我们所使用的测量工具（如问卷、测试等）是不是真的在测量我们想要测量的那个抽象概念。比如，我们想测量一个人的“创造力”，那么所设计的测量工具是否真的能准确地把这个人的创造力水平反映出来，而不是测到了其他无关的东西，这就是构念效度要解决的问题。\n\n回归系数在结构模型中表示潜变量之间的路径系数，而测量模型中的因子载荷也是回归的一种形式。方差和协方差则涉及到潜变量的残差以及观测变量之间的关系。这些参数是模型估计的核心部分，用来评估模型是否合适，以及各个变量之间的关系强度。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>结构方程模型</span>"
    ]
  },
  {
    "objectID": "结构方程模型.html#分析步骤",
    "href": "结构方程模型.html#分析步骤",
    "title": "34  结构方程模型",
    "section": "34.2 分析步骤",
    "text": "34.2 分析步骤\n\n简单看下这部分，有个印象即可。\n\n在应用结构方程模型分析变量之间的复杂关系的时候，一般可以分为5个步骤进行：\n\n模型设定（modelspecification）；\n模型识别（model identification）；\n模型估计（model estimation）；\n模型评价（model evaluation）；\n模型修正（model modification）。\n\n\n34.2.1 模型设定\n根据研究目的和专业知识建立起观测变量与潜变量以及潜变量之间的关系，即为模型的设定。一般有3种方式设立欲拟合的结构方程模型。\n\n直接确定模型：对研究者提出的单一的假设模型进行验证。这种纯粹的验证理论模型的分析较少。\n选择最优模型：研究者提出若干个理论模型，从中选出一个拟合优度最佳的。\n导出模型：研究者先提出一个或几个理论模型，结合专业知识不断进行修正，直至模型能够很好地拟合数据为止。这种分析方法比较多见。\n\n\n\n34.2.2 模型识别\n模型识别的主要任务就是在初始模型建立之后，考虑模型中的每一个未知参数能否由观测数据得到唯一解。根据结构方程组的个数和未知参数的个数之间的关系，模型可以分为恰好识别模型、识别不足模型、过度识别模型。\n\n对于识别不足的模型，待估计参数的个数多于样本中所能得到的方程的个数，此时进行参数估计能得到无穷多个解，比如对于方程a*b=16，求a和b的值，这个结果有无数个，因为有2个未知数却只有1个方程，好理解吧？\n对于恰好识别模型，虽然可以得到唯一的参数估计值，但是无法检验其对数据的拟合优度，因为此时的自由度和卡方值都是0。意思就是只有唯一解，没法比较优劣了，那还有啥用！\n对于过度识别模型，可以对参数进行假设检验，因此过度识别模型是结构方程模型本身所追求的。意思就是可以有多个解，但是是有限的，这样还能比较优劣，这是咱们希望的情况！\n\n\n\n34.2.3 模型估计\n也就是结构方程模型估计参数的过程。主要有以下几种方法（简单看下名字就好了…）：\n\n最大似然估计 最大似然估计（maximum likelihood estimation，MLE）最常用。对于一个给定的资料，ML法是使得参数的似然函数最大化。最大似然估计有许多优良特性。它是无偏的渐近有效的一致性估计，而且不受测量单位的影响。但是，ML估计需要假设观测变量为连续性变量，且具有多元正态分布（multi-variate normal distribution）。观测变量的非正态性（non-normality），尤其是在高峰度情况下，虽然参数的估计值仍有效，但往往会得到不正确的标准误。对这个问题可以采取：①考虑对偏态分布的变量进行转换，使其近似于多元正态分布；②将可能导致偏态的离群值（outlier）删除；③应用自助重抽样（bootstrap-resampling）来估计参数估计的方差以进行显著性检验。\n未加权最小二乘法 未加权最小二乘法（unweighted least squares，ULS）是将残差阵中的每一元素求平方和并使之最小，然后利用偏导数方法求出参数估计值。这种估计方法对观测变量的分布无特殊要求，只要参数可以识别，就能获得一致性估计值。\n广义最小二乘法 广义最小二乘法（generalized least squares，GLS）估计对（S-∑(θ)）的所有元素取相同的权重，即假定所有元素具有相同的方差协方差。当违反这些假定时，最小二乘估计是有偏差的，对此可采用加权处理。GLS估计是一致有效的估计。\n加权最小二乘法 加权最小二乘法（weighted least squares，WLS）是Browne于1984年提出的一种渐近式分布无干扰（asymptotically distribution free，ADF）的估计方法。这种方法不要求观测变量具有多元正态性。\n对角加权最小二乘法 当变量数很大时，要计算待估参数的渐近协方差阵相当费时间，并且占用大量的计算机内存。可以用对角加权最小二乘法（diagonal weighted least squares，DWLS）估计参数。DWLS得不到参数的渐近有效估计，但它能给出介于ML和WLS之间的折中估计。\n\n\n\n34.2.4 模型评价\n在获得了参数的估计值后需要对模型的拟合效果进行评价。一般而言需提供几个方面的信息：①参数估计的合理性及显著性检验；②测量模型的评价；③整体模型的评价。\n1.参数估计与假设检验\n在模型设定正确的前提下，参数的估计值应该具有合理的取值范围及正确的符号；反之，如果出现与此背离的情形，如方差为负值，相关系数的绝对值大于1，协方差或相关矩阵为非正定阵等，则表明模型设定有误或输入的矩阵缺少足够的信息。此外，还应该对每一个自由参数做是否为零的检验。在多元正态分布的前提下，ML和GLS估计能够获得正确的标准误，WLS估计在应用了正确的加权矩阵时所得到的标准误也是正确的，对于ULS和DWLS估计仅能得到渐近标准误，而IV和TSLS则不能提供标准误。并且，即使在观测变量有所偏离正态的情况下，ML和GLS也能得到稳健的标准误。单个参数的检验期望拒绝零假设，因为这表明将其设为自由参数是合理的：反之，当结论为不拒绝零假设时提示将其设为自由参数可能是不恰当的。此时，应结合实际理论将其固定为0。\n2.模型评价\n得到参数的估计值，就意味着得到一个特定的理论模型。接下来的问题是，如何知道这个特定的模型拟合实际数据的程度，涉及模型评价问题，至少需要进行两方面的评价：①检验模型中的参数是否具有统计学意义；②模型整体拟合程度的评价。\n即便是理论模型整体拟合效果很好，也不能保证所有的待估参数均具有统计学意义，所以应该对每个参数是否具有统计学意义进行假设检验。当某个参数的检验结果不具有统计学意义时，意味着将该参数设为自由参数是不恰当的，应将其固定为0，并对模型重新拟合与评价。用于验证性因子分析的许多软件（如LISREL）会在输出结果中给出参数的标准误、统计量值等。此外，当潜在因子的协方差矩阵以及度量误差的协方差矩阵的对角元素出现负值时，表明模型的设定有不恰当之处，因为这些元素实质均为方差，方差应该是非负值。\n对模型整体拟合效果的评价指标主要是拟合指数，拟合指数有很多，每个指数的计算及意义不尽相同。绝大多数的拟合指数是基于拟合函数计算出来的，卡方值是反映模型与数据拟合程度最直接的指标，卡方值越大，模型与数据拟合效果越不好。因为卡方值容易受到样本含量N影响，即在N较大时，卡方值也很大：N较小时，卡方值则很小，也就是说，卡方值往往不能很好地反映模型与数据的实际拟合程度。为了弥补卡方值的缺陷，许多学者先后提出了儿十个拟合指数。这些拟合指数大致可以分为：绝对拟合指数（absolute fit index）、相对拟合指数（comparative fit index）、信息标准指数（information criteria fit index）、节俭拟合指数（parsimony fit index）。\n一个比较理想的拟合指数应该具有这样的特点：①不受样本含量的影响；②惩罚复杂模型（自由参数较多的模型）；③对误设模型敏感。\n验证性因子分析与结构方程模型的拟合指标基本相同，下表列出了部分常用的拟合指标。\n\n上表中没有的几个指标补充：\n\nTucker-Lewis Index (TLI)：Tucker Lewis指数一般会在结构方程模型中报告，也被称为非范拟合指数（Non-Normed Fit Index，NNFI），是在因子分析以及结构方程模型等领域用于评估模型拟合优度的一个重要指标，通常希望大于0.9，值越大说明拟合效果越好。\nSRMR：Standardized Root Mean Square Residual，标准化残差均方根，SRMR的值越接近0，表示观测数据与模型预测数据之间的差异越小，即模型对数据的拟合效果越好。一般来说，当SRMR小于0.08时，认为模型拟合良好；当SRMR小于0.05时，表明模型拟合非常好。\n\n\n\n34.2.5 模型修正\n对初始理论模型可进行调整，以得到拟合效果较佳的模型，可通过修正指数（modification index，MI），适当地改变模型中某些变量之间的关系，或设定某些误差项，或限制某些结构参数。MacCallum给出一些建议：\n\n首先，在描述结构模型的问题前，需先解决测量模型的设定误差；\n其次，一次只能做一个修正，以免影响其他参数的估计。\n最后，修正过程应该先增加有意义的参数，如果需要，再减少无意义的参数。而非先减少无意义的参数，再增加有意义的参数。\n另外，在进行模型修正时，应该有实际的理论做指导，而不能仅凭样本数据提供的信息做出判断。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>结构方程模型</span>"
    ]
  },
  {
    "objectID": "结构方程模型.html#r语言实战",
    "href": "结构方程模型.html#r语言实战",
    "title": "34  结构方程模型",
    "section": "34.3 R语言实战",
    "text": "34.3 R语言实战\n以上是理论知识（太复杂了！），下面是R语言实战。\n\n34.3.1 lavaan简介\nlavaan是专门用于潜变量分析的R包，它是latent variable analysis（潜变量分析）的缩写。\n在常规的R公式语法中，波浪号~左边的变量是因变量，右边的变量是自变量，多个自变量以+连接，比如：\ny ~ x1 + x2 + x3 + x4\n在lavaan中，这种形式也被保留下来，表示常规的回归的模型。稍有不同的地方是，SEM和CFA中可能会同时出现多个回归模型（叫公式、方程都可以），用来表示多个变量之间的关系，比如：\n# 同时出现3个公式\ny ~ f1 + f2 + x1 + x2 \nf1 ~ f2 + f3 \nf2 ~ f3 + x1 + x2\nSEM中的变量分为潜变量和显变量，如果~两侧是不同类型的变量（比如：~左侧是潜变量，右侧是显变量），那么就会和常规的回归模型混淆，并且也难以区分谁是潜变量、谁是显变量，所以lavaan引入了=~，如果潜变量x由z1,z2,z3这3个显变量测量，可以写成如下格式：\nx =~ z1+z2+z3\n在SEM的求解过程中，变量之间的协方差和方差结构是必须的，lavaan中通过~~定义变量之间的协方差和方差结构。如果~~的两边是同一个变量，则是表示方差，如果是不同的变量，则是表示协方差。比如：\ny1 ~~ x2 # 估计y1和x2之间的协方差\nz1 ~~ x3 # 估计z1和x3之间的协方差\n一个完整的lavaan模型公式是由以上4个部分组成的，并且必须使用单引号'引起来，例如：\n\n# 一个完整的lavaan语法\nmyModel &lt;- ' # 回归模型\n             y1 + y2 ~ f1 + f2 + x1 + x2\n             f1 ~ f2 + f3\n             f2 ~ f3 + x1 + x2\n\n             # 潜变量 \n             f1 =~ y1 + y2 + y3 \n             f2 =~ y4 + y5 + y6 \n             f3 =~ y7 + y8 + y9 + y10\n\n             # 方差和协方差 \n             y1 ~~ y1 \n             y1 ~~ y2 \n             f1 ~~ f2\n\n             # 截距项\n             y1 ~ 1 \n             f1 ~ 1\n           '\n\n但是在实际使用时，只有潜变量部分的公式才是必须的，其余部分的公式有时可以省略（会默认计算其余部分，也有例外，要根据实际情况来）。\n\n\n34.3.2 数据简介\n结合一个具体的例子说明（孙振球医学统计学第5版例23-1）。\n某课题组采用脑卒中患者报告临床结局量表（下文简述Stroke-PRO量表）调查了295例脑卒中患者，用于评估其治疗结局。Stroke-PRO量表中，生理领域包含4个维度、20个条目；心理领域包含3个维度、14个条目；社会领域包含2个维度、7个条目；治疗领域包含1个维度、5个条目。量表结构如下：\n\n上表中每个条目是一个观测变量，每个维度是一个潜变量。\n以生理领域中躯体症状（SOS）这一维度为例，患者的躯体症状无法直接测量，需通过一些相关条目获取，如采用Likert5级计分，维度与条目信息如下：\n\n假如我们需要探讨脑卒中患者生理领域的躯体症状、认知能力和心理领域的焦虑、抑郁、回避之间的相互作用关系，将上述维度作为潜变量，各条目作为观测变量，可以从以下两方面着手分析：\n\n采用验证性因子分析评价量表的结构效度；\n采用结构方程模型分析不同维度间的关联关系。\n\n\n\n34.3.3 验证性因子分析\n验证性因子分析本质上是结构方程模型中的测量模型，常用于评价某个测验或者量表的结构效度（construct validity），又称为构念效度。\n\n构念是指在科学研究中，研究者为了描述、解释和预测现象而构建的抽象概念，如智力、焦虑、工作满意度等。构念本身无法直接观察和测量，需要通过一些可观测的指标（如题目、行为表现等）来间接测量。构念效度指的是测量工具能够准确测量理论上的构念或特质的程度。简单来说，就是看我们所使用的测量工具（如问卷、测试等）是不是真的在测量我们想要测量的那个抽象概念。比如，我们想测量一个人的 “创造力”，那么所设计的测量工具是否真的能准确地把这个人的创造力水平反映出来，而不是测到了其他无关的东西，这就是构念效度要解决的问题。\n\n用验证性因子分析评价Stroke-PRO量表生理领域即其躯体症状（SOS）、认知能力（COG）、言语交流（VEC）及自理能力（SHS）的结构效度。\n先读取数据和加载R包：\n\nlibrary(lavaan) # 做SEM\nlibrary(haven)  # 读取spss格式的数据\ndf23_2 &lt;- haven::read_sav(\"datasets/例23-02.sav\")\ndim(df23_2)\n## [1] 295  20\n#df23_2 &lt;- haven::zap_formats(df23_2)\ndf23_2[1:4,1:4] # 展示前4行和前4列\n## # A tibble: 4 × 4\n##    PHD1  PHD2  PHD3  PHD4\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     5     5     5     5\n## 2     4     4     4     5\n## 3     5     5     5     5\n## 4     5     4     5     5\n\n数据结构如上，每一行是一个患者，每一列是一个条目，其中的数字是评分。\n首先是构建模型，把潜变量和显变量之间的关系用公式的形式写出来。在这个示例中潜变量就是躯体症状（SOS）、认知能力（COG）、言语交流（VEC）及自理能力（SHS），显变量就是各自对应的条目，所以公式有4个：\n\n# 验证性因子分析只有测量模型，没有结构模型\ncfa_models &lt;- ' sos =~ PHD1+PHD2+PHD3+PHD4+PHD5+PHD6+PHD7 \n                cog =~ PHD8+PHD9+PHD10+PHD11\n                vec =~ PHD12+PHD13+PHD14+PHD15 \n                shs =~ PHD16+PHD17+PHD18+PHD19+PHD20 '\n\n使用cfa函数拟合验证性因子分析模型即可：\n\nfit &lt;- cfa(cfa_models, data = df23_2)\n\n查看拟合结果(结果和书中基本是一样的)：\n\nsummary(fit, standardized=T, rsquare = T)\n## lavaan 0.6-19 ended normally after 46 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of model parameters                        46\n## \n##   Number of observations                           295\n## \n## Model Test User Model:\n##                                                       \n##   Test statistic                               630.894\n##   Degrees of freedom                               164\n##   P-value (Chi-square)                           0.000\n## \n## Parameter Estimates:\n## \n##   Standard errors                             Standard\n##   Information                                 Expected\n##   Information saturated (h1) model          Structured\n## \n## Latent Variables:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   sos =~                                                                \n##     PHD1              1.000                               0.675    0.458\n##     PHD2              0.975    0.156    6.259    0.000    0.658    0.508\n##     PHD3              1.088    0.184    5.925    0.000    0.734    0.464\n##     PHD4              1.415    0.185    7.647    0.000    0.955    0.779\n##     PHD5              1.433    0.183    7.811    0.000    0.966    0.831\n##     PHD6              1.363    0.175    7.785    0.000    0.920    0.822\n##     PHD7              1.121    0.167    6.721    0.000    0.756    0.579\n##   cog =~                                                                \n##     PHD8              1.000                               0.931    0.734\n##     PHD9              1.107    0.087   12.723    0.000    1.031    0.800\n##     PHD10             1.088    0.086   12.635    0.000    1.013    0.794\n##     PHD11             0.804    0.084    9.554    0.000    0.749    0.595\n##   vec =~                                                                \n##     PHD12             1.000                               1.000    0.724\n##     PHD13             0.965    0.081   11.942    0.000    0.965    0.737\n##     PHD14             0.659    0.065   10.068    0.000    0.659    0.621\n##     PHD15             0.779    0.071   11.018    0.000    0.779    0.680\n##   shs =~                                                                \n##     PHD16             1.000                               1.031    0.737\n##     PHD17             1.233    0.080   15.470    0.000    1.271    0.873\n##     PHD18             1.394    0.090   15.562    0.000    1.437    0.878\n##     PHD19             1.390    0.088   15.706    0.000    1.432    0.885\n##     PHD20             1.407    0.085   16.493    0.000    1.450    0.926\n## \n## Covariances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   sos ~~                                                                \n##     cog               0.372    0.068    5.477    0.000    0.592    0.592\n##     vec               0.474    0.082    5.792    0.000    0.702    0.702\n##     shs               0.210    0.053    3.921    0.000    0.302    0.302\n##   cog ~~                                                                \n##     vec               0.737    0.096    7.682    0.000    0.792    0.792\n##     shs               0.263    0.068    3.893    0.000    0.274    0.274\n##   vec ~~                                                                \n##     shs               0.653    0.093    7.057    0.000    0.634    0.634\n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##    .PHD1              1.712    0.146   11.739    0.000    1.712    0.790\n##    .PHD2              1.240    0.107   11.612    0.000    1.240    0.742\n##    .PHD3              1.964    0.167   11.726    0.000    1.964    0.785\n##    .PHD4              0.590    0.061    9.725    0.000    0.590    0.393\n##    .PHD5              0.418    0.048    8.631    0.000    0.418    0.309\n##    .PHD6              0.406    0.046    8.863    0.000    0.406    0.324\n##    .PHD7              1.132    0.100   11.370    0.000    1.132    0.664\n##    .PHD8              0.742    0.075    9.853    0.000    0.742    0.461\n##    .PHD9              0.597    0.069    8.622    0.000    0.597    0.360\n##    .PHD10             0.603    0.069    8.778    0.000    0.603    0.370\n##    .PHD11             1.022    0.092   11.087    0.000    1.022    0.646\n##    .PHD12             0.907    0.088   10.255    0.000    0.907    0.476\n##    .PHD13             0.783    0.078   10.088    0.000    0.783    0.457\n##    .PHD14             0.692    0.062   11.111    0.000    0.692    0.614\n##    .PHD15             0.708    0.066   10.704    0.000    0.708    0.538\n##    .PHD16             0.892    0.079   11.328    0.000    0.892    0.457\n##    .PHD17             0.504    0.051    9.894    0.000    0.504    0.238\n##    .PHD18             0.614    0.063    9.784    0.000    0.614    0.229\n##    .PHD19             0.567    0.059    9.594    0.000    0.567    0.216\n##    .PHD20             0.349    0.044    7.899    0.000    0.349    0.142\n##     sos               0.455    0.116    3.935    0.000    1.000    1.000\n##     cog               0.867    0.126    6.909    0.000    1.000    1.000\n##     vec               1.000    0.146    6.847    0.000    1.000    1.000\n##     shs               1.062    0.146    7.277    0.000    1.000    1.000\n## \n## R-Square:\n##                    Estimate\n##     PHD1              0.210\n##     PHD2              0.258\n##     PHD3              0.215\n##     PHD4              0.607\n##     PHD5              0.691\n##     PHD6              0.676\n##     PHD7              0.336\n##     PHD8              0.539\n##     PHD9              0.640\n##     PHD10             0.630\n##     PHD11             0.354\n##     PHD12             0.524\n##     PHD13             0.543\n##     PHD14             0.386\n##     PHD15             0.462\n##     PHD16             0.543\n##     PHD17             0.762\n##     PHD18             0.771\n##     PHD19             0.784\n##     PHD20             0.858\n\n输出结果包括3个部分，第一部分是前9行，主要包含以下信息：\n\nlavaan的版本号\n优化是否正常结束，以及需要多少次迭代\n使用的估计方法，ML即最大似然法\n使用的优化器，这里是NLMINB\n模型参数数量，这里是46\n分析中实际使用的观测数量，这里是295\n模型的检验统计量（卡方值）、自由度、P值\n\n第二部分是模型的参数估计。\n首先是潜变量和观测变量之间的因子载荷系数，P值小于0.05且载荷系数越大，说明因子（也就是潜变量）和观测变量之间的相关性越强。各列的含义如下：\n\nEstimate：非标准化的载荷系数（和回归分析中的变量系数类似）\nStd.Err：标准误\nz-value：Z值\nP(&gt;|z|)：P值\nStd.lv：标准化的载荷系数，只有潜变量被标准化\nStd.all：标准化的载荷系数，潜变量和观测变量都被标准化，又被称为“完全标准化解”\n\n然后是3个因子（即潜变量）之间协方差的结果，展示因子与因子之间的相关性。P值小于0.05且Estimate（非标准化的系数）越大说明两个因子相关性越大。\n最后是误差的方差（Variances）。其中Estimate是每个观测变量的非标准化误差方差估计值，误差方差代表了观测变量中不能被潜在因子解释的部分，也就是观测变量的变异中无法由模型所设定的潜在因子结构来解释的那部分。较小的误差方差意味着观测变量能较好地被潜在因子解释。剩余3列分别是标准误、Z值和P值。\n注意，在Variances部分，观测变量名称之前有一个点。这是因为它们是因变量（或内生变量）（由潜在变量预测），因此，输出中打印的方差值是残差方差（即未被预测变量解释的剩余方差）的估计值。潜变量名称之前没有点，因为在这个模型中它们是外生变量（没有单箭头指向它们），这里的方差值是潜变量的估计总方差。\n最后一部分是每个观测变量的R2，这部分结果与书中也是完全一致的。\n通过以下代码可查看拟合指数（个别值与书中有差别）：\n\n# fit.measures = \"all\"可查看所有拟合指数\nfitMeasures(fit, fit.measures = c(\"chisq\",\"df\",\"aic\",\"gfi\",\"rmsea\",\"cfi\"))\n##     chisq        df       aic       gfi     rmsea       cfi \n##   630.894   164.000 17132.886     0.812     0.098     0.867\n\nsummary(fit)给出的因子载荷系数是非标准化的，可通过以下方式输出标准化的结果：\n\nstd_res &lt;- standardizedSolution(fit)\nstd_res # est.std就是标准化的载荷\n##      lhs op   rhs est.std    se      z pvalue ci.lower ci.upper\n## 1    sos =~  PHD1   0.458 0.050  9.218      0    0.361    0.556\n## 2    sos =~  PHD2   0.508 0.047 10.833      0    0.416    0.600\n## 3    sos =~  PHD3   0.464 0.049  9.393      0    0.367    0.561\n## 4    sos =~  PHD4   0.779 0.028 28.190      0    0.725    0.833\n## 5    sos =~  PHD5   0.831 0.024 35.304      0    0.785    0.877\n## 6    sos =~  PHD6   0.822 0.024 33.908      0    0.775    0.870\n## 7    sos =~  PHD7   0.579 0.043 13.620      0    0.496    0.663\n## 8    cog =~  PHD8   0.734 0.033 22.471      0    0.670    0.798\n## 9    cog =~  PHD9   0.800 0.028 28.742      0    0.746    0.855\n## 10   cog =~ PHD10   0.794 0.028 28.033      0    0.738    0.849\n## 11   cog =~ PHD11   0.595 0.043 13.984      0    0.512    0.679\n## 12   vec =~ PHD12   0.724 0.033 22.224      0    0.660    0.788\n## 13   vec =~ PHD13   0.737 0.032 23.324      0    0.675    0.799\n## 14   vec =~ PHD14   0.621 0.040 15.513      0    0.543    0.699\n## 15   vec =~ PHD15   0.680 0.036 18.944      0    0.609    0.750\n## 16   shs =~ PHD16   0.737 0.028 26.007      0    0.682    0.793\n## 17   shs =~ PHD17   0.873 0.016 54.662      0    0.842    0.904\n## 18   shs =~ PHD18   0.878 0.016 56.550      0    0.847    0.908\n## 19   shs =~ PHD19   0.885 0.015 59.683      0    0.856    0.914\n## 20   shs =~ PHD20   0.926 0.011 83.441      0    0.904    0.948\n## 21  PHD1 ~~  PHD1   0.790 0.046 17.348      0    0.701    0.879\n## 22  PHD2 ~~  PHD2   0.742 0.048 15.539      0    0.648    0.835\n## 23  PHD3 ~~  PHD3   0.785 0.046 17.120      0    0.695    0.875\n## 24  PHD4 ~~  PHD4   0.393 0.043  9.119      0    0.308    0.477\n## 25  PHD5 ~~  PHD5   0.309 0.039  7.903      0    0.233    0.386\n## 26  PHD6 ~~  PHD6   0.324 0.040  8.135      0    0.246    0.402\n## 27  PHD7 ~~  PHD7   0.664 0.049 13.474      0    0.568    0.761\n## 28  PHD8 ~~  PHD8   0.461 0.048  9.610      0    0.367    0.555\n## 29  PHD9 ~~  PHD9   0.360 0.045  8.070      0    0.272    0.447\n## 30 PHD10 ~~ PHD10   0.370 0.045  8.237      0    0.282    0.458\n## 31 PHD11 ~~ PHD11   0.646 0.051 12.742      0    0.546    0.745\n## 32 PHD12 ~~ PHD12   0.476 0.047 10.082      0    0.383    0.568\n## 33 PHD13 ~~ PHD13   0.457 0.047  9.798      0    0.365    0.548\n## 34 PHD14 ~~ PHD14   0.614 0.050 12.357      0    0.517    0.712\n## 35 PHD15 ~~ PHD15   0.538 0.049 11.036      0    0.443    0.634\n## 36 PHD16 ~~ PHD16   0.457 0.042 10.922      0    0.375    0.538\n## 37 PHD17 ~~ PHD17   0.238 0.028  8.521      0    0.183    0.292\n## 38 PHD18 ~~ PHD18   0.229 0.027  8.417      0    0.176    0.283\n## 39 PHD19 ~~ PHD19   0.216 0.026  8.246      0    0.165    0.268\n## 40 PHD20 ~~ PHD20   0.142 0.021  6.921      0    0.102    0.183\n## 41   sos ~~   sos   1.000 0.000     NA     NA    1.000    1.000\n## 42   cog ~~   cog   1.000 0.000     NA     NA    1.000    1.000\n## 43   vec ~~   vec   1.000 0.000     NA     NA    1.000    1.000\n## 44   shs ~~   shs   1.000 0.000     NA     NA    1.000    1.000\n## 45   sos ~~   cog   0.592 0.048 12.352      0    0.498    0.686\n## 46   sos ~~   vec   0.702 0.042 16.625      0    0.620    0.785\n## 47   sos ~~   shs   0.302 0.059  5.146      0    0.187    0.416\n## 48   cog ~~   vec   0.792 0.037 21.314      0    0.719    0.865\n## 49   cog ~~   shs   0.274 0.061  4.488      0    0.154    0.394\n## 50   vec ~~   shs   0.634 0.045 14.210      0    0.546    0.721\n# 或者使用：\n#summary(fit,standardized = T)\n\n\nsummary(fit)中支持的参数非常多，比如也可以直接输出系数的可信区间、拟合指数等，大家可以参考该函数的帮助文档。\n\n绘制路径图(结果与书中一致)，这个semPlot功能很强大，以后再详细介绍：\n\nlibrary(semPlot)\n\nsemPaths(fit\n         ,what = 'col' # 线条用不同的颜色表示\n         ,groups = \"latents\" # 根据潜变量上色\n         ,pastel = TRUE # 选颜色\n         ,whatLabels = 'std' # 显示标准化的载荷\n         ,style = \"lisrel\" # 使用lisrel软件的风格\n         ,rotation = 2 # 旋转方向\n         ,edge.label.cex = 1 # 载荷字体大小\n         ,mar = c(1, 6, 1, 6) # 控制图形边距\n         )\n\n\n\n\n\n\n\n\n如果只看文字和数字很难看出结果，但是一个图形就能展示所有结果，而且非常直观，不仅展示了潜变量、显变量之间的关系，还标注了因子载荷、相关系数等各种信息(注意上图中展示的载荷系数都是标准化的)。\n结论：潜变量于各条目之间的关系可通过因子载荷反应，所有维度与条目间的因子载荷均有统计学意义。拟合指数结果提示拟合效果尚可接受。所有因子载荷均大于0.4，介于0.46到0.93之间。由潜变量之间的相关系数可知，认知（COG）和语言交（VEC）流具有较强相关，认知（COG）和自理能力（SHS）相关性较弱。\n\n\n34.3.4 结构方程模型\n这部分将通过两个具体的示例进行介绍，先介绍一个简单的，再介绍一个复杂的。\n首先是孙振球医学统计学第5版例23-3。\n为了研究Stroke-PRO量表生理领域与心理领域中不同维度之间的内在联系以及各维度之间的因果关联，采用该量表收集295例脑卒中患者生理领域及心理领域的数据。以生理领域的躯体症状(SOS)和心理领域的焦虑（ANX)构建一个最简单的结构方程模型，模型只包含一个外生潜变量和内生潜变量。\n\nlibrary(lavaan) # 做SEM\nlibrary(haven)  # 读取spss格式的数据\ndf23_3 &lt;- haven::read_sav(\"datasets/例23-03.sav\")\ndim(df23_3)\n## [1] 295  12\n\n注意根据模型的假设（也就是变量之间的关系）写公式，不要乱写（和使用aov做各种各样的方差分析非常类似），这里的假设有3个：\n\n躯体症状（SOS）和所属的各条目之间的关系\n焦虑（ANX）和所属的各条目之间的关系\n躯体症状会导致焦虑（这条其实题干中没有明确说明，是根据书中的结果反推的）\n\n所以模型也有3个，前两个是测量模型，第3个是结构模型：\n\n# 前两个公式是测量模型，第3个公式是结构模型\nsem_models &lt;- ' sos =~ PHD1+PHD2+PHD3+PHD4+PHD5+PHD6+PHD7\n                anx =~ PSD1+PSD2+PSD3+PSD4+PSD5\n                anx ~ sos'\n\n使用sem函数拟合结构方程模型：\n\nfit &lt;- sem(sem_models, data = df23_3)\n\n查看结果，这里输出的结果中包含了标准化的结果：\n\nsummary(fit,standardized=T,rsquare = T)\n## lavaan 0.6-19 ended normally after 33 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of model parameters                        25\n## \n##   Number of observations                           295\n## \n## Model Test User Model:\n##                                                       \n##   Test statistic                               161.889\n##   Degrees of freedom                                53\n##   P-value (Chi-square)                           0.000\n## \n## Parameter Estimates:\n## \n##   Standard errors                             Standard\n##   Information                                 Expected\n##   Information saturated (h1) model          Structured\n## \n## Latent Variables:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   sos =~                                                                \n##     PHD1              1.000                               0.695    0.472\n##     PHD2              0.946    0.148    6.378    0.000    0.658    0.508\n##     PHD3              1.060    0.176    6.041    0.000    0.737    0.466\n##     PHD4              1.382    0.175    7.894    0.000    0.961    0.785\n##     PHD5              1.377    0.172    8.029    0.000    0.958    0.824\n##     PHD6              1.322    0.165    8.023    0.000    0.919    0.822\n##     PHD7              1.078    0.158    6.839    0.000    0.750    0.575\n##   anx =~                                                                \n##     PSD1              1.000                               0.931    0.721\n##     PSD2              0.923    0.079   11.747    0.000    0.860    0.720\n##     PSD3              0.996    0.074   13.379    0.000    0.927    0.824\n##     PSD4              0.988    0.084   11.718    0.000    0.920    0.719\n##     PSD5              1.113    0.080   13.882    0.000    1.036    0.859\n## \n## Regressions:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   anx ~                                                                 \n##     sos               0.790    0.126    6.287    0.000    0.590    0.590\n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##    .PHD1              1.684    0.144   11.686    0.000    1.684    0.777\n##    .PHD2              1.240    0.107   11.587    0.000    1.240    0.741\n##    .PHD3              1.959    0.167   11.702    0.000    1.959    0.783\n##    .PHD4              0.576    0.061    9.513    0.000    0.576    0.384\n##    .PHD5              0.435    0.050    8.665    0.000    0.435    0.321\n##    .PHD6              0.406    0.047    8.714    0.000    0.406    0.325\n##    .PHD7              1.142    0.101   11.355    0.000    1.142    0.670\n##    .PSD1              0.800    0.076   10.579    0.000    0.800    0.480\n##    .PSD2              0.685    0.065   10.585    0.000    0.685    0.481\n##    .PSD3              0.408    0.045    9.033    0.000    0.408    0.322\n##    .PSD4              0.792    0.075   10.601    0.000    0.792    0.484\n##    .PSD5              0.381    0.048    8.012    0.000    0.381    0.262\n##     sos               0.484    0.119    4.064    0.000    1.000    1.000\n##    .anx               0.565    0.086    6.536    0.000    0.652    0.652\n## \n## R-Square:\n##                    Estimate\n##     PHD1              0.223\n##     PHD2              0.259\n##     PHD3              0.217\n##     PHD4              0.616\n##     PHD5              0.679\n##     PHD6              0.675\n##     PHD7              0.330\n##     PSD1              0.520\n##     PSD2              0.519\n##     PSD3              0.678\n##     PSD4              0.516\n##     PSD5              0.738\n##     anx               0.348\n\n结果和验证性因子分析的结果解读一致。只有一个地方需要注意，那就是结果中多了一个Regressions部分，这个结果就是结构模型的结果，展示的是因子之间的关系。\n输出结果包括3个部分，第一部分是前9行，主要包含以下信息：\n\nlavaan的版本号\n优化是否正常结束，以及需要多少次迭代\n使用的估计方法，ML即最大似然法\n使用的优化器，这里是NLMINB\n模型参数数量，这里是25\n分析中实际使用的观测数量，这里是295\n模型的检验统计量（卡方值）、自由度、P值\n\n第二部分是模型的参数估计。\n首先是潜变量和观测变量之间的因子载荷系数，P值小于0.05且载荷系数越大，说明因子（也就是潜变量）和观测变量之间的相关性越强。各列的含义如下：\n\nEstimate：非标准化的载荷系数（和回归分析中的变量系数类似）\nStd.Err：标准误\nz-value：Z值\nP(&gt;|z|)：P值\nStd.lv：标准化的载荷系数，只有潜变量被标准化\nStd.all：标准化的载荷系数，潜变量和观测变量都被标准化，又被称为“完全标准化解”\n\n然后是3个因子（即潜变量）之间协方差的结果，展示因子与因子之间的相关性。P值小于0.05且Estimate（非标准化的系数）越大说明两个因子相关性越大。\n最后是误差的方差（Variances）。其中Estimate是每个观测变量的非标准化误差方差估计值，误差方差代表了观测变量中不能被潜在因子解释的部分，也就是观测变量的变异中无法由模型所设定的潜在因子结构来解释的那部分。较小的误差方差意味着观测变量能较好地被潜在因子解释。剩余3列分别是标准误、Z值和P值。\n注意，在Variances部分，观测变量名称之前有一个点。这是因为它们是因变量（或内生变量）（由潜在变量预测），因此，输出中打印的方差值是残差方差（即未被预测变量解释的剩余方差）的估计值。潜变量名称之前没有点，因为在这个模型中它们是外生变量（没有单箭头指向它们），这里的方差值是潜变量的估计总方差。\n最后一部分是每个观测变量的R2，这部分结果与书中也是完全一致的。\n查看拟合指数：\n\nfitMeasures(fit, fit.measures = c(\"chisq\",\"df\",\"aic\",\"gfi\",\"rmsea\",\"cfi\"))\n##     chisq        df       aic       gfi     rmsea       cfi \n##   161.889    53.000 10261.913     0.908     0.083     0.932\n\n绘制路径图（注意箭头指向，说明了因果关系）：\n\nlibrary(semPlot)\n# 换个样式\nsemPaths(fit,what = 'est' ,whatLabels = 'std' ,style = \"lisrel\")\n\n\n\n\n\n\n\n\n下面再介绍孙振球医学统计学第5版例23-4。\n某一理论模型假设脑卒中患者躯体不适可以引起焦虑（ANX），并间接引起抑郁（DEP）和回避（AVO）。试采用结构方程模型验证此理论假设。以脑卒中患者生理领域的躯体症状（SOS）、认知功能（COG）两个维度和心理领域的焦虑（ANX）、抑郁（DEP）、回避（AVO）三个维度采用最大似然估计法构建结构方程模型。\n\nlibrary(lavaan) # 做SEM\nlibrary(haven)  # 读取spss格式的数据\ndf23_4 &lt;- haven::read_sav(\"datasets/例23-04.sav\")\ndim(df23_4)\n## [1] 295  25\n\n首先还是要搞清楚该研究的假设是什么，然后根据假设写出公式即可。\n\n# 写好公式，谁测量谁...\n# 部分公式也是根据书中结果反推的，\n# 因为只根据题干没办法得到这么多信息...\nsem_models &lt;- ' sos =~ PHD1+PHD2+PHD3+PHD4+PHD5+PHD6+PHD7 \n                cog =~ PHD8+PHD9+PHD10+PHD11\n                anx =~ PSD1+PSD2+PSD3+PSD4+PSD5\n                dep =~ PSD6+PSD7+PSD8+PSD9+PSD10\n                avo =~ PSD11+PSD12+PSD13+PSD14\n                anx ~ sos\n                dep ~ anx + avo + cog\n                avo ~ anx +cog '\n\n后续的结果解读就是和之前一样的了，不再赘述。\n\nfit &lt;- sem(sem_models,data = df23_4)\nsummary(fit,standardized=T,rsquare = T)\n## lavaan 0.6-19 ended normally after 41 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of model parameters                        57\n## \n##   Number of observations                           295\n## \n## Model Test User Model:\n##                                                       \n##   Test statistic                               747.369\n##   Degrees of freedom                               268\n##   P-value (Chi-square)                           0.000\n## \n## Parameter Estimates:\n## \n##   Standard errors                             Standard\n##   Information                                 Expected\n##   Information saturated (h1) model          Structured\n## \n## Latent Variables:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   sos =~                                                                \n##     PHD1              1.000                               0.691    0.469\n##     PHD2              0.970    0.151    6.438    0.000    0.670    0.518\n##     PHD3              1.084    0.178    6.092    0.000    0.749    0.473\n##     PHD4              1.386    0.177    7.851    0.000    0.957    0.781\n##     PHD5              1.376    0.173    7.977    0.000    0.950    0.817\n##     PHD6              1.313    0.165    7.954    0.000    0.907    0.811\n##     PHD7              1.112    0.161    6.908    0.000    0.768    0.588\n##   cog =~                                                                \n##     PHD8              1.000                               0.933    0.736\n##     PHD9              1.129    0.088   12.858    0.000    1.053    0.818\n##     PHD10             1.067    0.086   12.388    0.000    0.996    0.780\n##     PHD11             0.781    0.084    9.267    0.000    0.728    0.579\n##   anx =~                                                                \n##     PSD1              1.000                               0.918    0.711\n##     PSD2              0.928    0.080   11.593    0.000    0.852    0.714\n##     PSD3              1.017    0.076   13.383    0.000    0.933    0.829\n##     PSD4              1.012    0.086   11.769    0.000    0.928    0.725\n##     PSD5              1.123    0.082   13.749    0.000    1.031    0.854\n##   dep =~                                                                \n##     PSD6              1.000                               0.851    0.767\n##     PSD7              1.098    0.070   15.686    0.000    0.934    0.855\n##     PSD8              1.085    0.079   13.668    0.000    0.923    0.762\n##     PSD9              0.991    0.080   12.388    0.000    0.843    0.700\n##     PSD10             1.040    0.067   15.508    0.000    0.885    0.847\n##   avo =~                                                                \n##     PSD11             1.000                               1.003    0.883\n##     PSD12             0.963    0.052   18.547    0.000    0.966    0.846\n##     PSD13             0.871    0.059   14.693    0.000    0.873    0.729\n##     PSD14             0.799    0.063   12.740    0.000    0.801    0.660\n## \n## Regressions:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   anx ~                                                                 \n##     sos               0.832    0.129    6.440    0.000    0.626    0.626\n##   dep ~                                                                 \n##     anx               0.342    0.058    5.910    0.000    0.369    0.369\n##     avo               0.371    0.058    6.420    0.000    0.437    0.437\n##     cog               0.224    0.047    4.797    0.000    0.246    0.246\n##   avo ~                                                                 \n##     anx               0.609    0.071    8.634    0.000    0.557    0.557\n##     cog               0.356    0.063    5.626    0.000    0.331    0.331\n## \n## Covariances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   sos ~~                                                                \n##     cog               0.401    0.071    5.666    0.000    0.622    0.622\n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##    .PHD1              1.690    0.144   11.716    0.000    1.690    0.780\n##    .PHD2              1.223    0.106   11.586    0.000    1.223    0.731\n##    .PHD3              1.942    0.166   11.706    0.000    1.942    0.776\n##    .PHD4              0.585    0.060    9.711    0.000    0.585    0.389\n##    .PHD5              0.449    0.050    9.014    0.000    0.449    0.332\n##    .PHD6              0.429    0.047    9.164    0.000    0.429    0.343\n##    .PHD7              1.115    0.098   11.337    0.000    1.115    0.654\n##    .PHD8              0.739    0.076    9.738    0.000    0.739    0.459\n##    .PHD9              0.550    0.069    8.007    0.000    0.550    0.332\n##    .PHD10             0.638    0.071    8.940    0.000    0.638    0.391\n##    .PHD11             1.052    0.094   11.138    0.000    1.052    0.665\n##    .PSD1              0.825    0.076   10.841    0.000    0.825    0.495\n##    .PSD2              0.698    0.065   10.816    0.000    0.698    0.490\n##    .PSD3              0.396    0.043    9.294    0.000    0.396    0.312\n##    .PSD4              0.777    0.072   10.726    0.000    0.777    0.474\n##    .PSD5              0.393    0.045    8.649    0.000    0.393    0.270\n##    .PSD6              0.507    0.047   10.682    0.000    0.507    0.412\n##    .PSD7              0.320    0.034    9.321    0.000    0.320    0.269\n##    .PSD8              0.617    0.057   10.731    0.000    0.617    0.420\n##    .PSD9              0.740    0.066   11.168    0.000    0.740    0.510\n##    .PSD10             0.308    0.032    9.514    0.000    0.308    0.283\n##    .PSD11             0.283    0.038    7.522    0.000    0.283    0.220\n##    .PSD12             0.370    0.042    8.831    0.000    0.370    0.284\n##    .PSD13             0.672    0.063   10.703    0.000    0.672    0.468\n##    .PSD14             0.831    0.074   11.172    0.000    0.831    0.564\n##     sos               0.477    0.118    4.043    0.000    1.000    1.000\n##     cog               0.871    0.126    6.902    0.000    1.000    1.000\n##    .anx               0.512    0.080    6.408    0.000    0.608    0.608\n##    .dep               0.146    0.026    5.658    0.000    0.202    0.202\n##    .avo               0.438    0.057    7.705    0.000    0.436    0.436\n## \n## R-Square:\n##                    Estimate\n##     PHD1              0.220\n##     PHD2              0.269\n##     PHD3              0.224\n##     PHD4              0.611\n##     PHD5              0.668\n##     PHD6              0.657\n##     PHD7              0.346\n##     PHD8              0.541\n##     PHD9              0.668\n##     PHD10             0.609\n##     PHD11             0.335\n##     PSD1              0.505\n##     PSD2              0.510\n##     PSD3              0.688\n##     PSD4              0.526\n##     PSD5              0.730\n##     PSD6              0.588\n##     PSD7              0.731\n##     PSD8              0.580\n##     PSD9              0.490\n##     PSD10             0.717\n##     PSD11             0.780\n##     PSD12             0.716\n##     PSD13             0.532\n##     PSD14             0.436\n##     anx               0.392\n##     dep               0.798\n##     avo               0.564\n# fitMeasures(fit, fit.measures = c(\"chisq\",\"df\",\"aic\",\"gfi\",\"rmsea\",\"cfi\"))\n\n画图：\n\nlibrary(semPlot)\nsemPaths(fit, 'std','std',style = \"lisrel\",exoCov = F)",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>结构方程模型</span>"
    ]
  },
  {
    "objectID": "结构方程模型.html#参考资料",
    "href": "结构方程模型.html#参考资料",
    "title": "34  结构方程模型",
    "section": "34.4 参考资料",
    "text": "34.4 参考资料\n\n孙振球《医学统计学》第4版和第5版\n结构方程模型是什么？一文轻松读懂！\nUsing R for Social Work Research\nR语言医学多元统计分析，赵军、戴静毅\nlavaan包官方tutorial\nhttp://sachaepskamp.com/files/semPlot.pdf",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>结构方程模型</span>"
    ]
  },
  {
    "objectID": "多水平模型.html",
    "href": "多水平模型.html",
    "title": "35  多水平模型",
    "section": "",
    "text": "35.1 理论知识\n理论知识我就不献丑了，直接给大家推荐高手的解读！\n关于多水平模型（multi-level models，MLM）的概念和理论知识，强烈推荐阅读冯国双老师的几篇文章，这是我目前见过的写的最通俗易懂的。多水平模型、混合模型、随机效应模型、固定效应模型、随机系数模型、方差成分模型等等，全都详细介绍到了：\n我之前接触多水平模型很少，对一些概念很模糊，但是读完这些推文后，我感到豁然开朗、恍然大悟，强烈推荐大家仔细读一读，对于初学者帮助很大。\n上面是理论知识，下面是R语言实战。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>多水平模型</span>"
    ]
  },
  {
    "objectID": "多水平模型.html#理论知识",
    "href": "多水平模型.html#理论知识",
    "title": "35  多水平模型",
    "section": "",
    "text": "多水平模型（一）——是什么？为什么？\n多水平模型（二）——什么是随机效应\n多水平模型（三）——随机截距模型\n多水平模型（四）——随机系数模型\n多水平模型（五）——常见的几个实际问题\n多组必须做两两比较吗？从固定效应和随机效应谈起\n重复测量数据分析及结果详解（之三）——多水平模型",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>多水平模型</span>"
    ]
  },
  {
    "objectID": "多水平模型.html#数据探索",
    "href": "多水平模型.html#数据探索",
    "title": "35  多水平模型",
    "section": "35.2 数据探索",
    "text": "35.2 数据探索\n数据可以从这个网站免费下载：https://www.learn-mlms.com/13-appendix.html。或者加入QQ群免费下载。\n\nlibrary(dplyr) # 数据操作\nlibrary(ggplot2) # 可视化\nlibrary(lme4) # 多水平模型\nlibrary(lmerTest) # 计算P值\nlibrary(performance) # 计算模型表现\ndata &lt;- read.csv('datasets/heck2011.csv') # 加载数据\n\n简单介绍下这个数据，一共有6871行，各个变量的含义如下，其中math是因变量，建模目的是用其他变量预测学生的数学成绩（math），或者叫探索和学生成绩有关系的变量：\n\nschcode:学校id，一共有419所学校\nRid:每个学校内的学生id，每个学校内都是从1开始\nid:学生id，从1到6871\nfemale:是否是女性，1=是，0=否\nses:衡量学校内学生的社会经济地位构成的Z-score\nfemses:以性别（女性）为中心的测量学生社会经济地位的变量\nmath:学生数学成绩测试分数\nses_mean:以学校为单位衡量学生社会经济地位的均值\npro4yrc:每个学校中计划就读四年制大学的学生比例\npublic:学校类型，1=公立学校，0=私立学校\n\n\nstr(data)\n## 'data.frame':    6871 obs. of  10 variables:\n##  $ schcode : int  1 1 1 1 1 1 1 1 1 1 ...\n##  $ Rid     : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ id      : int  6701 6702 6703 6704 6705 6706 6707 6708 6709 6710 ...\n##  $ female  : int  1 1 1 0 0 0 0 1 0 1 ...\n##  $ ses     : num  0.586 0.304 -0.544 -0.848 0.001 -0.106 -0.33 -0.891 0.207 -0.341 ...\n##  $ femses  : num  0.586 0.304 -0.544 0 0 0 0 -0.891 0 -0.341 ...\n##  $ math    : num  47.1 63.6 57.7 53.9 58 ...\n##  $ ses_mean: num  -0.267 -0.267 -0.267 -0.267 -0.267 ...\n##  $ pro4yrc : num  0.0833 0.0833 0.0833 0.0833 0.0833 ...\n##  $ public  : int  0 0 0 0 0 0 0 0 0 0 ...\n\n这个数据很明显是有层次的，学生是分别属于不同的学校的，所以这是一个两个水平的数据。学生是1级水平，学校是2级水平。\n不同学校之间的水平是有差异的，为了演示先选择其中10个学校（一共有419个学校）：\n\ndata_sub &lt;- data %&gt;% \n  filter(schcode &lt;= 10)\n\n如果不考虑学校水平的差异，探索社会经济地位（ses）和成绩（math）之间的关系，可以画出如下的散点图和拟合线：\n\ndata_sub %&gt;% \n  ggplot(mapping = aes(x = ses, y = math)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, fullrange = TRUE)\n\n\n\n\n\n\n\n\n如果考虑到学校之间的不同水平，按照学校分别拟合，会得到如下的图：\n\ndata_sub %&gt;% \n  ggplot(mapping = aes(x = ses, y = math, colour = factor(schcode))) +\n  geom_point() +\n  geom_smooth(mapping = aes(group = schcode), method = \"lm\", se = FALSE, fullrange = TRUE) +\n  labs(colour = \"schcode\")\n\n\n\n\n\n\n\n\n每个学校的ses的截距和斜率都是不一样的，所以不同学校对学生成绩的影响也是不同的，普通的多元线性回归没有考虑到这种差异，但是多水平模型可以发现这些差异并量化它们。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>多水平模型</span>"
    ]
  },
  {
    "objectID": "多水平模型.html#空模型",
    "href": "多水平模型.html#空模型",
    "title": "35  多水平模型",
    "section": "35.3 空模型",
    "text": "35.3 空模型\n先给大家演示一下最简单的，也就是“只有随机截距的模型”（random intercept only model），又被称为空模型（null model）。在这种模型中，没有任何自变量来解释学生成绩的差异，模型只考虑了学校间的差异，这些差异是由随机截距来表示的。\n\nnull_model &lt;- lmer(math ~ (1|schcode), data = data)\nsummary(null_model)\n## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: math ~ (1 | schcode)\n##    Data: data\n## \n## REML criterion at convergence: 48877.3\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -3.6336 -0.5732  0.1921  0.6115  5.2989 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  schcode  (Intercept) 10.64    3.262   \n##  Residual             66.55    8.158   \n## Number of obs: 6871, groups:  schcode, 419\n## \n## Fixed effects:\n##             Estimate Std. Error       df t value Pr(&gt;|t|)    \n## (Intercept)  57.6742     0.1883 416.0655   306.3   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n在lme4和lmerTest中（lmerTest可以在结果中给出P值，lme4不会给出P值，这是这两个包的差别），拟合多水平模型的基本语法是：\n# DV是因变量，IV是自变量\nlmer(DV ~ 1 + IV1 + IV2 + ... + IVp + (random_effect1 + random_effect2 + ... + random_effect3 | grouping_variable), data = dataset)\n该数据的因变量是math，波浪号右边的(1|schcode)表示分组变量schcode的随机截距，空模型没有其他自变量。\n结果主要是两部分：\n\nRandom effects：随机效应的估计\nFixed effects：固定效应的估计\n\n先看固定效应：截距的固定效应为57.67，表示所有学校的平均数学成绩，此时这个截距其实就是因变量的平均值，即：mean(data$math)=57.73391（微小差异，可以忽略）；\n再看随机效应：不同学校之间成绩的方差（Variance）为10.64，不同学校之间的学生成绩会围绕总体均值（也就是此时的截距57.67）波动，其标准差（Std.Dev）是3.262，同一个学校内不同学生之间成绩的方差为66.55，标准差为8.158。\n组内相关系数（intraclass correlation coefficient，ICC）是衡量多水平模型中群体间变异（即随机效应）与总变异（群体间变异+群体内变异）之比的一种指标。在多水平模型中，ICC用来评估因变量的变异有多少可以归因于群体间的差异，而有多少是群体内的个体差异。\n若ICC为0，说明群体间的差异为0，说明数据不存在层次结构，在本例中也就是学校之间没有差异，可以不使用多水平模型，使用普通的多元线性回归即可。\n根据模型输出，该例的总方差为10.64+66.55=77.19（var(data$math)），ICC=10.64/77.19=0.138；也就是说成绩总变异中的13.8%是学校不同导致的，剩下的86.2%的变异是由同一学校内学生之间的差异造成的。也就是说，大部分成绩变异来源于学校内部的差异，而学校之间的差异相对较小。\nICC也可以使用performance包自动计算：\n\nperformance::icc(null_model)\n## # Intraclass Correlation Coefficient\n## \n##     Adjusted ICC: 0.138\n##   Unadjusted ICC: 0.138\n\n由于没有其他自变量，所以调整的ICC和未调整的ICC是一样的。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>多水平模型</span>"
    ]
  },
  {
    "objectID": "多水平模型.html#添加1级水平的固定效应",
    "href": "多水平模型.html#添加1级水平的固定效应",
    "title": "35  多水平模型",
    "section": "35.4 添加1级水平的固定效应",
    "text": "35.4 添加1级水平的固定效应\n上面演示的空模型没有自变量，下面我们添加一个社会经济状况（ses）作为自变量，这个ses是在水平1单位（不同的学生，不同的学校属于水平2单位）间变化的，所以这个变量产生的效应属于水平1单位的固定效应。\n像这种只有随机截距，没有随机斜率（但是有自变量，不是“空模型”）的多水平模型被称为随机截距模型，又叫方差成分模型，相关概念的理解请参考本文开头推荐的几篇文章。\n用lme4拟合随机截距模型：\n\n# RMEL表示限制性最大似然估计，这也是默认方法\nses_l1 &lt;- lmer(math ~ ses + (1|schcode), data = data, REML = TRUE)\nsummary(ses_l1)\n## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: math ~ ses + (1 | schcode)\n##    Data: data\n## \n## REML criterion at convergence: 48215.4\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -3.7733 -0.5540  0.1303  0.6469  5.6908 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  schcode  (Intercept)  3.469   1.863   \n##  Residual             62.807   7.925   \n## Number of obs: 6871, groups:  schcode, 419\n## \n## Fixed effects:\n##              Estimate Std. Error        df t value Pr(&gt;|t|)    \n## (Intercept)   57.5960     0.1329  375.6989  433.36   &lt;2e-16 ***\n## ses            3.8739     0.1366 3914.6382   28.35   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##     (Intr)\n## ses -0.025\n\n先看固定效应：Intercept的估计值为57.5960，表示当所有自变量都为0的时候，因变量的预测值；ses的估计值为3.8739，表示ses每增加一个单位，数学成绩可以提高3.8739分；固定效应的解读和普通的多元线性回归没有差别。\n加入自变量后，我们可以发现固定效应中截距的值发生了变化，因为此时的截距不再是因变量的平均值，它是一个基于当前模型的预测值。\n再看随机效应：schcode的方差为3.469，残差的方差为62.807，该结果与上面的模型的结果解读是类似的。schcode作为分组变量，它的方差越大说明这个变量对因变量的影响越大，残差的方差表示模型无法解释的随机误差部分，这个值越大说明模型无法解释的随机误差越大。\n未调整的ICC可以量化分层变量（这里是schcode）所能解释的方差（变异）比例。在模型中加入ses这个自变量后，未调整的ICC=0.046：\n\nperformance::icc(ses_l1)\n## # Intraclass Correlation Coefficient\n## \n##     Adjusted ICC: 0.052\n##   Unadjusted ICC: 0.046\n\n说明在考虑社会经济地位的影响后，数学成绩差异中有4.6%是学校不同造成的。可以看到加入ses这个变量后，不同学校能够解释的变异减少了（13.8%到4.6%），这个也很好理解，因为有一部分变异被ses解释了，另外残差的变异也减小了（66.55到62.807），也是这个原因导致的。\n多水平模型肯定是要比单水平模型的拟合程度更好的，因为它能够解释分组变量导致的变异，也就是让不能解释的变异更少了。下面我们拟合一个普通的多元线性回归，并比较一下两个模型。\n\nf &lt;- lm(math ~ ses, data = data)\ncompare_performance(f,ses_l1,metrics = \"common\") # 比较下两个模型\n## # Comparison of Model Performance Indices\n## \n## Name   |           Model |   AIC (weights) |   BIC (weights) |  RMSE |    R2\n## ----------------------------------------------------------------------------\n## f      |              lm | 48304.0 (&lt;.001) | 48324.5 (&lt;.001) | 8.131 | 0.143\n## ses_l1 | lmerModLmerTest | 48219.1 (&gt;.999) | 48246.4 (&gt;.999) | 7.810 |      \n## \n## Name   | R2 (adj.) | R2 (cond.) | R2 (marg.) |   ICC\n## ----------------------------------------------------\n## f      |     0.143 |            |            |      \n## ses_l1 |           |      0.167 |      0.121 | 0.052\n\n结果表明，AIC、BIC、RMSE都是多水平模型更小，也就是模型表现更好。\n多元线性回归中常用的提取结果的方法也都适用于多水平模型，比如计算可信区间等：\n\nconfint(ses_l1)\n##                 2.5 %    97.5 %\n## .sig01       1.575429  2.144559\n## .sigma       7.789560  8.063828\n## (Intercept) 57.335234 57.856673\n## ses          3.596455  4.152745",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>多水平模型</span>"
    ]
  },
  {
    "objectID": "多水平模型.html#添加2级水平的固定效应",
    "href": "多水平模型.html#添加2级水平的固定效应",
    "title": "35  多水平模型",
    "section": "35.5 添加2级水平的固定效应",
    "text": "35.5 添加2级水平的固定效应\n前面我们添加了ses作为1级水平的自变量，以解释学生在数学成绩方面的部分差异。下面我们再添加一个在水平2单位（不同的学校）间变化的自变量，该自变量在不同的学校间是不同的，但是在同一所学校内的所有学生中是相同的，符合条件的自变量有3个：\n\nses_mean:以学校为单位衡量学生社会经济地位的均值\npro4yrc:每个学校中计划就读四年制大学的学生比例\npublic:学校类型，1=公立学校，0=私立学校\n\n我们选择public作为水平2单位的固定效应，这个模型依然是一个随机截距模型，或者叫方差成分模型：\n\nses_l1_public_l2 &lt;- lmer(math ~ ses + public + (1|schcode), \n                         data = data, REML = TRUE)\nsummary(ses_l1_public_l2)\n## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: math ~ ses + public + (1 | schcode)\n##    Data: data\n## \n## REML criterion at convergence: 48216\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -3.7718 -0.5541  0.1309  0.6477  5.6916 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  schcode  (Intercept)  3.486   1.867   \n##  Residual             62.807   7.925   \n## Number of obs: 6871, groups:  schcode, 419\n## \n## Fixed effects:\n##               Estimate Std. Error         df t value Pr(&gt;|t|)    \n## (Intercept)   57.63143    0.25535  381.81733 225.693   &lt;2e-16 ***\n## ses            3.87338    0.13673 3928.37427  28.329   &lt;2e-16 ***\n## public        -0.04859    0.29862  385.93649  -0.163    0.871    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##        (Intr) ses   \n## ses     0.013       \n## public -0.854 -0.031\n\n先看固定效应：Intercept的估计值为57.5960，表示当所有自变量都为0的时候，因变量的预测值；ses的估计值为3.87338，表示ses每增加一个单位，数学成绩可以提高3.87338分；public的估计值为-0.04859，说明相对于私立学校，公立学校在数学成绩上平均降低0.04859分。固定效应的解读和普通的多元线性回归没有差别。\n再看随机效应：结果解读和上面一个模型的解读类似的，就不再重复了。schcode的方差变大了一些，残差的方差没有变化。\n理论上如果新加入的自变量能够解释更多的因变量变异，那么残差的变异（方差）通常会减少，群体间（在本例中也就是学校间）的变异也会减少，因为这部分变异都被新加入的自变量解释了。但是很明显我们新加入的这个public自变量不太行，它几乎解释不了因变量的变异，从它的系数也可以看出来，只有-0.04859，比ses差远了。说明公立学校和私立学校对数学成绩影响很小（P值也表明这个变量没有统计学意义）。\n我们也可以通过计算模型的一些指标看看这个自变量到底行不行，并且和前面的模型比较一下：\n\ncompare_performance(null_model,ses_l1,ses_l1_public_l2)\n## # Comparison of Model Performance Indices\n## \n## Name             |           Model |   AIC (weights) |  AICc (weights)\n## ----------------------------------------------------------------------\n## null_model       | lmerModLmerTest | 48881.8 (&lt;.001) | 48881.8 (&lt;.001)\n## ses_l1           | lmerModLmerTest | 48219.1 (0.729) | 48219.1 (0.729)\n## ses_l1_public_l2 | lmerModLmerTest | 48221.1 (0.271) | 48221.1 (0.271)\n## \n## Name             |   BIC (weights) | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n## ------------------------------------------------------------------------------------\n## null_model       | 48902.3 (&lt;.001) |      0.138 |      0.000 | 0.138 | 7.977 | 8.158\n## ses_l1           | 48246.4 (0.988) |      0.167 |      0.121 | 0.052 | 7.810 | 7.925\n## ses_l1_public_l2 | 48255.2 (0.012) |      0.167 |      0.121 | 0.053 | 7.810 | 7.925\n\n结果发现，加入public后，AIC、BIC还变大了，R2没啥太大的变化，充分说明这个变量真的作用不大，可以不加。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>多水平模型</span>"
    ]
  },
  {
    "objectID": "多水平模型.html#具有随机斜率的mlm",
    "href": "多水平模型.html#具有随机斜率的mlm",
    "title": "35  多水平模型",
    "section": "35.6 具有随机斜率的MLM",
    "text": "35.6 具有随机斜率的MLM\n前面我们选择了10个学校，以展示了不同学校间数学成绩（math）与社会经济状况（ses）之间的关系：\n\n\n\n\n\n\n\n\n\n从图中可以看出，不同学校ses的截距和斜率值差异很大。例如，学校3的截距约为38，斜率较小且为正值，而学校8的截距约为55，斜率较大且为正值。\n在ses_l1这个模型中，我们假定每个学校ses的斜率是相同的，只估计了随机截距的变异，但是从图中可以看出，其实每个学校ses的斜率都是不一样的。也就是说，我们只估计了ses的平均效应，却忽略了不同学校的ses是不同的。\n下面我们在模型中添加一个随机斜率项来模拟学校间ses斜率的这种变异。\n在lme4的语法中，只需要将想要估计的具有不同斜率的变量名字放在|前面即可：\n\n# 估计ses的随机斜率和随机截距\nses_l1_random &lt;- lmer(math ~ ses + (ses|schcode), \n                      data = data, REML = TRUE)\nsummary(ses_l1_random)\n## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: math ~ ses + (ses | schcode)\n##    Data: data\n## \n## REML criterion at convergence: 48190.1\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -3.8578 -0.5553  0.1290  0.6437  5.7098 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev. Corr \n##  schcode  (Intercept)  3.2042  1.7900        \n##           ses          0.7794  0.8828   -1.00\n##  Residual             62.5855  7.9111        \n## Number of obs: 6871, groups:  schcode, 419\n## \n## Fixed effects:\n##              Estimate Std. Error        df t value Pr(&gt;|t|)    \n## (Intercept)   57.6959     0.1315  378.6378  438.78   &lt;2e-16 ***\n## ses            3.9602     0.1408 1450.7730   28.12   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##     (Intr)\n## ses -0.284\n## optimizer (nloptwrap) convergence code: 0 (OK)\n## boundary (singular) fit: see help('isSingular')\n\n上面的公式也可以写成math~ses+(1+ses|schcode)，1表示随机截距项，这是默认设置，也可以仅使用(ses|schcode)估计随机截距和随机斜率。如果你想从模型中排除随机截距，需要写成(0+ses|schcode)来覆盖默认设置。像这种既有随机截距又有随机斜率的模型又被称为随机系数模型。\n先看固定效应：Intercept的估计值为57.6959，表示当所有自变量都为0的时候，因变量的预测值；ses的估计值为3.9602，表示ses每增加一个单位，数学成绩可以提高3.9602分。固定效应的解读和普通的多元线性回归没有差别。\n再看随机效应：schcode（Intercept）的方差为3.2042，标准差是1.7900，它衡量的是不同学校之间截距的变异。中间的ses的方差为0.7794，标准差为0.8828，它衡量的不同学校之间斜率的变异，意思是不同学校的ses的斜率围绕总体平均斜率变化的方差为0.7794。残差的方差为62.5855，标准差为7.9111，它衡量的是模型无法解释的变异，可以发现在考虑了随机斜率后，残差的方差又变小了（62.807到62.5855）。Corr是-1表示随机截距和随机斜率的相关系数是-1，说明有些学校的截距越高，斜率就越低（结合上面的图看，是不是有这种趋势），也就是说：随着平均数学成绩的增加，ses与数学成绩之间的关系降低。\n如果想查看每个学校的截距和斜率，可以使用ranef：\n\nhead(ranef(ses_l1_random))\n## $schcode\n##       (Intercept)           ses\n## 1    0.9746642943 -0.4806908392\n## 2    1.0450460989 -0.5154021638\n## 3   -3.4842479301  1.7183824946\n## 4    1.8810910566 -0.9277278791\n## 5   -3.8147866269  1.8813995708\n## 6    1.5468720742 -0.7628957373\n## 7   -0.5122881335  0.2526533641\n## 8    0.2048295030 -0.1010190547\n## 9   -0.8897622544  0.4388183369\n## 10   1.2626598135 -0.6227262134\n## 11  -1.1521785159  0.5682383780\n## 12  -1.4680255383  0.7240097253\n## 13   1.3533164954 -0.6674368248\n## 14   0.6994867551 -0.3449771139\n## 15   1.3147915034 -0.6484368359\n## 16  -0.3814181351  0.1881101057\n## 17  -1.4833089703  0.7315473008\n## 18   0.7002622200 -0.3453595624\n## 19   2.4523232121 -1.2094516129\n## 20   0.5684136873 -0.2803337046\n## 21  -0.4109023544  0.2026513115\n## 22   0.8768327142 -0.4324416680\n## 23   0.3248492751 -0.1602111328\n## 24   0.8884441467 -0.4381682646\n## 25  -0.7109596957  0.3506354082\n## 26   2.2119758881 -1.0909156641\n## 27  -0.8875599376  0.4377321850\n## 28   1.8412254351 -0.9080667104\n## 29   0.9834084843 -0.4850033518\n## 30  -0.9295033738  0.4584181029\n## 31   0.4621769318 -0.2279392181\n## 32   0.5841744018 -0.2881066692\n## 33  -2.2225902669  1.0961505277\n## 34  -0.0348041303  0.0171649118\n## 35   0.7121683165 -0.3512314832\n## 36  -1.9271552831  0.9504461132\n## 37   2.5177534943 -1.2417209157\n## 38  -0.7534024732  0.3715675943\n## 39   0.6561454204 -0.3236017720\n## 40  -0.4178972897  0.2061011161\n## 41   0.7824983634 -0.3859172816\n## 42   0.0002114307 -0.0001042747\n## 43   0.2795891037 -0.1378894473\n## 44   0.5440628359 -0.2683242042\n## 45   1.6722426247 -0.8247267446\n## 46   0.2647420348 -0.1305670800\n## 47  -2.4382522920  1.2025120313\n## 48   1.4687945663 -0.7243889991\n## 49  -0.6317500158  0.3115702987\n## 50  -2.2638586950  1.1165035410\n## 51   0.0285971816 -0.0141037312\n## 52   1.4662213769 -0.7231199380\n## 53   1.5691025893 -0.7738595174\n## 54   0.8109670452 -0.3999576385\n## 55  -2.3250045379  1.1466598181\n## 56   1.0791228008 -0.5322083180\n## 57   0.3420626272 -0.1687005180\n## 58   0.6625560973 -0.3267634285\n## 59   1.2356550397 -0.6094078356\n## 60  -0.9610219938  0.4739626468\n## 61   0.3726567587 -0.1837891170\n## 62  -3.9059760855  1.9263729402\n## 63   0.2585462819 -0.1275114211\n## 64  -0.0218096074  0.0107561943\n## 65   1.1034727974 -0.5442173968\n## 66  -0.3527911498  0.1739916757\n## 67   1.1310601669 -0.5578231028\n## 68   0.2000217301 -0.0986479282\n## 69  -1.4852485358  0.7325038675\n## 70   1.1767003895 -0.5803322241\n## 71  -1.0605617265  0.5230542549\n## 72  -3.3378392986  1.6461757990\n## 73  -0.7939026197  0.3915417018\n## 74   1.0145970807 -0.5003851326\n## 75   0.8006019742 -0.3948457300\n## 76  -0.1038921476  0.0512381585\n## 77   0.7675631221 -0.3785514289\n## 78  -1.0253444384  0.5056855796\n## 79  -0.6845356850  0.3376034546\n## 80   0.6544003463 -0.3227411258\n## 81  -1.1140059544  0.5494122029\n## 82  -0.2377006531  0.1172306476\n## 83   0.6114819657 -0.3015743789\n## 84   1.6532069006 -0.8153385910\n## 85  -0.4189105707  0.2066008521\n## 86  -0.6649549090  0.3279464889\n## 87   1.0218902721 -0.5039820329\n## 88   1.1589252684 -0.5715657823\n## 89  -0.1442865016  0.0711600907\n## 90   1.4381502588 -0.7092756540\n## 91   0.1532970629 -0.0756039739\n## 92  -0.2220361267  0.1095051216\n## 93   0.3475295931 -0.1713967493\n## 94   0.7604429695 -0.3750398689\n## 95  -0.6487651100  0.3199619058\n## 96   0.7055351372 -0.3479600916\n## 97   0.6358441684 -0.3135894776\n## 98   1.4967687963 -0.7381854993\n## 99   0.2185471692 -0.1077844164\n## 100  1.0204171356 -0.5032555025\n## 101 -0.2170988034  0.1070701026\n## 102  0.1976650393 -0.0974856411\n## 103  2.7514380538 -1.3569708819\n## 104 -1.1006297014  0.5428152215\n## 105  0.2999952348 -0.1479534666\n## 106 -1.6892984503  0.8331384400\n## 107  1.7878158751 -0.8817258602\n## 108  0.8571204213 -0.4227198401\n## 109  1.1102660505 -0.5475677345\n## 110  0.0905551010 -0.0446605131\n## 111  0.2871226832 -0.1416049037\n## 112  0.7510879481 -0.3704261028\n## 113  0.8985547095 -0.4431546532\n## 114 -0.5824879247  0.2872749222\n## 115  0.1562819186 -0.0770760630\n## 116 -3.7996192959  1.8739192546\n## 117 -0.5496200382  0.2710649389\n## 118 -0.0936072885  0.0461658094\n## 119  1.7268967018 -0.8516813734\n## 120  1.6211436909 -0.7995254630\n## 121 -0.0763549991  0.0376572208\n## 122 -2.1359312688  1.0534115182\n## 123 -0.0304097001  0.0149976401\n## 124  1.4955778202 -0.7375981265\n## 125 -0.3437100589  0.1695130083\n## 126  1.0192294721 -0.5026697635\n## 127  0.6728133639 -0.3318221693\n## 128  0.0477975641 -0.0235730921\n## 129 -1.1506818437  0.5675002401\n## 130 -0.1342325846  0.0662016390\n## 131 -0.1717853249  0.0847221268\n## 132 -1.7511516458  0.8636435735\n## 133  0.8900878689 -0.4389789255\n## 134 -0.8411250729  0.4148311572\n## 135 -2.4617874768  1.2141192562\n## 136 -0.7598763953  0.3747604424\n## 137  0.5604287004 -0.2763956204\n## 138  0.0819907492 -0.0404366942\n## 139 -1.0097455760  0.4979924381\n## 140  2.4157382336 -1.1914084116\n## 141  0.2598739959 -0.1281662311\n## 142  2.2707632264 -1.1199087596\n## 143  0.6903922865 -0.3404918488\n## 144 -0.3423374576  0.1688360605\n## 145  0.0585298290 -0.0288660955\n## 146  0.7057228191 -0.3480526537\n## 147  0.3488853580 -0.1720653937\n## 148 -2.0620886628  1.0169933746\n## 149  0.6981581422 -0.3443218606\n## 150  1.1846962336 -0.5842756629\n## 151 -2.5650429027  1.2650433925\n## 152  0.2640477814 -0.1302246839\n## 153  1.3764275954 -0.6788348971\n## 154 -0.5846884961  0.2883602133\n## 155 -1.0464547400  0.5160968859\n## 156  0.6036026729 -0.2976884216\n## 157  0.8081699954 -0.3985781726\n## 158  1.0971985732 -0.5411230369\n## 159  0.3518236109 -0.1735144990\n## 160  0.4965509424 -0.2448920008\n## 161 -1.2657324253  0.6242415827\n## 162 -0.3915164626  0.1930904600\n## 163 -1.6177130162  0.7978335021\n## 164 -0.4148439198  0.2045952367\n## 165 -0.9712978763  0.4790305688\n## 166 -2.1244695578  1.0477587622\n## 167  0.3340176309 -0.1647328380\n## 168 -0.5978838734  0.2948679894\n## 169 -0.7021720433  0.3463014606\n## 170  0.8530185408 -0.4206968498\n## 171  0.0269581178 -0.0132953678\n## 172  0.8585620387 -0.4234308256\n## 173  2.3201228308 -1.1442522282\n## 174 -0.9129112045  0.4502350763\n## 175  0.5031175926 -0.2481305812\n## 176  0.0704575842 -0.0347486981\n## 177 -0.1934570094  0.0954102995\n## 178  0.7752418378 -0.3823384644\n## 179 -0.9510775107  0.4690581664\n## 180  1.1433971674 -0.5639075394\n## 181 -0.0320652197  0.0158141192\n## 182  2.5816388793 -1.2732282967\n## 183  0.1682358753 -0.0829715878\n## 184 -0.3950606675  0.1948384124\n## 185 -3.0513481605  1.5048823646\n## 186 -1.6607353541  0.8190515193\n## 187 -0.8421948477  0.4153587553\n## 188  1.4251174078 -0.7028480336\n## 189 -1.9246277399  0.9491995642\n## 190 -0.3657213084  0.1803686497\n## 191  0.0073808771 -0.0036401457\n## 192  0.1257823149 -0.0620340838\n## 193 -0.6046525848  0.2982062235\n## 194  0.6611534573 -0.3260716660\n## 195  1.4167898554 -0.6987410009\n## 196 -0.7023482897  0.3463883829\n## 197 -2.5950459837  1.2798404937\n## 198 -0.5304781949  0.2616244487\n## 199 -0.1623774361  0.0800822873\n## 200 -0.8282901087  0.4085011319\n## 201  0.0426995729 -0.0210588339\n## 202  1.1880332017 -0.5859214091\n## 203  0.4110629404 -0.2027305103\n## 204  0.1523160867 -0.0751201701\n## 205  0.0058128102 -0.0028667970\n## 206 -0.3315096214  0.1634959227\n## 207  0.1347094682 -0.0664368313\n## 208 -1.0067440410  0.4965121229\n## 209  2.2895582614 -1.1291782088\n## 210 -0.6335191739  0.3124428228\n## 211 -0.2296165769  0.1132436939\n## 212 -1.4954834523  0.7375515856\n## 213  0.8638762790 -0.4260517349\n## 214 -0.5492892741  0.2709018107\n## 215  0.9763025106 -0.4814987846\n## 216  0.3413567959 -0.1683524118\n## 217  1.4873826618 -0.7335563887\n## 218 -0.4878925064  0.2406217810\n## 219 -0.1067704979  0.0526577208\n## 220 -2.0470052694  1.0095544553\n## 221  1.9820868559 -0.9775375990\n## 222 -1.7682981018  0.8720999665\n## 223 -2.8119056742  1.3867926691\n## 224  1.5540978370 -0.7664593828\n## 225 -0.6179808748  0.3047795503\n## 226  0.0792663084 -0.0390930380\n## 227 -0.6449475575  0.3180791422\n## 228 -0.2350442112  0.1159205275\n## 229  1.4292301252 -0.7048763685\n## 230  0.4419689906 -0.2179729433\n## 231 -1.2058645381  0.5947155755\n## 232  0.2822759815 -0.1392145780\n## 233  0.7296107369 -0.3598338417\n## 234  0.7531542424 -0.3714451703\n## 235  1.4333385758 -0.7069025990\n## 236 -0.6033441072  0.2975609006\n## 237 -1.4942808028  0.7369584556\n## 238  0.6023853522 -0.2970880560\n## 239 -2.6448943338  1.3044250049\n## 240  0.9734668925 -0.4801002973\n## 241  0.5447238143 -0.2686501896\n## 242 -0.3760571745  0.1854661547\n## 243 -1.5293327600  0.7542455921\n## 244  1.2369304037 -0.6100368272\n## 245  1.4660539648 -0.7230373727\n## 246 -2.5293448909  1.2474376309\n## 247 -0.9343861076  0.4608261991\n## 248  0.1450783148 -0.0715506019\n## 249 -1.5338271177  0.7564621467\n## 250 -1.5246594526  0.7519407821\n## 251  0.5999647111 -0.2958942296\n## 252  1.5288418519 -0.7540034831\n## 253  0.0048549214 -0.0023943795\n## 254 -0.1882755736  0.0928548876\n## 255 -1.0287824088  0.5073811386\n## 256  1.7912193396 -0.8834044014\n## 257  1.2724729851 -0.6275659328\n## 258  0.2408141727 -0.1187661921\n## 259 -1.1587315583  0.5714702471\n## 260 -0.5097772987  0.2514150553\n## 261 -0.6661027532  0.3285125897\n## 262  1.5256506787 -0.7524296410\n## 263  0.1676251687 -0.0826703958\n## 264 -1.6625856066  0.8199640380\n## 265 -0.4826379709  0.2380303173\n## 266 -1.0467283158  0.5162318097\n## 267 -0.3146335363  0.1551728728\n## 268  0.3504531990 -0.1728386310\n## 269 -0.1114490905  0.0549651374\n## 270  0.3890170270 -0.1918577732\n## 271  1.4367984792 -0.7086089752\n## 272 -0.3396675325  0.1675192907\n## 273  0.1238821402 -0.0610969441\n## 274  1.8387067372 -0.9068245237\n## 275  0.8522383185 -0.4203120550\n## 276  0.0181713730 -0.0089618678\n## 277 -0.9572553398  0.4721049856\n## 278  2.5927939355 -1.2787298149\n## 279 -0.0055491493  0.0027367631\n## 280 -0.7498175750  0.3697995725\n## 281  1.8594326203 -0.9170462403\n## 282  1.5217833030 -0.7505223052\n## 283 -1.7251868830  0.8508381146\n## 284  1.1315901207 -0.5580844685\n## 285 -0.3027216665  0.1492981047\n## 286 -0.3038078704  0.1498338054\n## 287 -1.9718860229  0.9725066904\n## 288  1.4357443793 -0.7080891078\n## 289 -0.3139312679  0.1548265238\n## 290 -0.5536031043  0.2730293316\n## 291 -0.2805018438  0.1383395980\n## 292  0.1138885173 -0.0561682286\n## 293 -0.4812546253  0.2373480706\n## 294  1.2647755636 -0.6237696719\n## 295 -1.9703693660  0.9717586964\n## 296  0.7260155883 -0.3580607646\n## 297 -3.5481599699  1.7499030214\n## 298 -0.1860444986  0.0917545525\n## 299  0.0744973004 -0.0367410298\n## 300 -0.2977590191  0.1468505962\n## 301  0.2277684481 -0.1123322226\n## 302 -1.0914774296  0.5383014486\n## 303  0.6857036791 -0.3381794930\n## 304  0.6901742223 -0.3403843026\n## 305 -0.3396563028  0.1675137524\n## 306 -0.7666461893  0.3780992104\n## 307  1.5909656465 -0.7846420723\n## 308  0.7750689167 -0.3822531821\n## 309  1.0448046232 -0.5152830713\n## 310  0.3724615196 -0.1836928278\n## 311  0.9914781406 -0.4889831937\n## 312 -0.2249787533  0.1109563840\n## 313 -0.4054137600  0.1999444133\n## 314 -0.3996633708  0.1971084016\n## 315  0.3387917497 -0.1670873668\n## 316 -1.6148481549  0.7964205924\n## 317 -0.5537232718  0.2730885965\n## 318  0.4939725107 -0.2436203543\n## 319 -0.4548537206  0.2243275125\n## 320  0.9839610550 -0.4852758719\n## 321  0.7192395489 -0.3547189164\n## 322 -0.0086535210  0.0042677959\n## 323  0.7579995522 -0.3738348096\n## 324  0.5417464629 -0.2671818013\n## 325  1.0158447921 -0.5010004865\n## 326 -1.4552477307  0.7177078887\n## 327  1.6596593580 -0.8185208530\n## 328  0.9932451865 -0.4898546761\n## 329  0.2024267199 -0.0998340356\n## 330 -1.7054671873  0.8411126356\n## 331  0.5806649434 -0.2863758532\n## 332 -0.0966051669  0.0476443213\n## 333  1.4657539675 -0.7228894182\n## 334  1.3180292179 -0.6500336316\n## 335  1.7468823105 -0.8615379969\n## 336 -0.0661413130  0.0326199733\n## 337 -0.8868953506  0.4374044200\n## 338 -0.6043625909  0.2980632026\n## 339 -1.7385221910  0.8574149025\n## 340 -1.7859541851  0.8808077006\n## 341  1.0302378643 -0.5080989490\n## 342 -1.9336709386  0.9536595437\n## 343  0.5309474715 -0.2618558894\n## 344 -0.4414201958  0.2177022854\n## 345  1.2436196156 -0.6133358533\n## 346  0.3623474768 -0.1787047229\n## 347  0.1387805536 -0.0684446339\n## 348  2.6888442769 -1.3261005040\n## 349  0.2670765381 -0.1317184245\n## 350 -0.0718175029  0.0354193910\n## 351  1.0691868199 -0.5273080308\n## 352  0.8647255711 -0.4264705939\n## 353  0.8359147994 -0.4122615230\n## 354 -0.1432092836  0.0706288218\n## 355  0.6020190777 -0.2969074145\n## 356  1.4762257706 -0.7280539655\n## 357 -0.4886619680  0.2410012687\n## 358  1.2766700699 -0.6296358765\n## 359 -0.0280991085  0.0138580885\n## 360 -1.2276330016  0.6054514782\n## 361  0.3213774195 -0.1584988620\n## 362 -3.6557440800  1.8029620043\n## 363 -2.0889663383  1.0302490694\n## 364 -1.5179623621  0.7486378704\n## 365  0.7754083200 -0.3824205711\n## 366 -0.5131851461  0.2530957582\n## 367  1.1806614246 -0.5822857514\n## 368  1.8099286403 -0.8926315676\n## 369  1.2017529405 -0.5926877930\n## 370 -0.6631456552  0.3270541902\n## 371 -1.4458785858  0.7130871571\n## 372 -0.0637732387  0.0314520721\n## 373  0.4612196471 -0.2274670987\n## 374 -0.0602223627  0.0297008295\n## 375 -0.9790568809  0.4828572016\n## 376  0.8606100313 -0.4244408671\n## 377 -0.0407532693  0.0200989442\n## 378 -0.1038543153  0.0512195001\n## 379  0.4374651819 -0.2157517277\n## 380  0.8705725599 -0.4293542472\n## 381  0.8533691205 -0.4208697508\n## 382 -1.9161547689  0.9450208130\n## 383 -2.4496816163  1.2081488146\n## 384 -0.2784553498  0.1373302957\n## 385 -1.4419557520  0.7111524702\n## 386  1.0357625431 -0.5108236435\n## 387 -1.9831802646  0.9780768529\n## 388 -0.0641474190  0.0316366126\n## 389  0.1307049716 -0.0644618695\n## 390  1.9606485697 -0.9669645352\n## 391 -0.2511287368  0.1238531912\n## 392 -0.2826041129  0.1393764078\n## 393 -0.2155452648  0.1063039190\n## 394 -1.7091813558  0.8429444118\n## 395  1.0604858823 -0.5230168497\n## 396 -0.0693949232  0.0342246085\n## 397  0.1393399326 -0.0687205118\n## 398  0.4283313817 -0.2112470648\n## 399 -1.4393424853  0.7098636435\n## 400 -0.6253055618  0.3083919838\n## 401  0.6778194389 -0.3342910957\n## 402 -1.4088704913  0.6948352809\n## 403  1.2739991286 -0.6283186055\n## 404  0.6968629583 -0.3436830939\n## 405  1.6428515186 -0.8102314610\n## 406  0.4501905100 -0.2220276820\n## 407  0.4371172355 -0.2155801254\n## 408 -0.4441169594  0.2190322916\n## 409 -2.6083262711  1.2863901463\n## 410 -0.4445078699  0.2192250832\n## 411  0.0767803831 -0.0378670143\n## 412 -0.1575697151  0.0777111863\n## 413  1.0768277429 -0.5310764274\n## 414 -0.7398484939  0.3648829607\n## 415  0.2376241534 -0.1171929190\n## 416 -0.2601864177  0.1283203131\n## 417 -2.7895376238  1.3757610586\n## 418 -0.6233121254  0.3074088487\n## 419 -1.2366360507  0.6098916565",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>多水平模型</span>"
    ]
  },
  {
    "objectID": "多水平模型.html#具有交互效应的mlm",
    "href": "多水平模型.html#具有交互效应的mlm",
    "title": "35  多水平模型",
    "section": "35.7 具有交互效应的MLM",
    "text": "35.7 具有交互效应的MLM\n前面分别探索了ses和public对数学成绩的影响，如果要探索它们的交互效应，只需要像普通的单水平模型一样，将交互项添加到公式中即可，比如将ses和public的交互项添加到模型中：\n\n# 添加交互效应\ncrosslevel_model &lt;- lmer(math ~ ses + public + ses:public + (ses|schcode), \n                         data = data, REML = TRUE)\n# 也可以写成math ~ ses*public + (ses|schcode)\nsummary(crosslevel_model)\n## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: math ~ ses + public + ses:public + (ses | schcode)\n##    Data: data\n## \n## REML criterion at convergence: 48187.1\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -3.8509 -0.5593  0.1294  0.6412  5.6998 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev. Corr \n##  schcode  (Intercept)  3.2144  1.7929        \n##           ses          0.8013  0.8951   -1.00\n##  Residual             62.5555  7.9092        \n## Number of obs: 6871, groups:  schcode, 419\n## \n## Fixed effects:\n##               Estimate Std. Error         df t value Pr(&gt;|t|)    \n## (Intercept)   57.72440    0.25183  382.39815 229.216   &lt;2e-16 ***\n## ses            4.42383    0.27427 1283.55623  16.130   &lt;2e-16 ***\n## public        -0.02632    0.29472  387.41741  -0.089   0.9289    \n## ses:public    -0.62520    0.31957 1363.95274  -1.956   0.0506 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##            (Intr) ses    public\n## ses        -0.232              \n## public     -0.852  0.197       \n## ses:public  0.198 -0.858 -0.250\n## optimizer (nloptwrap) convergence code: 0 (OK)\n## boundary (singular) fit: see help('isSingular')\n\n先看固定效应（由于public是分类变量，1表示公立学校，0表示私立学校，所以在进行回归分析时会自动进行哑变量编码，以0（也就是私立学校）为参考，这里涉及一个基础知识，即回归分析中的哑变量编码）：\n\n截距（Intercept）是57.72440，即：当学校为是私立学校（public=0）且ses也为0时的平均数学预期成绩；\nses的估计值为4.42383，即：对于私立学校（public=0）来说，ses每增加一个单位，数学成绩会增加4.42383分；\npublic的估计值为-0.02632，即：公立学校（public=1）相比于私立学校（public=0），平均数学成绩会减少0.02632分；\nses:public的估计值为-0.62520，即：ses对数学成绩的影响在公立学校（public=1）比在私立学校（public=0）平均减少0.62520分。\n借助这些系数，我们可以估算ses在公立学校（public=1）的预期斜率，即4.42383-0.62520=3.79863。因此公立学校中ses每增加一个单位，数学成绩会增加3.79863分，略小于私立学校（4.42383分）。\n\n再看随机效应：（和上面ses_l1_random的结果解读基本类似，这里简单说一下）。schcode（Intercept）的方差为3.2144，标准差是1.7929，它衡量的是不同学校之间截距的变异。中间的ses的方差为0.8013，标准差为0.8951，它衡量的不同学校之间斜率的变异，意思是不同学校的ses的斜率围绕总体平均斜率变化的方差为0.8013。残差的方差为62.5555，标准差为7.9092，它衡量的是模型无法解释的变异。Corr是-1表示随机截距和随机斜率的相关系数是-1。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>多水平模型</span>"
    ]
  },
  {
    "objectID": "多水平模型.html#重复测量数据的mlm",
    "href": "多水平模型.html#重复测量数据的mlm",
    "title": "35  多水平模型",
    "section": "35.8 重复测量数据的MLM",
    "text": "35.8 重复测量数据的MLM\n重复测量数据的结构非常适合多水平模型，测量的时间点可以看做是1级水平，每个患者可以看做是2级水平，每个患者都包括多次测量数据，这是一个具有两个层次的结构。\n下面用一个简单的例子进行演示。\n使用某药治疗10个高血压患者，分别测量每个患者治疗前后的血压，请对该数据进行分析。\n\n# 模拟数据\ndata12_1 &lt;- data.frame(id=c(1:10,1:10),\n                       stat=rep(c(\"治疗前\",\"治疗后\"),each=10),\n                       bp=c(130,124,136,128,122,118,116,138,126,124,\n                            114,110,126,116,102,100,98,122,108,106)\n                       )\n# 设置下因子水平，变成治疗后vs治疗前\ndata12_1$stat &lt;- factor(data12_1$stat,levels = c(\"治疗前\",\"治疗后\"))\ndata12_1$id &lt;- factor(data12_1$id)\nstr(data12_1)\n## 'data.frame':    20 obs. of  3 variables:\n##  $ id  : Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  $ stat: Factor w/ 2 levels \"治疗前\",\"治疗后\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ bp  : num  130 124 136 128 122 118 116 138 126 124 ...\ndata12_1 # 数据长这样\n##    id   stat  bp\n## 1   1 治疗前 130\n## 2   2 治疗前 124\n## 3   3 治疗前 136\n## 4   4 治疗前 128\n## 5   5 治疗前 122\n## 6   6 治疗前 118\n## 7   7 治疗前 116\n## 8   8 治疗前 138\n## 9   9 治疗前 126\n## 10 10 治疗前 124\n## 11  1 治疗后 114\n## 12  2 治疗后 110\n## 13  3 治疗后 126\n## 14  4 治疗后 116\n## 15  5 治疗后 102\n## 16  6 治疗后 100\n## 17  7 治疗后  98\n## 18  8 治疗后 122\n## 19  9 治疗后 108\n## 20 10 治疗后 106\n\n先画个图看看不同患者治疗前后的血压值。可以发现每个患者的斜率和截距都是不一样的：\n\nlibrary(ggplot2)\nggplot(data12_1, aes(stat,bp))+\n  geom_line(aes(color=id,group = id))\n\n\n\n\n\n\n\n\n下面就是建立多水平模型即可，这里为了省事，我们默认斜率是相等的，只考虑随机截距：\n\nlibrary(lmerTest)\n\nf &lt;- lmer(bp ~ stat + (1 | id), data = data12_1)\nsummary(f)\n## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: bp ~ stat + (1 | id)\n##    Data: data12_1\n## \n## REML criterion at convergence: 113.9\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -1.1422 -0.5311  0.1307  0.4070  1.5714 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  id       (Intercept) 63.511   7.969   \n##  Residual              4.889   2.211   \n## Number of obs: 20, groups:  id, 10\n## \n## Fixed effects:\n##             Estimate Std. Error       df t value Pr(&gt;|t|)    \n## (Intercept) 126.2000     2.6153   9.6662   48.25 7.56e-13 ***\n## stat治疗后  -16.0000     0.9888   9.0000  -16.18 5.83e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n## Warning in abbreviate(rn, minlength = 6): abbreviate used with non-ASCII chars\n##            (Intr)\n## stat治疗后 -0.189\n\n结果解读不再重复。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>多水平模型</span>"
    ]
  },
  {
    "objectID": "多水平模型.html#广义混合效应模型",
    "href": "多水平模型.html#广义混合效应模型",
    "title": "35  多水平模型",
    "section": "35.9 广义混合效应模型",
    "text": "35.9 广义混合效应模型\n上面介绍的例子，都是假定因变量为连续分布，而在医学和公共卫生领域，许多应变量是离散型的，例如个体的健康状态可能与吸烟、饮酒、锻炼等日常生活方式有关。在离散型应变量的情形下，若数据具有层次结构特征，则最低水平的观察单位发生某事件的概率并不完全相互独立，故不再服从二项分布或Poisson分布，而服从超二项（extra-binomial）分布或超Poisson（extra-Poisson）分布。因此，在拟合这种类型的模型时，结局的聚集效应和离散型误差的复杂分布应考虑在模型中。假定在某试验中对某事件的测量为发生与不发生，即二分类的资料，若将其作为因变量，则在多水平框架内，处理这类资料的统计模型一般称为多水平广义线性模型。\n使用方法和结果解读基本类似，以后遇到了再详细介绍。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>多水平模型</span>"
    ]
  },
  {
    "objectID": "多水平模型.html#参考资料",
    "href": "多水平模型.html#参考资料",
    "title": "35  多水平模型",
    "section": "35.10 参考资料",
    "text": "35.10 参考资料\n\nIntroduction to Multilevel Modelling 2.A Cheatsheet for Building Multilevel Models in R\nCRAN Task View: Mixed, Multilevel, and Hierarchical Models in R",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>多水平模型</span>"
    ]
  },
  {
    "objectID": "广义估计方程.html",
    "href": "广义估计方程.html",
    "title": "36  广义估计方程",
    "section": "",
    "text": "36.1 理论知识\n在医学研究中，我们经常需要对同一群患者在不同时间点进行多次测量，这种数据叫做重复测量数据，这也是最常见的纵向数据之一。\n常见的纵向数据分析方法包括：\n重复测量方差分析的使用前提太多了，比如，它要求因变量是符合正态分布的连续型变量、要求残差正态性、要求符合球对称假设、不能有缺失值等，而且对于混杂因素的调整较弱。\n很多时候我们的数据并不能满足这么多的要求，此时可以使用广义估计方程（generalized estimating equations，GEE）。GEE和重复测量方差分析的区别如下：\n关于广义估计方程的理论知识，大家可以参考以下几篇文章，写得非常通俗，我都可以看懂，相信你们更加没问题：\n重复测量数据分析及结果详解（之二）——广义估计方程 广义估计方程的结果，你真的会解释吗？ 如何分析临床纵向数据：研究目的及分析方法\n下面是一个R语言实战广义估计方程的实例。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>广义估计方程</span>"
    ]
  },
  {
    "objectID": "广义估计方程.html#理论知识",
    "href": "广义估计方程.html#理论知识",
    "title": "36  广义估计方程",
    "section": "",
    "text": "重复测量方差分析\n混合效应模型\n广义估计方程(GEE)\n……\n\n\n\n\n\n广义估计方程的计算过程很复杂，但思想却并不难理解。该方法假定在多次测量之间存在一定的相关结构（广义估计方程中叫做作业相关矩阵）。对于重复测量数据而言，最主要的问题就是存在各次测量之间的相关性，从而不能用常规的线性模型等方法。所以广义估计方程思想很简单，就是把这种相关进行校正一下，然后得到校正后的参数估计值，这样就比较可靠了。–摘自冯国双老师的小白学统计",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>广义估计方程</span>"
    ]
  },
  {
    "objectID": "广义估计方程.html#数据探索",
    "href": "广义估计方程.html#数据探索",
    "title": "36  广义估计方程",
    "section": "36.2 数据探索",
    "text": "36.2 数据探索\n我们通过一个具体的例子来学习GEE的使用方法。比较两种药物治疗治疗抑郁症的效果。\n\ndat &lt;- read.csv(\"datasets/depression.csv\",stringsAsFactors = TRUE)\ndat$id &lt;- factor(dat$id)\ndat$drug &lt;- relevel(dat$drug, ref = \"standard\")\nhead(dat, n = 3)\n##   diagnose     drug id time depression\n## 1     mild standard  1    0          1\n## 2     mild standard  1    1          1\n## 3     mild standard  1    2          1\n\n本研究包含340名受试者，主要变量包括：\n\ndiagnose：抑郁症严重程度，轻微(mild)和严重(severe)\ndrug：药物，新药(new)和标准药(standard)\nid：受试者编号\ntime：时间点（0，1，2）\ndepression：结果变量，用药后的反应，1=有效，0=无效\n\n下面探索下抑郁症有效（depression=1）的比例在不同的诊断和不同的药物类别中，随时间变化的情况。\n\nwith(dat, \n     tapply(depression, list(diagnose, drug, time), mean)) |&gt;\n  ftable() |&gt; \n  round(2)\n##                     0    1    2\n##                                \n## mild   standard  0.51 0.59 0.68\n##        new       0.53 0.79 0.97\n## severe standard  0.21 0.28 0.46\n##        new       0.18 0.50 0.83\n\n从结果中可以看出，“有效”的比例在所有四种诊断和治疗组合中都是随时间增加的，并且对于“新”药物的增加更为显著。\n直接看数字可能不明显，下面我们画图展示。\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n# 先计算下画图数据，前半段代码和上面的代码一个意思\nsummary_dat &lt;- dat %&gt;% \n  group_by(diagnose, drug, time) %&gt;% \n  summarise(n = n(),\n            normal = sum(depression),\n            normal_rate = mean(depression),\n            .groups = \"drop\"\n            ) %&gt;% \n  mutate(se = sqrt(normal_rate * (1-normal_rate) / n),\n         lower = pmax(0, normal_rate - 1.96 * se),\n         upper = pmin(1, normal_rate + 1.96 * se)\n        )\n  \n## 画图\nggplot(summary_dat, aes(time, normal_rate, color=drug))+\n  geom_line(linewidth=1.3,aes(linetype = drug))+\n  geom_point(size=3)+\n  geom_errorbar(aes(ymin=lower, ymax = upper),width=0.5,size=0.9)+\n  facet_wrap(vars(diagnose),\n             labeller = labeller(diagnose=c(mild=\"轻度抑郁\",severe=\"重度抑郁\"))\n             )+\n  labs(\n    title = \"图1：两种药物治疗轻重度抑郁症各时间点有效率比较（95%置信区间）\",\n    x = \"记录时间\",\n    y = \"有效率\",\n    color = \"药物类型\",\n    linetype = \"药物类型\"\n  ) +\n  theme_minimal(base_size = 13) +\n  scale_color_manual(values = c(\"new\" = \"#2E8B57\", \"standard\" = \"#DC143C\"))\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n直接看图就很直观了，从图中可以看出，不管是轻度抑郁还是重度抑郁，在用药后有效率都是随时间明显增加的，而且都是新药组增加的更加明显。\n这个数据的因变量是二分类的，而且包含多个自变量，是没法用重复测量方差分析的，下面我们用广义估计方程探索两种药物治疗抑郁症的效果。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>广义估计方程</span>"
    ]
  },
  {
    "objectID": "广义估计方程.html#建立gee",
    "href": "广义估计方程.html#建立gee",
    "title": "36  广义估计方程",
    "section": "36.3 建立GEE",
    "text": "36.3 建立GEE\n我们用geepack实现GEE，其他R包比如gee、glmtoolbox都可以用，用法都差不多。\n从上面的的图中可以看出，两种药物治疗有效率随时间变化明显不同，这提示我们药物和时间之间是有交互作用的，所以需要在模型中加入交互项。\n如果时间点是等距且你想建模线性趋势，用time；如果时间点不规则或你想捕捉非线性变化，用factor(time)。\n\nlibrary(geepack)\ndep_gee &lt;- geeglm(depression ~ diagnose + drug*time,\n                  data = dat, \n                  id = id, \n                  family = binomial, # 因变量分布类型；连续变量选高斯分布\n                  corstr = \"independence\") # 指定作业相关矩阵\n\n上面这个公式中比较难理解的是作业相关矩阵，主要有5种(以下内容参考重复测量数据分析及结果详解（之二）——广义估计方程)：\n\n独立结构(independence structure):即不同时间点上的测量值之间彼此独立，无相关关系。这种结构因为数据完全独立，实际上也无需考虑广义估计方程，直接采用常规的广义线性模型即可。\n等相关结构(exchangeable correlation structure):即假定任意两次观测之间的相关性是相等的，不随两个时间点之间的间隔大小而改变。不管是第1次观测与第2次观测，还是第3次观测与第5次观测，相关系数都相等。\n一阶相关结构(one‐dependent structure)：表示某时间点的测量值只与其临近时间点的观测存在相关性，而与其他时间点的观测无关。例如，第2次观测只与第1次和第3次有相关，而与第4次无关。\n自相关(autocorrelation)：即相关大小与间隔次数有 关，相邻两次观测之间相关较强，间隔越远，相关性越小。例如，第2次观测与第1次和第3次观测相关性较大，与第4次观测的相关性较小。\n无结构相关(unstructured correlation)：即假定不同时间点观测值的相关系数各不相等，不存在前面几种相关结构的规律。\n\n如何选择合适的作业相关矩阵？请参考重复测量数据分析及结果详解（之二）——广义估计方程",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>广义估计方程</span>"
    ]
  },
  {
    "objectID": "广义估计方程.html#结果解读",
    "href": "广义估计方程.html#结果解读",
    "title": "36  广义估计方程",
    "section": "36.4 结果解读",
    "text": "36.4 结果解读\n查看详细的模型结果：\n\nsummary(dep_gee)\n## \n## Call:\n## geeglm(formula = depression ~ diagnose + drug * time, family = binomial, \n##     data = dat, id = id, corstr = \"independence\")\n## \n##  Coefficients:\n##                Estimate  Std.err   Wald Pr(&gt;|W|)    \n## (Intercept)    -0.02799  0.17419  0.026    0.872    \n## diagnosesevere -1.31391  0.14598 81.006  &lt; 2e-16 ***\n## drugnew        -0.05960  0.22854  0.068    0.794    \n## time            0.48241  0.11994 16.179 5.76e-05 ***\n## drugnew:time    1.01744  0.18769 29.385 5.93e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation structure = independence \n## Estimated Scale Parameters:\n## \n##             Estimate Std.err\n## (Intercept)   0.9806 0.06536\n## Number of clusters:   340  Maximum cluster size: 3\n\n模型输出的系数是在logit(ln)尺度上的，我们可以像解释普通的逻辑回归一样来解释这些系数：\n\n(Intercept)：系数是-0.028，表示在time=0、使用标准药的患者、治疗有效的log(odds)=-0.028。\ndiagnosesevere：系数是-1.3139，那么OR值=exp(β)=exp(-1.3139)=0.27，说明在相同药物和相同随访时间下，重度抑郁患者的治疗有效率仅为轻度患者的27%。且差异具有统计学意义。这表明基线诊断越严重，治疗有效的可能性显著越低，是疗效的重要负向预测因子。\ndrugnew：系数是-0.0596，OR=0.94，P=0.79，说明在治疗开始时（time=0），新药组与标准药组的治疗有效 率无统计学差异，说明两组基线均衡，后续疗效差异不太可能由初始不平衡导致。\ntime：系数是0.48，那么OR值=exp(β)=exp(0.48)=1.62，说明使用标准药物时，每增加一个时间单位，有效率增加约62%。\ndrugnew:time：系数是1.02，OR=2.77，说明在整个人群平均水平上，新药组随时间改善的速度是标准药组的2.77倍，这意味着新药的疗效优势随治疗时间延长而不断扩大。比如，在time=2时，新药组的有效率是标准药组的exp(-0.0596+1.0174*2)=7.21倍！\n\nOR值和可信区间可直接通过代码得到：\n\nbroom::tidy(dep_gee,exponentiate = T,conf.int = T)\n## # A tibble: 5 × 7\n##   term           estimate std.error statistic      p.value conf.low conf.high\n##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)       0.972     0.174    0.0258 0.872           0.691     1.37 \n## 2 diagnosesevere    0.269     0.146   81.0    0               0.202     0.358\n## 3 drugnew           0.942     0.229    0.0680 0.794           0.602     1.47 \n## 4 time              1.62      0.120   16.2    0.0000576       1.28      2.05 \n## 5 drugnew:time      2.77      0.188   29.4    0.0000000593    1.91      4.00\n\n结论：在调整诊断严重程度后，新药与时间存在显著的交互作用（OR = 2.77, 95% CI: 1.92–3.97, p &lt; 0.001），表明其抗抑郁疗效随治疗时间显著增强。尽管基线疗效无差异（OR = 0.94, p = 0.79），但在随访第2个时间点，新药组治疗有效率达到标准药组的7.2倍（OR = 7.21）。同时，重度诊断患者整体疗效较差（OR = 0.27, p &lt; 0.001）。结果支持新药在中长期治疗中的优越性。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>广义估计方程</span>"
    ]
  },
  {
    "objectID": "广义估计方程.html#计算qic",
    "href": "广义估计方程.html#计算qic",
    "title": "36  广义估计方程",
    "section": "36.5 计算QIC",
    "text": "36.5 计算QIC\nQIC可以帮助选择作业相关矩阵，QIC越小越好。下面我们尝试下不同的作业相关矩阵，并计算QIC。\n\nlibrary(geepack)\n\ncorstrs &lt;- c(\"independence\",\"exchangeable\",\"ar1\",\"unstructured\")\nqics=list()\nfor(i in 1:length(corstrs)){\n  dep_gee1 &lt;- geeglm(depression ~ diagnose + drug*time,\n                     data = dat, id = id, family = binomial, \n                     corstr = corstrs[i]) \n  qics[[i]]=QIC(dep_gee1)\n}\ndo.call(rbind,qics)\n##       QIC QICu Quasi Lik   CIC params QICC\n## [1,] 1172 1172      -581 5.140      5 1172\n## [2,] 1172 1172      -581 5.139      5 1172\n## [3,] 1172 1172      -581 5.139      5 1172\n## [4,] 1172 1172      -581 5.087      5 1173\n\n这个例子中QIC都是一样的~",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>广义估计方程</span>"
    ]
  },
  {
    "objectID": "广义估计方程.html#边际效应",
    "href": "广义估计方程.html#边际效应",
    "title": "36  广义估计方程",
    "section": "36.6 边际效应",
    "text": "36.6 边际效应\n在广义估计方程中，边际效应（Marginal-Effect）通常指调整协变量后，干预或时间对结局的平均影响，其结果解释为人群平均水平（population-averaged），这正是 GEE 的核心优势（与混合效应模型的subject-specific 效应相比较）。\n在上面的结果中，我们通过各个系数计算的OR值，就已经是边际OR值了。",
    "crumbs": [
      "高级统计分析",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>广义估计方程</span>"
    ]
  },
  {
    "objectID": "1034-finegray.html",
    "href": "1034-finegray.html",
    "title": "37  Fine-Gray检验和竞争风险模型",
    "section": "",
    "text": "37.1 加载数据和R包\n探讨骨髓移植和血液移植治疗白血病的疗效，结局事件定义为复发，某些患者因为移植不良反应死亡，定义为竞争风险事件。\n# 如果提示缺少R包直接安装即可\nrm(list = ls())\ndata(\"bmtcrr\",package = \"casebase\")\nstr(bmtcrr)\n## 'data.frame':    177 obs. of  7 variables:\n##  $ Sex   : Factor w/ 2 levels \"F\",\"M\": 2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : Factor w/ 2 levels \"ALL\",\"AML\": 1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : Factor w/ 4 levels \"CR1\",\"CR2\",\"CR3\",..: 4 2 3 2 2 4 1 1 1 4 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Status: int  2 1 0 2 2 2 0 2 0 1 ...\n##  $ Source: Factor w/ 2 levels \"BM+PB\",\"PB\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ftime : num  0.67 9.5 131.77 24.03 1.47 ...\n这个数据一共7个变量，177行。\n# 竞争风险分析需要用的R包\nlibrary(cmprsk)",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Fine-Gray检验和竞争风险模型</span>"
    ]
  },
  {
    "objectID": "1034-finegray.html#加载数据和r包",
    "href": "1034-finegray.html#加载数据和r包",
    "title": "37  Fine-Gray检验和竞争风险模型",
    "section": "",
    "text": "Sex: 性别，F是女，M是男\nD: 疾病类型，ALL是急性淋巴细胞白血病，AML是急性髓系细胞白血病。\nPhase: 不同阶段，4个水平，CR1，CR2，CR3，Relapse。\nAge: 年龄。\nStatus: 结局变量，0=删失，1=复发，2=竞争风险事件。\nSource: 因子变量，2个水平：BM+PB(骨髓移植+血液移植)，PB(血液移植)。\nftime: 生存时间。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Fine-Gray检验和竞争风险模型</span>"
    ]
  },
  {
    "objectID": "1034-finegray.html#fine-gray检验单因素分析",
    "href": "1034-finegray.html#fine-gray检验单因素分析",
    "title": "37  Fine-Gray检验和竞争风险模型",
    "section": "37.2 Fine-Gray检验（单因素分析）",
    "text": "37.2 Fine-Gray检验（单因素分析）\n在普通的生存分析中，可以用log-rank检验做单因素分析，在竞争风险模型中，使用Fine-Gray检验进行单因素分析。\n\n\n\n\n\n\n\n\n\n比如现在我们想要比较不同疾病类型（D）有没有差异，可以进行Fine-Gray检验：\n\nbmtcrr$Status &lt;- factor(bmtcrr$Status)\nf &lt;- cuminc(bmtcrr$ftime, bmtcrr$Status, bmtcrr$D)\nf\n## Tests:\n##        stat         pv df\n## 1 2.8623325 0.09067592  1\n## 2 0.4481279 0.50322531  1\n## Estimates and Variances:\n## $est\n##              20        40        60        80       100       120\n## ALL 1 0.3713851 0.3875571 0.3875571 0.3875571 0.3875571 0.3875571\n## AML 1 0.2414530 0.2663827 0.2810390 0.2810390 0.2810390        NA\n## ALL 2 0.3698630 0.3860350 0.3860350 0.3860350 0.3860350 0.3860350\n## AML 2 0.4439103 0.4551473 0.4551473 0.4551473 0.4551473        NA\n## \n## $var\n##                20          40          60          80         100         120\n## ALL 1 0.003307032 0.003405375 0.003405375 0.003405375 0.003405375 0.003405375\n## AML 1 0.001801156 0.001995487 0.002130835 0.002130835 0.002130835          NA\n## ALL 2 0.003268852 0.003373130 0.003373130 0.003373130 0.003373130 0.003373130\n## AML 2 0.002430406 0.002460425 0.002460425 0.002460425 0.002460425          NA\n\n结果中1代表复发,2代表竞争风险事件。\n第一行统计量=2.8623325, P=0.09067592,表示在控制了竞争风险事件（即第二行计算的统计量和P值）后，两种疾病类型ALL和AML的累计复发风险无统计学差异P=0.09067592。\n第2行说明ALL和AML的累计竞争风险无统计学差异。\n$est表示估计的各时间点ALL和AML组的累计复发率与与累计竞争风险事件发生率（分别用1和2来区分，与第一行第二行一致）。\n$var表示估计的各时间点ALL和AML组的累计复发率与与累计竞争风险事件发生率的方差（分别用1和2来区分，与第一行第二行一致）。\n\n37.2.1 图形展示结果\n对于上述结果可以使用图形展示：\n\nplot(f,xlab = 'Month', ylab = 'CIF',lwd=2,lty=1,\n     col = c('red','blue','black','forestgreen'))\n\n\n\n\n\n\n\n\n图形解读：\n纵坐标表示累计发生率CIF，横坐标是时间。我们从ALL1对应的红色曲线和AML1对应的蓝色曲线可以得出，ALL组的复发风险较AML 组高，但无统计学意义，P=0.09067592。同理，ALL2对应的黑色曲线在AML2对应的草绿色曲线下方，我们可以得出，ALL组的竞争风险事件发生率较AML组低，同样无统计学意义，P=0.50322531。\n简单来讲，这个图可以用一句话来概括：在控制了竞争风险事件后，ALL和AML累计复发风险无统计学差异P=0.09067592。\n\n\n37.2.2 ggplot2\n这个图不好看，非常的不ggplot，所以我们要用ggplot2重新画它！所以首先要提取数据，因为数就是图，图就是数。但是万能的broom包竟然没有不能提取这个对象的数据，只能手动来，太不优雅了！\n\n# 提取数据\nALL1 &lt;- data.frame(ALL1_t = f[[1]][[1]], ALL1_C = f[[1]][[2]])\nAML1 &lt;- data.frame(AML1_t = f[[2]][[1]], AML1_C = f[[2]][[2]])\nALL2 &lt;- data.frame(ALL2_t = f[[3]][[1]], ALL2_C = f[[3]][[2]])\nAML2 &lt;- data.frame(AML2_t = f[[4]][[1]], AML2_C = f[[4]][[2]])\n\nlibrary(ggplot2)\n\nggplot()+\n  geom_line(data = ALL1, aes(ALL1_t,ALL1_C))+\n  geom_line(data = ALL2, aes(ALL2_t,ALL2_C))+\n  geom_line(data = AML1, aes(AML1_t,AML1_C))+\n  geom_line(data = AML2, aes(AML2_t,AML2_C))+\n  labs(x=\"month\",y=\"cif\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n但是这种不好上色，所以我们美化一下，变成长数据再画图即可。\n\ntmp &lt;- data.frame(month = c(ALL1$ALL1_t,AML1$AML1_t,ALL2$ALL2_t,AML2$AML2_t),\n                  cif = c(ALL1$ALL1_C,AML1$AML1_C,ALL2$ALL2_C,AML2$AML2_C),\n                  type = rep(c(\"ALL1\",\"AML1\",\"ALL2\",\"AML2\"), c(58,58,58,88))\n                  )\n\nggplot(tmp, aes(month, cif))+\n  geom_line(aes(color=type, group=type),size=1.2)+\n  theme_bw()+\n  theme(legend.position = \"top\")\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Fine-Gray检验和竞争风险模型</span>"
    ]
  },
  {
    "objectID": "1034-finegray.html#竞争风险模型多因素分析",
    "href": "1034-finegray.html#竞争风险模型多因素分析",
    "title": "37  Fine-Gray检验和竞争风险模型",
    "section": "37.3 竞争风险模型（多因素分析）",
    "text": "37.3 竞争风险模型（多因素分析）\n做完了单因素分析，再看看竞争风险模型的多因素分析。\n首先要把自变量单独放在一个数据框里，使用中发现一个问题，这里如果把分类变量变为因子型不会自动进行哑变量编码，所以需要手动进行哑变量编码！\n但是我这里偷懒了，并没有进行哑变量设置！实际中是需要的哦！！\n\ncovs &lt;- subset(bmtcrr, select = - c(ftime,Status))\ncovs[,c(1:3,5)] &lt;- lapply(covs[,c(1:3,5)],as.integer)\n\nstr(covs)\n## 'data.frame':    177 obs. of  5 variables:\n##  $ Sex   : int  2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : int  1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : int  4 2 3 2 2 4 1 1 1 4 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Source: int  1 1 1 1 1 1 1 1 1 1 ...\n\n指定failcode=1, cencode=0, 分别代表结局事件1与截尾0，其他默认为竞争风险事件2。\n\n# 构建竞争风险模型\nf2 &lt;- crr(bmtcrr$ftime, bmtcrr$Status, covs, failcode=1, cencode=0)\nsummary(f2)\n## Competing Risks Regression\n## \n## Call:\n## crr(ftime = bmtcrr$ftime, fstatus = bmtcrr$Status, cov1 = covs, \n##     failcode = 1, cencode = 0)\n## \n##           coef exp(coef) se(coef)      z p-value\n## Sex     0.0494     1.051   0.2867  0.172 0.86000\n## D      -0.4860     0.615   0.3040 -1.599 0.11000\n## Phase   0.4144     1.514   0.1194  3.470 0.00052\n## Age    -0.0174     0.983   0.0118 -1.465 0.14000\n## Source  0.9526     2.592   0.5469  1.742 0.08200\n## \n##        exp(coef) exp(-coef)  2.5% 97.5%\n## Sex        1.051      0.952 0.599  1.84\n## D          0.615      1.626 0.339  1.12\n## Phase      1.514      0.661 1.198  1.91\n## Age        0.983      1.018 0.960  1.01\n## Source     2.592      0.386 0.888  7.57\n## \n## Num. cases = 177\n## Pseudo Log-likelihood = -267 \n## Pseudo likelihood ratio test = 23.6  on 5 df,\n\n结果解读：在控制了竞争分险事件后，phase变量，即疾病所处阶段是患者复发的独立影响因素(p =0.00052)。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Fine-Gray检验和竞争风险模型</span>"
    ]
  },
  {
    "objectID": "1035-psm.html",
    "href": "1035-psm.html",
    "title": "38  倾向性评分：匹配",
    "section": "",
    "text": "38.1 准备数据\n下面的数据及演示的方法主要参考了这篇文章：10.21037/atm-20-3998。大家感兴趣的可以去阅读原文。\n我们虚构一个数据，用于演示研究吸烟对心血管疾病的影响，性别和年龄作为混杂因素。\nset.seed(2020)\n\nx.Gender &lt;- rep(0:1, c(400,600)) # 400女 600男\nx.Age &lt;- round(abs(rnorm(1000, mean=45, sd=15)))\n\n# 对于这个数据来说，实际PS（tps）是可以计算出来的，如果这里不理解，也问题不大！\nz &lt;- (x.Age - 45) / 15 - (x.Age-45) ^ 2 / 100 + 2 * x.Gender\ntps &lt;- exp(z) / (1+exp(z)) \nSmoke &lt;- as.numeric(runif(1000) &lt; tps)\nz.y &lt;- x.Gender + 0.3*x.Age + 5*Smoke - 20\ny &lt;- exp(z.y) / (1+exp(z.y))\nCVD &lt;- as.numeric(runif(1000) &lt; y)\nx.Age.mask &lt;- rbinom(1000, 1, 0.2) # 随机产生几个缺失值\nx.Age &lt;- ifelse(x.Age.mask==1, NA, x.Age)\n\n# 原始数据长这样：\ndata &lt;- data.frame(x.Age, x.Gender, Smoke, CVD)\nhead(data)\n##   x.Age x.Gender Smoke CVD\n## 1    51        0     1   0\n## 2    50        0     0   0\n## 3    29        0     0   0\n## 4    28        0     0   0\n## 5     3        0     0   0\n## 6    56        0     1   1\n首先可以看一下原始数据的基线资料表，用的是tableone这个包，之前也做过介绍，做基线资料表的R包还有非常多，比如：\n为什么用tableone呢？因为它能计算SMD（后面会介绍这个SMD的作用），而且其他教程都是用的它…\nlibrary(tableone)\n\ntable2 &lt;- CreateTableOne(vars = c('x.Age', 'x.Gender', 'CVD'),\n                         data = data,\n                         factorVars = c('x.Gender', 'CVD'),\n                         strata = 'Smoke',\n                         smd=TRUE)\ntable2 &lt;- print(table2,smd=TRUE,\n                showAllLevels = TRUE,\n                noSpaces = TRUE,\n                printToggle = FALSE)\ntable2\n##                    Stratified by Smoke\n##                     level 0               1              p        test SMD    \n##   n                 \"\"    \"549\"           \"451\"          \"\"       \"\"   \"\"     \n##   x.Age (mean (SD)) \"\"    \"42.76 (19.69)\" \"47.04 (8.14)\" \"&lt;0.001\" \"\"   \"0.284\"\n##   x.Gender (%)      \"0\"   \"299 (54.5)\"    \"101 (22.4)\"   \"&lt;0.001\" \"\"   \"0.698\"\n##                     \"1\"   \"250 (45.5)\"    \"350 (77.6)\"   \"\"       \"\"   \"\"     \n##   CVD (%)           \"0\"   \"452 (82.3)\"    \"230 (51.0)\"   \"&lt;0.001\" \"\"   \"0.705\"\n##                     \"1\"   \"97 (17.7)\"     \"221 (49.0)\"   \"\"       \"\"   \"\"\n#write.csv(table2, file = \"Table2_before_matching.csv\")\n结果中可以看出x.Age和x.Gender两个变量在两组间是有差异的，其中SMD(standardized mean differences)可以用来衡量协变量在不同组间的差异；除此之外，这两个变量的P值在不同性别间也是小于0.001的，说明不同性别间这两个变量是有明显差别的。\n如果此时直接探讨是否吸烟对CVD的影响，很有可能会得到错误的答案，经典的辛普森悖论就是由于混杂因素的存在才导致出现神奇的结果（比如有种药对男人有效，对女人也有效，但是对全人类就没效了！）。\n所以要想办法解决x.Age和x.Gender两个变量在两组间的差异，达到基线可比的目的。今天要介绍的方法就是倾向性评分匹配。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#准备数据",
    "href": "1035-psm.html#准备数据",
    "title": "38  倾向性评分：匹配",
    "section": "",
    "text": "CVD：结果变量，1是有心血管疾病，0是没有\nx.Age：年龄\nx.Gender：0是女，1是男\nSmoke：1吸烟，0不吸烟，是我们的处理因素\n\n\n\n\n使用compareGroups包1行代码生成基线资料表\n使用R语言快速绘制三线表\ntableone？table1？傻傻分不清楚\n超强的gtSummary ≈ gt + comparegroups ??",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#matchit包进行psm",
    "href": "1035-psm.html#matchit包进行psm",
    "title": "38  倾向性评分：匹配",
    "section": "38.2 matchIt包进行PSM",
    "text": "38.2 matchIt包进行PSM\nmatchIt包支持非常多计算PS的方法，比如自带的logistic回归、广义可加模型、分类和回归树、神经网络，除了自带的方法，也支持其他方法计算的PS。这些方法通过distance参数指定：\n\ndistance:指定PS的计算方法，默认是logit，即logistic回归，GAMlogit（广义可加模型），rpart（决策树）,nnet(神经网络)，除此之外，也可以是使用其他包或方法计算的PS值！\ndistance.options:当你选择好了方法之后，不同的方法会有不同的额外选项。\n\n下面演示使用logistic回归的方法计算PS，这里我们的处理因素是二分类变量(是否吸烟)，可以通过逻辑回归计算这些协变量（也就是混杂因素）的P值，这个P值就是倾向性评分。倾向性评分就是P值！（有网友指出这样说不对，应该是参数pi表示事件的概率）\n\nlibrary(MatchIt)\n\n# 这里为了方便演示直接删掉了缺失值\ndata.complete &lt;- na.omit(data)\n\n# 因变量是处理因素，自变量是需要平衡的协变量\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\n                 data = data.complete,\n                 distance = \"logit\" # 选择logistic回归\n                 )\n\nm.out\n## A `matchit` object\n##  - method: 1:1 nearest neighbor matching without replacement\n##  - distance: Propensity score\n##              - estimated with logistic regression\n##  - number of obs.: 831 (original), 738 (matched)\n##  - target estimand: ATT\n##  - covariates: x.Age, x.Gender\n\n上面是一个简单的结果，告诉你匹配方法是1:1无放回最近邻匹配，计算方法是logistic回归，匹配了738例等信息。\n可通过以下方法获得算法估计的PS值：\n\neps &lt;- m.out$distance\nlength(eps)\n## [1] 831\nhead(eps)\n##         1         2         3         4         5         6 \n## 0.2583040 0.2545807 0.1847661 0.1818430 0.1200378 0.2774451\n\n一开始我们已经计算出了实际PS值（tps），所以我们可以画一个tps和估计ps的散点图，以tps为横坐标，以eps为纵坐标：\n\nlibrary(ggplot2)\n\n# 去掉缺失值\ntps.comp &lt;- tps[complete.cases(data)]\nSmoke.comp &lt;- as.factor(Smoke[complete.cases(data)])\ndf &lt;- data.frame(True=tps.comp, Estimated=eps, Smoke=Smoke.comp)\n\nggplot(df, aes(x=True, y=Estimated, colour=Smoke)) +\ngeom_point() +\ngeom_abline(intercept=0,slope=1, colour=\"#990000\", linetype=\"dashed\") +\nexpand_limits(x=c(0,1),y=c(0,1))\n\n\n\n\n\n\n\n\n可以看到拟合结果非常烂！因为一开始计算tps时用了平方（二次项），但是使用logistic估计ps时并没有用平方。\n我们把公式也变成平方即可，此时再画一个拟合图就完美一致了！如下所示：\n\n# 对x.Age平方\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age +x.Gender,\n                 data=data.complete) \n\neps &lt;- m.out$distance\n\ntps.comp &lt;- tps[complete.cases(data)]\nSmoke.comp &lt;- as.factor(Smoke[complete.cases(data)])\ndf &lt;- data.frame(True=tps.comp, Estimated=eps, Smoke=Smoke.comp)\nggplot(df, aes(x=True, y=Estimated, colour=Smoke)) +\ngeom_point() +\ngeom_abline(intercept=0,slope=1, colour=\"#990000\", linetype=\"dashed\") +\nexpand_limits(x=c(0,1),y=c(0,1))\n\n\n\n\n\n\n\n\n此时的PS是通过logistic回归计算的，既然PS就是P值，当然你完全可以用glm自己计算，通过以下方法：\n\ntmp &lt;- glm(Smoke~I(x.Age^2)+x.Age +x.Gender, data=data.complete,\n           family = binomial())\n\ntmp.df &lt;- data.frame(estimated = tmp$fitted.values,\n                     true = tps.comp,\n                     Smoke=Smoke.comp)\n\nggplot(tmp.df, aes(true, estimated))+\n  geom_point(aes(color=Smoke))+\n  geom_abline(intercept=0,slope=1, colour=\"#990000\", linetype=\"dashed\") +\n  expand_limits(x=c(0,1),y=c(0,1))\n\n\n\n\n\n\n\n\n看这个结果和matchit得到的结果完全一样！\n改变matchit()的参数即可使用不同的算法估计PS，比如下面是分类和回归树及神经网络方法：\n\n# cart\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\ndata=data.complete,\ndistance='rpart')\n\n# nnet\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\ndata=data.complete,\ndistance='nnet',\ndistance.options=list(size=16))\n\n默认的只有4种方法，但是完全可以自己通过其他方法计算PS，然后提供给distance参数即可！非常强大！\n\n38.2.1 使用随机森林计算PS\n默认没提供随机森林的算法，我们可以通过其他R包计算，反正PS就是P值，只要拿到P值就可以了！\n\n# 使用随机森林构建模型\nlibrary(randomForest)\ndata.complete$Smoke &lt;- factor(data.complete$Smoke)\n\nrf.out &lt;- randomForest(Smoke~x.Age+x.Gender, data=data.complete)\nrf.out\n## \n## Call:\n##  randomForest(formula = Smoke ~ x.Age + x.Gender, data = data.complete) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 1\n## \n##         OOB estimate of  error rate: 22.5%\n## Confusion matrix:\n##     0   1 class.error\n## 0 360 102   0.2207792\n## 1  85 284   0.2303523\n\n从随机森林结果中提取预测类别为1（有CVD）的概率：\n\neps &lt;- rf.out$votes[,2] # Estimated PS\n\n接下来只要把这个eps提供给distance参数即可：\n\nmatchit(formula=Smoke~x.Age+x.Gender,\n        data=data.complete,\n        distance=eps, # 自己估计的eps\n        method='nearest',\n        replace=TRUE,\n        discard='both',\n        ratio=2)\n## A `matchit` object\n##  - method: 2:1 nearest neighbor matching with replacement\n##  - distance: User-defined [common support]\n##  - common support: units from both groups dropped\n##  - number of obs.: 831 (original), 548 (matched)\n##  - target estimand: ATT\n##  - covariates: x.Age, x.Gender\n\n其他方法也是同理，只需要提供P值即可，但并不是越复杂的方法效果越好哦！不信的话可以把几种方法得到的eps都画一个散点拟合图看看效果，这个数据是逻辑回归最好哈！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#主要匹配方法选择",
    "href": "1035-psm.html#主要匹配方法选择",
    "title": "38  倾向性评分：匹配",
    "section": "38.3 主要匹配方法选择",
    "text": "38.3 主要匹配方法选择\n在确定了使用哪种算法计算PS后，匹配方法也是需要注意的一个问题，需要注意以下几个方面，首先是匹配方法的选择（method），然后是采样手段（有无放回），相似度的度量（卡钳值或其他），匹配比例（1:1或1：多）。\n\nmethod:\n\n默认的匹配方法是最近邻匹配nearest，其他方法还有\n“exact” (exact matching),\n“full” (optimal full matching),\n“optimal” (optimal pair matching),\n“cardinality”(cardinality and template matching),\n“subclass” (subclassification),\n“genetic” (genetic matching),\n“cem” (coarsened exact matching)\n\n\n每个匹配方法都提供了详细的解释，大家感兴趣的自己查看即可。\n\ncaliper:卡钳值，也就是配对标准，两组的概率值（PS）差距在这个标准内才会配对。这里的卡钳值是PS标准差的倍数，默认是不设置卡钳值。还有一个std.caliper参数，默认是TRUE，如果设置FALSE，你设置的卡钳值就直接是PS的倍数。\nreplace:能否重复匹配，默认是FALSE，意思是假如干预组的1号匹配到了对照组的A，那A就不能再和其他的干预组进行匹配了。\nratio:设置匹配比例，干预组:对照组到底是1比几，默认为1:1。ratio=2即是干预组：对照组是1:2。所以一般要求数据的对照组数量多于干预组才行。如果对照组比干预组多出很多，完全可以设置1:n进行匹配，这样还能损失更少的样本信息，但是一般也不会超过1:4。\nreestimate:如果是TRUE，丢掉没匹配上的样本，PS会使用剩下的样本重新计算PS，如果是FALSE或者不写就不会重新计算PS。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n比如下面是一个有放回的，1:2的，最近邻匹配：\n\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 replace=TRUE,\n                 ratio=2)\n\n可以通过m.out$match.matrix获取配好的对子：\n\nhead(m.out$match.matrix)\n##    [,1]  [,2] \n## 1  \"204\" \"23\" \n## 6  \"283\" \"163\"\n## 10 \"56\"  \"30\" \n## 12 \"41\"  \"28\" \n## 20 \"79\"  \"38\" \n## 26 \"84\"  \"70\"\n\n第一列是干预组的序号，第二列是和干预组配好对的，对照组的序号。\nm.out$discarded查看某个样本是否被丢弃：\n\ntable(m.out$discarded)\n## \n## FALSE \n##   831",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#匹配后数据的平衡性检验",
    "href": "1035-psm.html#匹配后数据的平衡性检验",
    "title": "38  倾向性评分：匹配",
    "section": "38.4 匹配后数据的平衡性检验",
    "text": "38.4 匹配后数据的平衡性检验\n检查匹配后的数据，主要是看协变量在不同组间是否已经均衡了（是不是没有差异了）。\n关于这个倾向性评分匹配后数据的平衡性检验，文献中比较推荐使用SMD和VR(variance ratio)，SMD&lt;0.25说明均衡了，VR&gt;2.0或者VR&lt;0.5说明很不均衡（越接近1越均衡）！\n但其实也可以用假设检验，比如t检验、卡方检验等，也是没有统一的标准！\n做一个1:1无放回的最近邻匹配：\n\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 replace=FALSE,\n                 ratio = 1)\n\n通过summary()查看匹配前后，不同组间协变量的各种统计量。通常建议选择standardize = TRUE查看标准后的各协变量的平衡性指标：\n\nsummary(m.out,standardize = TRUE)\n## \n## Call:\n## matchit(formula = Smoke ~ x.Age + x.Gender, data = data.complete, \n##     method = \"nearest\", distance = \"logit\", replace = FALSE, \n##     ratio = 1)\n## \n## Summary of Balance for All Data:\n##          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance        0.5170        0.3858          0.9050     0.6205    0.2069\n## x.Age          47.0352       42.7619          0.5247     0.1711    0.1306\n## x.Gender        0.7832        0.4502          0.8081          .    0.3330\n##          eCDF Max\n## distance   0.4833\n## x.Age      0.3629\n## x.Gender   0.3330\n## \n## Summary of Balance for Matched Data:\n##          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance        0.5170        0.4380          0.5448     0.7605    0.1298\n## x.Age          47.0352       47.3035         -0.0329     0.1908    0.1072\n## x.Gender        0.7832        0.5610          0.5393          .    0.2222\n##          eCDF Max Std. Pair Dist.\n## distance   0.4255          0.7108\n## x.Age      0.2439          2.2436\n## x.Gender   0.2222          0.5393\n## \n## Sample Sizes:\n##           Control Treated\n## All           462     369\n## Matched       369     369\n## Unmatched      93       0\n## Discarded       0       0\n\n结果主要是3个部分：\n\nSummary of Balance for All Data：原始数据中干预组和对照组的平均PS值和平均协变量，SMD,VR，每个协变量和PS的CDF（cumulative distribution functions）的均值和最大值\nSummary of Balance for Matched Data：匹配后数据的指标\nSample Sizes：样本数量\n\n通过观察比较匹配前后的数据指标可知，x.Age均衡了（0.0329&lt;0.1），但是x.Gender并没有均衡(0.5393&gt;0.1)！\n这个默认的函数在计算SMD的时候会把分类变量按照连续性变量进行计算，所以计算结果是有一些问题的。在一开始计算匹配前数据的SMD时我们用的是tableone这个包，匹配后数据的SMD理论上也是可以用这个包的：\n\n# 首先提取匹配后的数据\nmdata &lt;- match.data(m.out)\n\nlibrary(tableone)\n\ntable5 &lt;- CreateTableOne(vars = c('x.Age', 'x.Gender', 'CVD'),\n                         data = mdata,\n                         factorVars = c('x.Gender', 'CVD'),\n                         strata = 'Smoke',\n                         smd=TRUE)\ntable5 &lt;- print(table5, smd=TRUE, showAllLevels = TRUE, \n                noSpaces = TRUE, printToggle = FALSE)\ntable5\n##                    Stratified by Smoke\n##                     level 0               1              p        test SMD    \n##   n                 \"\"    \"369\"           \"369\"          \"\"       \"\"   \"\"     \n##   x.Age (mean (SD)) \"\"    \"47.30 (18.65)\" \"47.04 (8.14)\" \"0.800\"  \"\"   \"0.019\"\n##   x.Gender (%)      \"0\"   \"162 (43.9)\"    \"80 (21.7)\"    \"&lt;0.001\" \"\"   \"0.487\"\n##                     \"1\"   \"207 (56.1)\"    \"289 (78.3)\"   \"\"       \"\"   \"\"     \n##   CVD (%)           \"0\"   \"287 (77.8)\"    \"190 (51.5)\"   \"&lt;0.001\" \"\"   \"0.572\"\n##                     \"1\"   \"82 (22.2)\"     \"179 (48.5)\"   \"\"       \"\"   \"\"\n\n这个tableone计算的x.Gender的SMD是0.487，也是表明这个变量并没有被均衡。\n但是tableone这个包计算的SMD也是有一些问题的，具体原因大家自己读文献吧：Zhang Z, Kim HJ, Lonjon G, et al. Balance diagnostics after propensity score matching. Ann Transl Med 2019;7:16.\n所以推荐大家使用cobalt包进行平衡性指标的计算。\n\n38.4.1 cobalt包\n使用cobalt包进行平衡性指标的计算，这个包很专业，专门处理这类匹配问题的，大家可以去它的官网学习更多的细节！\n\nlibrary(cobalt)\n\n# m.threshold表示SMD的阈值，小于这个阈值的协变量是平衡的\nbal.tab(m.out, m.threshold = 0.1, un = TRUE)\n## Balance Measures\n##              Type Diff.Un Diff.Adj        M.Threshold\n## distance Distance  0.9050   0.5448                   \n## x.Age     Contin.  0.5247  -0.0329     Balanced, &lt;0.1\n## x.Gender   Binary  0.3330   0.2222 Not Balanced, &gt;0.1\n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1         1\n## Not Balanced, &gt;0.1     1\n## \n## Variable with the greatest mean difference\n##  Variable Diff.Adj        M.Threshold\n##  x.Gender   0.2222 Not Balanced, &gt;0.1\n## \n## Sample sizes\n##           Control Treated\n## All           462     369\n## Matched       369     369\n## Unmatched      93       0\n\nx.Age的SMD和默认是一样的，但是x.Gender是0.2222，比默认的小多了！\n这个结果比默认的结果更可靠，具体原因大家自己去读上面那篇文献。\n计算VR，结果中并没有计算x.Gender的VR，而且根据VR来看，x.Age也没有均衡。\n\nbal.tab(m.out, v.threshold = 2)\n## Balance Measures\n##              Type Diff.Adj V.Ratio.Adj      V.Threshold\n## distance Distance   0.5448      0.7605     Balanced, &lt;2\n## x.Age     Contin.  -0.0329      0.1908 Not Balanced, &gt;2\n## x.Gender   Binary   0.2222           .                 \n## \n## Balance tally for variance ratios\n##                  count\n## Balanced, &lt;2         1\n## Not Balanced, &gt;2     1\n## \n## Variable with the greatest variance ratio\n##  Variable V.Ratio.Adj      V.Threshold\n##     x.Age      0.1908 Not Balanced, &gt;2\n## \n## Sample sizes\n##           Control Treated\n## All           462     369\n## Matched       369     369\n## Unmatched      93       0\n\n\n\n38.4.2 统计检验衡量均衡性\n除了SMD和VR之外，传统的统计检验也可以用于检查匹配后的数据有没有均衡！\n首先取出匹配好的数据：\n\nmdata &lt;- match.data(m.out)\nhead(mdata)\n##    x.Age x.Gender Smoke CVD  distance weights subclass\n## 1     51        0     1   0 0.2583040       1        1\n## 2     50        0     0   0 0.2545807       1       16\n## 6     56        0     1   1 0.2774451       1        2\n## 9     71        0     0   1 0.3397803       1      237\n## 10    47        0     1   1 0.2436248       1        3\n## 12    59        0     1   1 0.2893402       1        4\n\n其中distance是估计的PS，weights是权重，因为我们用的是1:1无放回匹配，所以全都是1。\n下面用t检验看看匹配后干预组和对照组的x.Age有没有差异：\n\nt.test(x.Age ~ Smoke, data = mdata)\n## \n##  Welch Two Sample t-test\n## \n## data:  x.Age by Smoke\n## t = 0.25327, df = 503.47, p-value = 0.8002\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -1.812969  2.349555\n## sample estimates:\n## mean in group 0 mean in group 1 \n##        47.30352        47.03523\n\n结果也是显示x.Age已经没有差异了！\n然后用卡方检验看看x.Gender是否还有差异：\n\nchisq.test(mdata$x.Gender, mdata$Smoke,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  mdata$x.Gender and mdata$Smoke\n## X-squared = 41.342, df = 1, p-value = 1.278e-10\n\n结果显示x.Gender还是有差异的，这个结果也和SMD的判断结果相同。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#结果可视化",
    "href": "1035-psm.html#结果可视化",
    "title": "38  倾向性评分：匹配",
    "section": "38.5 结果可视化",
    "text": "38.5 结果可视化\n默认提供3种图形，但是美观性太差，就不放图了，大家感兴趣的可以自己试试看。\n\nplot(m.out) # 默认QQ图\nplot(m.out, type = 'jitter') # 散点图\nplot(m.out, type = 'hist') # 直方图\n\n默认的不好看，还是用cobalt包进行结果的可视化。\n\ncowplot::plot_grid(\n  bal.plot(m.out, var.name = 'x.Age', which = 'both', grid=TRUE),\n  bal.plot(m.out, var.name = 'x.Gender', which = 'both', grid=TRUE),\n  bal.plot(m.out, var.name = 'x.Age', which = 'both', grid=TRUE, type=\"ecdf\"),\n  # 还有很多参数可调整\n  love.plot(bal.tab(m.out, m.threshold=0.1),\n            stat = \"mean.diffs\",\n            grid=TRUE,\n            stars=\"raw\",\n            abs = F)\n  )\n\n\n\n\n\n\n\n\n上面两幅图展示的是协变量在匹配前（unadjusted sample）和匹配后（adjusted sample）的数据中的分布情况，连续型变量默认是画密度图，分类变量默认是画柱状图。\n左下图是累计密度图。右下的love plot图可视化匹配前后协变量的SMD，两条竖线是0.1阈值线，匹配后x.Age在两条竖线之间，说明平衡，x.Gender不在两条竖线之间，说明还是没平衡。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#不平衡怎么办",
    "href": "1035-psm.html#不平衡怎么办",
    "title": "38  倾向性评分：匹配",
    "section": "38.6 不平衡怎么办？",
    "text": "38.6 不平衡怎么办？\n比如这里的x.Gender这个变量就是不平衡的。\n有非常多的方法可以尝试，这里提供5种方法，但是非常有可能你各种方法都试过了还是不平衡！\n首先可以换一种计算PS的方法，可以换算法，换公式（增加二次项、交互项等）。\n\n# 增加二次项，结果依然不平衡\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 replace=FALSE,\n                 ratio=1)\n\n第二可以换匹配方法及对应的参数。\n\n# 还是不平衡\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='genetic',\n                 pop.size=100)\nbal.tab(m.out, m.threshold=0.1)\n\n第三，可以使用精确匹配，性别不平衡，那就在匹配时要求按照性别精确匹配，可以使用参数exact=c('x.Gender')。\n但是这样做的代价是大部分样本都浪费了！只有一小部分才能匹配上！\n\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 exact = c('x.Gender','x.Age'), # 精准！\n                 replace=FALSE,\n                 ratio=1)\n## Warning: Fewer control units than treated units in some `exact` strata; not all\n## treated units will get a match.\nbal.tab(m.out, m.threshold=0.1)\n## Balance Measures\n##                Type Diff.Adj    M.Threshold\n## distance   Distance        0 Balanced, &lt;0.1\n## I(x.Age^2)  Contin.        0 Balanced, &lt;0.1\n## x.Age       Contin.        0 Balanced, &lt;0.1\n## x.Gender     Binary        0 Balanced, &lt;0.1\n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1         4\n## Not Balanced, &gt;0.1     0\n## \n## Variable with the greatest mean difference\n##    Variable Diff.Adj    M.Threshold\n##  I(x.Age^2)        0 Balanced, &lt;0.1\n## \n## Sample sizes\n##           Control Treated\n## All           462     369\n## Matched       147     147\n## Unmatched     315     222\n\n第四，增加样本量（一切误差问题都可以通过增加样本量解决）。\n第五，匹配后结合其他方法，比如回归、分层等。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#其他问题",
    "href": "1035-psm.html#其他问题",
    "title": "38  倾向性评分：匹配",
    "section": "38.7 其他问题",
    "text": "38.7 其他问题\n这篇推文关于倾向性评分匹配说的还算详细，尤其是matchIt包的使用，但大部分都是基于开头说的那篇文献。\n除此之外，关于倾向性评分，还有一些很重要的问题并没有涉及到。比如：\n\n样本权重不同，匹配后数据如何检查平衡性？\n倾向性评分只能平衡记录到的协变量，对于潜在的、未被记录的误差不能平衡，怎么办？\n处理因素多分组或者是连续型变量时如何处理？\n倾向性评分的加权、回归、分层如何做？\n\n这些问题待以后有时间慢慢解决！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1035-psm.html#参考资料",
    "href": "1035-psm.html#参考资料",
    "title": "38  倾向性评分：匹配",
    "section": "38.8 参考资料",
    "text": "38.8 参考资料\n\nhttps://zhuanlan.zhihu.com/p/386501046\nhttps://mp.weixin.qq.com/s/ITWBruRe5LhuPq8TXjxPZQ\nhttps://zhuanlan.zhihu.com/p/559469895\nPropensity score matching with R: conventional methods and new features",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>倾向性评分：匹配</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html",
    "href": "1036-pssc.html",
    "title": "39  倾向性评分：回归和分层",
    "section": "",
    "text": "39.1 演示数据\n下面这个例子探讨不同学校对学生成绩的影响，这个数据一共有11078行，23列，我们只用其中一部分数据演示倾向性评分回归和分层。\n我们用到以下几个变量：\n首先加载数据，已上传到QQ群，需要的加群下载即可。\nlibrary(tidyverse)\n\necls &lt;- read.csv(\"datasets/ecls.csv\") %&gt;% \n  dplyr::select(c5r2mtsc_std,catholic,race_white,w3momed_hsb,p5hmage,\n                w3momscr,w3dadscr) %&gt;%\n  na.omit()\n\ndim(ecls)\n## [1] 5548    7\nglimpse(ecls)\n## Rows: 5,548\n## Columns: 7\n## $ c5r2mtsc_std &lt;dbl&gt; 0.98175332, 0.59437751, 0.49061062, 1.45127793, 2.5956991…\n## $ catholic     &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ race_white   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, …\n## $ w3momed_hsb  &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, …\n## $ p5hmage      &lt;int&gt; 47, 41, 43, 38, 47, 41, 31, 38, 26, 38, 27, 40, 33, 36, 4…\n## $ w3momscr     &lt;dbl&gt; 53.50, 34.95, 63.43, 53.50, 61.56, 38.18, 34.95, 63.43, 3…\n## $ w3dadscr     &lt;dbl&gt; 77.50, 53.50, 53.50, 53.50, 77.50, 53.50, 29.60, 33.42, 2…",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#演示数据",
    "href": "1036-pssc.html#演示数据",
    "title": "39  倾向性评分：回归和分层",
    "section": "",
    "text": "catholic：是我们的处理因素，1是天主教（catholic）学校，0是公立（public）学校，\nc5r2mtsc_std：结果变量（因变量），标准化之后的学生成绩，\nrace_white：是否是白人，1是0否，\nw3momed_hsb：妈妈的教育水平，1高中及以下，0大学及以上，\np5hmage：妈妈的年龄，要控制的混杂因素，\nw3momscr：妈妈的成绩，\nw3dadscr：爸爸的成绩。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#原始数据的概况",
    "href": "1036-pssc.html#原始数据的概况",
    "title": "39  倾向性评分：回归和分层",
    "section": "39.2 原始数据的概况",
    "text": "39.2 原始数据的概况\n首先看一下原始数据的情况。\n\necls %&gt;%\n  group_by(catholic) %&gt;%\n  summarise(n_students = n(),\n            mean_math = mean(c5r2mtsc_std),\n            std_error = sd(c5r2mtsc_std) / sqrt(n_students))\n## # A tibble: 2 × 4\n##   catholic n_students mean_math std_error\n##      &lt;int&gt;      &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1        0       4597     0.156    0.0144\n## 2        1        951     0.221    0.0277\n\n可以看到去公立学校的4597人，去天主教学校的才951人，并且去天主教的学校的学生成绩明显高于去公立学校的学生。\n此时如果不控制混杂因素直接进行t检验，结果是有统计学意义的，但是由于基线资料不可比，一开始两组学生的各种情况就不一样，所以结果很难说明成绩不同到底是不同学校导致的还是混杂因素导致的。\n\nwith(ecls, t.test(c5r2mtsc_std ~ catholic))\n## \n##  Welch Two Sample t-test\n## \n## data:  c5r2mtsc_std by catholic\n## t = -2.0757, df = 1508.1, p-value = 0.03809\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -0.126029105 -0.003564746\n## sample estimates:\n## mean in group 0 mean in group 1 \n##       0.1562757       0.2210727\n\n我们可以看看不同组别间混杂因素的差异，首先是3个连续型变量在两组间的平均值，可以看到都是不一样的：\n\necls %&gt;%\n  group_by(catholic) %&gt;%\n  select(p5hmage, w3momscr, w3dadscr) %&gt;%\n  summarise_all(list(~mean(., na.rm = T)))\n## # A tibble: 2 × 4\n##   catholic p5hmage w3momscr w3dadscr\n##      &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n## 1        0    37.8     43.8     42.6\n## 2        1    39.8     47.5     45.8\n\n可以看到不同组别间混杂因素明显是不同的，还可以分别对3个连续型变量做t检验，结果也显示这些混杂因素在一开始就是存在差异的。\n\necls %&gt;% \n  pivot_longer(cols = c(p5hmage,w3momscr,w3dadscr),\n               names_to = \"covs\",\n               values_to = \"values\"\n               ) %&gt;% \n  group_split(covs) %&gt;% \n  map(~t.test(values ~ catholic, data = .x)) %&gt;% \n  map_dbl(\"p.value\")\n## [1] 1.062659e-28 3.722314e-16 2.208513e-18\n\n对于两个分类变量，我们可以看看分别在两组间的数量构成比有没有差异。\n\ntab &lt;- xtabs(~race_white+catholic,data = ecls)\ntab\n##           catholic\n## race_white    0    1\n##          0 1610  222\n##          1 2987  729\nchisq.test(tab,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  tab\n## X-squared = 48.596, df = 1, p-value = 3.145e-12\n\n\ntab &lt;- xtabs(~w3momed_hsb+catholic,data = ecls)\ntab\n##            catholic\n## w3momed_hsb    0    1\n##           0 2777  751\n##           1 1820  200\nchisq.test(tab,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  tab\n## X-squared = 117.24, df = 1, p-value &lt; 2.2e-16\n\n可以看到两个分类变量在两组间的差异是非常明显的！\n所以我们现在要做的事就是控制混杂因素，让这些混杂因素变成可比的状态，不要影响我们的处理因素。\n开头也说过，控制混杂因素的方法其实是很多的，比如分层、协方差分析、多因素分析等，每种情况都要具体分析，选择一种最合适的。\n下面我们介绍倾向性评分回归和分层。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#计算倾向性评分",
    "href": "1036-pssc.html#计算倾向性评分",
    "title": "39  倾向性评分：回归和分层",
    "section": "39.3 计算倾向性评分",
    "text": "39.3 计算倾向性评分\n倾向性评分就是倾向干预的概率，所以可以通过逻辑回归计算P值，这个P值就是倾向性评分，所以也不一定要用到专用的R包！\n首先以处理因素（这里是catholic）为因变量，混杂因素为自变量构建逻辑回归模型：\n\nm_ps &lt;- glm(catholic ~ race_white+w3momed_hsb+p5hmage+w3momscr+w3dadscr,\n            family = binomial(), data = ecls)\n\n提取P值，也就是倾向性评分：\n\nprs_df &lt;- data.frame(pr_score = predict(m_ps, type = \"response\"),\n                     catholic = m_ps$model$catholic)\nhead(prs_df)\n##    pr_score catholic\n## 1 0.3755223        0\n## 2 0.2340976        0\n## 4 0.2990706        0\n## 5 0.2394663        1\n## 6 0.3920115        0\n## 8 0.2391453        0\n\n可以看一下不同处理因素间的P值（倾向性评分）分布：\n\nlabs &lt;- paste(\"Actual school type attended:\", c(\"Catholic\", \"Public\"))\nprs_df %&gt;%\n  mutate(catholic = ifelse(catholic == 1, labs[1], labs[2])) %&gt;%\n  ggplot(aes(x = pr_score)) +\n  geom_histogram(color = \"white\") +\n  facet_wrap(~catholic) +\n  xlab(\"Probability of going to Catholic school\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n计算倾向性评分只是第一步，有了这个倾向性评分后，就可以进行下面的分析了，比如回归、匹配、加权、分层等。\n可以看出我们这个PS是偏态的，其实是可以对PS做一些变换的，比如log，然后使用变换后的PS继续进行后面的分析。这里就不做变换了。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#倾向性评分回归",
    "href": "1036-pssc.html#倾向性评分回归",
    "title": "39  倾向性评分：回归和分层",
    "section": "39.4 倾向性评分回归",
    "text": "39.4 倾向性评分回归\n此时如果直接把这个评分和catholic作为自变量进行回归分析，就是倾向性评分回归了（也叫协变量调整/倾向性评分矫正等）！应该是倾向性评分4种方法里面最简单的一种了。\n\n# 计算倾向性评分\npr_score &lt;- predict(m_ps, type = \"response\")\n\n# 把倾向性评分加入到原数据中\necls_ps &lt;- ecls %&gt;% \n  mutate(ps = pr_score)\n\n# 把处理因素和倾向性评分作为自变量进行回归\npsl &lt;- lm(c5r2mtsc_std ~ catholic + ps, data = ecls_ps)\nsummary(psl)\n## \n## Call:\n## lm(formula = c5r2mtsc_std ~ catholic + ps, data = ecls_ps)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4.0525 -0.5741  0.0462  0.6106  3.1468 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.58249    0.02929 -19.885  &lt; 2e-16 ***\n## catholic    -0.10772    0.03241  -3.324 0.000893 ***\n## ps           4.48236    0.15873  28.239  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8934 on 5545 degrees of freedom\n## Multiple R-squared:  0.1263, Adjusted R-squared:  0.126 \n## F-statistic: 400.8 on 2 and 5545 DF,  p-value: &lt; 2.2e-16\n\n结果表明处理因素(分组变量)还是有意义的！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#倾向性评分分层",
    "href": "1036-pssc.html#倾向性评分分层",
    "title": "39  倾向性评分：回归和分层",
    "section": "39.5 倾向性评分分层",
    "text": "39.5 倾向性评分分层\n顾名思义，根据PS值进行分层，然后在每层内进行分析。每一层的协变量分布可认为是同质或均衡的。先对每一层干预与结局之间的关联进行估算，然后对所有层的关联作加权平均，最后得出干预与结局之间的总的关联效应。\n一般来说最好保证干预组和对照组两组的PS范围在差不多的范围内，如果相差很大，那分层效果肯定不好。比如干预组PS范围是0.50.9，对照组PS范围是0.010.4，这样两组PS完全没有交集，按照PS进行分层没啥意义。\n首先看一下PS的范围：\n\necls_ps %&gt;% group_by(catholic) %&gt;% \n  summarise(range = range(ps))\n## Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\n## dplyr 1.1.0.\n## ℹ Please use `reframe()` instead.\n## ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n##   always returns an ungrouped data frame and adjust accordingly.\n## # A tibble: 4 × 2\n## # Groups:   catholic [2]\n##   catholic  range\n##      &lt;int&gt;  &lt;dbl&gt;\n## 1        0 0.0370\n## 2        0 0.477 \n## 3        1 0.0492\n## 4        1 0.404\n\n两组分别是0.0370.477和0.0490.404，范围基本一致，所以我们就直接按照总体PS的最大值和最小值进行分层，如果两组PS差很多，可以按照两组PS的交集进行分层。\n文献一般建议分5-10层，可以根据PS进行平分，也可以按照百分位数进行分层，具体方法很多，大家自己看文献即可。\n我们这里简单点，结合上面PS的分布图，分4层，切点就用0.1,0.2,0.3。\n\necls_pslevel &lt;- ecls_ps %&gt;% \n  mutate(ps_level = case_when(ps&lt;=0.1 ~ \"level_1\",\n                              ps&gt;0.1 & ps&lt;=0.2 ~ \"level_2\",\n                              ps&gt;0.2 & ps&lt;=0.3 ~ \"level_3\",\n                              TRUE ~ \"level_4\"\n                              ),\n         #ps_level = factor(ps_level),\n         p5hmage = as.double(p5hmage),\n         across(where(is.integer), as.factor)\n         )\n\nglimpse(ecls_pslevel)\n## Rows: 5,548\n## Columns: 9\n## $ c5r2mtsc_std &lt;dbl&gt; 0.98175332, 0.59437751, 0.49061062, 1.45127793, 2.5956991…\n## $ catholic     &lt;fct&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ race_white   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, …\n## $ w3momed_hsb  &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, …\n## $ p5hmage      &lt;dbl&gt; 47, 41, 43, 38, 47, 41, 31, 38, 26, 38, 27, 40, 33, 36, 4…\n## $ w3momscr     &lt;dbl&gt; 53.50, 34.95, 63.43, 53.50, 61.56, 38.18, 34.95, 63.43, 3…\n## $ w3dadscr     &lt;dbl&gt; 77.50, 53.50, 53.50, 53.50, 77.50, 53.50, 29.60, 33.42, 2…\n## $ ps           &lt;dbl&gt; 0.37552233, 0.23409764, 0.29907061, 0.23946627, 0.3920115…\n## $ ps_level     &lt;chr&gt; \"level_4\", \"level_3\", \"level_3\", \"level_3\", \"level_4\", \"l…",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#分层后的数据",
    "href": "1036-pssc.html#分层后的数据",
    "title": "39  倾向性评分：回归和分层",
    "section": "39.6 分层后的数据",
    "text": "39.6 分层后的数据\n下面我们对每一层内的3个连续型协变量和我们的因变量进行t检验，其实这里可以直接用rstatix包解决，非常好用，但其实rstatix包就是基于purrr的，所以直接用purrr也可以。\n\necls_pslevel %&gt;% \n  pivot_longer(cols = c(1,5:7),names_to = \"variates\",values_to = \"values\") %&gt;% \n  group_nest(ps_level,variates) %&gt;% \n  dplyr::mutate(tt = map(data, ~ t.test(values ~ catholic,data = .x)),\n                res = map_dfr(tt, broom::tidy)\n                ) %&gt;% \n  unnest(res)\n## # A tibble: 16 × 14\n##    ps_level variates         data tt      estimate estimate1 estimate2 statistic\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;list&lt;tibb&gt; &lt;list&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n##  1 level_1  c5r2mtsc… [1,202 × 5] &lt;htest&gt; -0.00108    -0.347   -0.346   -0.00973\n##  2 level_1  p5hmage   [1,202 × 5] &lt;htest&gt; -1.00       32.9     33.9     -1.66   \n##  3 level_1  w3dadscr  [1,202 × 5] &lt;htest&gt; -0.639      36.8     37.4     -0.886  \n##  4 level_1  w3momscr  [1,202 × 5] &lt;htest&gt; -1.07       37.0     38.1     -1.40   \n##  5 level_2  c5r2mtsc… [2,388 × 5] &lt;htest&gt;  0.0685      0.142    0.0737   1.54   \n##  6 level_2  p5hmage   [2,388 × 5] &lt;htest&gt; -0.724      37.4     38.1     -2.92   \n##  7 level_2  w3dadscr  [2,388 × 5] &lt;htest&gt; -0.818      40.7     41.5     -1.73   \n##  8 level_2  w3momscr  [2,388 × 5] &lt;htest&gt; -1.13       41.3     42.5     -2.21   \n##  9 level_3  c5r2mtsc… [1,618 × 5] &lt;htest&gt;  0.171       0.533    0.361    3.46   \n## 10 level_3  p5hmage   [1,618 × 5] &lt;htest&gt;  0.00290    41.1     41.1      0.0141 \n## 11 level_3  w3dadscr  [1,618 × 5] &lt;htest&gt; -1.36       47.5     48.8     -2.17   \n## 12 level_3  w3momscr  [1,618 × 5] &lt;htest&gt; -0.371      50.9     51.3     -0.573  \n## 13 level_4  c5r2mtsc…   [340 × 5] &lt;htest&gt;  0.0580      0.728    0.670    0.548  \n## 14 level_4  p5hmage     [340 × 5] &lt;htest&gt;  0.820      46.2     45.4      1.84   \n## 15 level_4  w3dadscr    [340 × 5] &lt;htest&gt;  0.868      59.6     58.7      0.582  \n## 16 level_4  w3momscr    [340 × 5] &lt;htest&gt; -0.739      60.0     60.7     -0.637  \n## # ℹ 6 more variables: p.value &lt;dbl&gt;, parameter &lt;dbl&gt;, conf.low &lt;dbl&gt;,\n## #   conf.high &lt;dbl&gt;, method &lt;chr&gt;, alternative &lt;chr&gt;\n\n直接看p.value这一列，可以看到大部分都是大于0.05的，因变量c5r2mtsc_std只有在第3层是有差异的！\nlevel_2中的p5hmage和w3momscr变量的P值是小于0.05的，level_3中的w3dadscr变量P值也是小于0.05的。\n这说明我们的分层并没有很好的解决这几个混杂因素的影响，而且分层后每一层内（除了第3层）的因变量都没有差异了。。。理想的结果应该是分层后每一层内混杂因素在两组间都是没有差异的，而因变量都是有差异的！这样才能说明我们的分层很好地控制了混杂因素！\n但我们的这个结果很明显很差劲！大家可以考虑不同的分层方法再重新尝试几次，或者这个数据并不适合使用这种方法，可以用其他方法试试看，比如匹配、回归等。\n下面再看看分类变量，首先是race_white，在每一层内使用卡方检验，我们直接提取P值：\n\necls_pslevel %&gt;% \n  group_split(ps_level) %&gt;% \n  map(~chisq.test(.$race_white,.$catholic,correct=F)) %&gt;% \n  map_dbl(\"p.value\")\n## Warning in chisq.test(.$race_white, .$catholic, correct = F): Chi-squared\n## approximation may be incorrect\n## [1] 0.4755703 0.8423902 0.5696924 0.2667193\n\n结果还不错，每一层内都没有差异了。\n然后是w3momed_hsb这个变量，但是由于我们的分层有问题，导致level_4这一层中w3momed_hsb全都是0！\n\n# level_4有问题\necls_pslevel %&gt;% \n  group_by(ps_level,w3momed_hsb,catholic) %&gt;% \n  summarise(count=n())\n## # A tibble: 14 × 4\n## # Groups:   ps_level, w3momed_hsb [7]\n##    ps_level w3momed_hsb catholic count\n##    &lt;chr&gt;    &lt;fct&gt;       &lt;fct&gt;    &lt;int&gt;\n##  1 level_1  0           0           61\n##  2 level_1  0           1            5\n##  3 level_1  1           0         1082\n##  4 level_1  1           1           54\n##  5 level_2  0           0         1262\n##  6 level_2  0           1          261\n##  7 level_2  1           0          724\n##  8 level_2  1           1          141\n##  9 level_3  0           0         1192\n## 10 level_3  0           1          407\n## 11 level_3  1           0           14\n## 12 level_3  1           1            5\n## 13 level_4  0           0          262\n## 14 level_4  0           1           78\n\n所以我们就对前3层做一个统计检验吧。\n\necls_pslevel %&gt;% \n  filter(!ps_level == \"level_4\") %&gt;% \n  group_split(ps_level) %&gt;% \n  map(~chisq.test(.$w3momed_hsb,.$catholic,correct=F)) %&gt;% \n  map_dbl(\"p.value\")\n## Warning in chisq.test(.$w3momed_hsb, .$catholic, correct = F): Chi-squared\n## approximation may be incorrect\n## Warning in chisq.test(.$w3momed_hsb, .$catholic, correct = F): Chi-squared\n## approximation may be incorrect\n## [1] 0.3022080 0.5994507 0.9316443\n\n可以看到每一层内也是没有明显差别的。\n说明我们的分层对2个分类变量的平衡效果还是可以的，但是对连续型变量的效果真是一言难尽！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#总结",
    "href": "1036-pssc.html#总结",
    "title": "39  倾向性评分：回归和分层",
    "section": "39.7 总结",
    "text": "39.7 总结\n倾向性评分回归和分层的大致过程就是这样的，但其实很多细节我都忽略了，比如到底分几层？依据是什么？用PS还是log(PS)？\n而且特地找了一个不是很成功的例子（可能不是很恰当），结果并不是很完美，还有很多可以调整测试的空间，大家可以适当修改其中的方法细节，最后得到一个笔记好的结果。\n实际使用时大家要根据自己的实际情况选择最合适的方法，多读文献，从文献中找灵感。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1036-pssc.html#参考资料",
    "href": "1036-pssc.html#参考资料",
    "title": "39  倾向性评分：回归和分层",
    "section": "39.8 参考资料",
    "text": "39.8 参考资料\n\nhttps://sejdemyr.github.io/r-tutorials/statistics/tutorial8.html",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>倾向性评分：回归和分层</span>"
    ]
  },
  {
    "objectID": "1037-psw.html",
    "href": "1037-psw.html",
    "title": "40  倾向性评分：加权",
    "section": "",
    "text": "40.1 演示数据\n# 如果提示缺少R包直接安装即可\ndata(lindner, package = \"twang\")\n\nlindner[,c(3,4,6,7,8,10)] &lt;- lapply(lindner[,c(3,4,6,7,8,10)],factor)\n\nstr(lindner)\n## 'data.frame':    996 obs. of  11 variables:\n##  $ lifepres       : num  0 11.6 11.6 11.6 11.6 11.6 11.6 11.6 11.6 11.6 ...\n##  $ cardbill       : int  14301 3563 4694 7366 8247 8319 8410 8517 8763 8823 ...\n##  $ abcix          : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ stent          : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ height         : int  163 168 188 175 168 178 185 173 152 180 ...\n##  $ female         : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 1 1 2 2 1 ...\n##  $ diabetic       : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 1 1 1 1 1 1 ...\n##  $ acutemi        : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ejecfrac       : int  56 56 50 50 55 50 58 30 60 60 ...\n##  $ ves1proc       : Factor w/ 6 levels \"0\",\"1\",\"2\",\"3\",..: 2 2 2 2 2 2 2 2 2 2 ...\n##  $ sixMonthSurvive: logi  FALSE TRUE TRUE TRUE TRUE TRUE ...\n其中abcix是处理因素变量，sixMonthSurvive是二分类的结局变量，cardbill是连续型的结局变量，其余变量是协变量。\n首先可以通过tableone包看一下加权前的数据情况：\nlibrary(tableone)\n\ncovs &lt;- colnames(lindner)[c(1,4:10)]\n\ntab &lt;- CreateTableOne(vars = covs,\n                      strata = \"abcix\",\n                      data = lindner\n                      )\nprint(tab,showAllLevels = T,smd = T)\n##                       Stratified by abcix\n##                        level 0              1              p      test SMD   \n##   n                             298            698                           \n##   lifepres (mean (SD))        11.02 (2.54)   11.42 (1.45)   0.002       0.194\n##   stent (%)            0        124 (41.6)     206 (29.5)  &lt;0.001       0.255\n##                        1        174 (58.4)     492 (70.5)                    \n##   height (mean (SD))         171.45 (10.59) 171.44 (10.69)  0.996      &lt;0.001\n##   female (%)           0        183 (61.4)     467 (66.9)   0.111       0.115\n##                        1        115 (38.6)     231 (33.1)                    \n##   diabetic (%)         0        218 (73.2)     555 (79.5)   0.034       0.150\n##                        1         80 (26.8)     143 (20.5)                    \n##   acutemi (%)          0        280 (94.0)     573 (82.1)  &lt;0.001       0.372\n##                        1         18 ( 6.0)     125 (17.9)                    \n##   ejecfrac (mean (SD))        52.29 (10.30)  50.40 (10.42)  0.009       0.182\n##   ves1proc (%)         0          1 ( 0.3)       3 ( 0.4)  &lt;0.001       0.446\n##                        1        243 (81.5)     437 (62.6)                    \n##                        2         47 (15.8)     205 (29.4)                    \n##                        3          6 ( 2.0)      39 ( 5.6)                    \n##                        4          1 ( 0.3)      13 ( 1.9)                    \n##                        5          0 ( 0.0)       1 ( 0.1)\n如果只是看一下协变量是否在不同组间均衡，可以通过之前介绍过的cobalt实现：\nlibrary(cobalt)\n\n# 选择只有协变量的数据框\ncovariates &lt;- subset(lindner, select = c(1,4:10))\n\nbal.tab(covariates,treat = lindner$abcix, s.d.denom = \"pooled\",\n        m.threshold = 0.1, un = TRUE,\n        v.threshold = 2\n        )\n## Balance Measures\n##                  Type Diff.Un     M.Threshold.Un V.Ratio.Un V.Threshold.Un\n## lifepres_11.6  Binary  0.0346     Balanced, &lt;0.1          .               \n## stent          Binary  0.1210 Not Balanced, &gt;0.1          .               \n## height        Contin. -0.0003     Balanced, &lt;0.1     1.0201   Balanced, &lt;2\n## female         Binary -0.0550     Balanced, &lt;0.1          .               \n## diabetic       Binary -0.0636     Balanced, &lt;0.1          .               \n## acutemi        Binary  0.1187 Not Balanced, &gt;0.1          .               \n## ejecfrac      Contin. -0.1821 Not Balanced, &gt;0.1     1.0238   Balanced, &lt;2\n## ves1proc_0     Binary  0.0009     Balanced, &lt;0.1          .               \n## ves1proc_1     Binary -0.1894 Not Balanced, &gt;0.1          .               \n## ves1proc_2     Binary  0.1360 Not Balanced, &gt;0.1          .               \n## ves1proc_3     Binary  0.0357     Balanced, &lt;0.1          .               \n## ves1proc_4     Binary  0.0153     Balanced, &lt;0.1          .               \n## ves1proc_5     Binary  0.0014     Balanced, &lt;0.1          .               \n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1         8\n## Not Balanced, &gt;0.1     5\n## \n## Variable with the greatest mean difference\n##    Variable Diff.Un     M.Threshold.Un\n##  ves1proc_1 -0.1894 Not Balanced, &gt;0.1\n## \n## Balance tally for variance ratios\n##                  count\n## Balanced, &lt;2         2\n## Not Balanced, &gt;2     0\n## \n## Variable with the greatest variance ratio\n##  Variable V.Ratio.Un V.Threshold.Un\n##  ejecfrac     1.0238   Balanced, &lt;2\n## \n## Sample sizes\n##     Control Treated\n## All     298     698\nDiff.Adj就是SMD",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>倾向性评分：加权</span>"
    ]
  },
  {
    "objectID": "1037-psw.html#iptw",
    "href": "1037-psw.html#iptw",
    "title": "40  倾向性评分：加权",
    "section": "40.2 IPTW",
    "text": "40.2 IPTW\n倾向性评分只是一个概率（倾向干预组的概率），计算概率的算法是在是太多了，选择自己喜欢的就好，我这里就用最简单的逻辑回归，之前的推文中也演示过随机森林等其他估计PS的方法。\n\npsfit &lt;- glm(abcix ~ stent + height + female + diabetic + acutemi + \n               ejecfrac + ves1proc,\n             data = lindner, family = binomial())\nps &lt;- psfit$fitted.values\n\n逆概率加权以全部研究对象（ATE）为目标人群，通过加权是每一组研究对象的协变量分布于全部研究对象相似。\n该种加权方法下，研究对象的权重为该对象所在组的概率的倒数。\n\n干预组：1/ps\n对照组：1/(1-ps)\n\n下面根据计算出的PS计算每一个样本的权重：\n\niptw &lt;- ifelse(lindner$abcix == 1, 1/ps, 1/(1-ps))\n\nlindner$iptw &lt;- iptw\n\n加权后可以再次看看数据是否已经均衡：\n\nbal.tab(covariates,treat = lindner$abcix, s.d.denom = \"pooled\",\n        weights = lindner$iptw,\n        m.threshold = 0.1, un = TRUE,\n        v.threshold = 2\n        )\n## Balance Measures\n##                  Type Diff.Un V.Ratio.Un Diff.Adj    M.Threshold V.Ratio.Adj\n## lifepres_11.6  Binary  0.0346          .   0.0590 Balanced, &lt;0.1           .\n## stent          Binary  0.1210          .   0.0036 Balanced, &lt;0.1           .\n## height        Contin. -0.0003     1.0201  -0.0175 Balanced, &lt;0.1      0.8647\n## female         Binary -0.0550          .   0.0101 Balanced, &lt;0.1           .\n## diabetic       Binary -0.0636          .  -0.0175 Balanced, &lt;0.1           .\n## acutemi        Binary  0.1187          .  -0.0028 Balanced, &lt;0.1           .\n## ejecfrac      Contin. -0.1821     1.0238  -0.0119 Balanced, &lt;0.1      0.9784\n## ves1proc_0     Binary  0.0009          .   0.0009 Balanced, &lt;0.1           .\n## ves1proc_1     Binary -0.1894          .   0.0211 Balanced, &lt;0.1           .\n## ves1proc_2     Binary  0.1360          .  -0.0073 Balanced, &lt;0.1           .\n## ves1proc_3     Binary  0.0357          .  -0.0155 Balanced, &lt;0.1           .\n## ves1proc_4     Binary  0.0153          .  -0.0002 Balanced, &lt;0.1           .\n## ves1proc_5     Binary  0.0014          .   0.0010 Balanced, &lt;0.1           .\n##                V.Threshold\n## lifepres_11.6             \n## stent                     \n## height        Balanced, &lt;2\n## female                    \n## diabetic                  \n## acutemi                   \n## ejecfrac      Balanced, &lt;2\n## ves1proc_0                \n## ves1proc_1                \n## ves1proc_2                \n## ves1proc_3                \n## ves1proc_4                \n## ves1proc_5                \n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1        13\n## Not Balanced, &gt;0.1     0\n## \n## Variable with the greatest mean difference\n##       Variable Diff.Adj    M.Threshold\n##  lifepres_11.6    0.059 Balanced, &lt;0.1\n## \n## Balance tally for variance ratios\n##                  count\n## Balanced, &lt;2         2\n## Not Balanced, &gt;2     0\n## \n## Variable with the greatest variance ratio\n##  Variable V.Ratio.Adj  V.Threshold\n##    height      0.8647 Balanced, &lt;2\n## \n## Effective sample sizes\n##            Control Treated\n## Unadjusted  298.    698.  \n## Adjusted    202.27  671.09\n\n可以看到除了lifepres之外，其他全都均衡了，效果还是挺不错的。加权后，干预组和对照组的样本量已经变了哦！\n如果想要画出加权后数据的基线资料表，可以借助survey包。\n\nlibrary(survey)\n\n# 获取加权后的数据\ndf &lt;- svydesign(ids = ~1, data = lindner, weights = ~ iptw)\n\n# 使用tableone中的函数创建加权后的三线表\ntab_IPTW=svyCreateTableOne(vars=covs, strata=\"abcix\",data=df ,test=T) \nprint(tab_IPTW,showAllLevels=TRUE,smd=TRUE)\n##                       Stratified by abcix\n##                        level 0               1              p      test SMD   \n##   n                          1004.38         994.47                           \n##   lifepres (mean (SD))         10.74 (3.05)   11.42 (1.43)   0.024       0.288\n##   stent (%)            0       333.7 (33.2)   326.8 (32.9)   0.921       0.008\n##                        1       670.7 (66.8)   667.7 (67.1)                    \n##   height (mean (SD))          171.60 (11.39) 171.41 (10.60)  0.849       0.017\n##   female (%)           0       668.5 (66.6)   651.8 (65.5)   0.782       0.021\n##                        1       335.9 (33.4)   342.7 (34.5)                    \n##   diabetic (%)         0       762.7 (75.9)   772.6 (77.7)   0.610       0.042\n##                        1       241.7 (24.1)   221.8 (22.3)                    \n##   acutemi (%)          0       857.6 (85.4)   851.8 (85.7)   0.942       0.008\n##                        1       146.8 (14.6)   142.6 (14.3)                    \n##   ejecfrac (mean (SD))         51.07 (10.23)  50.95 (10.12)  0.879       0.012\n##   ves1proc (%)         0         3.0 ( 0.3)     3.9 ( 0.4)   0.937       0.088\n##                        1       664.1 (66.1)   678.6 (68.2)                    \n##                        2       261.6 (26.0)   251.7 (25.3)                    \n##                        3        61.3 ( 6.1)    45.3 ( 4.6)                    \n##                        4        14.4 ( 1.4)    14.0 ( 1.4)                    \n##                        5         0.0 ( 0.0)     1.0 ( 0.1)\n\n加权之后，就可以做各种分析了，比如回归分析等，分析时把权重因素也考虑进去即可。\n这里演示逻辑回归，根据因变量的类型，可选择不同的回归方法。\n\nf &lt;- glm(sixMonthSurvive~abcix+stent+height+female+diabetic+acutemi+\n           ejecfrac+ves1proc,\n             data = lindner, family = binomial(),\n         weights = iptw # 把权重加进去\n         )\n## Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nsummary(f)\n## \n## Call:\n## glm(formula = sixMonthSurvive ~ abcix + stent + height + female + \n##     diabetic + acutemi + ejecfrac + ves1proc, family = binomial(), \n##     data = lindner, weights = iptw)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  8.598e+00  1.585e+03   0.005  0.99567    \n## abcix1       1.785e+00  3.215e-01   5.551 2.84e-08 ***\n## stent1      -6.401e-01  3.024e-01  -2.117  0.03426 *  \n## height       3.491e-02  1.154e-02   3.025  0.00248 ** \n## female1      8.123e-03  3.143e-01   0.026  0.97938    \n## diabetic1   -7.314e-01  2.813e-01  -2.599  0.00934 ** \n## acutemi1    -1.540e+00  3.011e-01  -5.116 3.11e-07 ***\n## ejecfrac     5.923e-02  1.057e-02   5.604 2.10e-08 ***\n## ves1proc1   -1.378e+01  1.585e+03  -0.009  0.99306    \n## ves1proc2   -1.184e+01  1.585e+03  -0.007  0.99404    \n## ves1proc3   -1.569e+01  1.585e+03  -0.010  0.99210    \n## ves1proc4   -7.180e-02  1.787e+03   0.000  0.99997    \n## ves1proc5   -1.817e+00  4.262e+03   0.000  0.99966    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 732.98  on 995  degrees of freedom\n## Residual deviance: 469.49  on 983  degrees of freedom\n## AIC: 485.36\n## \n## Number of Fisher Scoring iterations: 16\n\n\n但是这种方法存在问题，我在stackoverflow中的帖子中看到有人指出，R自带的lm和glm中的weights参数并不是样本的权重，这点可以查看帮助文档确定，所以如果想要使用加权后的数据进行线性回归和逻辑回归，需要使用其他的R包，比如survey包。\n\n除了上面介绍的手动计算权重的方法，也可以通过多个R包实现，比如PSW/PSweight/twang等，大家感兴趣的可以自己查看相关说明。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>倾向性评分：加权</span>"
    ]
  },
  {
    "objectID": "1037-psw.html#重叠加权",
    "href": "1037-psw.html#重叠加权",
    "title": "40  倾向性评分：加权",
    "section": "40.3 重叠加权",
    "text": "40.3 重叠加权\n重叠加权的目标人群是两组协变量相似的人，即PS值分布重叠的人，其估计的效应为重叠人群平均处理效应（ATO）。\n\n干预组：1-ps\n对照组：ps\n\n重叠加权的优缺点可以看这篇文章：最强的倾向性评分方法—重叠加权\n使用PSweight包演示重叠加权，这个包不仅可以用于二分类，还可以用于多分类。\n还是使用lindner这个数据集。\n\n## 数据准备\nrm(list = ls())\ndata(lindner, package = \"twang\") \n\n# 构建估计PS的formula\nformula.ps &lt;- abcix ~ stent + height + female + diabetic + acutemi + ejecfrac + ves1proc\n\n进行重叠加权：\n\nlibrary(PSweight)\n\nPSweight &lt;- PSweight(ps.formula = formula.ps, data = lindner, \n                     weight = \"overlap\", # 重叠加权\n                     yname = \"cardbill\", # 因变量\n                     family = \"gaussian\", \n                     ps.method = \"glm\", \n                     out.method = \"glm\"\n                     )\n\n#返回结果，效应估计及其标准误、置信区间、P值\nsummary(PSweight)\n## \n## Closed-form inference: \n## \n## Original group value:  0, 1 \n## \n## Contrast: \n##             0 1\n## Contrast 1 -1 1\n## \n##             Estimate Std.Error       lwr    upr  Pr(&gt;|z|)    \n## Contrast 1 1134.9079    1.3253 1132.3103 1137.5 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n计算数据均衡性：\n\nSumStat&lt;-SumStat(ps.formula = formula.ps, data = lindner, weight = \"overlap\")\nSumStat[[\"ess\"]] #有效样本量\n##   unweighted  overlap\n## 0        298 287.4367\n## 1        698 569.5826\nplot(SumStat) #均衡性检验图形\n\n\n\n\n\n\n\nsummary(SumStat) #均衡性检验\n## unweighted result\n##           Mean 0  Mean 1   SMD\n## stent      0.584   0.705 0.254\n## height   171.446 171.443 0.000\n## female     0.386   0.331 0.115\n## diabetic   0.268   0.205 0.150\n## acutemi    0.060   0.179 0.371\n## ejecfrac  52.289  50.403 0.182\n## ves1proc   1.205   1.463 0.427\n## \n## overlap result\n##           Mean 0  Mean 1 SMD\n## stent      0.633   0.633   0\n## height   171.464 171.464   0\n## female     0.359   0.359   0\n## diabetic   0.242   0.242   0\n## acutemi    0.078   0.078   0\n## ejecfrac  51.830  51.830   0\n## ves1proc   1.257   1.257   0\n\n加权后，可以进行后续的各种分析，这里就不演示了。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>倾向性评分：加权</span>"
    ]
  },
  {
    "objectID": "1037-psw.html#参考资料",
    "href": "1037-psw.html#参考资料",
    "title": "40  倾向性评分：加权",
    "section": "40.4 参考资料",
    "text": "40.4 参考资料\n涂博祥, 秦婴逸, 吴骋, 等. 倾向性评分加权方法介绍及R软件实现[J]. 中国循证医学杂志, 2022, 22(3): 365–372.",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>倾向性评分：加权</span>"
    ]
  },
  {
    "objectID": "1038-p4trend.html",
    "href": "1038-p4trend.html",
    "title": "41  p-for-trend/ p-for-interaction/ per-1-sd",
    "section": "",
    "text": "41.1 P for trend\nP for trend是线性趋势检验的P值，用于反映自变量和因变量是否存在线性趋势关系。线性趋势检验，之前介绍过Cochran Armitage检验，不过是针对分类变量的。\n今天要介绍的P for trend主要是针对连续型变量的。\n关于p for trend具体含义和数值型变量分箱的方法，大家可以参考医咖会的文章：p for trend是个啥\n把连续性变量转换为分类变量(在R里转变为因子)，设置哑变量，进行回归分析，即可得到OR值及95%的可信区间；把转换好的分类变量当做数值型，进行回归分析，即可得到P for trend\n使用之前逻辑回归的例子演示，来自孙振球版医学统计学第4版，电子版和配套数据均放在QQ群文件中，需要的加群下载即可。\ndf16_2 &lt;- foreign::read.spss(\"datasets/例16-02.sav\", \n                             to.data.frame = T,\n                             use.value.labels = F,\n                             reencode  = \"utf-8\")\n\nstr(df16_2)\n## 'data.frame':    54 obs. of  11 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : num  3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : num  1 0 1 0 0 0 0 0 0 0 ...\n##  $ x3   : num  0 1 0 0 0 1 1 1 0 0 ...\n##  $ x4   : num  1 1 1 1 1 1 0 1 0 1 ...\n##  $ x5   : num  0 0 0 0 0 0 0 1 0 0 ...\n##  $ x6   : num  0 0 0 0 1 0 0 0 0 0 ...\n##  $ x7   : num  1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : num  1 0 0 0 1 1 0 0 1 0 ...\n##  $ y    : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n数据一共11列，第1列是编号，第2-9列是自变量，第10列是因变量。\n具体说明： - x1：年龄，小于45岁是1,45-55是2,55-65是3,65以上是4； - x2：高血压病史，1代表有，0代表无； - x3：高血压家族史，1代表有，0代表无； - x4：吸烟，1代表吸烟，0代表不吸烟； - x5：高血脂病史，1代表有，0代表无； - x6：动物脂肪摄入，0表示低，1表示高 - x7：BMI，小于24是1,24-26是2，大于26是3； - x8：A型性格，1代表是，0代表否； - y：是否是冠心病，1代表是，0代表否\n这里的x1~y虽然是数值型，但并不是真的代表数字大小，只是为了方便标识，\n年龄x1应该是数值型的，但是为了方便解释逻辑回归的意义，我们对它进行了分箱处理，也就是把它转换为了分类变量。数值型变量进行分箱，是回归分析中计算p for trend的第一步\n此时x1是数值型，我们直接进行逻辑回归，得到的P值就是 p for trend\nf &lt;- glm(y ~ x1 + x2, \n         data = df16_2, \n         family = binomial())\n\nbroom::tidy(f)\n## # A tibble: 3 × 5\n##   term        estimate std.error statistic p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 (Intercept)   -2.22      1.03      -2.15  0.0313\n## 2 x1             0.712     0.423      1.68  0.0928\n## 3 x2             1.08      0.625      1.73  0.0840\n0.09279918就是x1的p for trend，而且还是校正了x2这个变量之后的p for trend，是不是很简单？\n此时如果我们把x1变成因子型，那在进行回归分析时会自动进行哑变量编码，就可以得到几个组的OR值和95%的可信区间，关于R语言中分类变量进行回归分析时常用的一些编码方法，强烈你看一下这篇推文：R语言分类变量进行回归分析的编码方案。\n# 变为因子型\ndf16_2$x1.f &lt;- factor(df16_2$x1)\n\n# 把因子放入自变量\nf &lt;- glm(y ~ x1.f + x2, \n         data = df16_2, \n         family = binomial())\nbroom::tidy(f,conf.int=T,exponentiate=T)\n## # A tibble: 5 × 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    0.200     1.10     -1.47   0.142    0.0104      1.24\n## 2 x1.f2          2.32      1.19      0.704  0.481    0.289      49.3 \n## 3 x1.f3          4.48      1.26      1.19   0.233    0.485     102.  \n## 4 x1.f4          9.42      1.63      1.38   0.169    0.508     438.  \n## 5 x2             2.94      0.639     1.69   0.0918   0.854      10.7\n这样就得到了x1.f中4/3/2分别和1进行比较的OR值和95%的可信区间。当然你写函数提取也行：\n# OR值\nexp(coef(f))\n## (Intercept)       x1.f2       x1.f3       x1.f4          x2 \n##    0.200000    2.319343    4.476753    9.415697    2.936212\n\n# OR值的95%的可信区间\nexp(confint(f))\n##                  2.5 %     97.5 %\n## (Intercept) 0.01043892   1.240092\n## x1.f2       0.28932733  49.284803\n## x1.f3       0.48497992 102.335996\n## x1.f4       0.50766137 437.541812\n## x2          0.85353009  10.723068\n这样就得到了每个组的OR值和95%的可信区间，可以看到没有第1组的，因为第一组是参考，所有组都是和第一组进行比较。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>p-for-trend/ p-for-interaction/ per-1-sd</span>"
    ]
  },
  {
    "objectID": "1038-p4trend.html#p-for-interaction",
    "href": "1038-p4trend.html#p-for-interaction",
    "title": "41  p-for-trend/ p-for-interaction/ per-1-sd",
    "section": "41.2 p for interaction",
    "text": "41.2 p for interaction\np for interaction是交互作用的P值，关于其含义可以参考松哥统计的这篇文章：p for interaction是什么\n\n目前计算P for interaction两种方法： 1. 对于数值与等级或二分类，可以直接模型中增加相乘项【如x1×X2】，然后看交互项有无意义。 2. 而对于多项分类【如血型】，产生哑变量后，相乘则会产生多个交互项，此时不能整体判断交互作用是否有意义。我们可以先构建一个无交互作用项的模型，再构建一个有交互作用项的模型。然后采用似然比检验（likelihood ratio test）进行比较有个模型差异，则可以判定交互项整体是否有意义。\n\n\n41.2.1 方法1\n假如探索年龄(x1)和BMI(x7)之间对因变量y有没有交互作用，我们首先新建一列相乘列，然后进行回归分析。\n\n# 新建1列\ndf16_2$x17 &lt;- df16_2$x1 * df16_2$x7\nstr(df16_2)\n## 'data.frame':    54 obs. of  13 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : num  3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : num  1 0 1 0 0 0 0 0 0 0 ...\n##  $ x3   : num  0 1 0 0 0 1 1 1 0 0 ...\n##  $ x4   : num  1 1 1 1 1 1 0 1 0 1 ...\n##  $ x5   : num  0 0 0 0 0 0 0 1 0 0 ...\n##  $ x6   : num  0 0 0 0 1 0 0 0 0 0 ...\n##  $ x7   : num  1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : num  1 0 0 0 1 1 0 0 1 0 ...\n##  $ y    : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  $ x1.f : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 3 2 2 2 3 3 2 3 2 1 ...\n##  $ x17  : num  3 2 2 2 3 6 2 3 2 1 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n\n\n# 进行逻辑回归\nf &lt;- glm(y ~ x1 + x7 + x17, \n         family = binomial(),\n         data = df16_2\n         )\nsummary(f)\n## \n## Call:\n## glm(formula = y ~ x1 + x7 + x17, family = binomial(), data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)\n## (Intercept)  -0.3953     2.6477  -0.149    0.881\n## x1           -0.4867     1.1142  -0.437    0.662\n## x7           -1.1509     1.7095  -0.673    0.501\n## x17           0.9249     0.7489   1.235    0.217\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 62.508  on 50  degrees of freedom\n## AIC: 70.508\n## \n## Number of Fisher Scoring iterations: 5\n\n结果中显示x17的P值(p for interaction)是：0.217，交互作用项是没有统计学意义的。\n\n\n41.2.2 方法2\n\n# 先构建一个没有交互项的逻辑回归模型\nf1 &lt;- glm(y ~ x1 + x7, \n         family = binomial(),\n         data = df16_2)\n\n# 再构建一个有交互作用的逻辑回归模型\nf2 &lt;- glm(y ~ x1 + x7 + x17, \n         family = binomial(),\n         data = df16_2)\n\n# 似然比检验\nlmtest::lrtest(f1,f2)\n## Likelihood ratio test\n## \n## Model 1: y ~ x1 + x7\n## Model 2: y ~ x1 + x7 + x17\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n## 1   3 -32.216                     \n## 2   4 -31.254  1 1.9238     0.1654\n\n结果显示P(p for interaction)=0.1654，也就是交互作用项没有统计学意义。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>p-for-trend/ p-for-interaction/ per-1-sd</span>"
    ]
  },
  {
    "objectID": "1038-p4trend.html#per-1-sd",
    "href": "1038-p4trend.html#per-1-sd",
    "title": "41  p-for-trend/ p-for-interaction/ per-1-sd",
    "section": "41.3 per 1 sd",
    "text": "41.3 per 1 sd\n关于什么是per 1 sd，可以参考松哥统计的这篇文章：per 1 sd\n\nPer 1 sd的实现，其实就是把原始数据进行标准化，另存为一个新的变量X，新变量X因为是被标准化后的数据，因此其均数和标准差为0和1。然后让x进入模型进行分析。请问大家此时x每增加1个单位，效应量增加的风险为HR。因为标准差为1，此时x增加1个单位，就是Per 1 sd。1=Per 1 sd。就是自变量每增加1个标准差。\n\n为了方便演示，我们新建一列数据weight，然后进行标准化，再进行逻辑回归。\n\n# 新建一列weight\ndf16_2$weight &lt;- rnorm(54, 70,11)\n\n# 进行标准化\ndf16_2$weight.scaled &lt;- scale(df16_2$weight)\n\n# 进行逻辑回归\nf &lt;- glm(y ~ weight.scaled, data = df16_2)\nbroom::tidy(f,conf.int=T,exponentiate=T)\n## # A tibble: 2 × 7\n##   term          estimate std.error statistic       p.value conf.low conf.high\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)       1.62    0.0689     6.98  0.00000000525    1.41       1.85\n## 2 weight.scaled     1.05    0.0696     0.727 0.470            0.918      1.21\n\n结果给出了P值，OR值以及95%的可信区间。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>p-for-trend/ p-for-interaction/ per-1-sd</span>"
    ]
  },
  {
    "objectID": "1039-nonlinear.html",
    "href": "1039-nonlinear.html",
    "title": "42  多项式拟合",
    "section": "",
    "text": "前面用了2篇推文，帮大家梳理了从线性拟合到非线性拟合的常用方法，包括多项式回归、分段回归、样条回归、限制性立方样条回归，以及它们之间的区别和联系，详情请看：\n\n多项式回归和样条回归1\n多项式回归和样条回归2\n\n我们用car包里面的USPop数据集进行演示。这个数据集一共两列，一列是年份，另一列是美国每一年的人口数量，数据一共22行。\n\n# 加载数据\nlibrary(car)\ndata(\"USPop\")\npsych::headTail(USPop)\n##     year population\n## 1   1790       3.93\n## 2   1800       5.31\n## 3   1810       7.24\n## 4   1820       9.64\n## ...  ...        ...\n## 19  1970      203.3\n## 20  1980     226.54\n## 21  1990     248.71\n## 22  2000     281.42\n\n我们首先画图看看两列数据的情况：\n\nplot(population ~ year, data = USPop)\n\n\n\n\n\n\n\n\n这个数据很明显是曲线的形状，并不是一条直线，所以此时我们直接用线性回归（直线）拟合这样的数据是不合适的。不信我们可以画图看看。\n\n# 拟合线性回归\nf &lt;- lm(population ~ year, data = USPop)\n\n# 画出原来的数据\nplot(population ~ year, data = USPop)\n\n# 添加拟合线\nlines(USPop$year, fitted(f), col = \"blue\")\n\n\n\n\n\n\n\n\n图中这条蓝色的线就是线性拟合的线，很明显，对数据的拟合很差。\n那我们应该用什么方法拟合这个关系呢？\n根据之前的两篇推文，拟合非线性关系有非常多的方法，至少有3种：\n\n多项式回归\n分段回归\n样条回归\n\n我们这里先介绍多项式回归。\n多项式回归非常简单，就是个高中学过的高次方程的曲线。\n现在我们先拟合一个二次项的多项式回归：\n\n# 2次项，注意用法\nf1 &lt;- lm(population ~ year + I(year^2), data = USPop)\n\n# 画出拟合线\nplot(population ~ year, data = USPop)\nlines(USPop$year, fitted(f1))\n\n\n\n\n\n\n\n\n结果拟合很好，二次项就已经拟合效果非常好了，如果你还想看一下更高次项拟合，可以继续试试，比如3次项：\n\n# 3次项，注意用法\nf2 &lt;- lm(population ~ year + I(year^2) + I(year^3), data = USPop)\n\n# 画出拟合线\nplot(population ~ year, data = USPop)\nlines(USPop$year, fitted(f2))\n\n\n\n\n\n\n\n\n结果可见增加了一个3次项，结果并没有好很多。所以我们可以就选2次项即可。\n当然也有一些统计方法可以检验，加了2次项、3次项之后是不是有统计学意义，可以用似然比检验，比如anova：\n\n# 线性回归和2次项比较\nanova(f, f1)\n## Analysis of Variance Table\n## \n## Model 1: population ~ year\n## Model 2: population ~ year + I(year^2)\n##   Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n## 1     20 12819.0                                  \n## 2     19   170.7  1     12648 1408.1 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2次项和3次项比较\nanova(f1, f2)\n## Analysis of Variance Table\n## \n## Model 1: population ~ year + I(year^2)\n## Model 2: population ~ year + I(year^2) + I(year^3)\n##   Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n## 1     19 170.66                              \n## 2     18 143.64  1    27.027 3.3868 0.08227 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果很明显，加入2次项之后，P值是小于0.05的，说明是有统计学意义的，但是2次项和3次项比较，就没有统计学意义了，说明我们只要用到2次项即可。\n在写论文的时候应该如何描述这些方法呢？请多看文献，这不在本文的讨论范围。\n为了加深理解，下面再给大家举一个例子。\n首先是构造一个数据，构造数据的过程不需要看。\n\nx &lt;- 1:100         \nk &lt;- c(25, 50, 75) \nu &lt;- function(x)ifelse(x &gt; 0, x, 0)\nx2 &lt;- u(x - k[1])\nx3 &lt;- u(x - k[2])\nx4 &lt;- u(x - k[3])\nset.seed(1)\ny &lt;- 0.8 + 1*x + -1.2*x2 + 1.4*x3 + -1.6*x4 + rnorm(100,sd = 2.2)\nplot(x, y)\n\n\n\n\n\n\n\n\n这样的一个数据，很明显也不是线性的，所以此时线性回归肯定不合适。我们尝试用多项式回归来拟合这个数据。\n这个数据，我已经帮大家试好了，需要拟合6次项才会比较完美。\n\n# 拟合6次项\nf.6 &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))\n\n# 画出拟合线\nplot(x,y)\nlines(x, fitted(f.6))\n\n\n\n\n\n\n\n\n可以看到拟合线比较贴合数据。但是在拟合线的开头和末尾可以发现有点上翘的趋势，这也是多项式拟合的缺点，如果此时在两头多点数据，可能拟合效果就不是很好了。解决方法也很简单，就是我们下次要介绍的样条回归。\n多项式回归的公式写法像上面这样略显复杂，如果是更高次的项，岂不是更复杂？当然是有简便写法的。可以使用poly()函数。\n\n# 多项式拟合的简便写法，拟合6次项，和上面结果完全一样\nf.6 &lt;- lm(y ~ poly(x, 6))\n\n# 画出拟合线\nplot(x,y)\nlines(x, fitted(f.6))\n\n\n\n\n\n\n\n\n可以看到使用poly()函数极大的简化了公式写法，而且很好理解，后面的数字就代表了次方。看到这里，不知道你有没有想起重复测量数据的多重比较中用过的正交多项式呢？没有印象的赶紧去复习下：重复测量数据的多重比较\n这样的拟合线，当然也是可以用ggplot2画的。\n\nlibrary(ggplot2)\n\nggplot()+\n  geom_point(aes(x,y),size=2)+\n  geom_line(aes(x, fitted(f.6)), color=\"red\",size=2)+\n  theme_bw()\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n或者下面这样，好理解，还可以添加可信区间：\n\ndf.tmp &lt;- data.frame(x = x, y= y)\n\nggplot(df.tmp, aes(x,y))+\n  geom_point(size=2)+\n  geom_smooth(method = \"lm\",\n              formula = y ~ poly(x,6),\n              color=\"red\",\n              size=2,\n              se = T, # 可信区间\n              )+\n  theme_bw()\n\n\n\n\n\n\n\n\n最后一个问题：多项式能用于逻辑回归吗？Cox回归呢？\n当然可以了，只是把自变量变成多次项而已，和lm用法一模一样，函数使用glm()/coxph()等即可！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>多项式拟合</span>"
    ]
  },
  {
    "objectID": "1040-rcs.html",
    "href": "1040-rcs.html",
    "title": "43  样条回归",
    "section": "",
    "text": "43.0.1 线性的立方样条\n演示所用数据还是用上一篇的数据：\nrm(list = ls())\nx &lt;- 1:100         \nk &lt;- c(25, 50, 75) \nu &lt;- function(x)ifelse(x &gt; 0, x, 0)\nx2 &lt;- u(x - k[1])\nx3 &lt;- u(x - k[2])\nx4 &lt;- u(x - k[3])\nset.seed(1)\ny &lt;- 0.8 + 1*x + -1.2*x2 + 1.4*x3 + -1.6*x4 + rnorm(100,sd = 2.2)\nplot(x, y)\n假设我们有这样一个数据，很明显这不是一个直线的关系，这时候再用直线回归就不适合了。但是为了演示，这里还是给大家用直线回归方法拟合一下看看效果。\nf &lt;- lm(y ~ x)\nplot(x, y)\nlines(x, fitted(f),col=\"red\")\n很明显，直线回归是不可能有很好的拟合效果的。这时候我们应该用什么方法拟合这个关系呢？\n根据之前的3篇推文，拟合非线性关系有非常多的方法，至少有3种：\n多项式回归在上一篇推文中已经介绍过了，效果不错，但是有一个小小的缺点，在数据两端有上翘趋势，具体可参考之前的推文。\n今天给大家演示限制性立方样条回归。做限制性立方样条回归的R包很多，这里以rms为例，以后有机会再介绍其他R包，比如splines。\nrms做限制性立方样条回归很简单，对需要使用的变量使用rcs()函数即可。\n# 加载R包\nlibrary(rms)\nlibrary(ggplot2)\n# 拟合限制性立方样条，这里对变量x使用，跟多项式回归差不多\nf &lt;- lm(y ~ rcs(x,5))\n\n# 画出原数据\nplot(x,y)\nlines(x, fitted(f),col=\"red\") # 画出拟合线\n可以看到，拟合结果非常完美，甚至比我们的6次多项式拟合还要好一点！\n下面解释下上述代码中的意思。rcs是我们的立方样条函数，其中的数字5表示我们要用5个节点（不理解这里的节点啥意思的请去看开头的两篇推文）。\n默认节点是4，一般建议选3-6个左右，可以分别试试效果，选择拟合较好的那一个，参考文献是这篇：F. Harrell. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis.\n我们可以自己指定，比如根据分位数、中位数、平均数等，都可以作为我们的节点。\n下面用ggplot2画图。\ndf.tmp &lt;- data.frame(x=x,y=y)\n\nggplot(df.tmp, aes(x,y))+\n  geom_point(size=2)+\n  geom_smooth(method = \"lm\",\n              formula = y ~ rcs(x,5),\n              se = T,\n              color=\"red\"\n              )+\n  theme_bw()\n此方法同样也是适用于logistic回归和cox回归的，建议使用rms包中的lrm函数和cph进行拟合。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>样条回归</span>"
    ]
  },
  {
    "objectID": "1040-rcs.html#推荐阅读",
    "href": "1040-rcs.html#推荐阅读",
    "title": "43  样条回归",
    "section": "43.1 推荐阅读",
    "text": "43.1 推荐阅读\n聂博士的10+篇高质量RCS合集：RCS系列合集",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>样条回归</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html",
    "href": "1041-subgroupanalysis.html",
    "title": "44  亚组分析及森林图绘制",
    "section": "",
    "text": "44.1 准备数据\n使用survival包中的colon数据集用于演示，这是一份关于结肠癌患者的生存数据，共有1858行，16列，共分为3个组，1个观察组+2个治疗组，观察他们发生终点事件的差异。\n各变量的解释如下：\nrm(list = ls())\nlibrary(survival)\n\nstr(colon)\n## 'data.frame':    1858 obs. of  16 variables:\n##  $ id      : num  1 1 2 2 3 3 4 4 5 5 ...\n##  $ study   : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ rx      : Factor w/ 3 levels \"Obs\",\"Lev\",\"Lev+5FU\": 3 3 3 3 1 1 3 3 1 1 ...\n##  $ sex     : num  1 1 1 1 0 0 0 0 1 1 ...\n##  $ age     : num  43 43 63 63 71 71 66 66 69 69 ...\n##  $ obstruct: num  0 0 0 0 0 0 1 1 0 0 ...\n##  $ perfor  : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ adhere  : num  0 0 0 0 1 1 0 0 0 0 ...\n##  $ nodes   : num  5 5 1 1 7 7 6 6 22 22 ...\n##  $ status  : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ differ  : num  2 2 2 2 2 2 2 2 2 2 ...\n##  $ extent  : num  3 3 3 3 2 2 3 3 3 3 ...\n##  $ surg    : num  0 0 0 0 0 0 1 1 1 1 ...\n##  $ node4   : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ time    : num  1521 968 3087 3087 963 ...\n##  $ etype   : num  2 1 2 1 2 1 2 1 2 1 ...\n可以使用cox回归探索危险因素。分类变量需要变为因子型，这样在进行回归时会自动进行哑变量设置。\n为了演示，我们只选择Obs组和Lev+5FU组的患者，所有的分类变量都变为factor，把年龄也变为分类变量并变成factor。\nsuppressMessages(library(tidyverse))\n\ndf &lt;- colon %&gt;% \n  mutate(rx=as.numeric(rx)) %&gt;% \n  filter(etype == 1, !rx == 2) %&gt;%  #rx %in% c(\"Obs\",\"Lev+5FU\"), \n  select(time, status,rx, sex, age,obstruct,perfor,adhere,differ,extent,surg,node4) %&gt;% \n  mutate(sex=factor(sex, levels=c(0,1),labels=c(\"female\",\"male\")),\n         age=ifelse(age &gt;65,\"&gt;65\",\"&lt;=65\"),\n         age=factor(age, levels=c(\"&gt;65\",\"&lt;=65\")),\n         obstruct=factor(obstruct, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         perfor=factor(perfor, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         adhere=factor(adhere, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         differ=factor(differ, levels=c(1,2,3),labels=c(\"well\",\"moderate\",\"poor\")),\n         extent=factor(extent, levels=c(1,2,3,4),\n                       labels=c(\"submucosa\",\"muscle\",\"serosa\",\"contiguous\")),\n         surg=factor(surg, levels=c(0,1),labels=c(\"short\",\"long\")),\n         node4=factor(node4, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         rx=ifelse(rx==3,0,1),\n         rx=factor(rx,levels=c(0,1))\n         )\n\nstr(df)\n## 'data.frame':    619 obs. of  12 variables:\n##  $ time    : num  968 3087 542 245 523 ...\n##  $ status  : num  1 0 1 1 1 1 0 0 0 1 ...\n##  $ rx      : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 2 1 1 2 ...\n##  $ sex     : Factor w/ 2 levels \"female\",\"male\": 2 2 1 1 2 1 2 1 2 2 ...\n##  $ age     : Factor w/ 2 levels \"&gt;65\",\"&lt;=65\": 2 2 1 1 1 2 2 1 2 2 ...\n##  $ obstruct: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n##  $ perfor  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ adhere  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 1 1 1 1 1 ...\n##  $ differ  : Factor w/ 3 levels \"well\",\"moderate\",..: 2 2 2 2 2 2 2 2 3 2 ...\n##  $ extent  : Factor w/ 4 levels \"submucosa\",\"muscle\",..: 3 3 2 3 3 3 3 3 3 3 ...\n##  $ surg    : Factor w/ 2 levels \"short\",\"long\": 1 1 1 2 2 1 1 2 2 1 ...\n##  $ node4   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 1 1 1 1 ...",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html#准备数据",
    "href": "1041-subgroupanalysis.html#准备数据",
    "title": "44  亚组分析及森林图绘制",
    "section": "",
    "text": "id：患者id\nstudy：没啥用，所有患者都是1\nrx：治疗方法，共3种，Obs(观察组), Lev(左旋咪唑), Lev+5FU(左旋咪唑+5-FU)\nsex：性别，1是男性\nage：年龄\nobstruct：肠梗阻，1是有\nperfor：肠穿孔，1是有\nadhere：和附近器官粘连，1是有\nnodes：转移的淋巴结数量\nstatus：生存状态，0代表删失，1代表发生终点事件\ndiffer：肿瘤分化程度，1-well,2-moderate,3-poor\nextent：局部扩散情况，1-submucosa，2-muscle，3-serosa，4-contiguous structures\nsurg：手术后多久了，1-long,2-short\nnode4：是否有超过4个阳性淋巴结，1代表是\ntime：生存时间\netype：终点事件类型，1-复发，2-死亡",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html#不分亚组的分析",
    "href": "1041-subgroupanalysis.html#不分亚组的分析",
    "title": "44  亚组分析及森林图绘制",
    "section": "44.2 不分亚组的分析",
    "text": "44.2 不分亚组的分析\n直接使用所有数据，拟合单因素Cox回归模型：\n\nfit &lt;- coxph(Surv(time, status) ~ rx, data = df)\nbroom::tidy(fit,exponentiate = T,conf.int = T)\n## # A tibble: 1 × 7\n##   term  estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 rx1       1.67     0.119      4.32 0.0000156     1.32      2.11\n\n这个结果我们在之前说过无数遍了，各项意义就不做具体解释了。\n通过这个结果可以看出，Lev+5FU组是明显好于Obs组的，那么问题来了。\n有一个著名的东西叫辛普森悖论，这个方法对所有人有效，但是把这个方法单独对男人或女人使用，就没效了！\n这就是由于性别这个混杂因素导致的，控制混杂因素的方法，我们在医学统计系列推文中说过至少3种，今天就给大家演示最好理解的亚组分析。\n思路其实很简单，单独在男性患者中拟合模型看看结果是不是和所有患者的结果一样；然后单独在女性患者中也拟合模型。\n对于其他的分类变量，都是一样的操作。\n所以我说这个方法最简单，没有什么高深的数学理论，只是操作起来比较复杂，因为需要在每个分类变量的每个亚组中分别拟合模型。\n刚开始我是想通过嵌套for循环实现的，但是有点费脑子，所以我给大家演示下tidyverse的做法，后期会考虑再写个R包，实现这个功能。\n其实我已经找到了一个R包Publish可以实现回归分析的亚组分析，但是它的方法是错误的。。。\n通常最笨的方法也是最靠谱的方法，如果你实在不会，也可以手动实现这个过程，就以sex为例，先在male中拟合模型：\n\nfit0 &lt;- coxph(Surv(time, status) ~ rx, data = df[df$sex == \"male\",])\nbroom::tidy(fit0,exponentiate = T,conf.int = T)\n## # A tibble: 1 × 7\n##   term  estimate std.error statistic    p.value conf.low conf.high\n##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 rx1       2.29     0.181      4.57 0.00000495     1.60      3.26\n\n然后在female中拟合模型：\n\nfit0 &lt;- coxph(Surv(time, status) ~ rx, data = df[df$sex == \"female\",])\nbroom::tidy(fit0,exponentiate = T,conf.int = T)\n## # A tibble: 1 × 7\n##   term  estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 rx1       1.32     0.161      1.71  0.0878    0.960      1.80\n\n就这样不断的重复即可，然后把数据手动摘录一下。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html#亚组分析",
    "href": "1041-subgroupanalysis.html#亚组分析",
    "title": "44  亚组分析及森林图绘制",
    "section": "44.3 亚组分析",
    "text": "44.3 亚组分析\n对于先分组，再做某事这种分析思路，tidyverse天生就比较擅长。\n以下是tidyverse实现方法，借助purrr。\n首先把数据变为长数据，经典的长宽转换：\n\ndfl &lt;- df %&gt;% \n  pivot_longer(cols = 4:ncol(.),names_to = \"var\",values_to = \"value\") %&gt;% \n  arrange(var)\n\nhead(dfl)\n## # A tibble: 6 × 5\n##    time status rx    var    value\n##   &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;  &lt;fct&gt;\n## 1   968      1 0     adhere No   \n## 2  3087      0 0     adhere No   \n## 3   542      1 1     adhere Yes  \n## 4   245      1 0     adhere No   \n## 5   523      1 1     adhere No   \n## 6   904      1 0     adhere No\n\n根据rx（治疗方式）和var（需要分亚组的变量）分组，分别在每个组内拟合cox回归，并提取结果，一气呵成，这个操作我们在之前的倾向性评分分层中也演示过：倾向性评分回归和分层\n\nress &lt;- dfl %&gt;% \n  #group_by(var,value) %&gt;% \n  group_nest(var,value) %&gt;% \n  drop_na(value) %&gt;% \n  mutate(#sample_size=map(data, ~ nrow(.x)),\n         model=map(data, ~ coxph(Surv(time, status) ~ rx,data = .x)),\n         res = map(model, broom::tidy,conf.int = T, exponentiate = T)\n         ) %&gt;% \n  dplyr::select(var,value,res)\n## Warning: There was 1 warning in `mutate()`.\n## ℹ In argument: `model = map(data, ~coxph(Surv(time, status) ~ rx, data = .x))`.\n## Caused by warning in `coxph.fit()`:\n## ! Loglik converged before variable  1 ; coefficient may be infinite.\n\nglimpse(ress)\n## Rows: 21\n## Columns: 3\n## $ var   &lt;chr&gt; \"adhere\", \"adhere\", \"age\", \"age\", \"differ\", \"differ\", \"differ\", …\n## $ value &lt;fct&gt; No, Yes, &gt;65, &lt;=65, well, moderate, poor, submucosa, muscle, ser…\n## $ res   &lt;list&gt; [&lt;tbl_df[1 x 7]&gt;], [&lt;tbl_df[1 x 7]&gt;], [&lt;tbl_df[1 x 7]&gt;], [&lt;tbl_…\n\nres是列表列，其中每个元素就是我们的结果。\n顺便把每个亚组中每种治疗方式的人数也一起计算出来：\n\nss &lt;- dfl %&gt;% \n  group_by(var,value,rx) %&gt;% \n  drop_na(value) %&gt;% \n  summarise(sample_size=n()) %&gt;% \n  dplyr::select(var,value,rx,sample_size)\n\n然后把两个结果合并到一起：\n\nresss &lt;- ress %&gt;% \n  left_join(ss,b=c(\"var\",\"value\")) %&gt;% \n  unnest(res,rx,sample_size) %&gt;% \n  pivot_wider(names_from = \"rx\",values_from = \"sample_size\",names_prefix = \"rx_\") %&gt;% \n  select(-c(term,std.error,statistic)) %&gt;% \n  mutate(across(where(is.numeric), round,digits=2)) %&gt;% \n  mutate(`HR(95%CI)`=paste0(estimate,\"(\",conf.low,\"-\",conf.high,\")\"))\n## Warning: `unnest()` has a new interface. See `?unnest` for details.\n## ℹ Try `df %&gt;% unnest(c(res, rx, sample_size))`, with `mutate()` if needed.\n## Warning: There was 1 warning in `mutate()`.\n## ℹ In argument: `across(where(is.numeric), round, digits = 2)`.\n## Caused by warning:\n## ! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\n## Supply arguments directly to `.fns` through an anonymous function instead.\n## \n##   # Previously\n##   across(a:b, mean, na.rm = TRUE)\n## \n##   # Now\n##   across(a:b, \\(x) mean(x, na.rm = TRUE))\n\nstr(resss)\n## tibble [21 × 9] (S3: tbl_df/tbl/data.frame)\n##  $ var      : chr [1:21] \"adhere\" \"adhere\" \"age\" \"age\" ...\n##  $ value    : Factor w/ 15 levels \"female\",\"male\",..: 5 6 3 4 7 8 9 10 11 12 ...\n##  $ estimate : num [1:21] 1.69 1.5 1.97 1.52 2.68 1.67 1.32 0 2.41 1.68 ...\n##  $ p.value  : num [1:21] 0 0.17 0 0 0.02 0 0.28 1 0.07 0 ...\n##  $ conf.low : num [1:21] 1.31 0.84 1.33 1.14 1.19 1.26 0.8 0 0.93 1.31 ...\n##  $ conf.high: num [1:21] 2.18 2.67 2.93 2.03 6.02 ...\n##  $ rx_0     : num [1:21] 265 39 114 190 29 215 54 10 32 251 ...\n##  $ rx_1     : num [1:21] 268 47 110 205 27 229 52 8 38 249 ...\n##  $ HR(95%CI): chr [1:21] \"1.69(1.31-2.18)\" \"1.5(0.84-2.67)\" \"1.97(1.33-2.93)\" \"1.52(1.14-2.03)\" ...\n\nhead(resss)\n## # A tibble: 6 × 9\n##   var    value    estimate p.value conf.low conf.high  rx_0  rx_1 `HR(95%CI)`   \n##   &lt;chr&gt;  &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         \n## 1 adhere No           1.69    0        1.31      2.18   265   268 1.69(1.31-2.1…\n## 2 adhere Yes          1.5     0.17     0.84      2.67    39    47 1.5(0.84-2.67)\n## 3 age    &gt;65          1.97    0        1.33      2.93   114   110 1.97(1.33-2.9…\n## 4 age    &lt;=65         1.52    0        1.14      2.03   190   205 1.52(1.14-2.0…\n## 5 differ well         2.68    0.02     1.19      6.02    29    27 2.68(1.19-6.0…\n## 6 differ moderate     1.67    0        1.26      2.21   215   229 1.67(1.26-2.2…\n\n这样亚组分析就做好了，HR，可信区间，P值，每个组的人数都有了，还记得前面做的整体的结果吗，我们把它合并进来，方便后面画森林图用。\n\nfit &lt;- coxph(Surv(time, status) ~ rx, data = df)\nres_all &lt;- broom::tidy(fit,exponentiate = T,conf.int = T)\n\n#看下不同治疗组的人数\ndf %&gt;% count(rx)\n##   rx   n\n## 1  0 304\n## 2  1 315\n\nres_all &lt;- res_all %&gt;% \n  mutate(var=\"All people\",\n         value=\" \",\n         rx_0=304,\n         rx_1=305,\n         across(where(is.numeric), round,digits=2)\n         ) %&gt;% \n  mutate(`HR(95%CI)`=paste0(estimate,\"(\",conf.low,\"-\",conf.high,\")\")\n         ) %&gt;% \n  select(var,value,estimate,p.value,conf.low,conf.high,rx_0,rx_1,`HR(95%CI)`)\n\nres_all\n## # A tibble: 1 × 9\n##   var        value estimate p.value conf.low conf.high  rx_0  rx_1 `HR(95%CI)`  \n##   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n## 1 All people \" \"       1.67       0     1.32      2.11   304   305 1.67(1.32-2.…\n\n合并到一起：\n\nresss &lt;- bind_rows(res_all,resss)\nhead(resss)\n## # A tibble: 6 × 9\n##   var        value  estimate p.value conf.low conf.high  rx_0  rx_1 `HR(95%CI)` \n##   &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n## 1 All people \" \"        1.67    0        1.32      2.11   304   305 1.67(1.32-2…\n## 2 adhere     \"No\"       1.69    0        1.31      2.18   265   268 1.69(1.31-2…\n## 3 adhere     \"Yes\"      1.5     0.17     0.84      2.67    39    47 1.5(0.84-2.…\n## 4 age        \"&gt;65\"      1.97    0        1.33      2.93   114   110 1.97(1.33-2…\n## 5 age        \"&lt;=65\"     1.52    0        1.14      2.03   190   205 1.52(1.14-2…\n## 6 differ     \"well\"     2.68    0.02     1.19      6.02    29    27 2.68(1.19-6…\n\n到这里所有数据就都准备好了！下面只要整理下格式，画图即可。\n但是forestploter包画森林图的格式还是蛮复杂的，所以我们直接另存为csv，用excel修改好，再读进来。\n\nwrite.csv(resss, file = \"resss.csv\",quote = F,row.names = T)",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html#画森林图",
    "href": "1041-subgroupanalysis.html#画森林图",
    "title": "44  亚组分析及森林图绘制",
    "section": "44.4 画森林图",
    "text": "44.4 画森林图\n把数据整理成这样：\n\n\n\n\n\n\n\n\n\n还有一些细节你可以自己修改下，比如各个亚组的顺序，首字母大写，各个变体的大小写，分组变量的名字，把P值为0的改成&lt;0.0001，等。我就不改了\n然后读取进来：\n\nplot_df &lt;- read.csv(file = \"datasets/resss.csv\",check.names = F)\nplot_df\n##         Subgroup estimate p.value conf.low conf.high rx_0 rx_1        HR(95%CI)\n## 1     All people     1.67    0.00     1.32      2.11  304  305  1.67(1.32-2.11)\n## 2         adhere       NA      NA       NA        NA   NA   NA                 \n## 3             No     1.69    0.00     1.31      2.18  265  268  1.69(1.31-2.18)\n## 4            Yes     1.50    0.17     0.84      2.67   39   47   1.5(0.84-2.67)\n## 5            age       NA      NA       NA        NA   NA   NA                 \n## 6            &gt;65     1.97    0.00     1.33      2.93  114  110  1.97(1.33-2.93)\n## 7           &lt;=65     1.52    0.00     1.14      2.03  190  205  1.52(1.14-2.03)\n## 8         differ       NA      NA       NA        NA   NA   NA                 \n## 9           well     2.68    0.02     1.19      6.02   29   27  2.68(1.19-6.02)\n## 10      moderate     1.67    0.00     1.26      2.21  215  229  1.67(1.26-2.21)\n## 11          poor     1.32    0.28     0.80      2.19   54   52   1.32(0.8-2.19)\n## 12        extent       NA      NA       NA        NA   NA   NA                 \n## 13     submucosa     0.00    1.00     0.00       Inf   10    8         0(0-Inf)\n## 14        muscle     2.41    0.07     0.93      6.22   32   38  2.41(0.93-6.22)\n## 15        serosa     1.68    0.00     1.31      2.16  251  249  1.68(1.31-2.16)\n## 16    contiguous     1.44    0.46     0.55      3.75   11   20  1.44(0.55-3.75)\n## 17         node4       NA      NA       NA        NA   NA   NA                 \n## 18            No     1.85    0.00     1.37      2.49  225  228  1.85(1.37-2.49)\n## 19           Yes     1.41    0.07     0.97      2.05   79   87  1.41(0.97-2.05)\n## 20      obstruct       NA      NA       NA        NA   NA   NA                 \n## 21            No     1.65    0.00     1.27      2.13  250  252  1.65(1.27-2.13)\n## 22           Yes     1.73    0.05     1.01      2.95   54   63  1.73(1.01-2.95)\n## 23        perfor       NA      NA       NA        NA   NA   NA                 \n## 24            No     1.64    0.00     1.30      2.08  296  306   1.64(1.3-2.08)\n## 25           Yes     2.87    0.13     0.74     11.21    8    9 2.87(0.74-11.21)\n## 26           sex       NA      NA       NA        NA   NA   NA                 \n## 27        female     1.32    0.09     0.96      1.80  163  149   1.32(0.96-1.8)\n## 28          male     2.29    0.00     1.60      3.26  141  166   2.29(1.6-3.26)\n## 29          surg       NA      NA       NA        NA   NA   NA                 \n## 30         short     1.82    0.00     1.37      2.40  228  224   1.82(1.37-2.4)\n## 31          long     1.31    0.21     0.86      1.99   76   91  1.31(0.86-1.99)\n\n把数据中的说明部分的NA变成空格，这样画森林图时就不会显示了，然后增加1列空值用于展示可信区间：\n\nplot_df[,c(3,6,7)][is.na(plot_df[,c(3,6,7)])] &lt;- \" \"\nplot_df$` ` &lt;- paste(rep(\" \", nrow(plot_df)), collapse = \" \")\nplot_df\n##         Subgroup estimate p.value conf.low conf.high rx_0 rx_1        HR(95%CI)\n## 1     All people     1.67       0     1.32      2.11  304  305  1.67(1.32-2.11)\n## 2         adhere       NA               NA        NA                           \n## 3             No     1.69       0     1.31      2.18  265  268  1.69(1.31-2.18)\n## 4            Yes     1.50    0.17     0.84      2.67   39   47   1.5(0.84-2.67)\n## 5            age       NA               NA        NA                           \n## 6            &gt;65     1.97       0     1.33      2.93  114  110  1.97(1.33-2.93)\n## 7           &lt;=65     1.52       0     1.14      2.03  190  205  1.52(1.14-2.03)\n## 8         differ       NA               NA        NA                           \n## 9           well     2.68    0.02     1.19      6.02   29   27  2.68(1.19-6.02)\n## 10      moderate     1.67       0     1.26      2.21  215  229  1.67(1.26-2.21)\n## 11          poor     1.32    0.28     0.80      2.19   54   52   1.32(0.8-2.19)\n## 12        extent       NA               NA        NA                           \n## 13     submucosa     0.00       1     0.00       Inf   10    8         0(0-Inf)\n## 14        muscle     2.41    0.07     0.93      6.22   32   38  2.41(0.93-6.22)\n## 15        serosa     1.68       0     1.31      2.16  251  249  1.68(1.31-2.16)\n## 16    contiguous     1.44    0.46     0.55      3.75   11   20  1.44(0.55-3.75)\n## 17         node4       NA               NA        NA                           \n## 18            No     1.85       0     1.37      2.49  225  228  1.85(1.37-2.49)\n## 19           Yes     1.41    0.07     0.97      2.05   79   87  1.41(0.97-2.05)\n## 20      obstruct       NA               NA        NA                           \n## 21            No     1.65       0     1.27      2.13  250  252  1.65(1.27-2.13)\n## 22           Yes     1.73    0.05     1.01      2.95   54   63  1.73(1.01-2.95)\n## 23        perfor       NA               NA        NA                           \n## 24            No     1.64       0     1.30      2.08  296  306   1.64(1.3-2.08)\n## 25           Yes     2.87    0.13     0.74     11.21    8    9 2.87(0.74-11.21)\n## 26           sex       NA               NA        NA                           \n## 27        female     1.32    0.09     0.96      1.80  163  149   1.32(0.96-1.8)\n## 28          male     2.29       0     1.60      3.26  141  166   2.29(1.6-3.26)\n## 29          surg       NA               NA        NA                           \n## 30         short     1.82       0     1.37      2.40  228  224   1.82(1.37-2.4)\n## 31          long     1.31    0.21     0.86      1.99   76   91  1.31(0.86-1.99)\n##                                                                 \n## 1                                                               \n## 2                                                               \n## 3                                                               \n## 4                                                               \n## 5                                                               \n## 6                                                               \n## 7                                                               \n## 8                                                               \n## 9                                                               \n## 10                                                              \n## 11                                                              \n## 12                                                              \n## 13                                                              \n## 14                                                              \n## 15                                                              \n## 16                                                              \n## 17                                                              \n## 18                                                              \n## 19                                                              \n## 20                                                              \n## 21                                                              \n## 22                                                              \n## 23                                                              \n## 24                                                              \n## 25                                                              \n## 26                                                              \n## 27                                                              \n## 28                                                              \n## 29                                                              \n## 30                                                              \n## 31\n\n然后画图即可，默认的出图就已经很美观了，但是大家要注意，这里每个组的人数和开头那张图的每个组的人数稍有不同哦~\n\nlibrary(forestploter)\nlibrary(grid)\n\np &lt;- forest(\n  data = plot_df[,c(1,6,7,9,8,3)],\n  lower = plot_df$conf.low,\n  upper = plot_df$conf.high,\n  est = plot_df$estimate,\n  ci_column = 4,\n  sizes = (plot_df$estimate+0.001)*0.3, \n  ref_line = 1, \n  xlim = c(0.1,4)\n  )\nprint(p)\n\n\n\n\n\n\n\n\n如果你还需要美化，我们在之前也详细介绍过这个包的使用细节了：\n\n画一个好看的森林图\n用更简单的方式画森林图\n森林图展示回归系数\nggforestplot绘制森林图\nR语言画误差线的5种方法\nggplot2绘制森林图(有亚组和没亚组)\n\n下面是我们美化后的森林图，其实变化不是非常大，只要数据好，默认的图形也很好看：\n\n\n\n\n\n\n\n\n\n和开头那张NEJM的风格一模一样！\n\npdf(\"aaa.pdf\",width = 8,height = 10)\ntm &lt;- forest_theme(base_size = 12, # 基础大小\n                   # 可信区间点的形状，线型、颜色、宽度\n                   #ci_lty = 1,\n                   ci_lwd = 1.5,\n                   #ci_Theight = 0.2, # 可信区间两端加短竖线\n                   # 参考线宽度、形状、颜色\n                   refline_lwd = 1.5,\n                   refline_lty = \"dashed\",\n                   refline_col = \"grey20\",\n                   # 汇总菱形的填充色和边框色\n                   #summary_fill = \"#4575b4\",\n                   #summary_col = \"#4575b4\",\n                   # 脚注大小、字体、颜色\n                   footnote_cex = 0.8,\n                   footnote_fontface = \"italic\",\n                   footnote_col = \"grey30\",\n                   # 自定义背景色、前景色。fontface:1常规，2粗体，3斜体，4粗斜体 \n                   core = list(bg_params = list(fill = c(\"#FFFFFF\",\"#f5f7f6\"), col=NA))\n                   )\n\np &lt;- forest(\n  data = plot_df[,c(1,6,7,9,8,3)],\n  lower = plot_df$conf.low,\n  upper = plot_df$conf.high,\n  est = plot_df$estimate,\n  ci_column = 4,\n  sizes = (plot_df$estimate+0.001)*0.3, # 不能是负值或NA，而且不能太大\n  ref_line = 1, # 把竖线放到1的位置\n  xlim = c(0.1,4), # x轴范围,如果有的可信区间超过这个范围会显示为箭头\n  arrow_lab = c(\"Obs better\",\"Lev+5-FU better\"), # x轴下面的文字\n  theme = tm\n  )\nprint(p)\ndev.off()",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1041-subgroupanalysis.html#其他资源",
    "href": "1041-subgroupanalysis.html#其他资源",
    "title": "44  亚组分析及森林图绘制",
    "section": "44.5 其他资源",
    "text": "44.5 其他资源\n亚组分析和森林图的内容还有非常多的细节问题，为了不影响该合集的主要内容，我把它们放在下面的链接中，大家感兴趣的话可点击下面的链接查看，或者在公众号后台回复亚组分析获取合集链接：\n\n使用R语言画森林图和误差线\nggplot2绘制森林图(有亚组和没亚组)\nR语言亚组分析及森林图绘制\nR语言亚组分析1行代码实现！\n亚组分析和多因素回归的森林图比较\n多列可信区间、多组别的森林图绘制\n逻辑回归亚组分析森林图绘制\n协变量调整的亚组分析和森林图绘制\n1行代码实现：多因素回归的亚组分析，并绘制森林图\nR语言亚组分析及森林图绘制手册",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>亚组分析及森林图绘制</span>"
    ]
  },
  {
    "objectID": "1042-subgroup1code.html",
    "href": "1042-subgroup1code.html",
    "title": "45  亚组分析1行代码实现",
    "section": "",
    "text": "45.1 安装\ninstall.packages(\"jstable\")\n\n## From github: latest version\nremotes::install_github('jinseob2kim/jstable')\nlibrary(jstable)",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>亚组分析1行代码实现</span>"
    ]
  },
  {
    "objectID": "1042-subgroup1code.html#准备数据",
    "href": "1042-subgroup1code.html#准备数据",
    "title": "45  亚组分析1行代码实现",
    "section": "45.2 准备数据",
    "text": "45.2 准备数据\n还是使用之前演示的数据。\n使用survival包中的colon数据集用于演示，这是一份关于结肠癌患者的生存数据，共有1858行，16列，共分为3个组，1个观察组+2个治疗组，观察他们发生终点事件的差异。\n各变量的解释如下：\n\nid：患者id\nstudy：没啥用，所有患者都是1\nrx：治疗方法，共3种，Obs(观察组), Lev(左旋咪唑), Lev+5FU(左旋咪唑+5-FU)\nsex：性别，1是男性\nage：年龄\nobstruct：肠梗阻，1是有\nperfor：肠穿孔，1是有\nadhere：和附近器官粘连，1是有\nnodes：转移的淋巴结数量\nstatus：生存状态，0代表删失，1代表发生终点事件\ndiffer：肿瘤分化程度，1-well,2-moderate,3-poor\nextent：局部扩散情况，1-submucosa，2-muscle，3-serosa，4-contiguous structures\nsurg：手术后多久了，1-long,2-short\nnode4：是否有超过4个阳性淋巴结，1代表是\ntime：生存时间\netype：终点事件类型，1-复发，2-死亡\n\n\nrm(list = ls())\nlibrary(survival)\n\nstr(colon)\n## 'data.frame':    1858 obs. of  16 variables:\n##  $ id      : num  1 1 2 2 3 3 4 4 5 5 ...\n##  $ study   : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ rx      : Factor w/ 3 levels \"Obs\",\"Lev\",\"Lev+5FU\": 3 3 3 3 1 1 3 3 1 1 ...\n##  $ sex     : num  1 1 1 1 0 0 0 0 1 1 ...\n##  $ age     : num  43 43 63 63 71 71 66 66 69 69 ...\n##  $ obstruct: num  0 0 0 0 0 0 1 1 0 0 ...\n##  $ perfor  : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ adhere  : num  0 0 0 0 1 1 0 0 0 0 ...\n##  $ nodes   : num  5 5 1 1 7 7 6 6 22 22 ...\n##  $ status  : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ differ  : num  2 2 2 2 2 2 2 2 2 2 ...\n##  $ extent  : num  3 3 3 3 2 2 3 3 3 3 ...\n##  $ surg    : num  0 0 0 0 0 0 1 1 1 1 ...\n##  $ node4   : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ time    : num  1521 968 3087 3087 963 ...\n##  $ etype   : num  2 1 2 1 2 1 2 1 2 1 ...\n\n可以使用cox回归探索危险因素。分类变量需要变为因子型，这样在进行回归时会自动进行哑变量设置。\n为了演示，我们只选择Obs组和Lev+5FU组的患者，所有的分类变量都变为factor，把年龄也变为分类变量并变成factor。\n\nsuppressMessages(library(tidyverse))\n\ndf &lt;- colon %&gt;% \n  mutate(rx=as.numeric(rx)) %&gt;% \n  filter(etype == 1, !rx == 2) %&gt;%  #rx %in% c(\"Obs\",\"Lev+5FU\"), \n  select(time, status,rx, sex, age,obstruct,perfor,adhere,differ,extent,surg,node4) %&gt;% \n  mutate(sex=factor(sex, levels=c(0,1),labels=c(\"female\",\"male\")),\n         age=ifelse(age &gt;65,\"&gt;65\",\"&lt;=65\"),\n         age=factor(age, levels=c(\"&gt;65\",\"&lt;=65\")),\n         obstruct=factor(obstruct, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         perfor=factor(perfor, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         adhere=factor(adhere, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         differ=factor(differ, levels=c(1,2,3),labels=c(\"well\",\"moderate\",\"poor\")),\n         extent=factor(extent, levels=c(1,2,3,4),\n                       labels=c(\"submucosa\",\"muscle\",\"serosa\",\"contiguous\")),\n         surg=factor(surg, levels=c(0,1),labels=c(\"short\",\"long\")),\n         node4=factor(node4, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         rx=ifelse(rx==3,0,1),\n         rx=factor(rx,levels=c(0,1))\n         )\n\nstr(df)\n## 'data.frame':    619 obs. of  12 variables:\n##  $ time    : num  968 3087 542 245 523 ...\n##  $ status  : num  1 0 1 1 1 1 0 0 0 1 ...\n##  $ rx      : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 2 1 1 2 ...\n##  $ sex     : Factor w/ 2 levels \"female\",\"male\": 2 2 1 1 2 1 2 1 2 2 ...\n##  $ age     : Factor w/ 2 levels \"&gt;65\",\"&lt;=65\": 2 2 1 1 1 2 2 1 2 2 ...\n##  $ obstruct: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n##  $ perfor  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ adhere  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 1 1 1 1 1 ...\n##  $ differ  : Factor w/ 3 levels \"well\",\"moderate\",..: 2 2 2 2 2 2 2 2 3 2 ...\n##  $ extent  : Factor w/ 4 levels \"submucosa\",\"muscle\",..: 3 3 2 3 3 3 3 3 3 3 ...\n##  $ surg    : Factor w/ 2 levels \"short\",\"long\": 1 1 1 2 2 1 1 2 2 1 ...\n##  $ node4   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 1 1 1 1 ...",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>亚组分析1行代码实现</span>"
    ]
  },
  {
    "objectID": "1042-subgroup1code.html#亚组分析",
    "href": "1042-subgroup1code.html#亚组分析",
    "title": "45  亚组分析1行代码实现",
    "section": "45.3 亚组分析",
    "text": "45.3 亚组分析\n使用jstable，只要1行代码即可！！！\n\nlibrary(jstable)\n\nres &lt;- TableSubgroupMultiCox(\n  \n  # 指定公式\n  formula = Surv(time, status) ~ rx, \n  \n  # 指定哪些变量有亚组\n  var_subgroups = c(\"sex\",\"age\",\"obstruct\",\"perfor\",\"adhere\",\n                    \"differ\",\"extent\",\"surg\",\"node4\"), \n  data = df #指定你的数据\n  )\n## Warning in coxph.fit(X, Y, istrat, offset, init, control, weights = weights, :\n## Loglik converged before variable 1 ; coefficient may be infinite.\n## Warning in coxph.fit(X, Y, istrat, offset, init, control, weights = weights, :\n## Loglik converged before variable 1,5,6,7 ; coefficient may be infinite.\nres\n##         Variable Count Percent Point Estimate Lower Upper rx=0 rx=1 P value\n## rx       Overall   619     100           1.67  1.32  2.11 34.4 48.9  &lt;0.001\n## 1            sex  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 2         female   312    50.4           1.32  0.96   1.8 41.1 47.8   0.088\n## 3           male   307    49.6           2.29   1.6  3.26 26.6 50.1  &lt;0.001\n## 4            age  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 5            &gt;65   224    36.2           1.97  1.33  2.93 29.8 50.6   0.001\n## 6           &lt;=65   395    63.8           1.52  1.14  2.03   37 48.1   0.004\n## 7       obstruct  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 8             No   502    81.1           1.65  1.27  2.13 34.4 46.8  &lt;0.001\n## 9            Yes   117    18.9           1.73  1.01  2.95 34.2 57.6   0.046\n## 10        perfor  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 11            No   602    97.3           1.64   1.3  2.08 34.3 48.1  &lt;0.001\n## 12           Yes    17     2.7           2.87  0.74 11.21 37.5 77.8   0.129\n## 13        adhere  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 14            No   533    86.1           1.69  1.31  2.18 32.9 47.4  &lt;0.001\n## 15           Yes    86    13.9            1.5  0.84  2.67 44.4 57.9   0.173\n## 16        differ  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 17          well    56     9.2           2.68  1.19  6.02   31 55.6   0.017\n## 18      moderate   444    73.3           1.67  1.26  2.21   32 44.8  &lt;0.001\n## 19          poor   106    17.5           1.32   0.8  2.19 47.5 64.5   0.277\n## 20        extent  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 21     submucosa    18     2.9              0     0   Inf   30    0   0.999\n## 22        muscle    70    11.3           2.41  0.93  6.22  9.4 28.9   0.069\n## 23        serosa   500    80.8           1.68  1.31  2.16 36.7 52.1  &lt;0.001\n## 24    contiguous    31       5           1.44  0.55  3.75 58.4 67.2   0.459\n## 25          surg  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 26         short   452      73           1.82  1.37   2.4 31.5 48.9  &lt;0.001\n## 27          long   167      27           1.31  0.86  1.99 43.2 49.1   0.208\n## 28         node4  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;\n## 29            No   453    73.2           1.85  1.37  2.49 27.7 41.5  &lt;0.001\n## 30           Yes   166    26.8           1.41  0.97  2.05 53.2 68.7   0.074\n##    P for interaction\n## rx              &lt;NA&gt;\n## 1              0.029\n## 2               &lt;NA&gt;\n## 3               &lt;NA&gt;\n## 4              0.316\n## 5               &lt;NA&gt;\n## 6               &lt;NA&gt;\n## 7              0.752\n## 8               &lt;NA&gt;\n## 9               &lt;NA&gt;\n## 10             0.442\n## 11              &lt;NA&gt;\n## 12              &lt;NA&gt;\n## 13             0.756\n## 14              &lt;NA&gt;\n## 15              &lt;NA&gt;\n## 16             0.402\n## 17              &lt;NA&gt;\n## 18              &lt;NA&gt;\n## 19              &lt;NA&gt;\n## 20               0.1\n## 21              &lt;NA&gt;\n## 22              &lt;NA&gt;\n## 23              &lt;NA&gt;\n## 24              &lt;NA&gt;\n## 25             0.183\n## 26              &lt;NA&gt;\n## 27              &lt;NA&gt;\n## 28             0.338\n## 29              &lt;NA&gt;\n## 30              &lt;NA&gt;\n\n直接就得出了结果！除了亚组分析的各种结果，还给出了交互作用的P值！",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>亚组分析1行代码实现</span>"
    ]
  },
  {
    "objectID": "1042-subgroup1code.html#画森林图",
    "href": "1042-subgroup1code.html#画森林图",
    "title": "45  亚组分析1行代码实现",
    "section": "45.4 画森林图",
    "text": "45.4 画森林图\n这个结果不需要另存为csv也能直接使用（除非你是细节控，需要修改各种大小写等信息），当然如果你需要HR(95%CI)这种信息，还是需要自己添加一下的。\n我们添加个空列用于显示可信区间，并把不想显示的NA去掉即可，还需要把P值，可信区间这些列变为数值型。\n\nplot_df &lt;- res\nplot_df[,c(2,3,9)][is.na(plot_df[,c(2,3,9)])] &lt;- \" \"\nplot_df$` ` &lt;- paste(rep(\" \", nrow(plot_df)), collapse = \" \")\nplot_df[,4:6] &lt;- apply(plot_df[,4:6],2,as.numeric)\nplot_df\n##         Variable Count Percent Point Estimate Lower Upper rx=0 rx=1 P value\n## rx       Overall   619     100           1.67  1.32  2.11 34.4 48.9  &lt;0.001\n## 1            sex                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 2         female   312    50.4           1.32  0.96  1.80 41.1 47.8   0.088\n## 3           male   307    49.6           2.29  1.60  3.26 26.6 50.1  &lt;0.001\n## 4            age                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 5            &gt;65   224    36.2           1.97  1.33  2.93 29.8 50.6   0.001\n## 6           &lt;=65   395    63.8           1.52  1.14  2.03   37 48.1   0.004\n## 7       obstruct                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 8             No   502    81.1           1.65  1.27  2.13 34.4 46.8  &lt;0.001\n## 9            Yes   117    18.9           1.73  1.01  2.95 34.2 57.6   0.046\n## 10        perfor                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 11            No   602    97.3           1.64  1.30  2.08 34.3 48.1  &lt;0.001\n## 12           Yes    17     2.7           2.87  0.74 11.21 37.5 77.8   0.129\n## 13        adhere                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 14            No   533    86.1           1.69  1.31  2.18 32.9 47.4  &lt;0.001\n## 15           Yes    86    13.9           1.50  0.84  2.67 44.4 57.9   0.173\n## 16        differ                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 17          well    56     9.2           2.68  1.19  6.02   31 55.6   0.017\n## 18      moderate   444    73.3           1.67  1.26  2.21   32 44.8  &lt;0.001\n## 19          poor   106    17.5           1.32  0.80  2.19 47.5 64.5   0.277\n## 20        extent                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 21     submucosa    18     2.9           0.00  0.00   Inf   30    0   0.999\n## 22        muscle    70    11.3           2.41  0.93  6.22  9.4 28.9   0.069\n## 23        serosa   500    80.8           1.68  1.31  2.16 36.7 52.1  &lt;0.001\n## 24    contiguous    31       5           1.44  0.55  3.75 58.4 67.2   0.459\n## 25          surg                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 26         short   452      73           1.82  1.37  2.40 31.5 48.9  &lt;0.001\n## 27          long   167      27           1.31  0.86  1.99 43.2 49.1   0.208\n## 28         node4                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;        \n## 29            No   453    73.2           1.85  1.37  2.49 27.7 41.5  &lt;0.001\n## 30           Yes   166    26.8           1.41  0.97  2.05 53.2 68.7   0.074\n##    P for interaction\n## rx              &lt;NA&gt;\n## 1              0.029\n## 2               &lt;NA&gt;\n## 3               &lt;NA&gt;\n## 4              0.316\n## 5               &lt;NA&gt;\n## 6               &lt;NA&gt;\n## 7              0.752\n## 8               &lt;NA&gt;\n## 9               &lt;NA&gt;\n## 10             0.442\n## 11              &lt;NA&gt;\n## 12              &lt;NA&gt;\n## 13             0.756\n## 14              &lt;NA&gt;\n## 15              &lt;NA&gt;\n## 16             0.402\n## 17              &lt;NA&gt;\n## 18              &lt;NA&gt;\n## 19              &lt;NA&gt;\n## 20               0.1\n## 21              &lt;NA&gt;\n## 22              &lt;NA&gt;\n## 23              &lt;NA&gt;\n## 24              &lt;NA&gt;\n## 25             0.183\n## 26              &lt;NA&gt;\n## 27              &lt;NA&gt;\n## 28             0.338\n## 29              &lt;NA&gt;\n## 30              &lt;NA&gt;\n##                                                                 \n## rx                                                              \n## 1                                                               \n## 2                                                               \n## 3                                                               \n## 4                                                               \n## 5                                                               \n## 6                                                               \n## 7                                                               \n## 8                                                               \n## 9                                                               \n## 10                                                              \n## 11                                                              \n## 12                                                              \n## 13                                                              \n## 14                                                              \n## 15                                                              \n## 16                                                              \n## 17                                                              \n## 18                                                              \n## 19                                                              \n## 20                                                              \n## 21                                                              \n## 22                                                              \n## 23                                                              \n## 24                                                              \n## 25                                                              \n## 26                                                              \n## 27                                                              \n## 28                                                              \n## 29                                                              \n## 30\n\n画图就非常简单！\n\nlibrary(forestploter)\nlibrary(grid)\n\np &lt;- forest(\n  data = plot_df[,c(1,2,3,11,9)],\n  lower = plot_df$Lower,\n  upper = plot_df$Upper,\n  est = plot_df$`Point Estimate`,\n  ci_column = 4,\n  #sizes = (plot_df$estimate+0.001)*0.3, \n  ref_line = 1, \n  xlim = c(0.1,4)\n  )\nprint(p)\n\n\n\n\n\n\n\n\n\n\n这样就搞定了，真的是非常简单了，省去了大量的步骤。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>亚组分析1行代码实现</span>"
    ]
  },
  {
    "objectID": "1042-subgroup1code.html#其他资源",
    "href": "1042-subgroup1code.html#其他资源",
    "title": "45  亚组分析1行代码实现",
    "section": "45.5 其他资源",
    "text": "45.5 其他资源\n亚组分析和森林图的内容还有非常多的细节问题，为了不影响该合集的主要内容，我把它们放在下面的链接中，大家感兴趣的话可点击下面的链接查看，或者在公众号后台回复亚组分析获取合集链接：\n\n使用R语言画森林图和误差线\nggplot2绘制森林图(有亚组和没亚组)\nR语言亚组分析及森林图绘制\nR语言亚组分析1行代码实现！\n亚组分析和多因素回归的森林图比较\n多列可信区间、多组别的森林图绘制\n逻辑回归亚组分析森林图绘制\n协变量调整的亚组分析和森林图绘制\n1行代码实现：多因素回归的亚组分析，并绘制森林图\nR语言亚组分析及森林图绘制手册",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>亚组分析1行代码实现</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html",
    "href": "亚组分析和多因素回归的森林图.html",
    "title": "46  亚组分析和多因素回归的森林图比较",
    "section": "",
    "text": "46.1 准备数据\n使用survival包中的colon数据集用于演示，这是一份关于结肠癌患者的生存数据，共有1858行，16列，共分为3个组，1个观察组+2个治疗组，观察他们发生终点事件的差异。\n各变量的解释如下：\nrm(list = ls())\nlibrary(survival)\n\nstr(colon)\n## 'data.frame':    1858 obs. of  16 variables:\n##  $ id      : num  1 1 2 2 3 3 4 4 5 5 ...\n##  $ study   : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ rx      : Factor w/ 3 levels \"Obs\",\"Lev\",\"Lev+5FU\": 3 3 3 3 1 1 3 3 1 1 ...\n##  $ sex     : num  1 1 1 1 0 0 0 0 1 1 ...\n##  $ age     : num  43 43 63 63 71 71 66 66 69 69 ...\n##  $ obstruct: num  0 0 0 0 0 0 1 1 0 0 ...\n##  $ perfor  : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ adhere  : num  0 0 0 0 1 1 0 0 0 0 ...\n##  $ nodes   : num  5 5 1 1 7 7 6 6 22 22 ...\n##  $ status  : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ differ  : num  2 2 2 2 2 2 2 2 2 2 ...\n##  $ extent  : num  3 3 3 3 2 2 3 3 3 3 ...\n##  $ surg    : num  0 0 0 0 0 0 1 1 1 1 ...\n##  $ node4   : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ time    : num  1521 968 3087 3087 963 ...\n##  $ etype   : num  2 1 2 1 2 1 2 1 2 1 ...\n为了演示，我们只选择Obs组和Lev+5FU组的患者，所有的分类变量都变为factor，把年龄也变为分类变量并变成factor。\nlibrary(tidyverse)\n\ndf &lt;- colon %&gt;% \n  mutate(rx=as.numeric(rx)) %&gt;% \n  filter(etype == 1, !rx == 2) %&gt;%  #rx %in% c(\"Obs\",\"Lev+5FU\"), \n  select(time, status,rx, sex, age,obstruct,perfor,adhere,differ,extent,surg,node4) %&gt;% \n  mutate(sex=factor(sex, levels=c(0,1),labels=c(\"female\",\"male\")),\n         age=ifelse(age &gt;65,\"&gt;65\",\"&lt;=65\"),\n         age=factor(age, levels=c(\"&gt;65\",\"&lt;=65\")),\n         obstruct=factor(obstruct, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         perfor=factor(perfor, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         adhere=factor(adhere, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         differ=factor(differ, levels=c(1,2,3),labels=c(\"well\",\"moderate\",\"poor\")),\n         extent=factor(extent, levels=c(1,2,3,4),\n                       labels=c(\"submucosa\",\"muscle\",\"serosa\",\"contiguous\")),\n         surg=factor(surg, levels=c(0,1),labels=c(\"short\",\"long\")),\n         node4=factor(node4, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         rx=ifelse(rx==3,0,1),\n         rx=factor(rx,levels=c(0,1))\n         )\n\nstr(df)\n## 'data.frame':    619 obs. of  12 variables:\n##  $ time    : num  968 3087 542 245 523 ...\n##  $ status  : num  1 0 1 1 1 1 0 0 0 1 ...\n##  $ rx      : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 2 1 1 2 ...\n##  $ sex     : Factor w/ 2 levels \"female\",\"male\": 2 2 1 1 2 1 2 1 2 2 ...\n##  $ age     : Factor w/ 2 levels \"&gt;65\",\"&lt;=65\": 2 2 1 1 1 2 2 1 2 2 ...\n##  $ obstruct: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n##  $ perfor  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ adhere  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 1 1 1 1 1 ...\n##  $ differ  : Factor w/ 3 levels \"well\",\"moderate\",..: 2 2 2 2 2 2 2 2 3 2 ...\n##  $ extent  : Factor w/ 4 levels \"submucosa\",\"muscle\",..: 3 3 2 3 3 3 3 3 3 3 ...\n##  $ surg    : Factor w/ 2 levels \"short\",\"long\": 1 1 1 2 2 1 1 2 2 1 ...\n##  $ node4   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 1 1 1 1 ...\n多因素cox回归现在的很多文章中都是用来筛选变量的，但其实它是一种识别危险因素的方法，通常是根据P值和可信区间判断某个变量对终点事件是不是有影响。\n如果某个变量是分类变量，那么它在进入回归分析后会自动被执行哑变量编码，以其中第一个水平作为参考，其他水平都和参考组进行比较。具体的编码细节我在很久之前就详细介绍过了：分类数据回归分析时的编码方案",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#准备数据",
    "href": "亚组分析和多因素回归的森林图.html#准备数据",
    "title": "46  亚组分析和多因素回归的森林图比较",
    "section": "",
    "text": "id：患者id\nstudy：没啥用，所有患者都是1\nrx：治疗方法，共3种，Obs(观察组), Lev(左旋咪唑), Lev+5FU(左旋咪唑+5-FU)\nsex：性别，1是男性\nage：年龄\nobstruct：肠梗阻，1是有\nperfor：肠穿孔，1是有\nadhere：和附近器官粘连，1是有\nnodes：转移的淋巴结数量\nstatus：生存状态，0代表删失，1代表发生终点事件\ndiffer：肿瘤分化程度，1-well,2-moderate,3-poor\nextent：局部扩散情况，1-submucosa，2-muscle，3-serosa，4-contiguous_structures\nsurg：手术后多久了，1-long,2-short\nnode4：是否有超过4个阳性淋巴结，1代表是\ntime：生存时间\netype：终点事件类型，1-复发，2-死亡",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#多因素回归",
    "href": "亚组分析和多因素回归的森林图.html#多因素回归",
    "title": "46  亚组分析和多因素回归的森林图比较",
    "section": "46.2 多因素回归",
    "text": "46.2 多因素回归\n\nlibrary(survival)\n\nfit_multi &lt;- coxph(Surv(time, status) ~ ., data = df)\nsummary(fit_multi)\n## Call:\n## coxph(formula = Surv(time, status) ~ ., data = df)\n## \n##   n= 606, number of events= 292 \n##    (13 observations deleted due to missingness)\n## \n##                       coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \n## rx1               0.521198  1.684043  0.120261  4.334 1.47e-05 ***\n## sexmale          -0.125724  0.881858  0.118615 -1.060   0.2892    \n## age&lt;=65           0.022860  1.023123  0.124973  0.183   0.8549    \n## obstructYes       0.001102  1.001103  0.150254  0.007   0.9941    \n## perforYes         0.219640  1.245629  0.335564  0.655   0.5128    \n## adhereYes         0.121203  1.128854  0.172725  0.702   0.4829    \n## differmoderate   -0.214304  0.807103  0.211843 -1.012   0.3117    \n## differpoor        0.196139  1.216696  0.240222  0.816   0.4142    \n## extentmuscle      0.413055  1.511429  0.620625  0.666   0.5057    \n## extentserosa      1.043101  2.838005  0.584977  1.783   0.0746 .  \n## extentcontiguous  1.336959  3.807447  0.637908  2.096   0.0361 *  \n## surglong          0.198218  1.219229  0.127288  1.557   0.1194    \n## node4Yes          0.811284  2.250796  0.123699  6.559 5.43e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##                  exp(coef) exp(-coef) lower .95 upper .95\n## rx1                 1.6840     0.5938    1.3304     2.132\n## sexmale             0.8819     1.1340    0.6989     1.113\n## age&lt;=65             1.0231     0.9774    0.8008     1.307\n## obstructYes         1.0011     0.9989    0.7457     1.344\n## perforYes           1.2456     0.8028    0.6453     2.404\n## adhereYes           1.1289     0.8859    0.8047     1.584\n## differmoderate      0.8071     1.2390    0.5329     1.223\n## differpoor          1.2167     0.8219    0.7598     1.948\n## extentmuscle        1.5114     0.6616    0.4478     5.101\n## extentserosa        2.8380     0.3524    0.9017     8.932\n## extentcontiguous    3.8074     0.2626    1.0906    13.293\n## surglong            1.2192     0.8202    0.9500     1.565\n## node4Yes            2.2508     0.4443    1.7662     2.868\n## \n## Concordance= 0.672  (se = 0.016 )\n## Likelihood ratio test= 93.76  on 13 df,   p=3e-14\n## Wald test            = 93.41  on 13 df,   p=3e-14\n## Score (logrank) test = 98.95  on 13 df,   p=3e-15\n\n可以看到这个多因素回归的结果，对于每一个分类变量，都会进行哑变量编码（参考上面的推文），所有结果中会有rx1，sexmale这样的结果，rx这个变量是有2个类别的，分别是类别0和类别1，结果只有rx1，因为列别0是参考，对于sex也是，其中的female时参考，所以只有sexmale的结果。\n此时的森林图是这样的，也是表达的一模一样的意思，你可以看到结果中都有一个reference，这个就是参考了，参考类别是没有P值的，也没有可信区间，HR都是1。\n\nlibrary(survminer)\n\nggforest(fit_multi,fontsize = 1)\n## Warning in .get_data(model, data = data): The `data` argument is not provided.\n## Data will be extracted from model fit.\n\n\n\n\n\n\n\n\n为了和亚组分析的森林图比较一下，我们重新提取一下数据，使用forestploter包再画一遍。\n\nmultidf &lt;- broom::tidy(fit_multi,exponentiate = T,conf.int = T) %&gt;% \n  mutate(across(where(is.numeric), round,digits=2),\n         `HR(95%CI)`=paste0(estimate,\"(\",conf.low,\"-\",conf.high,\")\")\n         ) %&gt;% \n  select(term,estimate,p.value,conf.low,conf.high,`HR(95%CI)`)\n## Warning: There was 1 warning in `mutate()`.\n## ℹ In argument: `across(where(is.numeric), round, digits = 2)`.\n## Caused by warning:\n## ! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\n## Supply arguments directly to `.fns` through an anonymous function instead.\n## \n##   # Previously\n##   across(a:b, mean, na.rm = TRUE)\n## \n##   # Now\n##   across(a:b, \\(x) mean(x, na.rm = TRUE))\n\nmultidf\n## # A tibble: 13 × 6\n##    term             estimate p.value conf.low conf.high `HR(95%CI)`     \n##    &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;           \n##  1 rx1                  1.68    0        1.33      2.13 1.68(1.33-2.13) \n##  2 sexmale              0.88    0.29     0.7       1.11 0.88(0.7-1.11)  \n##  3 age&lt;=65              1.02    0.85     0.8       1.31 1.02(0.8-1.31)  \n##  4 obstructYes          1       0.99     0.75      1.34 1(0.75-1.34)    \n##  5 perforYes            1.25    0.51     0.65      2.4  1.25(0.65-2.4)  \n##  6 adhereYes            1.13    0.48     0.8       1.58 1.13(0.8-1.58)  \n##  7 differmoderate       0.81    0.31     0.53      1.22 0.81(0.53-1.22) \n##  8 differpoor           1.22    0.41     0.76      1.95 1.22(0.76-1.95) \n##  9 extentmuscle         1.51    0.51     0.45      5.1  1.51(0.45-5.1)  \n## 10 extentserosa         2.84    0.07     0.9       8.93 2.84(0.9-8.93)  \n## 11 extentcontiguous     3.81    0.04     1.09     13.3  3.81(1.09-13.29)\n## 12 surglong             1.22    0.12     0.95      1.56 1.22(0.95-1.56) \n## 13 node4Yes             2.25    0        1.77      2.87 2.25(1.77-2.87)\n\n#write.csv(multidf,file = \"multidf.csv\",quote = F)\n\n保存后重新整理下格式再读取进来：\n\nplot_df &lt;- read.csv(file=\"./datasets/multidf.csv\",check.names = F)\nplot_df\n##         subgroup estimate p.value conf.low conf.high        HR(95%CI)\n## 1             rx       NA      NA       NA        NA                 \n## 2            rx0     1.00      NA     1.00      1.00                 \n## 3            rx1     1.68    0.00     1.33      2.13  1.68(1.33-2.13)\n## 4            sex       NA      NA       NA        NA                 \n## 5         female     1.00      NA     1.00      1.00                 \n## 6           male     0.88    0.29     0.70      1.11   0.88(0.7-1.11)\n## 7            age       NA      NA       NA        NA                 \n## 8            &gt;65     1.00      NA     1.00      1.00                 \n## 9           &lt;=65     1.02    0.85     0.80      1.31   1.02(0.8-1.31)\n## 10      obstruct       NA      NA       NA        NA                 \n## 11            No     1.00      NA     1.00      1.00                 \n## 12           Yes     1.00    0.99     0.75      1.34     1(0.75-1.34)\n## 13        perfor       NA      NA       NA        NA                 \n## 14            No     1.00      NA     1.00      1.00                 \n## 15           Yes     1.25    0.51     0.65      2.40   1.25(0.65-2.4)\n## 16        adhere       NA      NA       NA        NA                 \n## 17            No     1.00      NA     1.00      1.00                 \n## 18           Yes     1.13    0.48     0.80      1.58   1.13(0.8-1.58)\n## 19        differ       NA      NA       NA        NA                 \n## 20          well     1.00      NA     1.00      1.00                 \n## 21      moderate     0.81    0.31     0.53      1.22  0.81(0.53-1.22)\n## 22          poor     1.22    0.41     0.76      1.95  1.22(0.76-1.95)\n## 23        extent       NA      NA       NA        NA                 \n## 24     submucosa     1.00      NA     1.00      1.00                 \n## 25        muscle     1.51    0.51     0.45      5.10   1.51(0.45-5.1)\n## 26        serosa     2.84    0.07     0.90      8.93   2.84(0.9-8.93)\n## 27    contiguous     3.81    0.04     1.09     13.29 3.81(1.09-13.29)\n## 28          surg       NA      NA       NA        NA                 \n## 29         short     1.00      NA     1.00      1.00                 \n## 30          long     1.22    0.12     0.95      1.56  1.22(0.95-1.56)\n## 31         node4       NA      NA       NA        NA                 \n## 32            No     1.00      NA     1.00      1.00                 \n## 33           Yes     2.25    0.00     1.77      2.87  2.25(1.77-2.87)\n\n把数据中的P值部分的NA变成空格，这样画森林图时就不会显示了，然后增加1列空值用于展示可信区间：\n\nplot_df[,c(3)][is.na(plot_df[,c(3)])] &lt;- \" \"\nplot_df$` ` &lt;- paste(rep(\" \", nrow(plot_df)), collapse = \" \")\nplot_df\n##         subgroup estimate p.value conf.low conf.high        HR(95%CI)\n## 1             rx       NA               NA        NA                 \n## 2            rx0     1.00             1.00      1.00                 \n## 3            rx1     1.68       0     1.33      2.13  1.68(1.33-2.13)\n## 4            sex       NA               NA        NA                 \n## 5         female     1.00             1.00      1.00                 \n## 6           male     0.88    0.29     0.70      1.11   0.88(0.7-1.11)\n## 7            age       NA               NA        NA                 \n## 8            &gt;65     1.00             1.00      1.00                 \n## 9           &lt;=65     1.02    0.85     0.80      1.31   1.02(0.8-1.31)\n## 10      obstruct       NA               NA        NA                 \n## 11            No     1.00             1.00      1.00                 \n## 12           Yes     1.00    0.99     0.75      1.34     1(0.75-1.34)\n## 13        perfor       NA               NA        NA                 \n## 14            No     1.00             1.00      1.00                 \n## 15           Yes     1.25    0.51     0.65      2.40   1.25(0.65-2.4)\n## 16        adhere       NA               NA        NA                 \n## 17            No     1.00             1.00      1.00                 \n## 18           Yes     1.13    0.48     0.80      1.58   1.13(0.8-1.58)\n## 19        differ       NA               NA        NA                 \n## 20          well     1.00             1.00      1.00                 \n## 21      moderate     0.81    0.31     0.53      1.22  0.81(0.53-1.22)\n## 22          poor     1.22    0.41     0.76      1.95  1.22(0.76-1.95)\n## 23        extent       NA               NA        NA                 \n## 24     submucosa     1.00             1.00      1.00                 \n## 25        muscle     1.51    0.51     0.45      5.10   1.51(0.45-5.1)\n## 26        serosa     2.84    0.07     0.90      8.93   2.84(0.9-8.93)\n## 27    contiguous     3.81    0.04     1.09     13.29 3.81(1.09-13.29)\n## 28          surg       NA               NA        NA                 \n## 29         short     1.00             1.00      1.00                 \n## 30          long     1.22    0.12     0.95      1.56  1.22(0.95-1.56)\n## 31         node4       NA               NA        NA                 \n## 32            No     1.00             1.00      1.00                 \n## 33           Yes     2.25       0     1.77      2.87  2.25(1.77-2.87)\n##                                                                     \n## 1                                                                   \n## 2                                                                   \n## 3                                                                   \n## 4                                                                   \n## 5                                                                   \n## 6                                                                   \n## 7                                                                   \n## 8                                                                   \n## 9                                                                   \n## 10                                                                  \n## 11                                                                  \n## 12                                                                  \n## 13                                                                  \n## 14                                                                  \n## 15                                                                  \n## 16                                                                  \n## 17                                                                  \n## 18                                                                  \n## 19                                                                  \n## 20                                                                  \n## 21                                                                  \n## 22                                                                  \n## 23                                                                  \n## 24                                                                  \n## 25                                                                  \n## 26                                                                  \n## 27                                                                  \n## 28                                                                  \n## 29                                                                  \n## 30                                                                  \n## 31                                                                  \n## 32                                                                  \n## 33\n\n然后画图即可，默认的出图就已经很美观了:\n\nlibrary(forestploter)\nlibrary(grid)\n\np &lt;- forest(\n  data = plot_df[,c(1,6,7,3)],\n  lower = plot_df$conf.low,\n  upper = plot_df$conf.high,\n  est = plot_df$estimate,\n  ci_column = 3,\n  sizes = 1, \n  ref_line = 1, \n  xlim = c(0.1,4)\n  )\nprint(p)",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#亚组分析",
    "href": "亚组分析和多因素回归的森林图.html#亚组分析",
    "title": "46  亚组分析和多因素回归的森林图比较",
    "section": "46.3 亚组分析",
    "text": "46.3 亚组分析\n亚组分析的思路非常简单，就是在每一个亚组中进行分析，详细过程我们就不介绍了，大家可以参考之前的推文（不理解亚组分析怎么做的一定要看）：\n\nR语言亚组分析及森林图绘制\nR语言亚组分析1行代码实现！\n\n对于我们这个演示数据，它画出来的亚组分析的森林图是这样的（绘制代码参考上面两篇推文）：",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#比较",
    "href": "亚组分析和多因素回归的森林图.html#比较",
    "title": "46  亚组分析和多因素回归的森林图比较",
    "section": "46.4 比较",
    "text": "46.4 比较\n不知道看到这里你明白了没有，亚组分析是在所有数据的子集中做分析，在每一个亚组中都进行一次分析，每次分析都能得到一个HR值和可信区间，把所有结果放在一起，就得到森林图了。\n而多因素回归其实只是把分类变量进行哑变量编码而已，其中一个是参考，其余都和参考比，这样也能得到不同类别的HR值和可信区间。如果是数值型变量而不是分类变量不用进行哑变量编码了，自然也不会出现“亚组”的形式。\n虽有都有HR值、可信区间、P值等信息，但是表达的意思和实现方法确实去安全不同的！\n还有一个我没见过的形式：多因素分析+亚组分析的森林图，但是粉丝群里有群友问到过，意思是在每一个亚组内都做多因素分析，这样的森林图就要在每个亚组内展示多个HR和可信区间了。",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#其他资源",
    "href": "亚组分析和多因素回归的森林图.html#其他资源",
    "title": "46  亚组分析和多因素回归的森林图比较",
    "section": "46.5 其他资源",
    "text": "46.5 其他资源\n亚组分析和森林图的内容还有非常多的细节问题，为了不影响该合集的主要内容，我把它们放在下面的链接中，大家感兴趣的话可点击下面的链接查看，或者在公众号后台回复亚组分析获取合集链接：\n\n使用R语言画森林图和误差线\nggplot2绘制森林图(有亚组和没亚组)\nR语言亚组分析及森林图绘制\nR语言亚组分析1行代码实现！\n亚组分析和多因素回归的森林图比较\n多列可信区间、多组别的森林图绘制\n逻辑回归亚组分析森林图绘制\n协变量调整的亚组分析和森林图绘制\n1行代码实现：多因素回归的亚组分析，并绘制森林图\nR语言亚组分析及森林图绘制手册",
    "crumbs": [
      "文献常见统计分析",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>亚组分析和多因素回归的森林图比较</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html",
    "href": "9999-appendix.html",
    "title": "附录 A — 其他合集",
    "section": "",
    "text": "A.1 PASS软件\n关注公众号：医学和生信笔记，后台回复PASS即可获得软件链接。或者直接海鲜市场买一个。",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#r语言零基础入门",
    "href": "9999-appendix.html#r语言零基础入门",
    "title": "附录 A — 其他合集",
    "section": "A.2 R语言零基础入门",
    "text": "A.2 R语言零基础入门\n\nR语言零基础入门合集：R语言零基础入门\n在线版电子书：R语言零基础入门\nPDF版电子书：R语言零基础入门\ngithub地址：R语言零基础入门\n视频版教程：R语言零基础入门",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#医学统计",
    "href": "9999-appendix.html#医学统计",
    "title": "附录 A — 其他合集",
    "section": "A.3 医学统计",
    "text": "A.3 医学统计\n\nR语言医学统计合集：R语言实战医学统计\n在线版电子书：R语言实战医学统计\nPDF版电子书：R语言实战医学统计\ngithub地址：R语言实战医学统计\n视频版教程：R语言实战医学统计",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#临床预测模型",
    "href": "9999-appendix.html#临床预测模型",
    "title": "附录 A — 其他合集",
    "section": "A.4 临床预测模型",
    "text": "A.4 临床预测模型\n\n临床预测模型合集：临床预测模型\n在线版电子书：R语言实战临床预测模型\nPDF版电子书：R语言实战临床预测模型\ngithub地址：R语言实战临床预测模型\n视频版教程：R语言实战临床预测模型",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#机器学习",
    "href": "9999-appendix.html#机器学习",
    "title": "附录 A — 其他合集",
    "section": "A.5 机器学习",
    "text": "A.5 机器学习\n医学和生信笔记后台回复caret即可获取caret包的合集教程；回复tidymodels即可获取tidymodels的合集教程；回复mlr3即可获取mlr3合集教程，回复机器学习即可获取机器学习推文合集。\n\nR语言机器学习合集：R语言机器学习\n在线版电子书：R语言实战机器学习\nPDF版电子书：R语言实战机器学习\ngithub地址：R语言实战机器学习\n视频版教程：R语言实战机器学习",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#模型解释",
    "href": "9999-appendix.html#模型解释",
    "title": "附录 A — 其他合集",
    "section": "A.6 模型解释",
    "text": "A.6 模型解释\n包括各种黑盒模型的解释方法，如SHAP、局部代理法、分解解释等：\n\n模型解释合集：模型解释合集",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#生信数据挖掘",
    "href": "9999-appendix.html#生信数据挖掘",
    "title": "附录 A — 其他合集",
    "section": "A.7 生信数据挖掘",
    "text": "A.7 生信数据挖掘\n生信数据挖掘合集：生信数据挖掘\n医学和生信笔记公众号所有关于生信数据挖掘的推文都可以免费下载使用，请看：“灌水”生信类文章会用到哪些生信下游分析？（附下载地址）\ngithub地址：R语言生信数据挖掘",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#扫码关注",
    "href": "9999-appendix.html#扫码关注",
    "title": "附录 A — 其他合集",
    "section": "A.8 扫码关注",
    "text": "A.8 扫码关注\n欢迎扫码关注：医学和生信笔记",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  }
]